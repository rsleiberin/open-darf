{"coref": {"AG_News": [], "Accuracy": [[41, 42], [1126, 1127], [1152, 1153], [1241, 1242], [1660, 1661]], "Amazon_Review_Full": [], "Amazon_Review_Polarity": [], "DBpedia": [], "Error": [], "FastText": [[29, 30], [57, 58], [304, 305], [886, 887], [1087, 1088], [1223, 1224], [1291, 1292], [1316, 1317], [1609, 1610], [1616, 1617], [1795, 1796]], "Sentiment_Analysis": [[314, 316], [905, 907], [959, 961], [1840, 1842]], "Sogou_News": [], "Text_Classification": [[7, 9], [18, 20], [94, 96], [175, 178], [238, 240], [1767, 1769], [1835, 1838]], "Yahoo__Answers": [], "Yelp_Binary_classification": [], "Yelp_Fine-grained_classification": [], "fastText": []}, "coref_non_salient": {"0": [[1549, 1550], [1572, 1574]], "1": [[35, 38], [1806, 1808]], "10": [[151, 155], [1245, 1247]], "11": [[1283, 1284], [1314, 1315], [1577, 1578]], "12": [[1000, 1003], [1131, 1134], [1249, 1252], [1304, 1307]], "13": [[1823, 1825]], "14": [[166, 168], [343, 345], [357, 359], [397, 399], [455, 457], [583, 585]], "15": [[1014, 1017], [1135, 1138]], "16": [[1192, 1193], [1332, 1333]], "17": [[1329, 1331]], "18": [[1826, 1828]], "19": [[569, 571], [621, 623], [640, 642]], "2": [[126, 128], [406, 409], [1815, 1818]], "20": [[1057, 1064]], "21": [[1684, 1686]], "22": [[1493, 1495]], "23": [[1653, 1654]], "24": [[1534, 1538]], "25": [[1776, 1777]], "26": [[1771, 1775]], "27": [[1730, 1732]], "28": [[1054, 1056]], "29": [[1527, 1528]], "3": [[311, 313], [922, 925], [1379, 1381]], "30": [[1554, 1557]], "31": [[1582, 1584]], "32": [[255, 257], [417, 419]], "33": [[1311, 1313]], "34": [[1021, 1025]], "35": [[1008, 1013]], "36": [[349, 351]], "37": [[116, 117]], "38": [[353, 354]], "39": [[1258, 1262]], "4": [[438, 440], [445, 447], [459, 461], [607, 609], [1788, 1790]], "40": [[263, 266]], "41": [[935, 938]], "42": [[803, 808]], "43": [[101, 104]], "44": [[1352, 1356]], "45": [[1301, 1303]], "46": [[562, 566]], "47": [[712, 715]], "48": [[2, 5]], "49": [[1169, 1170]], "5": [[72, 74], [555, 556], [1270, 1271]], "50": [[52, 53]], "51": [[478, 480]], "52": [[557, 560]], "53": [[248, 251]], "54": [[899, 901]], "55": [[1565, 1567]], "56": [[773, 777]], "57": [[856, 858]], "58": [[338, 339]], "59": [[425, 427]], "6": [[593, 595], [634, 636], [744, 745]], "60": [[110, 112]], "61": [[626, 629]], "62": [[986, 988]], "63": [[496, 498]], "64": [[1093, 1095]], "65": [[113, 115]], "66": [[995, 999]], "7": [[118, 120], [327, 329]], "8": [[50, 51], [631, 632]], "9": [[1026, 1027], [1144, 1145], [1253, 1254]]}, "doc_id": "023cc7f9f3544436553df9548a7d0575bb309c2e", "method_subrelations": {"FastText": [[[0, 8], "FastText"]], "fastText": [[[0, 8], "fastText"]]}, "n_ary_relations": [{"Material": "AG_News", "Method": "fastText", "Metric": "Error", "Task": "Text_Classification", "score": "7.5"}, {"Material": "Amazon_Review_Full", "Method": "FastText", "Metric": "Accuracy", "Task": "Sentiment_Analysis", "score": "60.2"}, {"Material": "Amazon_Review_Polarity", "Method": "FastText", "Metric": "Accuracy", "Task": "Sentiment_Analysis", "score": "94.6"}, {"Material": "DBpedia", "Method": "FastText", "Metric": "Error", "Task": "Text_Classification", "score": "1.4"}, {"Material": "Sogou_News", "Method": "fastText", "Metric": "Accuracy", "Task": "Sentiment_Analysis", "score": "96.8"}, {"Material": "Yahoo__Answers", "Method": "FastText", "Metric": "Accuracy", "Task": "Text_Classification", "score": "72.3"}, {"Material": "Yelp_Binary_classification", "Method": "FastText", "Metric": "Error", "Task": "Sentiment_Analysis", "score": "4.3"}, {"Material": "Yelp_Fine-grained_classification", "Method": "FastText", "Metric": "Error", "Task": "Sentiment_Analysis", "score": "36.1"}], "ner": [[2, 5, "Method"], [7, 9, "Task"], [18, 20, "Task"], [29, 30, "Method"], [35, 38, "Method"], [41, 42, "Metric"], [50, 51, "Task"], [52, 53, "Task"], [57, 58, "Method"], [72, 74, "Method"], [94, 96, "Task"], [101, 104, "Task"], [110, 112, "Task"], [113, 115, "Task"], [116, 117, "Task"], [118, 120, "Task"], [126, 128, "Method"], [151, 155, "Metric"], [166, 168, "Method"], [175, 178, "Task"], [238, 240, "Task"], [248, 251, "Task"], [255, 257, "Method"], [263, 266, "Method"], [304, 305, "Method"], [311, 313, "Task"], [314, 316, "Task"], [327, 329, "Task"], [338, 339, "Method"], [343, 345, "Method"], [349, 351, "Method"], [353, 354, "Method"], [357, 359, "Method"], [397, 399, "Method"], [406, 409, "Method"], [417, 419, "Method"], [425, 427, "Method"], [438, 440, "Method"], [445, 447, "Method"], [455, 457, "Method"], [459, 461, "Method"], [478, 480, "Method"], [496, 498, "Method"], [555, 556, "Method"], [557, 560, "Method"], [562, 566, "Method"], [569, 571, "Method"], [583, 585, "Method"], [593, 595, "Metric"], [607, 609, "Method"], [621, 623, "Method"], [626, 629, "Method"], [631, 632, "Task"], [634, 636, "Metric"], [640, 642, "Method"], [712, 715, "Method"], [744, 745, "Metric"], [773, 777, "Method"], [803, 808, "Method"], [856, 858, "Method"], [886, 887, "Method"], [899, 901, "Method"], [905, 907, "Task"], [922, 925, "Task"], [935, 938, "Method"], [959, 961, "Task"], [986, 988, "Method"], [995, 999, "Method"], [1000, 1003, "Method"], [1008, 1013, "Method"], [1014, 1017, "Method"], [1021, 1025, "Method"], [1026, 1027, "Method"], [1054, 1056, "Method"], [1057, 1064, "Method"], [1087, 1088, "Method"], [1093, 1095, "Metric"], [1126, 1127, "Metric"], [1131, 1134, "Method"], [1135, 1138, "Method"], [1144, 1145, "Method"], [1152, 1153, "Metric"], [1169, 1170, "Method"], [1192, 1193, "Method"], [1223, 1224, "Method"], [1241, 1242, "Metric"], [1245, 1247, "Metric"], [1249, 1252, "Method"], [1253, 1254, "Method"], [1258, 1262, "Method"], [1270, 1271, "Method"], [1283, 1284, "Method"], [1291, 1292, "Method"], [1301, 1303, "Metric"], [1304, 1307, "Method"], [1311, 1313, "Method"], [1314, 1315, "Method"], [1316, 1317, "Method"], [1329, 1331, "Method"], [1332, 1333, "Method"], [1352, 1356, "Method"], [1379, 1381, "Task"], [1493, 1495, "Metric"], [1527, 1528, "Metric"], [1534, 1538, "Method"], [1549, 1550, "Method"], [1554, 1557, "Method"], [1565, 1567, "Method"], [1572, 1574, "Method"], [1577, 1578, "Method"], [1582, 1584, "Method"], [1609, 1610, "Method"], [1616, 1617, "Method"], [1653, 1654, "Method"], [1660, 1661, "Metric"], [1684, 1686, "Method"], [1730, 1732, "Method"], [1767, 1769, "Task"], [1771, 1775, "Method"], [1776, 1777, "Method"], [1788, 1790, "Method"], [1795, 1796, "Method"], [1806, 1808, "Method"], [1815, 1818, "Method"], [1823, 1825, "Method"], [1826, 1828, "Method"], [1835, 1838, "Task"], [1840, 1842, "Task"]], "sections": [[0, 91], [91, 317], [317, 567], [567, 771], [771, 881], [881, 957], [957, 961], [961, 1066], [1066, 1243], [1243, 1377], [1377, 1381], [1381, 1594], [1594, 1751], [1751, 1869], [1869, 1912], [1912, 1915]], "sentences": [[0, 9], [9, 21], [21, 54], [54, 91], [91, 94], [94, 121], [121, 133], [133, 164], [164, 179], [179, 201], [201, 213], [213, 241], [241, 291], [291, 298], [298, 317], [317, 321], [321, 355], [355, 368], [368, 388], [388, 410], [410, 423], [423, 437], [437, 458], [458, 472], [472, 493], [493, 508], [508, 548], [548, 567], [567, 571], [571, 573], [573, 589], [589, 610], [610, 630], [630, 639], [639, 656], [656, 677], [677, 689], [689, 707], [707, 735], [735, 750], [750, 771], [771, 777], [777, 798], [798, 822], [822, 840], [840, 881], [881, 884], [884, 892], [892, 908], [908, 926], [926, 957], [957, 961], [961, 967], [967, 979], [979, 1031], [1031, 1041], [1041, 1066], [1066, 1070], [1070, 1080], [1080, 1109], [1109, 1124], [1124, 1146], [1146, 1175], [1175, 1194], [1194, 1220], [1220, 1243], [1243, 1248], [1248, 1275], [1275, 1293], [1293, 1328], [1328, 1346], [1346, 1377], [1377, 1381], [1381, 1387], [1387, 1417], [1417, 1438], [1438, 1462], [1462, 1477], [1477, 1492], [1492, 1508], [1508, 1525], [1525, 1531], [1531, 1545], [1545, 1570], [1570, 1594], [1594, 1601], [1601, 1614], [1614, 1639], [1639, 1662], [1662, 1707], [1707, 1726], [1726, 1742], [1742, 1751], [1751, 1756], [1756, 1770], [1770, 1791], [1791, 1814], [1814, 1850], [1850, 1869], [1869, 1873], [1873, 1892], [1892, 1912], [1912, 1915]], "words": ["document", ":", "Bag", "of", "Tricks", "for", "Efficient", "Text", "Classification", "This", "paper", "explores", "a", "simple", "and", "efficient", "baseline", "for", "text", "classification", ".", "Our", "experiments", "show", "that", "our", "fast", "text", "classifier", "fastText", "is", "often", "on", "par", "with", "deep", "learning", "classifiers", "in", "terms", "of", "accuracy", ",", "and", "many", "orders", "of", "magnitude", "faster", "for", "training", "and", "evaluation", ".", "We", "can", "train", "fastText", "on", "more", "than", "one", "billion", "words", "in", "less", "than", "ten", "minutes", "using", "a", "standard", "multicore", "CPU", ",", "and", "classify", "half", "a", "million", "sentences", "among", "312", "K", "classes", "in", "less", "than", "a", "minute", ".", "section", ":", "Introduction", "Text", "classification", "is", "an", "important", "task", "in", "Natural", "Language", "Processing", "with", "many", "applications", ",", "such", "as", "web", "search", ",", "information", "retrieval", ",", "ranking", "and", "document", "classification", ".", "Recently", ",", "models", "based", "on", "neural", "networks", "have", "become", "increasingly", "popular", ".", "While", "these", "models", "achieve", "very", "good", "performance", "in", "practice", ",", "they", "tend", "to", "be", "relatively", "slow", "both", "at", "train", "and", "test", "time", ",", "limiting", "their", "use", "on", "very", "large", "datasets", ".", "Meanwhile", ",", "linear", "classifiers", "are", "often", "considered", "as", "strong", "baselines", "for", "text", "classification", "problems", ".", "Despite", "their", "simplicity", ",", "they", "often", "obtain", "state", "-", "of", "-", "the", "-", "art", "performances", "if", "the", "right", "features", "are", "used", ".", "They", "also", "have", "the", "potential", "to", "scale", "to", "very", "large", "corpus", ".", "In", "this", "work", ",", "we", "explore", "ways", "to", "scale", "these", "baselines", "to", "very", "large", "corpus", "with", "a", "large", "output", "space", ",", "in", "the", "context", "of", "text", "classification", ".", "Inspired", "by", "the", "recent", "work", "in", "efficient", "word", "representation", "learning", ",", "we", "show", "that", "linear", "models", "with", "a", "rank", "constraint", "and", "a", "fast", "loss", "approximation", "can", "train", "on", "a", "billion", "words", "within", "ten", "minutes", ",", "while", "achieving", "performance", "on", "par", "with", "the", "state", "-", "of", "-", "the", "-", "art", ".", "We", "evaluate", "the", "quality", "of", "our", "approach", "fastTexthttps:", "//", "github.com", "/", "facebookresearch", "/", "fastText", "on", "two", "different", "tasks", ",", "namely", "tag", "prediction", "and", "sentiment", "analysis", ".", "section", ":", "Model", "architecture", "A", "simple", "and", "efficient", "baseline", "for", "sentence", "classification", "is", "to", "represent", "sentences", "as", "bag", "of", "words", "(", "BoW", ")", "and", "train", "a", "linear", "classifier", ",", "e.g.", ",", "a", "logistic", "regression", "or", "an", "SVM", ".", "However", ",", "linear", "classifiers", "do", "not", "share", "parameters", "among", "features", "and", "classes", ".", "This", "possibly", "limits", "their", "generalization", "in", "the", "context", "of", "large", "output", "space", "where", "some", "classes", "have", "very", "few", "examples", ".", "Common", "solutions", "to", "this", "problem", "are", "to", "factorize", "the", "linear", "classifier", "into", "low", "rank", "matrices", "or", "to", "use", "multilayer", "neural", "networks", ".", "Figure", "[", "reference", "]", "shows", "a", "simple", "linear", "model", "with", "rank", "constraint", ".", "The", "first", "weight", "matrix", "is", "a", "look", "-", "up", "table", "over", "the", "words", ".", "The", "word", "representations", "are", "then", "averaged", "into", "a", "text", "representation", ",", "which", "is", "in", "turn", "fed", "to", "a", "linear", "classifier", ".", "The", "text", "representation", "is", "an", "hidden", "variable", "which", "can", "be", "potentially", "be", "reused", ".", "This", "architecture", "is", "similar", "to", "the", "cbow", "model", "of", "mikolov2013efficient", ",", "where", "the", "middle", "word", "is", "replaced", "by", "a", "label", ".", "We", "use", "the", "softmax", "function", "to", "compute", "the", "probability", "distribution", "over", "the", "predefined", "classes", ".", "For", "a", "set", "of", "documents", ",", "this", "leads", "to", "minimizing", "the", "negative", "log", "-", "likelihood", "over", "the", "classes", ":", "where", "is", "the", "normalized", "bag", "of", "features", "of", "the", "-", "th", "document", ",", "the", "label", ",", "and", "the", "weight", "matrices", ".", "This", "model", "is", "trained", "asynchronously", "on", "multiple", "CPUs", "using", "stochastic", "gradient", "descent", "and", "a", "linearly", "decaying", "learning", "rate", ".", "subsection", ":", "Hierarchical", "softmax", "hidden", "output", "When", "the", "number", "of", "classes", "is", "large", ",", "computing", "the", "linear", "classifier", "is", "computationally", "expensive", ".", "More", "precisely", ",", "the", "computational", "complexity", "is", "where", "is", "the", "number", "of", "classes", "and", "the", "dimension", "of", "the", "text", "representation", ".", "In", "order", "to", "improve", "our", "running", "time", ",", "we", "use", "a", "hierarchical", "softmax", "based", "on", "the", "Huffman", "coding", "tree", ".", "During", "training", ",", "the", "computational", "complexity", "drops", "to", ".", "The", "hierarchical", "softmax", "is", "also", "advantageous", "at", "test", "time", "when", "searching", "for", "the", "most", "likely", "class", ".", "Each", "node", "is", "associated", "with", "a", "probability", "that", "is", "the", "probability", "of", "the", "path", "from", "the", "root", "to", "that", "node", ".", "If", "the", "node", "is", "at", "depth", "with", "parents", ",", "its", "probability", "is", "This", "means", "that", "the", "probability", "of", "a", "node", "is", "always", "lower", "than", "the", "one", "of", "its", "parent", ".", "Exploring", "the", "tree", "with", "a", "depth", "first", "search", "and", "tracking", "the", "maximum", "probability", "among", "the", "leaves", "allows", "us", "to", "discard", "any", "branch", "associated", "with", "a", "small", "probability", ".", "In", "practice", ",", "we", "observe", "a", "reduction", "of", "the", "complexity", "to", "at", "test", "time", ".", "This", "approach", "is", "further", "extended", "to", "compute", "the", "-", "top", "targets", "at", "the", "cost", "of", ",", "using", "a", "binary", "heap", ".", "subsection", ":", "N", "-", "gram", "features", "Bag", "of", "words", "is", "invariant", "to", "word", "order", "but", "taking", "explicitly", "this", "order", "into", "account", "is", "often", "computationally", "very", "expensive", ".", "Instead", ",", "we", "use", "a", "bag", "of", "n", "-", "grams", "as", "additional", "features", "to", "capture", "some", "partial", "information", "about", "the", "local", "word", "order", ".", "This", "is", "very", "efficient", "in", "practice", "while", "achieving", "comparable", "results", "to", "methods", "that", "explicitly", "use", "the", "order", ".", "We", "maintain", "a", "fast", "and", "memory", "efficient", "mapping", "of", "the", "n", "-", "grams", "by", "using", "the", "hashing", "trick", "with", "the", "same", "hashing", "function", "as", "in", "mikolov2011strategies", "and", "10", "M", "bins", "if", "we", "only", "used", "bigrams", ",", "and", "100", "M", "otherwise", ".", "section", ":", "Experiments", "We", "evaluate", "fastText", "on", "two", "different", "tasks", ".", "First", ",", "we", "compare", "it", "to", "existing", "text", "classifers", "on", "the", "problem", "of", "sentiment", "analysis", ".", "Then", ",", "we", "evaluate", "its", "capacity", "to", "scale", "to", "large", "output", "space", "on", "a", "tag", "prediction", "dataset", ".", "Note", "that", "our", "model", "could", "be", "implemented", "with", "the", "Vowpal", "Wabbit", "library", ",", "but", "we", "observe", "in", "practice", ",", "that", "our", "tailored", "implementation", "is", "at", "least", "2", "-", "5", "faster", ".", "subsection", ":", "Sentiment", "analysis", "paragraph", ":", "Datasets", "and", "baselines", ".", "We", "employ", "the", "same", "8", "datasets", "and", "evaluation", "protocol", "of", "zhang2015character", ".", "We", "report", "the", "n", "-", "grams", "and", "TFIDF", "baselines", "from", "zhang2015character", ",", "as", "well", "as", "the", "character", "level", "convolutional", "model", "(", "char", "-", "CNN", ")", "of", "zhang2015text", ",", "the", "character", "based", "convolution", "recurrent", "network", "(", "char", "-", "CRNN", ")", "of", "and", "the", "very", "deep", "convolutional", "network", "(", "VDCNN", ")", "of", "conneau2016", ".", "We", "also", "compare", "to", "tang2015document", "following", "their", "evaluation", "protocol", ".", "We", "report", "their", "main", "baselines", "as", "well", "as", "their", "two", "approaches", "based", "on", "recurrent", "networks", "(", "Conv", "-", "GRNN", "and", "LSTM", "-", "GRNN", ")", ".", "paragraph", ":", "Results", ".", "We", "present", "the", "results", "in", "Figure", "[", "reference", "]", ".", "We", "use", "10", "hidden", "units", "and", "run", "fastText", "for", "5", "epochs", "with", "a", "learning", "rate", "selected", "on", "a", "validation", "set", "from", "0.05", ",", "0.1", ",", "0.25", ",", "0.5", ".", "On", "this", "task", ",", "adding", "bigram", "information", "improves", "the", "performance", "by", "1", "-", "4", ".", "Overall", "our", "accuracy", "is", "slightly", "better", "than", "char", "-", "CNN", "and", "char", "-", "CRNN", "and", ",", "a", "bit", "worse", "than", "VDCNN", ".", "Note", "that", "we", "can", "increase", "the", "accuracy", "slightly", "by", "using", "more", "n", "-", "grams", ",", "for", "example", "with", "trigrams", ",", "the", "performance", "on", "Sogou", "goes", "up", "to", "97.1", ".", "Finally", ",", "Figure", "[", "reference", "]", "shows", "that", "our", "method", "is", "competitive", "with", "the", "methods", "presented", "in", "tang2015document", ".", "We", "tune", "the", "hyper", "-", "parameters", "on", "the", "validation", "set", "and", "observe", "that", "using", "n", "-", "grams", "up", "to", "5", "leads", "to", "the", "best", "performance", ".", "Unlike", "tang2015document", ",", "fastText", "does", "not", "use", "pre", "-", "trained", "word", "embeddings", ",", "which", "can", "be", "explained", "the", "1", "difference", "in", "accuracy", ".", "paragraph", ":", "Training", "time", ".", "Both", "char", "-", "CNN", "and", "VDCNN", "are", "trained", "on", "a", "NVIDIA", "Tesla", "K40", "GPU", ",", "while", "our", "models", "are", "trained", "on", "a", "CPU", "using", "20", "threads", ".", "Table", "[", "reference", "]", "shows", "that", "methods", "using", "convolutions", "are", "several", "orders", "of", "magnitude", "slower", "than", "fastText", ".", "While", "it", "is", "possible", "to", "have", "a", "10", "speed", "up", "for", "char", "-", "CNN", "by", "using", "more", "recent", "CUDA", "implementations", "of", "convolutions", ",", "fastText", "takes", "less", "than", "a", "minute", "to", "train", "on", "these", "datasets", ".", "The", "GRNNs", "method", "of", "tang2015document", "takes", "around", "12", "hours", "per", "epoch", "on", "CPU", "with", "a", "single", "thread", ".", "Our", "speed", "-", "up", "compared", "to", "neural", "network", "based", "methods", "increases", "with", "the", "size", "of", "the", "dataset", ",", "going", "up", "to", "at", "least", "a", "15", ",", "000", "speed", "-", "up", ".", "subsection", ":", "Tag", "prediction", "paragraph", ":", "Dataset", "and", "baselines", ".", "To", "test", "scalability", "of", "our", "approach", ",", "further", "evaluation", "is", "carried", "on", "the", "YFCC100", "M", "dataset", "which", "consists", "of", "almost", "100", "M", "images", "with", "captions", ",", "titles", "and", "tags", ".", "We", "focus", "on", "predicting", "the", "tags", "according", "to", "the", "title", "and", "caption", "(", "we", "do", "not", "use", "the", "images", ")", ".", "We", "remove", "the", "words", "and", "tags", "occurring", "less", "than", "100", "times", "and", "split", "the", "data", "into", "a", "train", ",", "validation", "and", "test", "set", ".", "The", "train", "set", "contains", "91", ",", "188", ",", "648", "examples", "(", "1.5B", "tokens", ")", ".", "The", "validation", "has", "930", ",", "497", "examples", "and", "the", "test", "set", "543", ",", "424", ".", "The", "vocabulary", "size", "is", "297", ",", "141", "and", "there", "are", "312", ",", "116", "unique", "tags", ".", "We", "will", "release", "a", "script", "that", "recreates", "this", "dataset", "so", "that", "our", "numbers", "could", "be", "reproduced", ".", "We", "report", "precision", "at", "1", ".", "We", "consider", "a", "frequency", "-", "based", "baseline", "which", "predicts", "the", "most", "frequent", "tag", ".", "We", "also", "compare", "with", "Tagspace", ",", "which", "is", "a", "tag", "prediction", "model", "similar", "to", "ours", ",", "but", "based", "on", "the", "Wsabie", "model", "of", "weston2011wsabie", ".", "While", "the", "Tagspace", "model", "is", "described", "using", "convolutions", ",", "we", "consider", "the", "linear", "version", ",", "which", "achieves", "comparable", "performance", "but", "is", "much", "faster", ".", "paragraph", ":", "Results", "and", "training", "time", ".", "Table", "[", "reference", "]", "presents", "a", "comparison", "of", "fastText", "and", "the", "baselines", ".", "We", "run", "fastText", "for", "5", "epochs", "and", "compare", "it", "to", "Tagspace", "for", "two", "sizes", "of", "the", "hidden", "layer", ",", "i.e.", ",", "50", "and", "200", ".", "Both", "models", "achieve", "a", "similar", "performance", "with", "a", "small", "hidden", "layer", ",", "but", "adding", "bigrams", "gives", "us", "a", "significant", "boost", "in", "accuracy", ".", "At", "test", "time", ",", "Tagspace", "needs", "to", "compute", "the", "scores", "for", "all", "the", "classes", "which", "makes", "it", "relatively", "slow", ",", "while", "our", "fast", "inference", "gives", "a", "significant", "speed", "-", "up", "when", "the", "number", "of", "classes", "is", "large", "(", "more", "than", "300", "K", "here", ")", ".", "Overall", ",", "we", "are", "more", "than", "an", "order", "of", "magnitude", "faster", "to", "obtain", "model", "with", "a", "better", "quality", ".", "The", "speedup", "of", "the", "test", "phase", "is", "even", "more", "significant", "(", "a", "600", "speedup", ")", ".", "Table", "[", "reference", "]", "shows", "some", "qualitative", "examples", ".", "section", ":", "Discussion", "and", "conclusion", "In", "this", "work", ",", "we", "propose", "a", "simple", "baseline", "method", "for", "text", "classification", ".", "Unlike", "unsupervisedly", "trained", "word", "vectors", "from", "word2vec", ",", "our", "word", "features", "can", "be", "averaged", "together", "to", "form", "good", "sentence", "representations", ".", "In", "several", "tasks", ",", "fastText", "obtains", "performance", "on", "par", "with", "recently", "proposed", "methods", "inspired", "by", "deep", "learning", ",", "while", "being", "much", "faster", ".", "Although", "deep", "neural", "networks", "have", "in", "theory", "much", "higher", "representational", "power", "than", "shallow", "models", ",", "it", "is", "not", "clear", "if", "simple", "text", "classification", "problems", "such", "as", "sentiment", "analysis", "are", "the", "right", "ones", "to", "evaluate", "them", ".", "We", "will", "publish", "our", "code", "so", "that", "the", "research", "community", "can", "easily", "build", "on", "top", "of", "our", "work", ".", "paragraph", ":", "Acknowledgement", ".", "We", "thank", "Gabriel", "Synnaeve", ",", "Herv\u00e9", "G\u00e9gou", ",", "Jason", "Weston", "and", "L\u00e9on", "Bottou", "for", "their", "help", "and", "comments", ".", "We", "also", "thank", "Alexis", "Conneau", ",", "Duyu", "Tang", "and", "Zichao", "Zhang", "for", "providing", "us", "with", "information", "about", "their", "methods", ".", "bibliography", ":", "References"]}