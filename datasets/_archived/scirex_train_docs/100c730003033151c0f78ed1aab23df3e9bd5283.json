{"coref": {"Attentive_LSTM": [[750, 752], [853, 855], [1956, 1958], [2012, 2014], [2139, 2141], [3156, 3159], [3202, 3204], [3222, 3225], [3592, 3595], [3772, 3775], [3798, 3800], [3830, 3832], [4394, 4397], [4443, 4446], [4452, 4455], [4472, 4475], [4508, 4510], [4621, 4623], [5127, 5129], [5551, 5553], [5627, 5630], [5655, 5658]], "LSTM": [[121, 125], [162, 165], [843, 844], [5478, 5479], [3141, 3142], [3144, 3145], [3148, 3149], [3218, 3219], [3576, 3577], [3601, 3602], [3716, 3717]], "LSTM__lexical_overlap___dist_output_": [], "MAP": [[3035, 3037], [3302, 3303], [3478, 3480], [3586, 3587], [3529, 3530], [4113, 4114], [4122, 4123], [4985, 4986]], "MRR": [[3304, 3305], [3589, 3590]], "QASent": [[3069, 3070], [3075, 3076], [3571, 3572], [3614, 3615]], "Question_Answering": [[84, 87], [551, 553], [1768, 1771], [1835, 1840], [2099, 2104], [4379, 4383], [4428, 4431], [5037, 5039], [5166, 5170], [5440, 5445]], "WikiQA": [[3044, 3045], [3072, 3074], [3087, 3088], [3669, 3670]], "dist_output": [], "lexical_overlap": [[3106, 3108], [3281, 3283]]}, "coref_non_salient": {"0": [[2344, 2350], [3809, 3811]], "1": [[1922, 1924], [1949, 1952], [2213, 2217], [2628, 2629], [3211, 3212], [3453, 3454], [3503, 3504], [4372, 4373], [4649, 4650], [5556, 5558]], "10": [[220, 222], [492, 494], [640, 642], [1005, 1007], [1045, 1047], [2664, 2666], [4757, 4760], [5294, 5296], [5326, 5328], [5515, 5517], [5545, 5547]], "100": [[3662, 3663]], "101": [[1387, 1390]], "102": [[1854, 1856]], "103": [[4204, 4205]], "104": [[2788, 2791]], "105": [[2519, 2521]], "106": [[173, 176]], "107": [[3375, 3376]], "108": [[2136, 2137]], "109": [[3646, 3650]], "11": [[340, 343], [913, 916], [940, 944], [1290, 1293]], "110": [[1510, 1514]], "111": [[2643, 2649]], "112": [[40, 42], [394, 396]], "113": [[2868, 2869]], "114": [[76, 79]], "115": [[437, 443]], "116": [[1369, 1371]], "117": [[1763, 1766], [3057, 3060], [3540, 3543], [3744, 3748]], "118": [[622, 629], [1498, 1504]], "119": [[241, 245]], "12": [[727, 728], [1872, 1873], [1891, 1892], [2265, 2266], [3366, 3369], [5536, 5537]], "120": [[281, 286]], "121": [[127, 130]], "122": [[803, 806]], "123": [[2650, 2651]], "124": [[1723, 1727], [2398, 2401]], "125": [[4829, 4833]], "126": [[2732, 2734]], "127": [[132, 134]], "128": [[3236, 3239]], "129": [[4708, 4710]], "13": [[54, 56], [367, 369], [432, 434], [452, 454], [505, 507], [617, 619], [1099, 1101], [1212, 1215], [1659, 1661], [1754, 1756], [2256, 2258], [2668, 2670], [3258, 3260], [4722, 4724], [4731, 4733], [5288, 5290], [5320, 5322], [5366, 5368], [5382, 5384], [5448, 5450], [5541, 5543], [5648, 5650]], "130": [[5151, 5155]], "131": [[2549, 2551]], "132": [[3032, 3034]], "133": [[101, 108]], "134": [[2888, 2890]], "135": [[246, 247]], "136": [[4367, 4369]], "137": [[147, 150]], "138": [[470, 473]], "139": [[906, 911], [1186, 1190], [1218, 1222]], "14": [[1051, 1054], [1574, 1576]], "140": [[1433, 1437]], "141": [[1747, 1748]], "142": [[1362, 1364]], "143": [[478, 481]], "144": [[1615, 1618]], "145": [[482, 483]], "146": [[11, 15]], "147": [[158, 159]], "148": [[722, 725]], "149": [[1532, 1534]], "15": [[1539, 1541], [2571, 2574], [2604, 2606]], "150": [[311, 313]], "16": [[1859, 1863], [1912, 1917], [2173, 2177], [3178, 3183]], "17": [[536, 537], [574, 575], [656, 657], [1529, 1530], [2395, 2396], [2444, 2445], [2783, 2784], [2824, 2825], [2837, 2838], [2856, 2857], [2908, 2909], [2927, 2928], [3025, 3026], [4228, 4229], [4289, 4290], [4326, 4327], [4656, 4657], [4690, 4691], [4948, 4949], [5003, 5004], [5352, 5353], [5597, 5598], [993, 994], [2565, 2566], [5566, 5567]], "18": [[177, 178], [951, 952], [4175, 4176]], "19": [[373, 376], [427, 430], [1135, 1138], [2251, 2254], [5275, 5279]], "2": [[277, 279], [2406, 2408], [2610, 2611], [3436, 3437]], "20": [[4449, 4451], [4516, 4518]], "21": [[2358, 2359], [5217, 5219]], "22": [[510, 519], [1709, 1712]], "23": [[4835, 4836], [5005, 5006], [5114, 5119]], "24": [[270, 272], [3493, 3495], [5346, 5348], [5349, 5351], [5434, 5436], [5608, 5610], [5624, 5626]], "25": [[498, 500], [1374, 1376]], "26": [[775, 777], [1994, 1996], [2337, 2339]], "27": [[2056, 2058], [3769, 3771], [4552, 4554]], "28": [[583, 588], [1522, 1525]], "29": [[548, 549], [713, 714], [1926, 1927], [1953, 1954], [1991, 1992], [2071, 2072], [2088, 2089], [3240, 3241], [3476, 3477], [3606, 3607], [3637, 3638], [3725, 3726], [3765, 3766], [3864, 3865], [4343, 4344], [4414, 4415], [4433, 4434], [4478, 4479], [4519, 4520], [4636, 4637], [4658, 4659], [4692, 4693], [5437, 5438], [5651, 5652], [996, 997], [3041, 3042], [3129, 3130], [3230, 3231], [4270, 4271], [5568, 5569], [5635, 5636]], "3": [[36, 38], [400, 402]], "30": [[2654, 2659], [2803, 2804]], "31": [[543, 547], [1759, 1763], [1842, 1846], [5313, 5317]], "32": [[95, 99], [2827, 2831]], "33": [[1254, 1256], [1400, 1402], [2614, 2616], [3265, 3267], [3440, 3442]], "34": [[228, 231], [966, 967], [2324, 2325], [4862, 4863]], "35": [[1252, 1253], [3496, 3497]], "36": [[467, 468], [1148, 1149], [2584, 2585], [3392, 3393], [3412, 3413], [5495, 5496], [614, 615], [1536, 1537]], "37": [[3309, 3311]], "38": [[4897, 4899]], "39": [[113, 114], [703, 704], [2697, 2699]], "4": [[27, 33], [344, 348]], "40": [[1670, 1672], [2119, 2121], [3299, 3301], [5244, 5246]], "41": [[920, 925], [5226, 5231]], "42": [[474, 475], [3581, 3582]], "43": [[4918, 4920]], "44": [[731, 735], [846, 850], [1940, 1943], [3761, 3764]], "45": [[4800, 4802], [4875, 4877]], "46": [[4867, 4871]], "47": [[666, 670], [4950, 4954]], "48": [[2225, 2227]], "49": [[1115, 1117], [2285, 2287], [3432, 3434], [5457, 5459], [5491, 5493]], "5": [[89, 93], [531, 535], [691, 694], [1449, 1453], [1454, 1458], [5260, 5264], [5281, 5285]], "50": [[1018, 1020], [2067, 2069], [3250, 3252], [3736, 3738], [4854, 4856]], "51": [[4924, 4927]], "52": [[539, 541], [2877, 2879], [5423, 5425]], "53": [[1169, 1171], [1359, 1360], [1719, 1721]], "54": [[1104, 1110], [1152, 1158]], "55": [[935, 937], [1021, 1024], [5040, 5043]], "56": [[4564, 4567], [5300, 5302]], "57": [[4700, 4703]], "58": [[80, 83], [2439, 2441], [2688, 2690], [2744, 2746], [4912, 4914], [5163, 5165]], "59": [[1396, 1399]], "6": [[884, 886], [4822, 4824]], "60": [[3253, 3255]], "61": [[1075, 1077], [1121, 1123], [1337, 1339], [1444, 1446]], "62": [[4933, 4934]], "63": [[5266, 5269]], "64": [[1282, 1284], [1469, 1471]], "65": [[3152, 3155]], "66": [[4890, 4893]], "67": [[1088, 1092], [1330, 1333], [2005, 2008]], "68": [[4214, 4216]], "69": [[2578, 2580], [3386, 3388]], "7": [[1819, 1821], [4857, 4859]], "70": [[3378, 3380]], "71": [[3192, 3197]], "72": [[3914, 3918]], "73": [[3857, 3862]], "74": [[4390, 4393]], "75": [[866, 871]], "76": [[2314, 2317]], "77": [[2910, 2911]], "78": [[3679, 3681]], "79": [[4260, 4262]], "8": [[252, 254], [817, 819], [4, 6], [23, 25], [404, 406], [881, 883], [983, 985], [1405, 1407], [2244, 2246], [4165, 4167], [4683, 4685], [4776, 4778], [5147, 5149], [5593, 5595]], "80": [[1161, 1163], [2276, 2278]], "81": [[5015, 5016]], "82": [[2805, 2806]], "83": [[3651, 3652]], "84": [[794, 798], [3206, 3210]], "85": [[5099, 5101]], "86": [[1648, 1650]], "87": [[1409, 1413]], "88": [[4792, 4794]], "89": [[464, 466]], "9": [[2634, 2637], [2747, 2748], [2798, 2799]], "90": [[4348, 4351]], "91": [[5615, 5619]], "92": [[637, 639], [1549, 1551]], "93": [[4626, 4627]], "94": [[4767, 4771]], "95": [[4964, 4968]], "96": [[5252, 5259]], "97": [[319, 321]], "98": [[944, 947]], "99": [[352, 356], [608, 612], [5602, 5606]]}, "doc_id": "100c730003033151c0f78ed1aab23df3e9bd5283", "method_subrelations": {"Attentive_LSTM": [[[0, 14], "Attentive_LSTM"]], "LSTM": [[[0, 4], "LSTM"]], "LSTM__lexical_overlap___dist_output_": [[[0, 4], "LSTM"], [[6, 21], "lexical_overlap"], [[24, 35], "dist_output"]]}, "n_ary_relations": [{"Material": "QASent", "Method": "Attentive_LSTM", "Metric": "MAP", "Task": "Question_Answering", "score": "0.7339"}, {"Material": "QASent", "Method": "LSTM", "Metric": "MAP", "Task": "Question_Answering", "score": "0.6436"}, {"Material": "QASent", "Method": "LSTM__lexical_overlap___dist_output_", "Metric": "MAP", "Task": "Question_Answering", "score": "0.7228"}, {"Material": "QASent", "Method": "Attentive_LSTM", "Metric": "MRR", "Task": "Question_Answering", "score": "0.8117"}, {"Material": "QASent", "Method": "LSTM", "Metric": "MRR", "Task": "Question_Answering", "score": " 0.7235"}, {"Material": "QASent", "Method": "LSTM__lexical_overlap___dist_output_", "Metric": "MRR", "Task": "Question_Answering", "score": "0.7986"}, {"Material": "WikiQA", "Method": "Attentive_LSTM", "Metric": "MAP", "Task": "Question_Answering", "score": "0.6886"}, {"Material": "WikiQA", "Method": "LSTM", "Metric": "MAP", "Task": "Question_Answering", "score": "0.6552"}, {"Material": "WikiQA", "Method": "LSTM__lexical_overlap___dist_output_", "Metric": "MAP", "Task": "Question_Answering", "score": " 0.6820"}, {"Material": "WikiQA", "Method": "Attentive_LSTM", "Metric": "MRR", "Task": "Question_Answering", "score": "0.7069"}, {"Material": "WikiQA", "Method": "LSTM", "Metric": "MRR", "Task": "Question_Answering", "score": " 0.6747"}, {"Material": "WikiQA", "Method": "LSTM__lexical_overlap___dist_output_", "Metric": "MRR", "Task": "Question_Answering", "score": "0.6988"}], "ner": [[11, 15, "Method"], [27, 33, "Task"], [36, 38, "Method"], [40, 42, "Method"], [54, 56, "Method"], [76, 79, "Task"], [80, 83, "Task"], [84, 87, "Task"], [89, 93, "Method"], [95, 99, "Method"], [101, 108, "Method"], [113, 114, "Metric"], [121, 125, "Method"], [127, 130, "Method"], [132, 134, "Method"], [147, 150, "Metric"], [158, 159, "Method"], [162, 165, "Method"], [173, 176, "Task"], [177, 178, "Task"], [220, 222, "Method"], [228, 231, "Task"], [241, 245, "Method"], [246, 247, "Method"], [252, 254, "Method"], [270, 272, "Metric"], [277, 279, "Task"], [281, 286, "Method"], [311, 313, "Method"], [319, 321, "Method"], [340, 343, "Method"], [344, 348, "Task"], [352, 356, "Method"], [367, 369, "Method"], [373, 376, "Method"], [394, 396, "Method"], [400, 402, "Method"], [427, 430, "Method"], [432, 434, "Method"], [437, 443, "Task"], [452, 454, "Method"], [464, 466, "Method"], [467, 468, "Method"], [470, 473, "Method"], [474, 475, "Method"], [478, 481, "Method"], [482, 483, "Method"], [492, 494, "Method"], [498, 500, "Method"], [505, 507, "Method"], [510, 519, "Method"], [531, 535, "Method"], [536, 537, "Method"], [539, 541, "Method"], [543, 547, "Method"], [548, 549, "Method"], [551, 553, "Task"], [574, 575, "Method"], [583, 588, "Method"], [608, 612, "Method"], [617, 619, "Method"], [622, 629, "Method"], [637, 639, "Method"], [640, 642, "Method"], [656, 657, "Method"], [666, 670, "Method"], [691, 694, "Method"], [703, 704, "Metric"], [713, 714, "Method"], [722, 725, "Method"], [727, 728, "Method"], [731, 735, "Method"], [750, 752, "Method"], [775, 777, "Method"], [794, 798, "Method"], [803, 806, "Task"], [817, 819, "Method"], [843, 844, "Method"], [846, 850, "Method"], [853, 855, "Method"], [866, 871, "Method"], [884, 886, "Task"], [906, 911, "Method"], [913, 916, "Method"], [920, 925, "Task"], [935, 937, "Method"], [940, 944, "Method"], [944, 947, "Method"], [966, 967, "Task"], [1005, 1007, "Method"], [1018, 1020, "Method"], [1021, 1024, "Method"], [1045, 1047, "Method"], [1051, 1054, "Metric"], [1075, 1077, "Method"], [1088, 1092, "Method"], [1099, 1101, "Method"], [1104, 1110, "Method"], [1115, 1117, "Method"], [1121, 1123, "Method"], [1135, 1138, "Method"], [1148, 1149, "Method"], [1152, 1158, "Method"], [1161, 1163, "Method"], [1169, 1171, "Method"], [1186, 1190, "Method"], [1212, 1215, "Method"], [1218, 1222, "Method"], [1252, 1253, "Metric"], [1254, 1256, "Method"], [1282, 1284, "Method"], [1290, 1293, "Method"], [1330, 1333, "Method"], [1337, 1339, "Method"], [1359, 1360, "Method"], [1362, 1364, "Method"], [1369, 1371, "Method"], [1374, 1376, "Method"], [1387, 1390, "Method"], [1396, 1399, "Task"], [1400, 1402, "Method"], [1409, 1413, "Task"], [1433, 1437, "Method"], [1444, 1446, "Method"], [1449, 1453, "Method"], [1454, 1458, "Method"], [1469, 1471, "Method"], [1498, 1504, "Method"], [1510, 1514, "Method"], [1522, 1525, "Method"], [1529, 1530, "Method"], [1532, 1534, "Method"], [1539, 1541, "Method"], [1549, 1551, "Method"], [1574, 1576, "Metric"], [1615, 1618, "Method"], [1648, 1650, "Method"], [1659, 1661, "Method"], [1670, 1672, "Method"], [1709, 1712, "Method"], [1719, 1721, "Method"], [1723, 1727, "Method"], [1747, 1748, "Method"], [1754, 1756, "Method"], [1759, 1763, "Method"], [1763, 1766, "Task"], [1768, 1771, "Task"], [1819, 1821, "Task"], [1835, 1840, "Task"], [1842, 1846, "Method"], [1854, 1856, "Method"], [1859, 1863, "Method"], [1872, 1873, "Method"], [1891, 1892, "Method"], [1912, 1917, "Method"], [1922, 1924, "Task"], [1926, 1927, "Method"], [1940, 1943, "Method"], [1949, 1952, "Task"], [1953, 1954, "Method"], [1956, 1958, "Method"], [1991, 1992, "Method"], [1994, 1996, "Method"], [2005, 2008, "Method"], [2012, 2014, "Method"], [2056, 2058, "Method"], [2067, 2069, "Method"], [2071, 2072, "Method"], [2088, 2089, "Method"], [2099, 2104, "Task"], [2119, 2121, "Method"], [2136, 2137, "Method"], [2139, 2141, "Method"], [2173, 2177, "Method"], [2213, 2217, "Task"], [2225, 2227, "Method"], [2251, 2254, "Method"], [2256, 2258, "Method"], [2265, 2266, "Method"], [2276, 2278, "Method"], [2285, 2287, "Method"], [2314, 2317, "Method"], [2324, 2325, "Task"], [2337, 2339, "Method"], [2344, 2350, "Task"], [2358, 2359, "Method"], [2395, 2396, "Method"], [2398, 2401, "Method"], [2406, 2408, "Task"], [2439, 2441, "Task"], [2444, 2445, "Method"], [2519, 2521, "Metric"], [2549, 2551, "Method"], [2571, 2574, "Method"], [2578, 2580, "Task"], [2584, 2585, "Method"], [2604, 2606, "Method"], [2610, 2611, "Task"], [2614, 2616, "Method"], [2628, 2629, "Task"], [2634, 2637, "Task"], [2643, 2649, "Method"], [2650, 2651, "Method"], [2654, 2659, "Method"], [2664, 2666, "Method"], [2668, 2670, "Method"], [2688, 2690, "Task"], [2697, 2699, "Metric"], [2732, 2734, "Method"], [2744, 2746, "Task"], [2747, 2748, "Task"], [2783, 2784, "Method"], [2788, 2791, "Method"], [2798, 2799, "Task"], [2803, 2804, "Method"], [2805, 2806, "Method"], [2824, 2825, "Method"], [2827, 2831, "Method"], [2837, 2838, "Method"], [2856, 2857, "Method"], [2868, 2869, "Method"], [2877, 2879, "Method"], [2888, 2890, "Metric"], [2908, 2909, "Method"], [2910, 2911, "Method"], [2927, 2928, "Method"], [3025, 3026, "Method"], [3032, 3034, "Metric"], [3035, 3037, "Metric"], [3044, 3045, "Material"], [3057, 3060, "Task"], [3069, 3070, "Material"], [3072, 3074, "Material"], [3075, 3076, "Material"], [3087, 3088, "Material"], [3106, 3108, "Method"], [3152, 3155, "Method"], [3156, 3159, "Method"], [3178, 3183, "Method"], [3192, 3197, "Method"], [3202, 3204, "Method"], [3206, 3210, "Method"], [3211, 3212, "Task"], [3222, 3225, "Method"], [3236, 3239, "Method"], [3240, 3241, "Method"], [3250, 3252, "Method"], [3253, 3255, "Method"], [3258, 3260, "Method"], [3265, 3267, "Method"], [3281, 3283, "Method"], [3299, 3301, "Method"], [3302, 3303, "Metric"], [3304, 3305, "Metric"], [3309, 3311, "Metric"], [3366, 3369, "Method"], [3375, 3376, "Method"], [3378, 3380, "Method"], [3386, 3388, "Task"], [3392, 3393, "Method"], [3412, 3413, "Method"], [3432, 3434, "Method"], [3436, 3437, "Task"], [3440, 3442, "Method"], [3453, 3454, "Task"], [3476, 3477, "Method"], [3478, 3480, "Metric"], [3493, 3495, "Metric"], [3496, 3497, "Metric"], [3503, 3504, "Task"], [3540, 3543, "Task"], [3586, 3587, "Metric"], [3589, 3590, "Metric"], [3592, 3595, "Method"], [3606, 3607, "Method"], [3637, 3638, "Method"], [3646, 3650, "Method"], [3651, 3652, "Method"], [3662, 3663, "Method"], [3679, 3681, "Method"], [3725, 3726, "Method"], [3736, 3738, "Method"], [3744, 3748, "Task"], [3761, 3764, "Method"], [3765, 3766, "Method"], [3769, 3771, "Method"], [3772, 3775, "Method"], [3798, 3800, "Method"], [3809, 3811, "Task"], [3830, 3832, "Method"], [3857, 3862, "Method"], [3864, 3865, "Method"], [3914, 3918, "Method"], [4204, 4205, "Task"], [4214, 4216, "Method"], [4228, 4229, "Method"], [4260, 4262, "Method"], [4289, 4290, "Method"], [4326, 4327, "Method"], [4343, 4344, "Method"], [4348, 4351, "Method"], [4367, 4369, "Task"], [4372, 4373, "Task"], [4379, 4383, "Task"], [4390, 4393, "Method"], [4394, 4397, "Method"], [4414, 4415, "Method"], [4428, 4431, "Task"], [4433, 4434, "Method"], [4443, 4446, "Method"], [4449, 4451, "Method"], [4452, 4455, "Method"], [4472, 4475, "Method"], [4478, 4479, "Method"], [4508, 4510, "Method"], [4516, 4518, "Method"], [4519, 4520, "Method"], [4552, 4554, "Method"], [4564, 4567, "Method"], [4621, 4623, "Method"], [4626, 4627, "Method"], [4636, 4637, "Method"], [4649, 4650, "Task"], [4656, 4657, "Method"], [4658, 4659, "Method"], [4690, 4691, "Method"], [4692, 4693, "Method"], [4700, 4703, "Method"], [4708, 4710, "Metric"], [4722, 4724, "Method"], [4731, 4733, "Method"], [4757, 4760, "Method"], [4767, 4771, "Method"], [4792, 4794, "Method"], [4800, 4802, "Method"], [4822, 4824, "Task"], [4829, 4833, "Method"], [4835, 4836, "Method"], [4854, 4856, "Method"], [4857, 4859, "Task"], [4862, 4863, "Task"], [4867, 4871, "Method"], [4875, 4877, "Method"], [4890, 4893, "Method"], [4897, 4899, "Method"], [4912, 4914, "Task"], [4918, 4920, "Task"], [4924, 4927, "Method"], [4933, 4934, "Method"], [4948, 4949, "Method"], [4950, 4954, "Method"], [4964, 4968, "Method"], [5003, 5004, "Method"], [5005, 5006, "Method"], [5015, 5016, "Method"], [5037, 5039, "Task"], [5040, 5043, "Method"], [5099, 5101, "Method"], [5114, 5119, "Method"], [5127, 5129, "Method"], [5151, 5155, "Method"], [5163, 5165, "Task"], [5166, 5170, "Task"], [5217, 5219, "Method"], [5226, 5231, "Task"], [5244, 5246, "Method"], [5252, 5259, "Method"], [5260, 5264, "Method"], [5266, 5269, "Task"], [5275, 5279, "Method"], [5281, 5285, "Method"], [5288, 5290, "Method"], [5294, 5296, "Method"], [5300, 5302, "Method"], [5313, 5317, "Method"], [5320, 5322, "Method"], [5326, 5328, "Method"], [5346, 5348, "Metric"], [5349, 5351, "Metric"], [5352, 5353, "Method"], [5366, 5368, "Method"], [5382, 5384, "Method"], [5423, 5425, "Method"], [5434, 5436, "Metric"], [5437, 5438, "Method"], [5440, 5445, "Task"], [5448, 5450, "Method"], [5457, 5459, "Method"], [5478, 5479, "Method"], [5491, 5493, "Method"], [5495, 5496, "Method"], [5515, 5517, "Method"], [5536, 5537, "Method"], [5541, 5543, "Method"], [5545, 5547, "Method"], [5551, 5553, "Method"], [5556, 5558, "Task"], [5597, 5598, "Method"], [5602, 5606, "Method"], [5608, 5610, "Metric"], [5615, 5619, "Method"], [5624, 5626, "Metric"], [5627, 5630, "Method"], [5648, 5650, "Method"], [5651, 5652, "Method"], [5655, 5658, "Method"], [4, 6, "Method"], [23, 25, "Method"], [404, 406, "Method"], [614, 615, "Method"], [881, 883, "Method"], [951, 952, "Task"], [983, 985, "Method"], [993, 994, "Method"], [996, 997, "Method"], [1405, 1407, "Method"], [1536, 1537, "Method"], [2244, 2246, "Method"], [2565, 2566, "Method"], [3041, 3042, "Method"], [3129, 3130, "Method"], [3141, 3142, "Method"], [3144, 3145, "Method"], [3148, 3149, "Method"], [3218, 3219, "Method"], [3230, 3231, "Method"], [3529, 3530, "Metric"], [3571, 3572, "Material"], [3576, 3577, "Method"], [3581, 3582, "Method"], [3601, 3602, "Method"], [3614, 3615, "Material"], [3669, 3670, "Material"], [3716, 3717, "Method"], [4113, 4114, "Metric"], [4122, 4123, "Metric"], [4165, 4167, "Method"], [4175, 4176, "Task"], [4270, 4271, "Method"], [4683, 4685, "Method"], [4776, 4778, "Method"], [4985, 4986, "Metric"], [5147, 5149, "Method"], [5566, 5567, "Method"], [5568, 5569, "Method"], [5593, 5595, "Method"], [5635, 5636, "Method"]], "sections": [[0, 159], [159, 938], [938, 1447], [1447, 1757], [1757, 2430], [2430, 2433], [2433, 2684], [2684, 3051], [3051, 3536], [3536, 4155], [4155, 4725], [4725, 5138], [5138, 5247], [5247, 5250], [5250, 5270], [5270, 5279], [5279, 5311], [5311, 5344], [5344, 5659]], "sentences": [[0, 16], [16, 34], [34, 68], [68, 88], [88, 120], [120, 145], [145, 158], [158, 159], [159, 162], [162, 180], [180, 206], [206, 241], [241, 245], [245, 268], [268, 305], [305, 336], [336, 360], [360, 390], [390, 422], [422, 452], [452, 495], [495, 520], [520, 524], [524, 573], [573, 601], [601, 652], [652, 686], [686, 712], [712, 749], [749, 778], [778, 817], [817, 838], [838, 872], [872, 891], [891, 912], [912, 938], [938, 944], [944, 975], [975, 1002], [1002, 1025], [1025, 1038], [1038, 1065], [1065, 1083], [1083, 1093], [1093, 1113], [1113, 1119], [1119, 1129], [1129, 1172], [1172, 1198], [1198, 1203], [1203, 1229], [1229, 1232], [1232, 1234], [1234, 1240], [1240, 1245], [1245, 1260], [1260, 1275], [1275, 1299], [1299, 1316], [1316, 1340], [1340, 1372], [1372, 1403], [1403, 1447], [1447, 1453], [1453, 1495], [1495, 1520], [1520, 1562], [1562, 1593], [1593, 1604], [1604, 1635], [1635, 1658], [1658, 1686], [1686, 1697], [1697, 1713], [1713, 1715], [1715, 1740], [1740, 1757], [1757, 1763], [1763, 1790], [1790, 1816], [1816, 1841], [1841, 1868], [1868, 1882], [1882, 1902], [1902, 1925], [1925, 1953], [1953, 1980], [1980, 2009], [2009, 2031], [2031, 2053], [2053, 2079], [2079, 2105], [2105, 2135], [2135, 2169], [2169, 2182], [2182, 2206], [2206, 2211], [2211, 2241], [2241, 2282], [2282, 2305], [2305, 2351], [2351, 2367], [2367, 2372], [2372, 2388], [2388, 2409], [2409, 2416], [2416, 2430], [2430, 2433], [2433, 2441], [2441, 2451], [2451, 2474], [2474, 2496], [2496, 2518], [2518, 2536], [2536, 2563], [2563, 2576], [2576, 2588], [2588, 2609], [2609, 2638], [2638, 2660], [2660, 2684], [2684, 2690], [2690, 2700], [2700, 2722], [2722, 2743], [2743, 2778], [2778, 2807], [2807, 2832], [2832, 2846], [2846, 2873], [2873, 2891], [2891, 2912], [2912, 2939], [2939, 2958], [2958, 2980], [2980, 3006], [3006, 3031], [3031, 3051], [3051, 3060], [3060, 3075], [3075, 3109], [3109, 3121], [3121, 3138], [3138, 3161], [3161, 3172], [3172, 3198], [3198, 3220], [3220, 3242], [3242, 3268], [3268, 3302], [3302, 3315], [3315, 3333], [3333, 3348], [3348, 3364], [3364, 3381], [3381, 3396], [3396, 3416], [3416, 3435], [3435, 3467], [3467, 3487], [3487, 3509], [3509, 3536], [3536, 3543], [3543, 3569], [3569, 3591], [3591, 3612], [3612, 3665], [3665, 3686], [3686, 3708], [3708, 3749], [3749, 3786], [3786, 3812], [3812, 3852], [3852, 3896], [3896, 3925], [3925, 3940], [3940, 3960], [3960, 3988], [3988, 3992], [3992, 3996], [3996, 3997], [3997, 4004], [4004, 4016], [4016, 4051], [4051, 4069], [4069, 4084], [4084, 4105], [4105, 4121], [4121, 4139], [4139, 4155], [4155, 4158], [4158, 4178], [4178, 4200], [4200, 4220], [4220, 4224], [4224, 4249], [4249, 4267], [4267, 4274], [4274, 4278], [4278, 4287], [4287, 4305], [4305, 4324], [4324, 4361], [4361, 4384], [4384, 4423], [4423, 4447], [4447, 4483], [4483, 4511], [4511, 4528], [4528, 4534], [4534, 4574], [4574, 4592], [4592, 4615], [4615, 4652], [4652, 4686], [4686, 4704], [4704, 4725], [4725, 4729], [4729, 4772], [4772, 4804], [4804, 4825], [4825, 4847], [4847, 4887], [4887, 4907], [4907, 4931], [4931, 4938], [4938, 4963], [4963, 4998], [4998, 5024], [5024, 5044], [5044, 5064], [5064, 5072], [5072, 5078], [5078, 5092], [5092, 5113], [5113, 5119], [5119, 5138], [5138, 5141], [5141, 5156], [5156, 5191], [5191, 5247], [5247, 5250], [5250, 5259], [5259, 5270], [5270, 5279], [5279, 5285], [5285, 5311], [5311, 5317], [5317, 5330], [5330, 5344], [5344, 5348], [5348, 5359], [5359, 5393], [5393, 5433], [5433, 5447], [5447, 5452], [5452, 5488], [5488, 5514], [5514, 5519], [5519, 5548], [5548, 5562], [5562, 5596], [5596, 5620], [5620, 5640], [5640, 5659]], "words": ["Recent", "advances", "in", "neural", "variational", "inference", "have", "spawned", "a", "renaissance", "in", "deep", "latent", "variable", "models", ".", "In", "this", "paper", "we", "introduce", "a", "generic", "variational", "inference", "framework", "for", "generative", "and", "conditional", "models", "of", "text", ".", "While", "traditional", "variational", "methods", "derive", "an", "analytic", "approximation", "for", "the", "intractable", "distributions", "over", "latent", "variables", ",", "here", "we", "construct", "an", "inference", "network", "conditioned", "on", "the", "discrete", "text", "input", "to", "provide", "the", "variational", "distribution", ".", "We", "validate", "this", "framework", "on", "two", "very", "different", "text", "modelling", "applications", ",", "generative", "document", "modelling", "and", "supervised", "question", "answering", ".", "Our", "neural", "variational", "document", "model", "combines", "a", "continuous", "stochastic", "document", "representation", "with", "a", "bag", "-", "of", "-", "words", "generative", "model", "and", "achieves", "the", "lowest", "reported", "perplexities", "on", "two", "standard", "test", "corpora", ".", "The", "neural", "answer", "selection", "model", "employs", "a", "stochastic", "representation", "layer", "within", "an", "attention", "mechanism", "to", "extract", "the", "semantics", "between", "a", "question", "and", "answer", "pair", ".", "On", "two", "question", "answering", "benchmarks", "this", "model", "exceeds", "all", "previous", "published", "benchmarks", ".", "NeuralVariationalInferenceforTextProcessing", "section", ":", "Introduction", "Probabilistic", "generative", "models", "underpin", "many", "successful", "applications", "within", "the", "field", "of", "natural", "language", "processing", "(", "NLP", ")", ".", "Their", "popularity", "stems", "from", "their", "ability", "to", "use", "unlabelled", "data", "effectively", ",", "to", "incorporate", "abundant", "linguistic", "features", ",", "and", "to", "learn", "interpretable", "dependencies", "among", "data", ".", "However", "these", "successes", "are", "tempered", "by", "the", "fact", "that", "as", "the", "structure", "of", "such", "generative", "models", "becomes", "deeper", "and", "more", "complex", ",", "true", "Bayesian", "inference", "becomes", "intractable", "due", "to", "the", "high", "dimensional", "integrals", "required", ".", "Markov", "chain", "Monte", "Carlo", "(", "MCMC", ")", "neal1993probabilistic", ",", "andrieu2003introduction", "and", "variational", "inference", "jordan1999introduction", ",", "attias2000variational", ",", "beal2003variational", "are", "the", "standard", "approaches", "for", "approximating", "these", "integrals", ".", "However", "the", "computational", "cost", "of", "the", "former", "results", "in", "impractical", "training", "for", "the", "large", "and", "deep", "neural", "networks", "which", "are", "now", "fashionable", ",", "and", "the", "latter", "is", "conventionally", "confined", "due", "to", "the", "underestimation", "of", "posterior", "variance", ".", "The", "lack", "of", "effective", "and", "efficient", "inference", "methods", "hinders", "our", "ability", "to", "create", "highly", "expressive", "models", "of", "text", ",", "especially", "in", "the", "situation", "where", "the", "model", "is", "non", "-", "conjugate", ".", "This", "paper", "introduces", "a", "neural", "variational", "framework", "for", "generative", "models", "of", "text", ",", "inspired", "by", "the", "variational", "auto", "-", "encoder", "rezende2014stochastic", ",", "kingma2013auto", ".", "The", "principle", "idea", "is", "to", "build", "an", "inference", "network", ",", "implemented", "by", "a", "deep", "neural", "network", "conditioned", "on", "text", ",", "to", "approximate", "the", "intractable", "distributions", "over", "the", "latent", "variables", ".", "Instead", "of", "providing", "an", "analytic", "approximation", ",", "as", "in", "traditional", "variational", "Bayes", ",", "neural", "variational", "inference", "learns", "to", "model", "the", "posterior", "probability", ",", "thus", "endowing", "the", "model", "with", "strong", "generalisation", "abilities", ".", "Due", "to", "the", "flexibility", "of", "deep", "neural", "networks", ",", "the", "inference", "network", "is", "capable", "of", "learning", "complicated", "non", "-", "linear", "distributions", "and", "processing", "structured", "inputs", "such", "as", "word", "sequences", ".", "Inference", "networks", "can", "be", "designed", "as", ",", "but", "not", "restricted", "to", ",", "multilayer", "perceptrons", "(", "MLP", ")", ",", "convolutional", "neural", "networks", "(", "CNN", ")", ",", "and", "recurrent", "neural", "networks", "(", "RNN", ")", ",", "approaches", "which", "are", "rarely", "used", "in", "conventional", "generative", "models", ".", "By", "using", "the", "reparameterisation", "method", "rezende2014stochastic", ",", "kingma2013auto", ",", "the", "inference", "network", "is", "trained", "through", "back", "-", "propagating", "unbiased", "and", "low", "variance", "gradients", "w.r.t", ".", "the", "latent", "variables", ".", "Within", "this", "framework", ",", "we", "propose", "a", "Neural", "Variational", "Document", "Model", "(", "NVDM", ")", "for", "document", "modelling", "and", "a", "Neural", "Answer", "Selection", "Model", "(", "NASM", ")", "for", "question", "answering", ",", "a", "task", "that", "selects", "the", "sentences", "that", "correctly", "answer", "a", "factoid", "question", "from", "a", "set", "of", "candidate", "sentences", ".", "The", "NVDM", "(", "Figure", "[", "reference", "]", ")", "is", "an", "unsupervised", "generative", "model", "of", "text", "which", "aims", "to", "extract", "a", "continuous", "semantic", "latent", "variable", "for", "each", "document", ".", "This", "model", "can", "be", "interpreted", "as", "a", "variational", "auto", "-", "encoder", ":", "an", "MLP", "encoder", "(", "inference", "network", ")", "compresses", "the", "bag", "-", "of", "-", "words", "document", "representation", "into", "a", "continuous", "latent", "distribution", ",", "and", "a", "softmax", "decoder", "(", "generative", "model", ")", "reconstructs", "the", "document", "by", "generating", "the", "words", "independently", ".", "A", "primary", "feature", "of", "NVDM", "is", "that", "each", "word", "is", "generated", "directly", "from", "a", "dense", "continuous", "document", "representation", "instead", "of", "the", "more", "common", "binary", "semantic", "vector", "hinton2009replicated", ",", "larochelle2012neural", ",", "Srivastava2013", ",", "mnih2014neural", ".", "Our", "experiments", "demonstrate", "that", "our", "neural", "document", "model", "achieves", "the", "state", "-", "of", "-", "the", "-", "art", "perplexities", "on", "the", "20NewsGroups", "and", "RCV1", "-", "v2", ".", "The", "NASM", "(", "Figure", "[", "reference", "]", ")", "is", "a", "supervised", "conditional", "model", "which", "imbues", "LSTMs", "hochreiter1997long", "with", "a", "latent", "stochastic", "attention", "mechanism", "to", "model", "the", "semantics", "of", "question", "-", "answer", "pairs", "and", "predict", "their", "relatedness", ".", "The", "attention", "model", "is", "designed", "to", "focus", "on", "the", "phrases", "of", "an", "answer", "that", "are", "strongly", "connected", "to", "the", "question", "semantics", "and", "is", "modelled", "by", "a", "latent", "distribution", ".", "This", "mechanism", "allows", "the", "model", "to", "deal", "with", "the", "ambiguity", "inherent", "in", "the", "task", "and", "learns", "pair", "-", "specific", "representations", "that", "are", "more", "effective", "at", "predicting", "answer", "matches", ",", "rather", "than", "independent", "embeddings", "of", "question", "and", "answer", "sentences", ".", "Bayesian", "inference", "provides", "a", "natural", "safeguard", "against", "overfitting", ",", "especially", "as", "the", "training", "sets", "available", "for", "this", "task", "are", "small", ".", "The", "experiments", "show", "that", "the", "LSTM", "with", "a", "latent", "stochastic", "attention", "mechanism", "learns", "an", "effective", "attention", "model", "and", "outperforms", "both", "previously", "published", "results", ",", "and", "our", "own", "strong", "non", "-", "stochastic", "attention", "baselines", ".", "In", "summary", ",", "we", "demonstrate", "the", "effectiveness", "of", "neural", "variational", "inference", "for", "text", "processing", "on", "two", "diverse", "tasks", ".", "These", "models", "are", "simple", ",", "expressive", "and", "can", "be", "trained", "efficiently", "with", "the", "highly", "scalable", "stochastic", "gradient", "back", "-", "propagation", ".", "Our", "neural", "variational", "framework", "is", "suitable", "for", "both", "unsupervised", "and", "supervised", "learning", "tasks", ",", "and", "can", "be", "generalised", "to", "incorporate", "any", "type", "of", "neural", "networks", ".", "section", ":", "Neural", "Variational", "Inference", "Framework", "Latent", "variable", "modelling", "is", "popular", "in", "many", "NLP", "problems", ",", "but", "it", "is", "non", "-", "trivial", "to", "carry", "out", "effective", "and", "efficient", "inference", "for", "models", "with", "complex", "and", "deep", "structure", ".", "In", "this", "section", "we", "introduce", "a", "generic", "neural", "variational", "inference", "framework", "that", "we", "apply", "to", "both", "the", "unsupervised", "NVDM", "and", "supervised", "NASM", "in", "the", "follow", "sections", ".", "We", "define", "a", "generative", "model", "with", "a", "latent", "variable", ",", "which", "can", "be", "considered", "as", "the", "stochastic", "units", "in", "deep", "neural", "networks", ".", "We", "designate", "the", "observed", "parent", "and", "child", "nodes", "of", "as", "and", "respectively", ".", "Hence", ",", "the", "joint", "distribution", "of", "the", "generative", "model", "is", ",", "and", "the", "variational", "lower", "bound", "is", "derived", "as", ":", "where", "parameterises", "the", "generative", "distributions", "and", ".", "In", "order", "to", "have", "a", "tight", "lower", "bound", ",", "the", "variational", "distribution", "should", "approach", "the", "true", "posterior", ".", "Here", ",", "we", "employ", "a", "parameterised", "diagonal", "Gaussian", "as", ".", "The", "three", "steps", "to", "construct", "the", "inference", "network", "are", ":", "Construct", "vector", "representations", "of", "the", "observed", "variables", ":", ",", ".", "Assemble", "a", "joint", "representation", ":", ".", "Parameterise", "the", "variational", "distribution", "over", "the", "latent", "variable", ":", ".", "and", "can", "be", "any", "type", "of", "deep", "neural", "networks", "that", "are", "suitable", "for", "the", "observed", "data", ";", "is", "an", "MLP", "that", "concatenates", "the", "vector", "representations", "of", "the", "conditioning", "variables", ";", "is", "a", "linear", "transformation", "which", "outputs", "the", "parameters", "of", "the", "Gaussian", "distribution", ".", "By", "sampling", "from", "the", "variational", "distribution", ",", ",", "we", "are", "able", "to", "carry", "out", "stochastic", "back", "-", "propagation", "to", "optimise", "the", "lower", "bound", "(", "Eq", ".", "[", "reference", "]", ")", ".", "During", "training", ",", "the", "model", "parameters", "together", "with", "the", "inference", "network", "parameters", "are", "updated", "by", "stochastic", "back", "-", "propagation", "based", "on", "the", "samples", "drawn", "from", ".", "For", "the", "gradients", "w.r.t", ".", ",", "we", "have", "the", "form", ":", "For", "the", "gradients", "w.r.t", ".", "we", "reparameterise", "and", "sample", "to", "reduce", "the", "variance", "in", "stochastic", "estimation", "rezende2014stochastic", ",", "kingma2013auto", ".", "The", "update", "of", "can", "be", "carried", "out", "by", "back", "-", "propagating", "the", "gradients", "w.r.t", ".", "and", ":", "It", "is", "worth", "mentioning", "that", "unsupervised", "learning", "is", "a", "special", "case", "of", "the", "neural", "variational", "framework", "where", "has", "no", "parent", "node", ".", "In", "that", "case", "is", "directly", "drawn", "from", "the", "prior", "instead", "of", "the", "conditional", "distribution", ",", "and", ".", "Here", "we", "only", "discuss", "the", "scenario", "where", "the", "latent", "variables", "are", "continuous", "and", "the", "parameterised", "diagonal", "Gaussian", "is", "employed", "as", "the", "variational", "distribution", ".", "However", "the", "framework", "is", "also", "suitable", "for", "discrete", "units", ",", "and", "the", "only", "modification", "needed", "is", "to", "replace", "the", "Gaussian", "with", "a", "multinomial", "parameterised", "by", "the", "outputs", "of", "a", "softmax", "function", ".", "Though", "the", "reparameterisation", "trick", "for", "continuous", "variables", "is", "not", "applicable", "for", "this", "case", ",", "a", "policy", "gradient", "approach", "mnih2014neural", "can", "help", "to", "alleviate", "the", "high", "variance", "problem", "during", "stochastic", "estimation", ".", "proposed", "a", "variational", "inference", "framework", "for", "semi", "-", "supervised", "learning", ",", "but", "the", "prior", "distribution", "over", "the", "hidden", "variable", "remains", "as", "the", "standard", "Gaussian", "prior", ",", "while", "we", "apply", "a", "conditional", "parameterised", "Gaussian", "distribution", ",", "which", "is", "jointly", "learned", "with", "the", "variational", "distribution", ".", "section", ":", "Neural", "Variational", "Document", "Model", "The", "Neural", "Variational", "Document", "Model", "(", "Figure", "[", "reference", "]", ")", "is", "a", "simple", "instance", "of", "unsupervised", "learning", "where", "a", "continuous", "hidden", "variable", ",", "which", "generates", "all", "the", "words", "in", "a", "document", "independently", ",", "is", "introduced", "to", "represent", "its", "semantic", "content", ".", "Let", "be", "the", "bag", "-", "of", "-", "words", "representation", "of", "a", "document", "and", "be", "the", "one", "-", "hot", "representation", "of", "the", "word", "at", "position", ".", "As", "an", "unsupervised", "generative", "model", ",", "we", "could", "interpret", "NVDM", "as", "a", "variational", "autoencoder", ":", "an", "MLP", "encoder", "compresses", "document", "representations", "into", "continuous", "hidden", "vectors", "(", ")", ";", "a", "softmax", "decoder", "reconstructs", "the", "documents", "by", "independently", "generating", "the", "words", "(", ")", ".", "To", "maximise", "the", "log", "-", "likelihood", "of", "documents", ",", "we", "derive", "the", "lower", "bound", ":", "where", "is", "the", "number", "of", "words", "in", "the", "document", "and", "is", "a", "Gaussian", "prior", "for", ".", "Here", ",", "we", "consider", "is", "observed", "for", "all", "the", "documents", ".", "The", "conditional", "probability", "over", "words", "(", "decoder", ")", "is", "modelled", "by", "multinomial", "logistic", "regression", "and", "shared", "across", "documents", ":", "where", "learns", "the", "semantic", "word", "embeddings", "and", "represents", "the", "bias", "term", ".", "As", "there", "is", "no", "supervision", "information", "for", "the", "latent", "semantics", ",", ",", "the", "posterior", "approximation", "is", "only", "conditioned", "on", "the", "current", "document", ".", "The", "inference", "network", "is", "modelled", "as", ":", "For", "each", "document", ",", "the", "neural", "network", "generates", "its", "own", "parameters", "and", "that", "parameterise", "the", "latent", "distribution", "over", "document", "semantics", ".", "Based", "on", "the", "samples", ",", "the", "lower", "bound", "(", "Eq", ".", "[", "reference", "]", ")", "can", "be", "optimised", "by", "back", "-", "propagating", "the", "stochastic", "gradients", "w.r.t", ".", "and", ".", "Since", "is", "a", "standard", "Gaussian", "prior", ",", "the", "Gaussian", "KL", "-", "Divergence", "can", "be", "computed", "analytically", "to", "further", "lower", "the", "variance", "of", "the", "gradients", ".", "Moreover", ",", "it", "also", "acts", "as", "a", "regulariser", "for", "updating", "the", "parameters", "of", "the", "inference", "network", ".", "section", ":", "Neural", "Answer", "Selection", "Model", "Answer", "sentence", "selection", "is", "a", "question", "answering", "paradigm", "where", "a", "model", "must", "identify", "the", "correct", "sentences", "answering", "a", "factual", "question", "from", "a", "set", "of", "candidate", "sentences", ".", "Assume", "a", "question", "is", "associated", "with", "a", "set", "of", "answer", "sentences", ",", "together", "with", "their", "judgements", ",", "where", "if", "the", "answer", "is", "correct", "and", "otherwise", ".", "This", "is", "a", "classification", "task", "where", "we", "treat", "each", "training", "data", "point", "as", "a", "triple", "while", "predicting", "for", "the", "unlabelled", "question", "-", "answer", "pair", ".", "The", "Neural", "Answer", "Selection", "Model", "(", "Figure", "[", "reference", "]", ")", "is", "a", "supervised", "model", "that", "learns", "the", "question", "and", "answer", "representations", "and", "predicts", "their", "relatedness", ".", "It", "employs", "two", "different", "LSTMs", "to", "embed", "raw", "question", "inputs", "and", "answer", "inputs", ".", "Let", "and", "be", "the", "state", "outputs", "of", "the", "two", "LSTMs", ",", "and", ",", "be", "the", "positions", "of", "the", "states", ".", "Conventionally", ",", "the", "last", "state", "outputs", "and", ",", "as", "the", "independent", "question", "and", "answer", "representations", ",", "can", "be", "used", "for", "relatedness", "prediction", ".", "In", "NASM", ",", "however", ",", "we", "aim", "to", "learn", "pair", "-", "specific", "representations", "through", "a", "latent", "attention", "mechanism", ",", "which", "is", "more", "effective", "for", "pair", "relatedness", "prediction", ".", "NASM", "applies", "an", "attention", "model", "to", "focus", "on", "the", "words", "in", "the", "answer", "sentence", "that", "are", "prominent", "for", "predicting", "the", "answer", "matched", "to", "the", "current", "question", ".", "Instead", "of", "using", "a", "deterministic", "question", "vector", ",", "such", "as", ",", "NASM", "employs", "a", "latent", "distribution", "to", "model", "the", "question", "semantics", ",", "which", "is", "a", "parameterised", "diagonal", "Gaussian", ".", "Therefore", ",", "the", "attention", "model", "extracts", "a", "context", "vector", "by", "iteratively", "attending", "to", "the", "answer", "tokens", "based", "on", "the", "stochastic", "vector", ".", "In", "doing", "so", "the", "model", "is", "able", "to", "adapt", "to", "the", "ambiguity", "inherent", "in", "questions", "and", "obtain", "salient", "information", "through", "attention", ".", "Compared", "to", "its", "deterministic", "counterpart", "(", "applying", "as", "the", "question", "semantics", ")", ",", "the", "stochastic", "units", "incorporated", "into", "NASM", "allow", "multi", "-", "modal", "attention", "distributions", ".", "Further", ",", "by", "marginalising", "over", "the", "latent", "variables", ",", "NASM", "is", "more", "robust", "against", "overfitting", ",", "which", "is", "important", "for", "small", "question", "answering", "training", "sets", ".", "In", "this", "model", ",", "the", "conditional", "distribution", "is", ":", "For", "each", "question", ",", "the", "neural", "network", "generates", "the", "corresponding", "parameters", "and", "that", "parameterise", "the", "latent", "distribution", "over", "question", "semantics", ".", "Following", "bahdanau2014neural", ",", "the", "attention", "model", "is", "defined", "as", ":", "where", "is", "the", "normalised", "attention", "score", "at", "answer", "token", ",", "and", "the", "context", "vector", "is", "the", "weighted", "sum", "of", "all", "the", "state", "outputs", ".", "We", "adopt", "as", "the", "question", "and", "answer", "representations", "for", "predicting", "their", "relatedness", ".", "is", "a", "deterministic", "vector", "that", "is", "equal", "to", ",", "while", "is", "a", "combination", "of", "the", "sequence", "output", "and", "the", "context", "vector", "(", "Eq", ".", "[", "reference", "]", ")", ".", "For", "the", "prediction", "of", "pair", "relatedness", ",", "we", "model", "the", "conditional", "probability", "distribution", "by", "sigmoid", "function", ":", "To", "maximise", "the", "log", "-", "likelihood", "we", "use", "the", "variational", "lower", "bound", ":", "Following", "the", "neural", "variational", "inference", "framework", ",", "we", "construct", "a", "deep", "neural", "network", "as", "the", "inference", "network", ":", "where", "and", "are", "also", "modelled", "by", "LSTMs", ",", "and", "the", "relatedness", "label", "is", "modelled", "by", "a", "simple", "linear", "transformation", "into", "the", "vector", ".", "According", "to", "the", "joint", "representation", ",", "we", "then", "generate", "the", "parameters", "and", ",", "which", "parameterise", "the", "variational", "distribution", "over", "the", "question", "semantics", ".", "To", "emphasise", ",", "though", "both", "and", "are", "modelled", "as", "parameterised", "Gaussian", "distributions", ",", "as", "an", "approximation", "only", "functions", "during", "inference", "by", "producing", "samples", "to", "compute", "the", "stochastic", "gradients", ",", "while", "is", "the", "generative", "distribution", "that", "generates", "the", "samples", "for", "predicting", "the", "question", "-", "answer", "relatedness", ".", "Based", "on", "the", "samples", ",", "we", "use", "SGVB", "to", "optimise", "the", "lower", "bound", "(", "Eq", ".", "[", "reference", "]", ")", ".", "The", "model", "parameters", "and", "the", "inference", "network", "parameters", "are", "updated", "jointly", "using", "their", "stochastic", "gradients", ".", "In", "this", "case", ",", "similar", "to", "the", "NVDM", ",", "the", "Gaussian", "KL", "divergence", "can", "be", "analytically", "computed", "during", "training", "process", ".", "[", "Perplexity", "on", "test", "dataset", ".", "]", "ModelDim20NewsRCV1LDA5010911437LDA20010581142RSM50953988docNADE50896742SBN50909784fDARN50917724fDARN200\u2014", "-", "598NVDM50836563NVDM200852550", "[", "The", "five", "nearest", "words", "in", "the", "semantic", "space", ".", "]", "section", ":", "Experiments", "subsection", ":", "Dataset", "&", "Setup", "for", "Document", "Modelling", "We", "experiment", "with", "NVDM", "on", "two", "standard", "news", "corpora", ":", "the", "20NewsGroupshttp:", "//", "qwone.com", "/", "jason", "/", "20Newsgroups", "and", "the", "Reuters", "RCV1", "-", "v2http:", "//", "trec.nist.gov", "/", "data", "/", "reuters", "/", "reuters.html", ".", "The", "former", "is", "a", "collection", "of", "newsgroup", "documents", ",", "consisting", "of", "11", ",", "314", "training", "and", "7", ",", "531", "test", "articles", ".", "The", "latter", "is", "a", "large", "collection", "from", "Reuters", "newswire", "stories", "with", "794", ",", "414", "training", "and", "10", ",", "000", "test", "cases", ".", "The", "vocabulary", "size", "of", "these", "two", "datasets", "are", "set", "as", "2", ",", "000", "and", "10", ",", "000", ".", "To", "make", "a", "direct", "comparison", "with", "the", "prior", "work", "we", "follow", "the", "same", "preprocessing", "procedure", "and", "setup", "as", "hinton2009replicated", ",", "larochelle2012neural", ",", "Srivastava2013", ",", "and", "mnih2014neural", ".", "We", "train", "NVDM", "models", "with", "50", "and", "200", "dimensional", "document", "representations", "respectively", ".", "For", "the", "inference", "network", ",", "we", "use", "an", "MLP", "(", "Eq", ".", "[", "reference", "]", ")", "with", "2", "layers", "and", "500", "dimension", "rectifier", "linear", "units", ",", "which", "converts", "document", "representations", "into", "embeddings", ".", "During", "training", "we", "carry", "out", "stochastic", "estimation", "by", "taking", "one", "sample", "for", "estimating", "the", "stochastic", "gradients", ",", "while", "in", "prediction", "we", "use", "20", "samples", "for", "predicting", "document", "perplexity", ".", "The", "model", "is", "trained", "by", "Adam", "DBLP", ":", "journals", "/", "corr", "/", "KingmaB14", "and", "tuned", "by", "hold", "-", "out", "validation", "perplexity", ".", "We", "alternately", "optimise", "the", "generative", "model", "and", "the", "inference", "network", "by", "fixing", "the", "parameters", "of", "one", "while", "updating", "the", "parameters", "of", "the", "other", ".", "subsection", ":", "Experiments", "on", "Document", "Modelling", "Table", "[", "reference", "]", "presents", "the", "test", "document", "perplexity", ".", "The", "first", "column", "lists", "the", "models", ",", "and", "the", "second", "column", "shows", "the", "dimension", "of", "latent", "variables", "used", "in", "the", "experiments", ".", "The", "final", "two", "columns", "present", "the", "perplexity", "achieved", "by", "each", "topic", "model", "on", "the", "20NewsGroups", "and", "RCV1", "-", "v2", "datasets", ".", "In", "document", "modelling", ",", "perplexity", "is", "computed", "by", ",", "where", "is", "the", "number", "of", "documents", ",", "represents", "the", "length", "of", "the", "th", "document", "and", "is", "the", "log", "probability", "of", "the", "words", "in", "the", "document", ".", "Since", "is", "intractable", "in", "the", "NVDM", ",", "we", "use", "the", "variational", "lower", "bound", "(", "which", "is", "an", "upper", "bound", "on", "perplexity", ")", "to", "compute", "the", "perplexity", "following", "mnih2014neural", ".", "While", "all", "the", "baseline", "models", "listed", "in", "Table", "[", "reference", "]", "apply", "discrete", "latent", "variables", ",", "here", "NVDM", "employs", "a", "continuous", "stochastic", "document", "representation", ".", "The", "experimental", "results", "indicate", "that", "NVDM", "achieves", "the", "best", "performance", "on", "both", "datasets", ".", "For", "the", "experiments", "on", "RCV1", "-", "v2", "dataset", ",", "the", "NVDM", "with", "latent", "variable", "of", "50", "dimension", "performs", "even", "better", "than", "the", "fDARN", "with", "200", "dimension", ".", "It", "demonstrates", "that", "our", "document", "model", "with", "continuous", "latent", "variables", "has", "higher", "expressiveness", "and", "better", "generalisation", "ability", ".", "Table", "[", "reference", "]", "compares", "the", "5", "nearest", "words", "selected", "according", "to", "the", "semantic", "vector", "learned", "from", "NVDM", "and", "docNADE", ".", "In", "addition", "to", "the", "perplexities", ",", "we", "also", "qualitatively", "evaluate", "the", "semantic", "information", "learned", "by", "NVDM", "on", "the", "20NewsGroups", "dataset", "with", "latent", "variables", "of", "50", "dimension", ".", "We", "assume", "each", "dimension", "in", "the", "latent", "space", "represents", "a", "topic", "that", "corresponds", "to", "a", "specific", "semantic", "meaning", ".", "Table", "[", "reference", "]", "presents", "5", "randomly", "selected", "topics", "with", "10", "words", "that", "have", "the", "strongest", "positive", "connection", "with", "the", "topic", ".", "Based", "on", "the", "words", "in", "each", "column", ",", "we", "can", "deduce", "their", "corresponding", "topics", "as", ":", "Space", ",", "Religion", ",", "Encryption", ",", "Sport", "and", "Policy", ".", "Although", "the", "model", "does", "not", "impose", "independent", "interpretability", "on", "the", "latent", "representation", "dimensions", ",", "we", "still", "see", "that", "the", "NVDM", "learns", "locally", "interpretable", "structure", ".", "figureThe", "standard", "deviations", "of", "MAP", "scores", "computed", "by", "running", "10", "NASM", "models", "on", "WikiQA", "with", "different", "numbers", "of", "samples", ".", "subsection", ":", "Dataset", "&", "Setup", "for", "Answer", "Sentence", "Selection", "We", "experiment", "on", "two", "answer", "selection", "datasets", ",", "the", "QASent", "and", "the", "WikiQA", "datasets", ".", "QASent", "wang2007jeopardy", "is", "created", "from", "the", "TREC", "QA", "track", ",", "and", "the", "WikiQA", "yang", "-", "yih", "-", "meek:2015:EMNLP", "is", "constructed", "from", "Wikipedia", ",", "which", "is", "less", "noisy", "and", "less", "biased", "towards", "lexical", "overlap", ".", "Table", "[", "reference", "]", "summarises", "the", "statistics", "of", "the", "two", "datasets", ".", "In", "order", "to", "investigate", "the", "effectiveness", "of", "our", "NASM", "model", "we", "also", "implemented", "two", "strong", "baseline", "models", "\u2014", "a", "vanilla", "LSTM", "model", "(", "LSTM", ")", "and", "an", "LSTM", "model", "with", "a", "deterministic", "attention", "mechanism", "(", "LSTM", "+", "Att", ")", ".", "The", "former", "directly", "applies", "the", "QA", "matching", "function", "(", "Eq", ".", "[", "reference", "]", ")", "on", "the", "independent", "question", "and", "answer", "representations", "which", "are", "the", "last", "state", "outputs", "and", "from", "the", "question", "and", "answer", "LSTM", "models", ".", "The", "latter", "adds", "an", "attention", "model", "to", "learn", "pair", "-", "specific", "representation", "for", "prediction", "on", "the", "basis", "of", "the", "vanilla", "LSTM", ".", "Moreover", ",", "LSTM", "+", "Att", "is", "the", "deterministic", "counterpart", "of", "NASM", ",", "which", "has", "the", "same", "neural", "network", "architecture", "as", "NASM", ".", "The", "only", "difference", "is", "that", "it", "replaces", "the", "stochastic", "units", "with", "deterministic", "ones", ",", "and", "no", "inference", "network", "is", "required", "to", "carry", "out", "stochastic", "estimation", ".", "Following", "previous", "work", ",", "for", "each", "of", "our", "models", "we", "also", "add", "a", "lexical", "overlap", "feature", "by", "combining", "a", "co", "-", "occurrence", "word", "count", "feature", "with", "the", "probability", "generated", "from", "the", "neural", "model", ".", "MAP", "and", "MRR", "are", "adopted", "as", "the", "evaluation", "metrics", "for", "this", "task", ".", "To", "facilitate", "direct", "comparison", "with", "previous", "work", "we", "follow", "the", "same", "experimental", "setup", "as", "Yu:2014", "and", "severyn2015disi", ".", "The", "word", "embeddings", "are", "obtained", "by", "running", "the", "word2vec", "tool", "DBLP", ":", "conf", "/", "nips", "/", "MikolovSCCD13", "on", "the", "English", "Wikipedia", "dump", "and", "the", "AQUAINThttps:", "//", "catalog.ldc.upenn.edu", "/", "LDC2002T31", "corpus", ".", "We", "use", "LSTMs", "with", "layers", "and", "hidden", "units", ",", "and", "apply", "dropout", "after", "the", "embedding", "layer", ".", "For", "the", "construction", "of", "the", "inference", "network", ",", "we", "use", "an", "MLP", "(", "Eq", ".", "[", "reference", "]", ")", "with", "2", "layers", "and", "tanh", "units", "of", "50", "dimension", ",", "and", "an", "MLP", "(", "Eq", ".", "[", "reference", "]", ")", "with", "2", "layers", "and", "tanh", "units", "of", "150", "dimension", "for", "modelling", "the", "joint", "representation", ".", "During", "training", "we", "carry", "out", "stochastic", "estimation", "by", "taking", "one", "sample", "for", "computing", "the", "gradients", ",", "while", "in", "prediction", "we", "use", "20", "samples", "to", "calculate", "the", "expectation", "of", "the", "lower", "bound", ".", "Figure", "[", "reference", "]", "presents", "the", "standard", "deviation", "of", "NASM", "\u2019s", "MAP", "scores", "while", "using", "different", "numbers", "of", "samples", ".", "Considering", "the", "trade", "-", "off", "between", "computational", "cost", "and", "variance", ",", "we", "chose", "20", "samples", "for", "prediction", "in", "all", "the", "experiments", ".", "The", "models", "are", "trained", "using", "Adam", "DBLP", ":", "journals", "/", "corr", "/", "KingmaB14", ",", "with", "hyperparameters", "selected", "by", "optimising", "the", "MAP", "score", "on", "the", "development", "set", ".", "subsection", ":", "Experiments", "on", "Answer", "Sentence", "Selection", "Table", "[", "reference", "]", "compares", "the", "results", "of", "our", "models", "with", "current", "state", "-", "of", "-", "the", "-", "art", "models", "on", "both", "answer", "selection", "datasets", ".", "On", "the", "QASent", "dataset", ",", "our", "vanilla", "LSTM", "model", "outperforms", "the", "deep", "CNN", "model", "by", "approximately", "on", "MAP", "and", "on", "MRR", ".", "The", "LSTM", "+", "Att", "performs", "slightly", "better", "than", "the", "vanilla", "LSTM", "model", ",", "and", "our", "NASM", "improves", "the", "results", "further", ".", "Since", "the", "QASent", "dataset", "is", "biased", "towards", "lexical", "overlapping", "features", ",", "after", "combining", "with", "a", "co", "-", "occurrence", "word", "count", "feature", ",", "our", "best", "model", "NASM", "outperforms", "all", "the", "previous", "models", ",", "including", "both", "neural", "network", "based", "models", "and", "classifiers", "with", "a", "set", "of", "hand", "-", "crafted", "features", "(", "e.g.", "LCLR", ")", ".", "Similarly", ",", "on", "the", "WikiQA", "dataset", ",", "all", "of", "our", "models", "outperform", "the", "previous", "distributional", "models", "by", "a", "large", "margin", ".", "By", "including", "a", "word", "count", "feature", ",", "our", "models", "improve", "further", "and", "achieve", "the", "state", "-", "of", "-", "the", "-", "art", ".", "Notably", ",", "on", "both", "datasets", ",", "our", "two", "LSTM", "-", "based", "models", "have", "set", "strong", "baselines", "and", "NASM", "works", "even", "better", ",", "which", "demonstrates", "the", "effectiveness", "of", "introducing", "stochastic", "units", "to", "model", "question", "semantics", "in", "this", "answer", "sentence", "selection", "task", ".", "In", "Figure", "[", "reference", "]", ",", "we", "compare", "the", "effectiveness", "of", "the", "latent", "attention", "mechanism", "(", "NASM", ")", "and", "its", "deterministic", "counterpart", "(", "LSTM", "+", "Att", ")", "by", "visualising", "the", "attention", "scores", "on", "the", "answer", "sentences", ".", "For", "most", "of", "the", "negative", "answer", "sentences", ",", "neither", "of", "the", "two", "attention", "models", "can", "attend", "to", "reasonable", "words", "that", "are", "beneficial", "for", "predicting", "relatedness", ".", "But", "for", "the", "correct", "answer", "sentences", ",", "such", "as", "the", "ones", "in", "Figure", "[", "reference", "]", ",", "both", "attention", "models", "are", "able", "to", "capture", "crucial", "information", "by", "attending", "to", "different", "parts", "of", "the", "sentence", "based", "on", "the", "question", "semantics", ".", "Interestingly", ",", "compared", "to", "the", "deterministic", "counterpart", "LSTM", "+", "Att", ",", "our", "NASM", "assigns", "higher", "attention", "scores", "on", "the", "prominent", "words", "that", "are", "relevant", "to", "the", "question", ",", "which", "forms", "a", "more", "peaked", "distribution", "and", "in", "turn", "helps", "the", "model", "achieve", "better", "performance", ".", "In", "order", "to", "have", "an", "intuitive", "observation", "on", "the", "latent", "distributions", ",", "we", "present", "Hinton", "diagrams", "of", "their", "log", "standard", "deviation", "parameters", "(", "Figure", "[", "reference", "]", ")", ".", "In", "a", "Hinton", "diagram", ",", "the", "size", "of", "a", "square", "is", "proportional", "to", "a", "value", "\u2019s", "magnitude", ",", "and", "the", "colour", "(", "black", "/", "white", ")", "indicates", "its", "sign", "(", "positive", "/", "negative", ")", ".", "In", "this", "case", ",", "we", "visualise", "the", "parameters", "of", "50", "conditional", "distributions", "with", "the", "questions", "selected", "from", "5", "different", "groups", ",", "which", "start", "with", "\u2018", "how", "\u2019", ",", "\u2018", "what", "\u2019", ",", "\u2018", "who", "\u2019", ",", "\u2018", "when", "\u2019", "and", "\u2018", "where", "\u2019", ".", "All", "the", "log", "standard", "deviations", "are", "initialised", "as", "zero", "before", "training", ".", "According", "to", "Figure", "[", "reference", "]", ",", "we", "can", "see", "that", "the", "questions", "starting", "with", "\u2018", "how", "\u2019", "have", "more", "white", "areas", ",", "which", "indicates", "higher", "variances", "or", "more", "uncertainties", "are", "in", "these", "dimensions", ".", "By", "contrast", ",", "the", "questions", "starting", "with", "\u2018", "what", "\u2019", "have", "black", "squares", "in", "almost", "every", "dimension", ".", "Intuitively", ",", "it", "is", "more", "difficult", "to", "understand", "and", "answer", "the", "questions", "starting", "with", "\u2018", "how", "\u2019", "than", "the", "others", ",", "while", "the", "\u2018", "what", "\u2019", "questions", "commonly", "have", "explicit", "words", "indicating", "the", "possible", "answers", ".", "To", "validate", "this", ",", "we", "compute", "the", "stratified", "MAP", "scores", "based", "on", "different", "question", "type", ".", "The", "MAP", "of", "\u2019", "how", "\u2019", "questions", "is", "0.524", "which", "is", "the", "lowest", "among", "the", "five", "groups", ".", "Hence", "empirically", ",", "\u2019", "how", "\u2019", "questions", "are", "harder", "to", "\u2019", "understand", "and", "answer", "\u2019", ".", "section", ":", "Discussion", "As", "shown", "in", "the", "experiments", ",", "neural", "variational", "inference", "brings", "consistent", "improvements", "on", "the", "performance", "of", "both", "NLP", "tasks", ".", "The", "basic", "intuition", "is", "that", "the", "latent", "distributions", "grant", "the", "ability", "to", "sum", "over", "all", "the", "possibilities", "in", "terms", "of", "semantics", ".", "From", "the", "perspective", "of", "optimisation", ",", "one", "of", "the", "most", "important", "reasons", "is", "that", "Bayesian", "learning", "guards", "against", "overfitting", ".", "According", "to", "Eq", ".", "[", "reference", "]", "in", "NVDM", ",", "since", "we", "adopt", "as", "a", "standard", "Gaussian", "prior", ",", "the", "KL", "divergence", "term", "can", "be", "analytically", "computed", "as", ".", "It", "is", "not", "difficult", "to", "find", "that", "it", "actually", "acts", "as", "L2", "regulariser", "when", "we", "update", "the", ".", "Similarly", ",", "in", "NASM", "(", "Eq", ".", "[", "reference", "]", ")", ",", "we", "also", "have", "the", "KL", "divergence", "term", ".", "Different", "from", "NVDM", ",", "it", "attempts", "to", "minimise", "the", "distance", "between", "and", "that", "are", "both", "conditional", "distributions", ".", "Because", "as", "well", "as", "are", "learned", "during", "training", ",", "the", "two", "distributions", "are", "mutually", "restrained", "while", "being", "updated", ".", "Therefore", ",", "NVDM", "simply", "penalises", "the", "large", "and", "encourages", "to", "approach", "the", "prior", "for", "every", "document", ",", "but", "in", "NASM", ",", "acts", "like", "a", "moving", "baseline", "distribution", "which", "regularises", "the", "update", "of", "for", "every", "different", "conditions", ".", "In", "practice", ",", "we", "carry", "out", "early", "stopping", "by", "observing", "the", "prediction", "performance", "on", "development", "dataset", "for", "the", "question", "answer", "selection", "task", ".", "Using", "the", "same", "learning", "rate", "and", "neural", "network", "structure", ",", "LSTM", "+", "Att", "reaches", "optimal", "performance", "and", "starts", "to", "overfit", "on", "training", "dataset", "generally", "at", "the", "th", "iteration", ",", "while", "NASM", "starts", "to", "overfit", "around", "the", "th", "iteration", ".", "More", "interestingly", ",", "in", "the", "question", "answer", "selection", "experiments", ",", "NASM", "learns", "more", "peaked", "attention", "scores", "than", "its", "deterministic", "counterpart", "LSTM", "+", "Att", ".", "For", "the", "update", "process", "of", "LSTM", "+", "Att", ",", "we", "find", "there", "exists", "a", "relatively", "big", "variance", "in", "the", "gradients", "w.r.t", ".", "question", "semantics", "(", "LSTM", "+", "Att", "applies", "deterministic", "while", "NASM", "applies", "stochastic", ")", ".", "This", "is", "because", "the", "training", "dataset", "is", "small", "and", "contains", "many", "negative", "answer", "sentences", "that", "brings", "no", "benefit", "but", "noise", "to", "the", "learning", "of", "the", "attention", "model", ".", "In", "contrast", ",", "for", "the", "update", "process", "of", "NASM", ",", "we", "observe", "more", "stable", "gradients", "w.r.t", ".", "the", "parameters", "of", "latent", "distributions", ".", "The", "optimisation", "of", "the", "lower", "bound", "on", "one", "hand", "maximises", "the", "conditional", "log", "-", "likelihood", "(", "that", "the", "deterministic", "counterpart", "cares", "about", ")", "and", "on", "the", "other", "hand", "minimises", "the", "KL", "-", "divergence", "(", "that", "regularises", "the", "gradients", ")", ".", "Hence", ",", "each", "update", "of", "the", "lower", "bound", "actually", "keeps", "the", "gradients", "w.r.t", ".", "from", "swinging", "heavily", ".", "Besides", ",", "since", "the", "values", "of", "are", "not", "very", "significant", "in", "this", "case", ",", "the", "distribution", "of", "attention", "scores", "mainly", "depends", "on", ".", "Therefore", ",", "the", "learning", "of", "the", "attention", "model", "benefits", "from", "the", "regularisation", "as", "well", ",", "and", "it", "explains", "the", "fact", "that", "NASM", "learns", "more", "peaked", "attention", "scores", "which", "in", "turn", "helps", "achieve", "a", "better", "prediction", "performance", ".", "Since", "the", "computations", "of", "NVDM", "and", "NASM", "can", "be", "parallelised", "on", "GPU", "and", "only", "one", "sample", "is", "required", "during", "training", "process", ",", "it", "is", "very", "efficient", "to", "carry", "out", "the", "neural", "variational", "inference", ".", "Moreover", ",", "for", "both", "NVDM", "and", "NASM", ",", "all", "the", "parameters", "are", "updated", "by", "back", "-", "propagation", ".", "Thus", ",", "the", "increased", "computation", "time", "for", "the", "stochastic", "units", "only", "comes", "from", "the", "added", "parameters", "of", "the", "inference", "network", ".", "section", ":", "Related", "Work", "Training", "an", "inference", "network", "to", "approximate", "the", "variational", "distribution", "was", "first", "proposed", "in", "the", "context", "of", "Helmholtz", "machines", "hinton1994autoencoders", ",", "hinton1995wake", ",", "dayan1996varieties", ",", "but", "applications", "of", "these", "directed", "generative", "models", "come", "up", "against", "the", "problem", "of", "establishing", "low", "variance", "gradient", "estimators", ".", "Recent", "advances", "in", "neural", "variational", "inference", "mitigate", "this", "problem", "by", "reparameterising", "the", "continuous", "random", "variables", "rezende2014stochastic", ",", "kingma2013auto", ",", "using", "control", "variates", "mnih2014neural", "or", "approximating", "the", "posterior", "with", "importance", "sampling", "bornschein2014reweighted", ".", "The", "instantiations", "of", "these", "ideas", "gregor2015draw", ",", "kingma2014semi", ",", "ba2015learning", "have", "demonstrated", "strong", "performance", "on", "the", "tasks", "of", "image", "processing", ".", "The", "recent", "variants", "of", "generative", "auto", "-", "encoder", "louizos2015variational", ",", "DBLP", ":", "journals", "/", "corr", "/", "MakhzaniSJG15", "are", "also", "very", "competitive", ".", "tang2013learning", "applies", "the", "similar", "idea", "of", "introducing", "stochastic", "units", "for", "expression", "classification", ",", "but", "its", "inference", "is", "carried", "out", "by", "Monte", "Carlo", "EM", "algorithm", "with", "the", "reliance", "on", "importance", "sampling", ",", "which", "is", "less", "efficient", "and", "lack", "of", "scalability", ".", "Another", "class", "of", "neural", "generative", "models", "make", "use", "of", "the", "autoregressive", "assumption", "larochelle2011neural", ",", "uria2014deep", ",", "germain2015made", ",", "gregor2013deep", ".", "Applications", "of", "these", "models", "on", "document", "modelling", "achieve", "significant", "improvements", "on", "generating", "documents", ",", "compared", "to", "conventional", "probabilistic", "topic", "models", "hofmann1999probabilistic", ",", "blei2003latent", "and", "also", "the", "RBMs", "hinton2009replicated", ",", "Srivastava2013", ".", "While", "these", "models", "that", "use", "binary", "semantic", "vectors", ",", "our", "NVDM", "employs", "dense", "continuous", "document", "representations", "which", "are", "both", "expressive", "and", "easy", "to", "train", ".", "The", "semantic", "word", "vector", "model", "maas2011learning", "also", "employs", "a", "continuous", "semantic", "vector", "to", "generate", "words", ",", "but", "the", "model", "is", "trained", "by", "MAP", "inference", "which", "does", "not", "permit", "the", "calculation", "of", "the", "posterior", "distribution", ".", "A", "very", "similar", "idea", "to", "NVDM", "is", "DBLP", ":", "journals", "/", "corr", "/", "BowmanVVDJB15", ",", "which", "employs", "VAE", "to", "generate", "sentences", "from", "a", "continuous", "space", ".", "Apart", "from", "the", "work", "mentioned", "above", ",", "there", "is", "other", "interesting", "work", "on", "question", "answering", "with", "deep", "neural", "networks", ".", "One", "of", "the", "popular", "streams", "is", "mapping", "factoid", "questions", "with", "answer", "triples", "in", "the", "knowledge", "base", "Bordes:2014:EMNLP", ",", "DBLP", ":", "journals", "/", "corr", "/", "BordesWU14", ",", "DBLP", ":", "conf", "/", "acl", "/", "YihHM14", ".", "Moreover", ",", "DBLP", ":", "journals", "/", "corr", "/", "WestonCB14", ",", "sukhbaatar2015end", ",", "DBLP", ":", "journals", "/", "corr", "/", "KumarISBEPOGS15", "further", "exploit", "memory", "networks", ",", "where", "long", "-", "term", "memories", "act", "as", "dynamic", "knowledge", "bases", ".", "Another", "attention", "-", "based", "model", "DBLP", ":", "journals", "/", "corr", "/", "HermannKGEKSB15", "applies", "the", "attentive", "network", "to", "help", "read", "and", "comprehend", "for", "long", "articles", ".", "section", ":", "Conclusion", "This", "paper", "introduced", "a", "deep", "neural", "variational", "inference", "framework", "for", "generative", "models", "of", "text", ".", "We", "experimented", "on", "two", "diverse", "tasks", ",", "document", "modelling", "and", "question", "answer", "selection", "tasks", "to", "demonstrate", "the", "effectiveness", "of", "this", "framework", ",", "where", "in", "both", "cases", "our", "models", "achieve", "state", "of", "the", "art", "performance", ".", "Apart", "from", "the", "promising", "results", ",", "our", "model", "also", "has", "the", "advantages", "of", "(", "1", ")", "simple", ",", "expressive", ",", "and", "efficient", "when", "training", "with", "the", "SGVB", "algorithm", ";", "(", "2", ")", "suitable", "for", "both", "unsupervised", "and", "supervised", "learning", "tasks", ";", "and", "(", "3", ")", "capable", "of", "generalising", "to", "incorporate", "any", "type", "of", "neural", "network", ".", "bibliography", ":", "References", "section", ":", "t", "-", "SNE", "Visualisation", "of", "Document", "Representations", "[", "Neural", "Variational", "Document", "Model", "]", "[", "Semantic", "Word", "Vector", "]", "section", ":", "Details", "of", "the", "Deep", "Neural", "Network", "Structures", "subsection", ":", "Neural", "Variational", "Document", "Model", "(", "1", ")", "Inference", "Network", ":", "(", "2", ")", "Generative", "Model", ":", "(", "3", ")", "KL", "Divergence", ":", "The", "variational", "lower", "bound", "to", "be", "optimised", ":", "subsection", ":", "Neural", "Answer", "Selection", "Model", "(", "1", ")", "Inference", "Network", ":", "(", "2", ")", "Generative", "Model", ":", ":", "(", "3", ")", "KL", "Divergence", ":", "The", "variational", "lower", "bound", "to", "be", "optimised", ":", "section", ":", "Computational", "Complexity", "The", "computational", "complexity", "of", "NVDM", "for", "a", "training", "document", "is", ".", "Here", ",", "represents", "the", "cost", "for", "the", "inference", "network", "to", "generate", "a", "sample", ",", "where", "is", "the", "number", "of", "the", "layers", "in", "the", "inference", "network", "and", "is", "the", "average", "dimension", "of", "these", "layers", ".", "Besides", ",", "is", "the", "cost", "of", "reconstructing", "the", "document", "from", "a", "sample", ",", "where", "is", "the", "average", "length", "of", "the", "documents", "and", "represents", "the", "volume", "of", "words", "applied", "in", "this", "document", "model", ",", "which", "is", "conventionally", "much", "lager", "than", ".", "The", "computational", "complexity", "of", "NASM", "for", "a", "training", "question", "-", "answer", "pair", "is", ".", "The", "inference", "network", "needs", ".", "It", "takes", "to", "produce", "the", "joint", "representation", "for", "a", "question", "-", "answer", "pair", "and", "its", "label", ",", "where", "is", "the", "total", "number", "of", "parameters", "of", "an", "LSTM", "and", "is", "the", "average", "length", "of", "the", "sentences", ".", "Based", "on", "the", "joint", "representation", ",", "an", "MLP", "spends", "to", "generate", "a", "sample", ",", "where", "is", "the", "number", "of", "layers", "and", "represents", "the", "average", "dimension", ".", "The", "generative", "model", "requires", ".", "Similarly", ",", "it", "costs", "to", "construct", "the", "generative", "latent", "distribution", ",", "where", "can", "be", "saved", "if", "the", "LSTMs", "are", "shared", "by", "the", "inference", "network", "and", "the", "generative", "model", ".", "Besides", ",", "the", "attention", "model", "takes", "and", "the", "relatedness", "prediction", "takes", "the", "last", ".", "Since", "the", "computations", "of", "NVDM", "and", "NASM", "can", "be", "parallelised", "in", "GPU", "and", "only", "one", "sample", "is", "required", "during", "training", "process", ",", "it", "is", "very", "efficient", "to", "carry", "out", "the", "neural", "variational", "inference", ".", "As", "NVDM", "is", "an", "instantiation", "of", "variational", "auto", "-", "encoder", ",", "its", "computational", "complexity", "is", "the", "same", "as", "the", "deterministic", "auto", "-", "encoder", ".", "In", "addition", ",", "the", "computational", "complexity", "of", "LSTM", "+", "Att", ",", "the", "deterministic", "counterpart", "of", "NASM", ",", "is", "also", ".", "There", "is", "only", "time", "increase", "by", "introducing", "an", "inference", "network", "for", "NASM", "when", "compared", "to", "LSTM", "+", "Att", "."]}