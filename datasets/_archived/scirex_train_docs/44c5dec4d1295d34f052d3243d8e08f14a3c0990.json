{"coref": {"12-layer_Transformer-XL": [[3597, 3602], [3667, 3677]], "18-layer_Transformer-XL": [], "24-layer_Transformer-XL": [], "Bit_per_Character__BPC_": [[179, 180], [811, 812], [820, 821]], "Hutter_Prize": [], "Language_Modelling": [[40, 42], [246, 248], [326, 328], [1025, 1027], [1103, 1105], [1248, 1250], [1279, 1281], [1394, 1396], [4760, 4762], [4845, 4848], [4849, 4854], [3399, 3402], [3447, 3452], [3716, 3719]], "Number_of_params": [], "One_Billion_Word": [[843, 846], [3430, 3433], [210, 213], [3834, 3837], [4350, 4353]], "PPL": [[181, 182], [829, 830], [841, 842], [857, 858], [4220, 4221], [4504, 4505], [5544, 5545], [3533, 3534], [5426, 5427], [5485, 5486]], "Params": [], "Penn_Treebank__Word_Level_": [[220, 222], [861, 863], [3436, 3438], [3997, 3999], [3950, 3952]], "Test_perplexity": [], "Text8": [[194, 195], [822, 823], [3427, 3428], [3752, 3753], [3801, 3802]], "Transformer-XL": [[2, 5], [54, 57], [60, 61], [115, 118], [133, 135], [157, 159], [597, 600], [767, 770], [789, 792], [851, 854], [886, 889], [984, 987], [1005, 1006], [1243, 1245], [1380, 1384], [1405, 1406], [1438, 1441], [1854, 1856], [2020, 2021], [2288, 2291], [2488, 2489], [2539, 2540], [3286, 3291], [3385, 3388], [3526, 3529], [3608, 3610], [3724, 3727], [3821, 3824], [3870, 3873], [3886, 3889], [3915, 3918], [3924, 3926], [3931, 3934], [3973, 3976], [3981, 3984], [4009, 4012], [4041, 4044], [4239, 4242], [4271, 4274], [4623, 4626], [4655, 4656], [4678, 4681], [4707, 4710], [4726, 4729], [4755, 4758], [4826, 4829], [4855, 4858], [4870, 4871], [4880, 4882], [4927, 4930], [4938, 4941], [5497, 5500], [3271, 3274], [3545, 3548], [3586, 3589], [4371, 4374], [4641, 4644]], "Transformer-XL_-_12_layers": [], "Transformer-XL_-_18_layers": [], "Transformer-XL_-_24_layers": [], "Transformer-XL_Base": [], "Transformer-XL_Large": [], "Transformer-XL_Standard": [[803, 804], [3907, 3908]], "Validation_perplexity": [], "WikiText-103": [[201, 204], [831, 834], [938, 941], [994, 997], [3419, 3422], [3440, 3443], [4061, 4064], [5574, 5577]], "enwiki8": [[187, 188], [813, 814], [942, 943], [2286, 2287], [3424, 3425], [3552, 3553], [3750, 3751], [3799, 3800]]}, "coref_non_salient": {"0": [[266, 268], [1858, 1859]], "1": [[1823, 1825], [2838, 2841], [4722, 4725]], "10": [[2501, 2503], [2552, 2554], [4664, 4666]], "100": [[4148, 4149]], "101": [[1365, 1367]], "11": [[5418, 5420], [5446, 5448]], "12": [[315, 316], [360, 361], [459, 460], [1191, 1192], [386, 387]], "13": [[1201, 1204], [1603, 1606]], "14": [[1067, 1068], [3979, 3980]], "15": [[87, 90], [2560, 2562], [2602, 2604], [2658, 2660], [4051, 4054], [4795, 4798]], "16": [[15, 17], [447, 450]], "17": [[1695, 1696], [2251, 2252], [2256, 2257], [4739, 4740], [4877, 4878]], "18": [[4022, 4024], [4029, 4031], [4917, 4919]], "19": [[4670, 4671], [5225, 5227], [5503, 5504]], "2": [[716, 719], [750, 754], [2220, 2224], [2431, 2434], [2792, 2795], [2870, 2873], [2902, 2905], [4428, 4431]], "20": [[3029, 3033], [3208, 3212]], "21": [[2548, 2550]], "22": [[3217, 3220], [3262, 3265], [4960, 4963]], "23": [[5177, 5179]], "24": [[2246, 2248], [2319, 2321]], "25": [[1813, 1815], [4692, 4694], [4699, 4701]], "26": [[5554, 5556]], "27": [[4086, 4088], [4162, 4164]], "28": [[4568, 4571]], "29": [[4375, 4377]], "3": [[1594, 1598], [4787, 4791], [4832, 4836]], "30": [[5570, 5572]], "31": [[958, 959], [4536, 4537], [4539, 4540], [4587, 4588]], "32": [[1415, 1418]], "33": [[358, 359]], "34": [[2880, 2882]], "35": [[5994, 5996]], "36": [[2133, 2136], [2188, 2191]], "37": [[1519, 1521], [1611, 1613], [1702, 1704], [2279, 2281], [2302, 2304]], "38": [[2179, 2181], [2196, 2198]], "39": [[1850, 1852], [2064, 2066], [2564, 2566], [3257, 3259], [4046, 4048], [4158, 4160], [4230, 4232], [4417, 4419], [4660, 4662]], "4": [[255, 260], [292, 296], [1169, 1173], [1926, 1930], [3862, 3866]], "40": [[953, 957], [4442, 4446], [4452, 4455], [4531, 4535]], "41": [[5307, 5311]], "42": [[4456, 4457], [4463, 4464], [4486, 4487]], "43": [[1319, 1321]], "44": [[1330, 1333]], "45": [[3616, 3620]], "46": [[3624, 3628]], "47": [[1299, 1300]], "48": [[4763, 4767]], "49": [[6106, 6110]], "5": [[285, 287], [1002, 1004], [4652, 4654]], "50": [[21, 26], [96, 101]], "51": [[423, 424], [566, 567], [1231, 1232]], "52": [[5729, 5734]], "53": [[3282, 3284]], "54": [[3960, 3963]], "55": [[363, 366]], "56": [[2542, 2546]], "57": [[864, 865]], "58": [[3967, 3969]], "59": [[1162, 1165]], "6": [[4985, 4987], [4998, 5000]], "60": [[1445, 1450]], "61": [[1224, 1226]], "62": [[1254, 1256]], "63": [[234, 235]], "64": [[451, 456], [783, 788], [1583, 1588]], "65": [[470, 472]], "66": [[1132, 1134]], "67": [[241, 242]], "68": [[51, 53]], "69": [[1211, 1213]], "7": [[577, 579], [704, 706], [1683, 1686], [4291, 4294], [4421, 4424]], "70": [[1388, 1393]], "71": [[5198, 5199]], "72": [[1291, 1295]], "73": [[4954, 4959]], "74": [[3222, 3224]], "75": [[5755, 5756]], "76": [[3634, 3638]], "77": [[6103, 6104]], "78": [[4460, 4462]], "79": [[3995, 3996]], "8": [[3192, 3193], [5122, 5124], [5182, 5186]], "80": [[3487, 3492]], "81": [[3970, 3972]], "82": [[1075, 1077]], "83": [[3230, 3232]], "84": [[4126, 4128]], "85": [[127, 128], [304, 305], [341, 342], [1227, 1228], [1608, 1609], [4842, 4843], [4868, 4869], [5506, 5507]], "86": [[300, 303]], "87": [[419, 421]], "88": [[6, 9]], "89": [[230, 232]], "9": [[79, 84], [1818, 1822], [4391, 4395]], "90": [[239, 240]], "91": [[427, 434]], "92": [[309, 314]], "93": [[2884, 2886]], "94": [[2398, 2402]], "95": [[728, 730]], "96": [[672, 678]], "97": [[1069, 1071]], "98": [[1215, 1218]], "99": [[614, 619]]}, "doc_id": "44c5dec4d1295d34f052d3243d8e08f14a3c0990", "method_subrelations": {"12-layer_Transformer-XL": [[[0, 23], "12-layer_Transformer-XL"]], "18-layer_Transformer-XL": [[[0, 23], "18-layer_Transformer-XL"]], "24-layer_Transformer-XL": [[[0, 23], "24-layer_Transformer-XL"]], "Transformer-XL": [[[0, 14], "Transformer-XL"]], "Transformer-XL_-_12_layers": [[[0, 26], "Transformer-XL_-_12_layers"]], "Transformer-XL_-_18_layers": [[[0, 26], "Transformer-XL_-_18_layers"]], "Transformer-XL_-_24_layers": [[[0, 26], "Transformer-XL_-_24_layers"]], "Transformer-XL_Base": [[[0, 19], "Transformer-XL_Base"]], "Transformer-XL_Large": [[[0, 20], "Transformer-XL_Large"]], "Transformer-XL_Standard": [[[0, 23], "Transformer-XL_Standard"]]}, "n_ary_relations": [{"Material": "Hutter_Prize", "Method": "12-layer_Transformer-XL", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.06"}, {"Material": "Hutter_Prize", "Method": "18-layer_Transformer-XL", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.03"}, {"Material": "Hutter_Prize", "Method": "24-layer_Transformer-XL", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "0.99"}, {"Material": "Hutter_Prize", "Method": "12-layer_Transformer-XL", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "41M"}, {"Material": "Hutter_Prize", "Method": "18-layer_Transformer-XL", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "88M"}, {"Material": "Hutter_Prize", "Method": "24-layer_Transformer-XL", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "277M"}, {"Material": "One_Billion_Word", "Method": "Transformer-XL_Base", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "0.46B"}, {"Material": "One_Billion_Word", "Method": "Transformer-XL_Large", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "0.8B"}, {"Material": "One_Billion_Word", "Method": "Transformer-XL_Base", "Metric": "PPL", "Task": "Language_Modelling", "score": "23.5"}, {"Material": "One_Billion_Word", "Method": "Transformer-XL_Large", "Metric": "PPL", "Task": "Language_Modelling", "score": "21.8"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "Transformer-XL", "Metric": "Params", "Task": "Language_Modelling", "score": "24M"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "Transformer-XL", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "54.55"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "Transformer-XL", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "56.72"}, {"Material": "Text8", "Method": "Transformer-XL_-_24_layers", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.08"}, {"Material": "Text8", "Method": "Transformer-XL_-_24_layers", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "277M"}, {"Material": "WikiText-103", "Method": "Transformer-XL_Large", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "257M"}, {"Material": "WikiText-103", "Method": "Transformer-XL_Standard", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "151M"}, {"Material": "WikiText-103", "Method": "Transformer-XL_Large", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "18.3"}, {"Material": "WikiText-103", "Method": "Transformer-XL_Standard", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "24.0"}, {"Material": "WikiText-103", "Method": "Transformer-XL_Large", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "18.2"}, {"Material": "WikiText-103", "Method": "Transformer-XL_Standard", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "23.1"}, {"Material": "enwiki8", "Method": "Transformer-XL_-_12_layers", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.06"}, {"Material": "enwiki8", "Method": "Transformer-XL_-_18_layers", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.03"}, {"Material": "enwiki8", "Method": "Transformer-XL_-_24_layers", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "0.99"}, {"Material": "enwiki8", "Method": "Transformer-XL_-_12_layers", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "41M"}, {"Material": "enwiki8", "Method": "Transformer-XL_-_18_layers", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "88M"}, {"Material": "enwiki8", "Method": "Transformer-XL_-_24_layers", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "277M"}], "ner": [[2, 5, "Method"], [6, 9, "Method"], [15, 17, "Method"], [21, 26, "Task"], [40, 42, "Task"], [51, 53, "Method"], [54, 57, "Method"], [60, 61, "Method"], [79, 84, "Method"], [87, 90, "Method"], [96, 101, "Task"], [115, 118, "Method"], [127, 128, "Method"], [133, 135, "Method"], [157, 159, "Method"], [179, 180, "Metric"], [181, 182, "Metric"], [187, 188, "Material"], [194, 195, "Material"], [201, 204, "Material"], [220, 222, "Material"], [230, 232, "Method"], [234, 235, "Method"], [239, 240, "Method"], [241, 242, "Method"], [246, 248, "Task"], [255, 260, "Task"], [266, 268, "Task"], [285, 287, "Method"], [292, 296, "Task"], [300, 303, "Method"], [304, 305, "Method"], [309, 314, "Method"], [315, 316, "Method"], [326, 328, "Task"], [341, 342, "Method"], [358, 359, "Method"], [360, 361, "Method"], [363, 366, "Method"], [419, 421, "Method"], [423, 424, "Task"], [427, 434, "Task"], [447, 450, "Method"], [451, 456, "Method"], [459, 460, "Method"], [470, 472, "Method"], [566, 567, "Task"], [577, 579, "Task"], [597, 600, "Method"], [614, 619, "Method"], [672, 678, "Task"], [704, 706, "Task"], [716, 719, "Method"], [728, 730, "Task"], [750, 754, "Method"], [767, 770, "Method"], [783, 788, "Method"], [789, 792, "Method"], [803, 804, "Method"], [811, 812, "Metric"], [813, 814, "Material"], [820, 821, "Metric"], [822, 823, "Material"], [829, 830, "Metric"], [831, 834, "Material"], [841, 842, "Metric"], [843, 846, "Material"], [851, 854, "Method"], [857, 858, "Metric"], [861, 863, "Material"], [864, 865, "Task"], [886, 889, "Method"], [938, 941, "Material"], [942, 943, "Material"], [953, 957, "Metric"], [958, 959, "Method"], [984, 987, "Method"], [994, 997, "Material"], [1002, 1004, "Method"], [1005, 1006, "Method"], [1025, 1027, "Task"], [1067, 1068, "Method"], [1069, 1071, "Method"], [1075, 1077, "Method"], [1103, 1105, "Task"], [1132, 1134, "Method"], [1162, 1165, "Task"], [1169, 1173, "Task"], [1191, 1192, "Method"], [1201, 1204, "Task"], [1211, 1213, "Method"], [1215, 1218, "Method"], [1224, 1226, "Method"], [1227, 1228, "Method"], [1231, 1232, "Task"], [1243, 1245, "Method"], [1248, 1250, "Task"], [1254, 1256, "Task"], [1279, 1281, "Task"], [1291, 1295, "Method"], [1299, 1300, "Method"], [1319, 1321, "Method"], [1330, 1333, "Method"], [1365, 1367, "Method"], [1380, 1384, "Method"], [1388, 1393, "Task"], [1394, 1396, "Task"], [1405, 1406, "Method"], [1415, 1418, "Method"], [1438, 1441, "Method"], [1445, 1450, "Method"], [1519, 1521, "Method"], [1583, 1588, "Method"], [1594, 1598, "Method"], [1603, 1606, "Task"], [1608, 1609, "Method"], [1611, 1613, "Method"], [1683, 1686, "Task"], [1695, 1696, "Task"], [1702, 1704, "Method"], [1813, 1815, "Metric"], [1818, 1822, "Method"], [1823, 1825, "Method"], [1850, 1852, "Method"], [1854, 1856, "Method"], [1858, 1859, "Task"], [1926, 1930, "Task"], [2020, 2021, "Method"], [2064, 2066, "Method"], [2133, 2136, "Method"], [2179, 2181, "Method"], [2188, 2191, "Method"], [2196, 2198, "Method"], [2220, 2224, "Method"], [2246, 2248, "Method"], [2251, 2252, "Task"], [2256, 2257, "Task"], [2279, 2281, "Method"], [2286, 2287, "Material"], [2288, 2291, "Method"], [2302, 2304, "Method"], [2319, 2321, "Method"], [2398, 2402, "Method"], [2431, 2434, "Method"], [2488, 2489, "Method"], [2501, 2503, "Method"], [2539, 2540, "Method"], [2542, 2546, "Method"], [2548, 2550, "Method"], [2552, 2554, "Method"], [2560, 2562, "Method"], [2564, 2566, "Method"], [2602, 2604, "Method"], [2658, 2660, "Method"], [2792, 2795, "Method"], [2838, 2841, "Method"], [2870, 2873, "Method"], [2880, 2882, "Task"], [2884, 2886, "Task"], [2902, 2905, "Method"], [3029, 3033, "Method"], [3192, 3193, "Method"], [3208, 3212, "Method"], [3217, 3220, "Method"], [3222, 3224, "Method"], [3230, 3232, "Method"], [3257, 3259, "Method"], [3262, 3265, "Method"], [3282, 3284, "Method"], [3286, 3291, "Method"], [3385, 3388, "Method"], [3419, 3422, "Material"], [3424, 3425, "Material"], [3427, 3428, "Material"], [3430, 3433, "Material"], [3436, 3438, "Material"], [3440, 3443, "Material"], [3487, 3492, "Method"], [3526, 3529, "Method"], [3552, 3553, "Material"], [3597, 3602, "Method"], [3608, 3610, "Method"], [3616, 3620, "Method"], [3624, 3628, "Method"], [3634, 3638, "Method"], [3667, 3677, "Method"], [3724, 3727, "Method"], [3750, 3751, "Material"], [3752, 3753, "Material"], [3799, 3800, "Material"], [3821, 3824, "Method"], [3862, 3866, "Task"], [3870, 3873, "Method"], [3886, 3889, "Method"], [3907, 3908, "Method"], [3915, 3918, "Method"], [3924, 3926, "Method"], [3931, 3934, "Method"], [3960, 3963, "Method"], [3967, 3969, "Method"], [3970, 3972, "Method"], [3973, 3976, "Method"], [3979, 3980, "Method"], [3981, 3984, "Method"], [3995, 3996, "Method"], [3997, 3999, "Material"], [4009, 4012, "Method"], [4022, 4024, "Task"], [4029, 4031, "Task"], [4041, 4044, "Method"], [4046, 4048, "Method"], [4051, 4054, "Method"], [4061, 4064, "Material"], [4086, 4088, "Method"], [4126, 4128, "Method"], [4148, 4149, "Task"], [4158, 4160, "Method"], [4162, 4164, "Method"], [4220, 4221, "Metric"], [4230, 4232, "Method"], [4239, 4242, "Method"], [4271, 4274, "Method"], [4291, 4294, "Task"], [4375, 4377, "Method"], [4391, 4395, "Method"], [4417, 4419, "Method"], [4421, 4424, "Task"], [4428, 4431, "Method"], [4442, 4446, "Metric"], [4452, 4455, "Metric"], [4456, 4457, "Metric"], [4460, 4462, "Method"], [4463, 4464, "Metric"], [4486, 4487, "Metric"], [4504, 4505, "Metric"], [4531, 4535, "Metric"], [4536, 4537, "Method"], [4539, 4540, "Method"], [4568, 4571, "Method"], [4587, 4588, "Method"], [4623, 4626, "Method"], [4652, 4654, "Method"], [4655, 4656, "Method"], [4660, 4662, "Method"], [4664, 4666, "Method"], [4670, 4671, "Metric"], [4678, 4681, "Method"], [4692, 4694, "Metric"], [4699, 4701, "Metric"], [4707, 4710, "Method"], [4722, 4725, "Method"], [4726, 4729, "Method"], [4739, 4740, "Task"], [4755, 4758, "Method"], [4760, 4762, "Task"], [4763, 4767, "Method"], [4787, 4791, "Method"], [4795, 4798, "Method"], [4826, 4829, "Method"], [4832, 4836, "Method"], [4842, 4843, "Method"], [4845, 4848, "Task"], [4849, 4854, "Task"], [4855, 4858, "Method"], [4868, 4869, "Method"], [4870, 4871, "Method"], [4877, 4878, "Task"], [4880, 4882, "Method"], [4917, 4919, "Task"], [4927, 4930, "Method"], [4938, 4941, "Method"], [4954, 4959, "Task"], [4960, 4963, "Method"], [4985, 4987, "Metric"], [4998, 5000, "Metric"], [5122, 5124, "Method"], [5177, 5179, "Metric"], [5182, 5186, "Method"], [5198, 5199, "Task"], [5225, 5227, "Metric"], [5307, 5311, "Metric"], [5418, 5420, "Metric"], [5446, 5448, "Metric"], [5497, 5500, "Method"], [5503, 5504, "Metric"], [5506, 5507, "Method"], [5544, 5545, "Metric"], [5554, 5556, "Method"], [5570, 5572, "Method"], [5729, 5734, "Task"], [5755, 5756, "Task"], [5994, 5996, "Metric"], [6103, 6104, "Method"], [6106, 6110, "Method"], [210, 213, "Material"], [386, 387, "Method"], [3271, 3274, "Method"], [3399, 3402, "Task"], [3447, 3452, "Task"], [3533, 3534, "Metric"], [3545, 3548, "Method"], [3586, 3589, "Method"], [3716, 3719, "Task"], [3801, 3802, "Material"], [3834, 3837, "Material"], [3950, 3952, "Material"], [4350, 4353, "Material"], [4371, 4374, "Method"], [4641, 4644, "Method"], [5426, 5427, "Metric"], [5485, 5486, "Metric"], [5574, 5577, "Material"]], "sections": [[0, 243], [243, 1012], [1012, 1267], [1267, 1378], [1378, 1816], [1816, 2429], [2429, 3376], [3376, 3379], [3379, 4020], [4020, 4440], [4440, 4690], [4690, 4746], [4746, 4883], [4883, 4912], [4912, 4915], [4915, 4952], [4952, 5194], [5194, 5552], [5552, 6224]], "sentences": [[0, 15], [15, 43], [43, 73], [73, 91], [91, 111], [111, 162], [162, 227], [227, 243], [243, 246], [246, 276], [276, 300], [300, 336], [336, 379], [379, 403], [403, 437], [437, 465], [465, 494], [494, 519], [519, 546], [546, 571], [571, 580], [580, 606], [606, 620], [620, 646], [646, 668], [668, 690], [690, 707], [707, 735], [735, 767], [767, 789], [789, 847], [847, 875], [875, 893], [893, 920], [920, 945], [945, 980], [980, 1012], [1012, 1016], [1016, 1095], [1095, 1126], [1126, 1158], [1158, 1182], [1182, 1218], [1218, 1233], [1233, 1267], [1267, 1270], [1270, 1297], [1297, 1310], [1310, 1327], [1327, 1358], [1358, 1378], [1378, 1384], [1384, 1419], [1419, 1451], [1451, 1464], [1464, 1507], [1507, 1515], [1515, 1527], [1527, 1531], [1531, 1549], [1549, 1562], [1562, 1590], [1590, 1623], [1623, 1668], [1668, 1694], [1694, 1726], [1726, 1757], [1757, 1762], [1762, 1791], [1791, 1800], [1800, 1816], [1816, 1825], [1825, 1857], [1857, 1894], [1894, 1898], [1898, 1935], [1935, 1948], [1948, 1970], [1970, 2016], [2016, 2046], [2046, 2058], [2058, 2062], [2062, 2089], [2089, 2106], [2106, 2137], [2137, 2148], [2148, 2171], [2171, 2175], [2175, 2192], [2192, 2231], [2231, 2253], [2253, 2282], [2282, 2314], [2314, 2333], [2333, 2364], [2364, 2406], [2406, 2429], [2429, 2434], [2434, 2465], [2465, 2482], [2482, 2532], [2532, 2555], [2555, 2592], [2592, 2605], [2605, 2631], [2631, 2655], [2655, 2683], [2683, 2712], [2712, 2731], [2731, 2766], [2766, 2784], [2784, 2810], [2810, 2843], [2843, 2865], [2865, 2888], [2888, 2936], [2936, 3009], [3009, 3025], [3025, 3037], [3037, 3051], [3051, 3086], [3086, 3101], [3101, 3127], [3127, 3169], [3169, 3187], [3187, 3213], [3213, 3225], [3225, 3255], [3255, 3276], [3276, 3306], [3306, 3331], [3331, 3335], [3335, 3376], [3376, 3379], [3379, 3383], [3383, 3440], [3440, 3459], [3459, 3493], [3493, 3507], [3507, 3518], [3518, 3550], [3550, 3562], [3562, 3576], [3576, 3621], [3621, 3649], [3649, 3682], [3682, 3721], [3721, 3745], [3745, 3781], [3781, 3806], [3806, 3819], [3819, 3834], [3834, 3851], [3851, 3867], [3867, 3885], [3885, 3913], [3913, 3941], [3941, 3958], [3958, 3977], [3977, 3997], [3997, 4020], [4020, 4024], [4024, 4055], [4055, 4073], [4073, 4083], [4083, 4097], [4097, 4098], [4098, 4123], [4123, 4150], [4150, 4184], [4184, 4210], [4210, 4228], [4228, 4251], [4251, 4280], [4280, 4303], [4303, 4341], [4341, 4365], [4365, 4382], [4382, 4425], [4425, 4440], [4440, 4446], [4446, 4463], [4463, 4484], [4484, 4524], [4524, 4539], [4539, 4572], [4572, 4587], [4587, 4604], [4604, 4615], [4615, 4638], [4638, 4658], [4658, 4672], [4672, 4690], [4690, 4694], [4694, 4711], [4711, 4746], [4746, 4749], [4749, 4774], [4774, 4799], [4799, 4826], [4826, 4855], [4855, 4883], [4883, 4886], [4886, 4912], [4912, 4915], [4915, 4922], [4922, 4938], [4938, 4952], [4952, 4963], [4963, 4988], [4988, 5001], [5001, 5027], [5027, 5044], [5044, 5063], [5063, 5083], [5083, 5114], [5114, 5135], [5135, 5173], [5173, 5194], [5194, 5199], [5199, 5215], [5215, 5228], [5228, 5237], [5237, 5256], [5256, 5283], [5283, 5312], [5312, 5352], [5352, 5375], [5375, 5429], [5429, 5461], [5461, 5465], [5465, 5474], [5474, 5493], [5493, 5518], [5518, 5535], [5535, 5552], [5552, 5556], [5556, 5580], [5580, 5601], [5601, 5617], [5617, 5640], [5640, 5646], [5646, 5650], [5650, 5671], [5671, 5715], [5715, 5748], [5748, 5782], [5782, 5786], [5786, 5818], [5818, 5876], [5876, 5900], [5900, 5947], [5947, 5972], [5972, 6004], [6004, 6024], [6024, 6028], [6028, 6063], [6063, 6075], [6075, 6091], [6091, 6098], [6098, 6148], [6148, 6164], [6164, 6168], [6168, 6191], [6191, 6204], [6204, 6224]], "words": ["document", ":", "Transformer", "-", "XL", ":", "Attentive", "Language", "Models", "Beyond", "a", "Fixed", "-", "Length", "Context", "Transformer", "networks", "have", "a", "potential", "of", "learning", "longer", "-", "term", "dependency", ",", "but", "are", "limited", "by", "a", "fixed", "-", "length", "context", "in", "the", "setting", "of", "language", "modeling", ".", "As", "a", "solution", ",", "we", "propose", "a", "novel", "neural", "architecture", ",", "Transformer", "-", "XL", ",", "that", "enables", "Transformer", "to", "learn", "dependency", "beyond", "a", "fixed", "length", "without", "disrupting", "temporal", "coherence", ".", "Concretely", ",", "it", "consists", "of", "a", "segment", "-", "level", "recurrence", "mechanism", "and", "a", "novel", "positional", "encoding", "scheme", ".", "Our", "method", "not", "only", "enables", "capturing", "longer", "-", "term", "dependency", ",", "but", "also", "resolves", "the", "problem", "of", "context", "fragmentation", ".", "As", "a", "result", ",", "Transformer", "-", "XL", "learns", "dependency", "that", "is", "about", "80", "%", "longer", "than", "RNNs", "and", "450", "%", "longer", "than", "vanilla", "Transformers", ",", "achieves", "better", "performance", "on", "both", "short", "and", "long", "sequences", ",", "and", "is", "up", "to", "1", ",", "800", "+", "times", "faster", "than", "vanilla", "Transformer", "during", "evaluation", ".", "Additionally", ",", "we", "improve", "the", "state", "-", "of", "-", "the", "-", "art", "(", "SoTA", ")", "results", "of", "bpc", "/", "perplexity", "from", "1.06", "to", "0.99", "on", "enwiki8", ",", "from", "1.13", "to", "1.08", "on", "text8", ",", "from", "20.5", "to", "18.3", "on", "WikiText", "-", "103", ",", "from", "23.7", "to", "21.8", "on", "One", "Billion", "Word", ",", "and", "from", "55.3", "to", "54.5", "on", "Penn", "Treebank", "(", "without", "finetuning", ")", ".", "Our", "code", ",", "pretrained", "models", ",", "and", "hyperparameters", "are", "available", "in", "both", "Tensorflow", "and", "PyTorch", ".", "section", ":", "Introduction", "Language", "modeling", "is", "among", "the", "important", "problems", "that", "require", "modeling", "long", "-", "term", "dependency", ",", "with", "successful", "applications", "such", "as", "unsupervised", "pretraining", "dai2015semi", ",", "peters2018deep", ",", "radford2018improving", ",", "devlin2018bert", ".", "However", ",", "it", "has", "been", "a", "challenge", "to", "equip", "neural", "networks", "with", "the", "capability", "to", "model", "long", "-", "term", "dependency", "in", "sequential", "data", ".", "Recurrent", "neural", "networks", "(", "RNNs", ")", ",", "in", "particular", "Long", "Short", "-", "Term", "Memory", "(", "LSTM", ")", "networks", "hochreiter1997long", ",", "have", "been", "a", "standard", "solution", "to", "language", "modeling", "and", "obtained", "strong", "results", "on", "multiple", "benchmarks", ".", "Despite", "the", "wide", "adaption", ",", "RNNs", "are", "difficult", "to", "optimize", "due", "to", "gradient", "vanishing", "and", "explosion", "hochreiter2001gradient", ",", "and", "the", "introduction", "of", "gating", "in", "LSTMs", "and", "the", "gradient", "clipping", "technique", "graves2013generating", ",", "pascanu2012understanding", "might", "not", "be", "sufficient", "to", "fully", "address", "this", "issue", ".", "Empirically", ",", "previous", "work", "has", "found", "that", "LSTM", "language", "models", "use", "200", "context", "words", "on", "average", "khandelwal2018sharp", ",", "indicating", "room", "for", "further", "improvement", ".", "On", "the", "other", "hand", ",", "the", "direct", "connections", "between", "long", "-", "distance", "word", "pairs", "baked", "in", "attention", "mechanisms", "might", "ease", "optimization", "and", "enable", "the", "learning", "of", "long", "-", "term", "dependency", "bahdanau2014neural", ",", "vaswani2017attention", ".", "Recently", ",", "designed", "a", "set", "of", "auxiliary", "losses", "to", "train", "deep", "Transformer", "networks", "for", "character", "-", "level", "language", "modeling", ",", "which", "outperform", "LSTMs", "by", "a", "large", "margin", ".", "Despite", "the", "success", ",", "the", "LM", "training", "in", "is", "performed", "on", "separated", "fixed", "-", "length", "segments", "of", "a", "few", "hundred", "characters", ",", "without", "any", "information", "flow", "across", "segments", ".", "As", "a", "consequence", "of", "the", "fixed", "context", "length", ",", "the", "model", "can", "not", "capture", "any", "longer", "-", "term", "dependency", "beyond", "the", "predefined", "context", "length", ".", "In", "addition", ",", "the", "fixed", "-", "length", "segments", "are", "created", "by", "selecting", "a", "consecutive", "chunk", "of", "symbols", "without", "respecting", "the", "sentence", "or", "any", "other", "semantic", "boundary", ".", "Hence", ",", "the", "model", "lacks", "necessary", "contextual", "information", "needed", "to", "well", "predict", "the", "first", "few", "symbols", ",", "leading", "to", "inefficient", "optimization", "and", "inferior", "performance", ".", "We", "refer", "to", "this", "problem", "as", "context", "fragmentation", ".", "To", "address", "the", "aforementioned", "limitations", "of", "fixed", "-", "length", "contexts", ",", "we", "propose", "a", "new", "architecture", "called", "Transformer", "-", "XL", "(", "meaning", "extra", "long", ")", ".", "We", "introduce", "the", "notion", "of", "recurrence", "into", "our", "deep", "self", "-", "attention", "network", ".", "In", "particular", ",", "instead", "of", "computing", "the", "hidden", "states", "from", "scratch", "for", "each", "new", "segment", ",", "we", "reuse", "the", "hidden", "states", "obtained", "in", "previous", "segments", ".", "The", "reused", "hidden", "states", "serve", "as", "memory", "for", "the", "current", "segment", ",", "which", "builds", "up", "a", "recurrent", "connection", "between", "the", "segments", ".", "As", "a", "result", ",", "modeling", "very", "long", "-", "term", "dependency", "becomes", "possible", "because", "information", "can", "be", "propagated", "through", "the", "recurrent", "connections", ".", "Meanwhile", ",", "passing", "information", "from", "the", "previous", "segment", "can", "also", "resolve", "the", "problem", "of", "context", "fragmentation", ".", "More", "importantly", ",", "we", "show", "the", "necessity", "of", "using", "relative", "positional", "encodings", "rather", "than", "absolute", "ones", ",", "in", "order", "to", "enable", "state", "reuse", "without", "causing", "temporal", "confusion", ".", "Hence", ",", "as", "an", "additional", "technical", "contribution", ",", "we", "introduce", "a", "simple", "but", "more", "effective", "relative", "positional", "encoding", "formulation", "that", "generalizes", "to", "attention", "lengths", "longer", "than", "the", "one", "observed", "during", "training", ".", "Transformer", "-", "XL", "obtained", "strong", "results", "on", "five", "datasets", ",", "varying", "from", "word", "-", "level", "to", "character", "-", "level", "language", "modeling", ".", "Transformer", "-", "XL", "improves", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "(", "SoTA", ")", "results", "from", "1.06", "to", "0.99", "in", "bpc", "on", "enwiki8", ",", "from", "1.13", "to", "1.08", "in", "bpc", "on", "text8", ",", "from", "20.5", "to", "18.3", "in", "perplexity", "on", "WikiText", "-", "103", ",", "and", "from", "23.7", "to", "21.8", "in", "perplexity", "on", "One", "Billion", "Word", ".", "On", "small", "data", ",", "Transformer", "-", "XL", "also", "achieves", "a", "perplexity", "of", "54.5", "on", "Penn", "Treebank", "without", "finetuning", ",", "which", "is", "SoTA", "when", "comparable", "settings", "are", "considered", ".", "We", "use", "two", "methods", "to", "quantitatively", "study", "the", "effective", "lengths", "of", "Transformer", "-", "XL", "and", "the", "baselines", ".", "Similar", "to", ",", "we", "gradually", "increase", "the", "attention", "length", "at", "test", "time", "until", "no", "further", "noticeable", "improvement", "(", "0.1", "%", "relative", "gains", ")", "can", "be", "observed", ".", "Our", "best", "model", "in", "this", "settings", "use", "attention", "lengths", "of", "1", ",", "600", "and", "3", ",", "800", "on", "WikiText", "-", "103", "and", "enwiki8", "respectively", ".", "In", "addition", ",", "we", "devise", "a", "metric", "called", "Relative", "Effective", "Context", "Length", "(", "RECL", ")", "that", "aims", "to", "perform", "a", "fair", "comparison", "of", "the", "gains", "brought", "by", "increasing", "the", "context", "lengths", "for", "different", "models", ".", "In", "this", "setting", ",", "Transformer", "-", "XL", "learns", "a", "RECL", "of", "900", "words", "on", "WikiText", "-", "103", ",", "while", "the", "numbers", "for", "recurrent", "networks", "and", "Transformer", "are", "only", "500", "and", "128", ".", "section", ":", "Related", "Work", "In", "the", "last", "few", "years", ",", "the", "field", "of", "language", "modeling", "has", "witnessed", "many", "significant", "advances", ",", "including", "but", "not", "limited", "to", "devising", "novel", "architectures", "to", "better", "encode", "the", "context", "bengio2003neural", ",", "mikolov2010recurrent", ",", "zilly2016recurrent", ",", "krause2016multiplicative", ",", "grave2016improving", ",", "dauphin2016language", ",", "chung2016hierarchical", ",", "merity2016pointer", ",", "kalchbrenner2016neural", ",", "al2018character", ",", "improving", "regularization", "and", "optimization", "algorithms", ",", "speeding", "up", "the", "Softmax", "computation", "morin2005hierarchical", ",", "kuchaiev2017factorization", ",", "grave2016efficient", ",", "jozefowicz2016exploring", ",", "and", "enriching", "the", "output", "distribution", "family", "yang2017breaking", ",", "kanai2018sigsoftmax", ".", "To", "capture", "the", "long", "-", "range", "context", "in", "language", "modeling", ",", "a", "line", "of", "work", "directly", "feeds", "a", "representation", "of", "the", "wider", "context", "into", "the", "network", "as", "an", "additional", "input", ".", "Existing", "works", "range", "from", "ones", "where", "context", "representations", "are", "manually", "defined", "mikolov2012context", ",", "ji2015document", ",", "wang2015larger", "to", "others", "that", "rely", "on", "document", "-", "level", "topics", "learned", "from", "data", "dieng2016topicrnn", ",", "wang2017topic", ".", "More", "broadly", ",", "in", "generic", "sequence", "modeling", ",", "how", "to", "capture", "long", "-", "term", "dependency", "has", "been", "a", "long", "-", "standing", "research", "problem", ".", "From", "this", "perspective", ",", "since", "the", "ubiquitous", "adaption", "of", "LSTM", ",", "many", "efforts", "have", "been", "spent", "on", "relieving", "the", "vanishing", "gradient", "problem", ",", "including", "better", "initialization", "le2015simple", ",", "additional", "loss", "signal", "trinh2018learning", ",", "augmented", "memory", "structure", "ke2018sparse", "and", "others", "that", "modify", "the", "internal", "architecture", "of", "RNNs", "to", "ease", "the", "optimization", ".", "Different", "from", "them", ",", "our", "work", "is", "based", "on", "the", "Transformer", "architecture", "and", "shows", "that", "language", "modeling", "as", "a", "real", "-", "world", "task", "benefits", "from", "the", "ability", "to", "learn", "longer", "-", "term", "dependency", ".", "section", ":", "Model", "Given", "a", "corpus", "of", "tokens", ",", "the", "task", "of", "language", "modeling", "is", "to", "estimate", "the", "joint", "probability", ",", "which", "is", "often", "auto", "-", "regressively", "factorized", "as", ".", "With", "the", "factorization", ",", "the", "problem", "reduces", "to", "estimating", "each", "conditional", "factor", ".", "In", "this", "work", ",", "we", "stick", "to", "the", "standard", "neural", "approach", "to", "modeling", "the", "conditional", "probability", ".", "Specifically", ",", "a", "trainable", "neural", "network", "is", "used", "to", "encode", "the", "context", "into", "a", "fixed", "size", "hidden", "state", ",", "which", "is", "multiplied", "with", "the", "word", "embeddings", "to", "obtain", "the", "logits", ".", "The", "logits", "are", "then", "fed", "into", "the", "Softmax", "function", ",", "yielding", "a", "categorical", "probability", "distribution", "over", "the", "next", "token", ".", "subsection", ":", "Vanilla", "Transformer", "Language", "Models", "In", "order", "to", "apply", "Transformer", "or", "self", "-", "attention", "to", "language", "modeling", ",", "the", "central", "problem", "is", "how", "to", "train", "a", "Transformer", "to", "effectively", "encode", "an", "arbitrarily", "long", "context", "into", "a", "fixed", "size", "representation", ".", "Given", "infinite", "memory", "and", "computation", ",", "a", "simple", "solution", "would", "be", "to", "process", "the", "entire", "context", "sequence", "using", "an", "unconditional", "Transformer", "decoder", ",", "similar", "to", "a", "feed", "-", "forward", "neural", "network", ".", "However", ",", "this", "is", "usually", "infeasible", "with", "the", "limited", "resource", "in", "practice", ".", "[", "b", "]", "0.292", "[", "b", "]", "0.69", "One", "feasible", "but", "crude", "approximation", "is", "to", "split", "the", "entire", "corpus", "into", "shorter", "segments", "of", "manageable", "sizes", ",", "and", "only", "train", "the", "model", "within", "each", "segment", ",", "ignoring", "all", "contextual", "information", "from", "previous", "segments", ".", "This", "is", "the", "idea", "adopted", "by", "al2018character", ".", "We", "call", "it", "the", "vanilla", "model", "and", "visualize", "it", "in", "Fig", ".", "[", "reference", "]", ".", "Under", "this", "training", "paradigm", ",", "information", "never", "flows", "across", "segments", "in", "either", "the", "forward", "or", "backward", "pass", ".", "There", "are", "two", "critical", "limitations", "of", "using", "a", "fixed", "-", "length", "context", ".", "First", ",", "the", "largest", "possible", "dependency", "length", "is", "upper", "bounded", "by", "the", "segment", "length", ",", "which", "is", "a", "few", "hundred", "on", "character", "-", "level", "language", "modeling", "al2018character", ".", "Therefore", ",", "although", "the", "self", "-", "attention", "mechanism", "is", "less", "affected", "by", "the", "vanishing", "gradient", "problem", "compared", "to", "RNNs", ",", "the", "vanilla", "model", "is", "not", "able", "to", "fully", "exploit", "this", "optimization", "advantage", ".", "Second", ",", "though", "it", "is", "possible", "to", "use", "padding", "to", "respect", "the", "sentence", "or", "other", "semantic", "boundaries", ",", "in", "practice", "it", "has", "been", "standard", "practice", "to", "simply", "chunk", "long", "text", "into", "fixed", "-", "length", "segments", "due", "to", "improved", "efficiency", "peters2018deep", ",", "devlin2018bert", ",", "al2018character", ".", "However", ",", "simply", "chunking", "a", "sequence", "into", "fixed", "-", "length", "segments", "will", "lead", "to", "the", "context", "fragmentation", "problem", "as", "discussed", "in", "Section", "[", "reference", "]", ".", "During", "evaluation", ",", "at", "each", "step", ",", "the", "vanilla", "model", "also", "consumes", "a", "segment", "of", "the", "same", "length", "as", "in", "training", ",", "but", "only", "makes", "one", "prediction", "at", "the", "last", "position", ".", "Then", ",", "at", "the", "next", "step", ",", "the", "segment", "is", "shifted", "to", "the", "right", "by", "only", "one", "position", ",", "and", "the", "new", "segment", "has", "to", "be", "processed", "all", "from", "scratch", ".", "As", "shown", "in", "Fig", ".", "[", "reference", "]", ",", "this", "procedure", "ensures", "that", "each", "prediction", "utilizes", "the", "longest", "possible", "context", "exposed", "during", "training", ",", "and", "also", "relieves", "context", "fragmentation", "issue", "encountered", "in", "training", ".", "However", ",", "this", "evaluation", "procedure", "is", "extremely", "expensive", ".", "We", "will", "show", "that", "our", "proposed", "architecture", "is", "able", "to", "substantially", "improve", "the", "evaluation", "speed", ".", "subsection", ":", "Segment", "-", "Level", "Recurrence", "with", "State", "Reuse", "[", "b", "]", "0.63", "[", "b", "]", "0.35", "To", "address", "the", "limitations", "of", "using", "a", "fixed", "-", "length", "context", ",", "we", "propose", "to", "introduce", "a", "recurrence", "mechanism", "to", "the", "Transformer", "architecture", ".", "During", "training", ",", "the", "hidden", "state", "sequence", "computed", "for", "the", "previous", "segment", "is", "fixed", "and", "cached", "to", "be", "reused", "as", "an", "extended", "context", "when", "the", "model", "processes", "the", "next", "new", "segment", ",", "as", "shown", "in", "Fig", ".", "[", "reference", "]", ".", "Although", "the", "gradient", "still", "remains", "within", "a", "segment", ",", "this", "additional", "input", "allows", "the", "network", "to", "exploit", "information", "in", "the", "history", ",", "leading", "to", "an", "ability", "of", "modeling", "longer", "-", "term", "dependency", "and", "avoiding", "context", "fragmentation", ".", "Formally", ",", "let", "the", "two", "consecutive", "segments", "of", "length", "be", "and", "respectively", ".", "Denoting", "the", "-", "th", "layer", "hidden", "state", "sequence", "produced", "for", "the", "-", "th", "segment", "by", ",", "where", "is", "the", "hidden", "dimension", ".", "Then", ",", "the", "-", "th", "layer", "hidden", "state", "for", "segment", "is", "produced", "(", "schematically", ")", "as", "follows", ",", "where", "the", "function", "stands", "for", "stop", "-", "gradient", ",", "the", "notation", "indicates", "the", "concatenation", "of", "two", "hidden", "sequences", "along", "the", "length", "dimension", ",", "and", "denotes", "model", "parameters", ".", "Compared", "to", "the", "standard", "Transformer", ",", "the", "critical", "difference", "lies", "in", "that", "the", "key", "and", "value", "are", "conditioned", "on", "the", "extended", "context", "and", "hence", "cached", "from", "the", "previous", "segment", ".", "We", "emphasize", "this", "particular", "design", "by", "the", "green", "paths", "in", "Fig", ".", "[", "reference", "]", ".", "With", "this", "recurrence", "mechanism", "applied", "to", "every", "two", "consecutive", "segments", "of", "a", "corpus", ",", "it", "essentially", "creates", "a", "segment", "-", "level", "recurrence", "in", "the", "hidden", "states", ".", "As", "a", "result", ",", "the", "effective", "context", "being", "utilized", "can", "go", "way", "beyond", "just", "two", "segments", ".", "However", ",", "notice", "that", "the", "recurrent", "dependency", "between", "and", "shifts", "one", "layer", "downwards", "per", "-", "segment", ",", "which", "differs", "from", "the", "same", "-", "layer", "recurrence", "in", "conventional", "RNN", "-", "LMs", ".", "Consequently", ",", "the", "largest", "possible", "dependency", "length", "grows", "linearly", "w.r.t", ".", "the", "number", "of", "layers", "as", "well", "as", "the", "segment", "length", ",", "i.e.", ",", ",", "as", "visualized", "by", "the", "shaded", "area", "in", "Fig", ".", "[", "reference", "]", ".", "This", "is", "analogous", "to", "truncated", "BPTT", "mikolov2010recurrent", ",", "a", "technique", "developed", "for", "training", "RNN", "-", "LMs", ".", "However", ",", "different", "from", "truncated", "BPTT", ",", "our", "method", "caches", "a", "sequence", "of", "hidden", "states", "instead", "of", "the", "last", "one", ",", "and", "should", "be", "applied", "together", "with", "the", "relative", "positional", "encoding", "technique", "described", "in", "Section", "[", "reference", "]", ".", "Besides", "achieving", "extra", "long", "context", "and", "resolving", "fragmentation", ",", "another", "benefit", "that", "comes", "with", "the", "recurrence", "scheme", "is", "significantly", "faster", "evaluation", ".", "Specifically", ",", "during", "evaluation", ",", "the", "representations", "from", "the", "previous", "segments", "can", "be", "reused", "instead", "of", "being", "computed", "from", "scratch", "as", "in", "the", "case", "of", "the", "vanilla", "model", ".", "In", "our", "experiments", "on", "enwiki8", ",", "Transformer", "-", "XL", "is", "up", "to", "1", ",", "800", "+", "times", "faster", "than", "the", "vanilla", "model", "during", "evaluation", "(", "see", "Section", "[", "reference", "]", ")", ".", "Finally", ",", "notice", "that", "the", "recurrence", "scheme", "does", "not", "need", "to", "be", "restricted", "to", "only", "the", "previous", "segment", ".", "In", "theory", ",", "we", "can", "cache", "as", "many", "previous", "segments", "as", "the", "GPU", "memory", "allows", ",", "and", "reuse", "all", "of", "them", "as", "the", "extra", "context", "when", "processing", "the", "current", "segment", ".", "Thus", ",", "we", "can", "cache", "a", "predefined", "length", "-", "old", "hidden", "states", "spanning", "(", "possibly", ")", "multiple", "segments", ",", "and", "refer", "to", "them", "as", "the", "memory", ",", "due", "to", "a", "clear", "connection", "to", "the", "memory", "augmented", "neural", "networks", "graves2014neural", ",", "weston2014memory", ".", "In", "our", "experiments", ",", "we", "set", "equal", "to", "the", "segment", "length", "during", "training", ",", "and", "increase", "it", "by", "multiple", "times", "during", "evaluation", ".", "subsection", ":", "Relative", "Positional", "Encodings", "While", "we", "found", "the", "idea", "presented", "in", "the", "previous", "subsection", "very", "appealing", ",", "there", "is", "a", "crucial", "technical", "challenge", "we", "have", "n\u2019t", "solved", "in", "order", "to", "reuse", "the", "hidden", "states", ".", "That", "is", ",", "how", "can", "we", "keep", "the", "positional", "information", "coherent", "when", "we", "reuse", "the", "states", "?", "Recall", "that", ",", "in", "the", "standard", "Transformer", ",", "the", "information", "of", "sequence", "order", "is", "provided", "by", "a", "set", "of", "positional", "encodings", ",", "denoted", "as", ",", "where", "the", "-", "th", "row", "corresponds", "to", "the", "-", "th", "absolute", "position", "within", "a", "segment", "and", "prescribes", "the", "maximum", "possible", "length", "to", "be", "modeled", ".", "Then", ",", "the", "actual", "input", "to", "the", "Transformer", "is", "the", "element", "-", "wise", "addition", "of", "the", "word", "embeddings", "and", "the", "positional", "encodings", ".", "If", "we", "simply", "adapt", "this", "positional", "encoding", "to", "our", "recurrence", "mechanism", "introduced", "above", ",", "the", "hidden", "state", "sequence", "would", "be", "computed", "schematically", "by", "where", "is", "the", "word", "embedding", "sequence", "of", ",", "and", "represents", "a", "transformation", "function", ".", "Notice", "that", ",", "both", "and", "are", "associated", "with", "the", "same", "positional", "encoding", ".", "As", "a", "result", ",", "the", "model", "has", "no", "information", "to", "distinguish", "the", "positional", "difference", "between", "and", "for", "any", ",", "resulting", "in", "a", "sheer", "performance", "loss", ".", "In", "order", "to", "avoid", "this", "failure", "mode", ",", "the", "fundamental", "idea", "is", "to", "only", "encode", "the", "relative", "positional", "information", "in", "the", "hidden", "states", ".", "Conceptually", ",", "the", "positional", "encoding", "gives", "the", "model", "a", "temporal", "clue", "or", "\u201c", "bias", "\u201d", "about", "how", "information", "should", "be", "gathered", ",", "i.e.", ",", "where", "to", "attend", ".", "For", "the", "same", "purpose", ",", "instead", "of", "incorporating", "bias", "statically", "into", "the", "initial", "embedding", ",", "one", "can", "inject", "the", "same", "information", "into", "the", "attention", "score", "of", "each", "layer", ".", "More", "importantly", ",", "it", "is", "more", "intuitive", "and", "generalizable", "to", "define", "the", "temporal", "bias", "in", "a", "relative", "manner", ".", "For", "instance", ",", "when", "a", "query", "vector", "attends", "on", "the", "key", "vectors", ",", "it", "does", "not", "need", "to", "know", "the", "absolute", "position", "of", "each", "key", "vector", "to", "identify", "the", "temporal", "order", "of", "the", "segment", ".", "Instead", ",", "it", "suffices", "to", "know", "the", "relative", "distance", "between", "each", "key", "vector", "and", "itself", ",", "i.e.", ".", "Practically", ",", "one", "can", "create", "a", "set", "of", "relative", "positional", "encodings", ",", "where", "the", "-", "th", "row", "indicates", "a", "relative", "distance", "of", "between", "two", "positions", ".", "By", "injecting", "the", "relative", "distance", "dynamically", "into", "the", "attention", "score", ",", "the", "query", "vector", "can", "easily", "distinguish", "the", "representations", "of", "and", "from", "their", "different", "distances", ",", "making", "the", "state", "reuse", "mechanism", "feasible", ".", "Meanwhile", ",", "we", "wo", "n\u2019t", "lose", "any", "temporal", "information", ",", "as", "the", "absolute", "position", "can", "be", "recovered", "recursively", "from", "relative", "distances", ".", "Previously", ",", "the", "idea", "of", "relative", "positional", "encodings", "has", "been", "explored", "in", "the", "context", "of", "machine", "translation", "shaw2018self", "and", "music", "generation", "huang2018improved", ".", "Here", ",", "we", "offer", "a", "different", "derivation", ",", "arriving", "at", "a", "new", "form", "of", "relative", "positional", "encodings", ",", "which", "not", "only", "has", "a", "one", "-", "to", "-", "one", "correspondence", "to", "its", "absolute", "counterpart", "but", "also", "enjoys", "much", "better", "generalization", "empirically", "(", "see", "Section", "[", "reference", "]", ")", ".", "Firstly", ",", "in", "the", "standard", "Transformer", "vaswani2017attention", ",", "the", "attention", "score", "between", "query", "and", "key", "vector", "within", "the", "same", "segment", "can", "be", "decomposed", "as", "Following", "the", "idea", "of", "only", "relying", "on", "relative", "positional", "information", ",", "we", "propose", "to", "re", "-", "parameterize", "the", "four", "terms", "as", "follows", "The", "first", "change", "we", "make", "is", "to", "replace", "all", "appearances", "of", "the", "absolute", "positional", "embedding", "for", "computing", "key", "vectors", "in", "term", "and", "with", "its", "relative", "counterpart", ".", "This", "essentially", "reflects", "the", "prior", "that", "only", "the", "relative", "distance", "matters", "for", "where", "to", "attend", ".", "Note", "that", "is", "a", "sinusoid", "encoding", "matrix", "vaswani2017attention", "without", "learnable", "parameters", ".", "Secondly", ",", "we", "introduce", "a", "trainable", "parameter", "to", "replace", "the", "query", "in", "term", ".", "In", "this", "case", ",", "since", "the", "query", "vector", "is", "the", "same", "for", "all", "query", "positions", ",", "it", "suggests", "that", "the", "attentive", "bias", "towards", "different", "words", "should", "remain", "the", "same", "regardless", "of", "the", "query", "position", ".", "With", "a", "similar", "reasoning", ",", "a", "trainable", "parameter", "is", "added", "to", "substitute", "in", "term", ".", "Finally", ",", "we", "deliberately", "separate", "the", "two", "weight", "matrices", "and", "for", "producing", "the", "content", "-", "based", "key", "vectors", "and", "location", "-", "based", "key", "vectors", "respectively", ".", "Under", "the", "new", "parameterization", ",", "each", "term", "has", "an", "intuitive", "meaning", ":", "term", "represents", "content", "-", "based", "addressing", ",", "term", "captures", "a", "content", "-", "dependent", "positional", "bias", ",", "term", "governs", "a", "global", "content", "bias", ",", "and", "encodes", "a", "global", "positional", "bias", ".", "In", "comparison", ",", "the", "formulation", "in", "only", "has", "terms", "and", ",", "dropping", "the", "two", "bias", "terms", "and", ".", "Moreover", ",", "shaw2018self", "merge", "the", "multiplication", "into", "a", "single", "trainable", "matrix", ",", "which", "abandons", "the", "inductive", "bias", "built", "into", "the", "original", "sinusoid", "positional", "encoding", "vaswani2017attention", ".", "In", "contrast", ",", "our", "relative", "positional", "embedding", "adapts", "the", "sinusoid", "formulation", ".", "As", "a", "benefit", "of", "the", "inductive", "bias", ",", "a", "model", "trained", "on", "a", "memory", "of", "some", "certain", "length", "can", "automatically", "generalize", "to", "a", "memory", "several", "times", "longer", "during", "evaluation", ".", "Equipping", "the", "recurrence", "mechanism", "with", "our", "proposed", "relative", "positional", "embedding", ",", "we", "finally", "arrive", "at", "the", "Transformer", "-", "XL", "architecture", ".", "For", "completeness", ",", "we", "summarize", "the", "computational", "procedure", "for", "a", "-", "layer", "Transformer", "-", "XL", "with", "a", "single", "attention", "head", "below", ":", "with", "defined", "as", "the", "word", "embedding", "sequence", ".", "In", "addition", ",", "it", "is", "worth", "mentioning", "that", "a", "naive", "way", "to", "compute", "requires", "computing", "for", "all", "pairs", ",", "whose", "cost", "is", "quadratic", "w.r.t", ".", "the", "sequence", "length", ".", "However", ",", "noticing", "that", "the", "value", "of", "only", "ranges", "from", "zero", "to", "the", "sequence", "length", ",", "we", "show", "a", "simple", "computation", "procedure", "in", "Appendix", "[", "reference", "]", ",", "which", "reduces", "the", "cost", "to", "be", "linear", "w.r.t", ".", "the", "sequence", "length", ".", "section", ":", "Experiments", "subsection", ":", "Main", "Results", "We", "apply", "Transformer", "-", "XL", "to", "a", "variety", "of", "datasets", "on", "both", "word", "-", "level", "and", "character", "-", "level", "language", "modeling", "to", "have", "a", "comparison", "with", "state", "-", "of", "-", "the", "-", "art", "systems", ",", "including", "WikiText", "-", "103", "merity2016pointer", ",", "enwiki8", "mahoney2011large", ",", "text8", "mahoney2011large", ",", "One", "Billion", "Word", "chelba2013one", ",", "and", "Penn", "Treebank", "mikolov2012context", ".", "WikiText", "-", "103", "is", "the", "largest", "available", "word", "-", "level", "language", "modeling", "benchmark", "with", "long", "-", "term", "dependency", ".", "It", "contains", "103", "M", "training", "tokens", "from", "28", "K", "articles", ",", "with", "an", "average", "length", "of", "3.6", "K", "tokens", "per", "article", ",", "which", "allows", "testing", "the", "ability", "of", "long", "-", "term", "dependency", "modeling", ".", "We", "set", "the", "attention", "length", "to", "384", "during", "training", "and", "1600", "during", "evaluation", ".", "We", "adopted", "adaptive", "softmax", "and", "input", "representations", "baevski2018adaptive", ",", "grave2016efficient", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "Transformer", "-", "XL", "reduces", "the", "previous", "SoTA", "perplexity", "from", "20.5", "to", "18.3", ",", "which", "demonstrates", "the", "superiority", "of", "the", "Transformer", "-", "XL", "architecture", ".", "The", "dataset", "enwiki8", "contains", "100", "M", "bytes", "of", "unprocessed", "Wikipedia", "text", ".", "We", "compare", "our", "architecture", "with", "the", "previous", "results", "in", "Table", "[", "reference", "]", ".", "Under", "the", "model", "size", "constraint", ",", "the", "12", "-", "layer", "Transformer", "-", "XL", "achieves", "a", "new", "SoTA", "result", ",", "outperforming", "the", "12", "-", "layer", "vanilla", "Transformer", "from", "by", "0.05", ",", "while", "both", "Transformer", "variants", "have", "a", "large", "margin", "over", "conventional", "RNN", "-", "based", "models", ".", "Notably", ",", "our", "12", "-", "layer", "architecture", "achieves", "the", "same", "result", "as", "the", "64", "-", "layer", "network", "from", ",", "using", "only", "17", "%", "of", "the", "parameter", "budget", ".", "In", "order", "to", "see", "whether", "better", "performances", "can", "be", "obtained", "by", "increasing", "the", "model", "size", ",", "we", "train", "18", "-", "layer", "and", "24", "-", "layer", "Transformer", "-", "XLs", "with", "increased", "model", "sizes", ".", "With", "the", "attention", "length", "784", "during", "training", "and", "3", ",", "800", "during", "evaluation", ",", "we", "obtained", "a", "new", "SoTA", "result", "and", "our", "method", "is", "the", "first", "to", "break", "through", "1.0", "on", "widely", "-", "studied", "character", "-", "level", "benchmarks", ".", "Different", "from", ",", "Transformer", "-", "XL", "does", "not", "need", "any", "auxiliary", "losses", ",", "and", "thus", "all", "benefits", "are", "credited", "to", "a", "better", "architecture", ".", "Similar", "to", "but", "different", "from", "enwiki8", ",", "text8", "contains", "100", "M", "processed", "Wikipedia", "characters", "created", "by", "lowering", "case", "the", "text", "and", "removing", "any", "character", "other", "than", "the", "26", "letters", "a", "through", "z", ",", "and", "space", ".", "Due", "to", "the", "similarity", ",", "we", "simply", "adapt", "the", "best", "model", "and", "the", "same", "hyper", "-", "parameters", "on", "enwiki8", "to", "text8", "without", "further", "tuning", ".", "The", "comparison", "with", "previous", "methods", "is", "summarized", "in", "Table", "[", "reference", "]", ".", "Again", ",", "Transformer", "-", "XL", "achieves", "the", "new", "SoTA", "result", "with", "a", "clear", "margin", ".", "One", "Billion", "Word", "does", "not", "preserve", "any", "long", "-", "term", "dependency", "because", "sentences", "have", "been", "shuffled", ".", "Consequently", ",", "this", "dataset", "mainly", "tests", "the", "ability", "of", "modeling", "only", "short", "-", "term", "dependency", ".", "The", "comparison", "between", "Transformer", "-", "XL", "and", "the", "other", "methods", "is", "shown", "in", "Table", "[", "reference", "]", ".", "Although", "Transformer", "-", "XL", "is", "mainly", "designed", "to", "better", "capture", "longer", "-", "term", "dependency", ",", "it", "dramatically", "improves", "the", "single", "-", "model", "SoTA", "from", "23.7", "to", "21.8", ".", "Specifically", ",", "Transformer", "-", "XL", "significantly", "outperforms", "a", "contemporary", "method", "using", "vanilla", "Transformers", ",", "suggesting", "the", "advantage", "of", "Transformer", "-", "XL", "is", "generalizable", "to", "modeling", "short", "sequences", ".", "We", "also", "report", "the", "results", "on", "word", "-", "level", "Penn", "Treebank", "in", "Table", "[", "reference", "]", ".", "Similar", "to", "AWD", "-", "LSTM", "merity2017regularizing", ",", "we", "apply", "variational", "dropout", "and", "weight", "average", "to", "Transformer", "-", "XL", ".", "With", "proper", "regularization", ",", "Transformer", "-", "XL", "achieves", "a", "new", "SoTA", "result", "among", "models", "without", "two", "-", "step", "finetuning", ".", "Penn", "Treebank", "has", "only", "1", "M", "training", "tokens", ",", "which", "implies", "that", "Transformer", "-", "XL", "also", "generalizes", "well", "even", "on", "small", "datasets", ".", "subsection", ":", "Ablation", "Study", "We", "conduct", "two", "sets", "of", "ablation", "studies", "to", "examine", "the", "effects", "of", "two", "proposed", "techniques", "used", "in", "Transformer", "-", "XL", ":", "the", "recurrence", "mechanism", "and", "the", "new", "positional", "encoding", "scheme", ".", "The", "first", "study", "is", "performed", "on", "WikiText", "-", "103", ",", "which", "requires", "modeling", "long", "-", "term", "dependency", ".", "The", "results", "are", "reported", "in", "Table", "[", "reference", "]", ".", "Among", "the", "compared", "encoding", "schemes", ",", "is", "relative", ",", "while", "and", "are", "absolute", ".", "\u201c", "Full", "\u201d", "and", "\u201c", "half", "\u201d", "losses", "refer", "to", "applying", "a", "cross", "entropy", "loss", "to", "all", "or", "the", "recent", "half", "positions", "in", "the", "segment", ".", "We", "found", "that", "absolute", "encodings", "only", "work", "well", "with", "half", "losses", "because", "half", "losses", "exclude", "positions", "with", "very", "short", "attention", "lengths", "during", "training", "for", "better", "generalization", ".", "Table", "[", "reference", "]", "shows", "that", "both", "the", "recurrence", "mechanism", "and", "our", "encoding", "scheme", "are", "necessary", "to", "achieve", "the", "best", "performance", ",", "as", "well", "as", "generalizing", "to", "longer", "attention", "sequences", "during", "evaluation", "time", ".", "Although", "the", "backpropagation", "length", "during", "training", "is", "only", "128", ",", "with", "the", "two", "techniques", "the", "attention", "length", "can", "be", "increased", "to", "640", "at", "test", "time", ".", "In", "the", "standard", "setting", "with", "151", "M", "parameters", ",", "the", "perplexity", "decreases", "as", "the", "attention", "length", "increases", ".", "Since", "the", "recurrence", "mechanism", "costs", "additional", "memory", ",", "we", "also", "compare", "Transformer", "-", "XL", "with", "baselines", "under", "the", "same", "GPU", "memory", "constraints", ".", "As", "shown", "in", "Table", "[", "reference", "]", "in", "Appendix", "[", "reference", "]", ",", "despite", "using", "a", "shorter", "backpropagation", "length", ",", "Transformer", "-", "XL", "remains", "superior", "to", "the", "baselines", ".", "The", "second", "study", "targets", "at", "isolating", "the", "effects", "of", "resolving", "the", "context", "fragmentation", "problem", "from", "the", "benefit", "of", "capturing", "longer", "context", "length", ".", "In", "order", "to", "achieve", "this", "goal", ",", "we", "deliberately", "choose", "a", "dataset", "that", "does", "not", "require", "long", "-", "term", "dependency", ",", "so", "that", "any", "improvement", "from", "establishing", "the", "recurrence", "can", "be", "attributed", "to", "solving", "the", "context", "fragmentation", ".", "Specifically", ",", "we", "perform", "this", "controlled", "experiment", "on", "the", "One", "Billion", "Word", "dataset", ",", "which", "can", "only", "benefit", "from", "removing", "the", "context", "fragmentation", ".", "We", "train", "a", "20", "-", "layer", "Transformer", "-", "XL", "with", "0.3B", "parameters", "for", "400", "K", "steps", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "using", "segment", "-", "level", "recurrence", "substantially", "improves", "performance", "even", "when", "long", "-", "term", "dependency", "is", "not", "needed", ",", "which", "is", "consistent", "with", "our", "previous", "discussion", "that", "the", "recurrence", "mechanism", "resolves", "the", "context", "fragmentation", "problem", ".", "Moreover", ",", "our", "relative", "positional", "encodings", "is", "also", "superior", "to", "shaw2018self", "on", "short", "sequences", ".", "subsection", ":", "Relative", "Effective", "Context", "Length", "proposed", "a", "method", "to", "evaluate", "the", "Effective", "Context", "Length", "(", "ECL", ")", "of", "a", "sequence", "model", ".", "ECL", "is", "the", "longest", "length", "to", "which", "increasing", "the", "context", "span", "would", "lead", "to", "a", "gain", "more", "than", "a", "threshold", ".", "However", ",", "ECL", "ignores", "the", "fact", "that", "it", "is", "harder", "to", "get", "improvement", "when", "a", "model", "already", "achieves", "a", "lower", "perplexity", "using", "only", "a", "shorter", "context", ",", "and", "thus", "it", "is", "not", "suitable", "for", "fair", "comparison", "among", "multiple", "models", ".", "We", "instead", "propose", "a", "new", "metric", "called", "Relative", "Effective", "Context", "Length", "(", "RECL", ")", ".", "RECL", "is", "defined", "on", "a", "model", "group", "instead", "of", "a", "single", "model", ",", "and", "the", "gain", "of", "a", "long", "context", "is", "measure", "by", "the", "relative", "improvement", "over", "the", "best", "short", "context", "model", ".", "As", "such", ",", "the", "model", "group", "shares", "the", "same", "baseline", "to", "enable", "fair", "comparison", ".", "RECL", "also", "has", "a", "parameter", ",", "which", "means", "constraining", "the", "comparison", "on", "top", "-", "hard", "examples", ".", "See", "Appedix", "[", "reference", "]", "for", "more", "details", "about", "RECL", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "Transformer", "-", "XL", "manages", "to", "model", "dependency", "of", "900", "words", "long", "on", "average", "with", ".", "The", "RECL", "of", "Transformer", "-", "XL", "is", "80", "%", "and", "450", "%", "longer", "than", "recurrent", "networks", "and", "Transformer", "respectively", ".", "Both", "the", "recurrence", "mechanism", "and", "our", "positional", "encodings", "contribute", "to", "a", "longer", "RECL", ".", "This", "further", "substantiates", "our", "argument", "that", "Transformer", "-", "XL", "is", "able", "to", "model", "longer", "-", "term", "dependency", ".", "subsection", ":", "Evaluation", "Speed", "Finally", ",", "we", "compare", "the", "evaluation", "speed", "of", "the", "proposed", "model", "with", "the", "vanilla", "Transformer", "model", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "due", "to", "the", "state", "reuse", "scheme", ",", "Transformer", "-", "XL", "achieves", "an", "up", "to", "1", ",", "874", "times", "speedup", "during", "evaluation", "compared", "to", "the", "architecture", "in", ".", "section", ":", "Conclusions", "We", "propose", "a", "novel", "architecture", ",", "Transformer", "-", "XL", ",", "for", "language", "modeling", "with", "self", "-", "attention", "architectures", "beyond", "a", "fixed", "-", "length", "context", ".", "Our", "main", "technical", "contributions", "include", "introducing", "the", "notion", "of", "recurrence", "in", "a", "purely", "self", "-", "attentive", "model", "and", "deriving", "a", "novel", "positional", "encoding", "scheme", ".", "These", "two", "techniques", "form", "a", "complete", "set", "of", "solutions", ",", "as", "any", "one", "of", "them", "alone", "does", "not", "address", "the", "issue", "of", "fixed", "-", "length", "contexts", ".", "Transformer", "-", "XL", "is", "the", "first", "self", "-", "attention", "model", "that", "achieves", "substantially", "better", "results", "than", "RNNs", "on", "both", "character", "-", "level", "and", "word", "-", "level", "language", "modeling", ".", "Transformer", "-", "XL", "is", "also", "able", "to", "model", "longer", "-", "term", "dependency", "than", "RNNs", "and", "Transformer", ",", "and", "achieves", "substantial", "speedup", "during", "evaluation", "compared", "to", "vanilla", "Transformers", ".", "subsubsection", ":", "Acknowledgments", "This", "work", "was", "supported", "in", "part", "by", "the", "Office", "of", "Naval", "Research", ",", "NSF", "grant", "IIS1763562", ",", "Google", "focused", "award", ",", "and", "the", "Nvidia", "fellowship", ".", "bibliography", ":", "References", "appendix", ":", "Ablation", "Study", "with", "Memory", "Constraints", "Table", "[", "reference", "]", "compares", "Transformer", "-", "XL", "with", "baseline", "under", "the", "same", "memory", "budget", ".", "Transformer", "-", "XL", "still", "outperforms", "the", "baseline", "even", "with", "a", "shorter", "backprop", "length", ".", "appendix", ":", "Efficient", "Computation", "of", "the", "Attention", "with", "Relative", "Positional", "Embedding", "As", "we", "discussed", "in", "section", "[", "reference", "]", ",", "the", "naive", "way", "of", "computing", "the", "for", "all", "pairs", "is", "subject", "to", "a", "quadratic", "cost", ".", "Here", ",", "we", "present", "a", "simple", "method", "with", "only", "a", "linear", "cost", ".", "Firstly", ",", "notice", "that", "the", "relative", "distance", "can", "only", "be", "integer", "from", "0", "to", ",", "where", "and", "are", "the", "memory", "length", "and", "segment", "length", "respectively", ".", "Hence", ",", "the", "rows", "of", "the", "matrix", "consist", "of", "all", "possible", "vector", "outputs", "of", "for", "any", ".", "Note", "that", "we", "have", "defined", "in", "a", "reversed", "order", ",", "i.e.", ",", ",", "to", "make", "further", "discussion", "easier", ".", "Next", ",", "we", "collect", "the", "term", "for", "all", "possible", "into", "the", "following", "matrix", ",", "Then", ",", "we", "further", "define", "Now", ",", "it", "is", "easy", "to", "see", "an", "immediate", "relationship", "between", "and", ",", "where", "the", "-", "th", "row", "of", "is", "simply", "a", "left", "-", "shifted", "version", "of", "-", "th", "row", "of", ".", "Hence", ",", "the", "computation", "of", "only", "requires", "a", "matrix", "multiplication", "to", "compute", "and", "then", "a", "set", "of", "left", "-", "shifts", ".", "Similarly", ",", "we", "can", "collect", "all", "term", "for", "all", "possible", "into", "another", "matrix", ",", "Then", ",", "we", "can", "follow", "the", "same", "procedure", "to", "define", "Again", ",", "each", "row", "of", "is", "simply", "a", "left", "-", "shift", "version", "of", ".", "Hence", ",", "the", "main", "computation", "cost", "comes", "from", "the", "matrix", "-", "vector", "multiplication", ",", "which", "is", "not", "expensive", "any", "more", ".", "appendix", ":", "Details", "About", "RECL", "[", "b", "]", "0.5", "[", "b", "]", "0.5", "[", "b", "]", "0.5", "[", "b", "]", "0.5", "In", "this", "section", ",", "we", "describe", "the", "details", "of", "the", "metric", "RECL", ".", "Let", "be", "a", "model", "group", "consisting", "of", "models", ".", "Let", "denote", "the", "loss", "of", "model", "on", "the", "-", "th", "token", "in", "the", "corpus", "with", "a", "context", "length", ".", "Concretely", ",", "the", "loss", "can", "be", "written", "as", "where", "is", "the", "probability", "distribution", "given", "by", "model", ",", "and", "is", "the", "-", "th", "token", "in", "the", "corpus", ".", "Given", "a", "short", "context", "length", "and", "a", "long", "context", "length", "such", "that", ",", "we", "can", "further", "define", "a", "baseline", "for", "each", "position", ",", "The", "relative", "loss", "of", "w.r.t", ".", "the", "model", "group", "is", "written", "as", "The", "above", "equation", "uses", "the", "minimum", "loss", "of", "all", "models", "on", "the", "short", "length", "as", "a", "baseline", ",", "and", "only", "losses", "smaller", "than", "the", "baseline", "will", "be", "effectively", "counted", "towards", "the", "relative", "loss", ".", "This", "enables", "fair", "comparison", "between", "multiple", "models", "because", "all", "models", "with", "a", "long", "context", "length", "need", "to", "improve", "over", "the", "same", "baseline", ".", "Sometimes", "we", "only", "care", "about", "those", "positions", "where", "the", "baseline", "performs", "poorly", "(", "which", "means", "short", "-", "term", "dependency", "with", "context", "length", "is", "not", "sufficient", ")", ",", "so", "given", "a", "ratio", "parameter", ",", "we", "define", "the", "set", "is", "the", "above", "equation", "as", "The", "relative", "gain", "is", "subsequently", "defined", "as", "the", "relative", "perplexity", "reduction", ":", "Given", "a", "step", "size", ",", "we", "then", "use", "an", "algorithm", "to", "find", "the", "RECL", "by", "thresholding", "the", "relative", "gain", ":", "Set", "initial", "short", "context", "length", ",", "and", "long", "context", "length", "Compute", ".", "If", ",", "return", ".", "If", ",", "set", "and", "go", "to", "step", "1", ".", "In", "Figure", "[", "reference", "]", ",", "we", "visualize", "the", "unnormalized", "relative", "perplexity", "gains", "with", "various", "pairs", "of", "when", ".", "It", "is", "clear", "that", "Transformer", "-", "XL", "has", "a", "longer", "RECL", "compared", "to", "RNNs", "and", "other", "baselines", "because", "the", "relative", "gains", "are", "substantially", "larger", ".", "For", "reference", ",", "we", "plot", "the", "perplexities", "with", "varying", "context", "lengths", "in", "Figure", "[", "reference", "]", ".", "The", "y", "-", "axis", "denotes", "the", "\u201c", "normal", "\u201d", "perplexity", "(", "not", "calibrated", "by", "baselines", ")", ".", "appendix", ":", "Attention", "Visualization", "In", "this", "section", ",", "we", "provide", "some", "visualization", "of", "the", "attention", "learned", "by", "the", "SoTA", "model", "on", "the", "WikiText", "-", "103", "validation", "set", ".", "Recall", "that", ",", "this", "model", "has", "16", "10", "-", "head", "transformer", "layers", "and", "relies", "on", "a", "memory", "of", "length", "640", ".", "The", "first", "visualization", "aims", "at", "revealing", "the", "overall", "trend", "of", "where", "the", "model", "is", "attending", ".", "Specifically", ",", "for", "each", "attention", "head", "of", "each", "layer", ",", "we", "average", "the", "attention", "distributions", "of", "all", "tokens", "in", "the", "validation", "set", ".", "This", "is", "shown", "in", "Fig", ".", "[", "reference", "]", ".", "As", "we", "can", "see", ",", "the", "overall", "trend", "is", "to", "focus", "more", "on", "the", "nearby", "tokens", "than", "the", "faraway", "ones", ".", "However", ",", "it", "is", "also", "very", "clear", "that", "some", "attention", "heads", "have", "a", "wider", "attention", "distribution", "over", "the", "entire", "memory", "span", ",", "notably", "the", "head", "8", "from", "layer", "1", ",", "head", "78", "from", "layer", "8", ",", "and", "the", "head", "158", "from", "layer", "16", ".", "[", "b", "]", "[", "b", "]", "[", "b", "]", "Since", "we", "are", "focused", "on", "learning", "long", "-", "range", "dependency", ",", "we", "are", "especially", "interested", "in", "these", "heads", "with", "a", "wider", "attention", "span", ".", "Thus", ",", "in", "the", "second", "set", "of", "visualization", ",", "we", "pick", "the", "three", "notable", "heads", "mentioned", "above", ",", "and", "visualize", "their", "attention", "behavior", "for", "a", "randomly", "chosen", "position", ",", "as", "shown", "in", "Fig", ".", "[", "reference", "]", ".", "Here", ",", "we", "see", "three", "different", "patterns", "of", "wider", "attention", ":", "For", "the", "head", "8", "in", "the", "1st", "layer", ",", "we", "see", "an", "almost", "uniform", "attention", "over", "the", "entire", "memory", "span", ".", "This", "is", "quite", "intuitive", ",", "as", "lower", "-", "level", "layers", "needs", "to", "screen", "the", "entire", "memory", "span", "to", "decide", "where", "to", "focus", "for", "higher", "-", "level", "layers", "For", "the", "head", "78", "in", "the", "8th", "layer", "(", "a", "middle", "-", "level", "layer", ")", ",", "we", "see", "a", "very", "sparse", "attention", "pattern", "scattered", "in", "all", "ranges", "of", "the", "memory", ".", "Again", ",", "this", "well", "fits", "our", "intuition", "that", "as", "information", "accumulates", ",", "the", "network", "may", "focus", "on", "some", "particular", "position", "with", "special", "interests", ".", "For", "the", "head", "158", "in", "the", "16th", "layer", "(", "i.e.", "the", "last", "layer", ")", ",", "each", "target", "location", "(", "corresponding", "to", "each", "row", ")", "has", "its", "own", "distinct", "sparse", "focus", ",", "differing", "from", "head", "78", "where", "target", "locations", "largely", "share", "the", "same", "attentive", "location", "in", "memory", ".", "Meanwhile", ",", "the", "pattern", "is", "also", "different", "from", "the", "case", "of", "head", "8", ",", "where", "a", "few", "locations", "are", "clearly", "attended", "more", "than", "others", ".", "[", "b", "]", "[", "b", "]", "[", "b", "]", "Finally", ",", "as", "we", "have", "discussed", "in", "section", "[", "reference", "]", ",", "the", "attention", "score", "can", "be", "decomposed", "into", "four", "intuitive", "terms", ".", "Here", ",", "we", "want", "to", "further", "investigate", "how", "these", "four", "terms", "contribute", "to", "the", "overall", "attention", "trend", "in", "Fig", ".", "[", "reference", "]", ".", "Since", "the", "term", "represents", "the", "global", "content", "bias", ",", "i.e.", ",", "the", "prior", "importance", "of", "each", "word", "regardless", "of", "the", "context", ",", "we", "will", "leave", "it", "out", "and", "focus", "on", "the", "terms", ",", "and", ".", "So", ",", "for", "each", "term", ",", "we", "take", "the", "Softmax", "w.r.t", ".", "the", "memory", "span", "and", "average", "the", "resulted", "distribution", "of", "all", "tokens", "in", "the", "validation", "set", ".", "The", "results", "are", "visualized", "in", "Fig", ".", "[", "reference", "]", ":", "Since", "term", "is", "fully", "content", "-", "based", "addressing", ",", "when", "averaging", "over", "all", "target", "words", ",", "the", "result", "is", "essentially", "uniform", "over", "the", "entire", "context", ",", "except", "for", "a", "few", "very", "close", "words", ",", "which", "are", "likely", "to", "be", "semantically", "similar", "to", "the", "target", "word", ".", "The", "overall", "trend", "of", "term", "highly", "resembles", "that", "of", "the", "entire", "attention", "distribution", "in", "Fig", ".", "[", "reference", "]", ".", "It", "suggests", "that", "the", "global", "trend", "of", "focusing", "on", "the", "nearby", "context", "is", "largely", "contributed", "by", "this", "content", "-", "dependent", "positional", "bias", ".", "The", "overall", "trend", "of", "term", "is", "also", "focusing", "more", "on", "nearby", "words", ".", "However", ",", "compared", "to", "the", "trend", "of", "term", ",", "it", "is", "clearly", "flatter", "and", "biases", "towards", "a", "longer", "context", "."]}