{"coref": {"Andor_et_al_": [], "Dependency_Parsing": [[36, 38], [449, 451], [854, 856], [2429, 2430], [3132, 3134], [3135, 3137], [3437, 3438], [4417, 4419], [4574, 4575], [4584, 4585], [4943, 4945], [408, 410], [2514, 2516]], "LAS": [[2658, 2661], [3083, 3085], [3316, 3319], [3322, 3323], [3532, 3533], [3562, 3563]], "POS": [[2551, 2552]], "Penn_Treebank": [[2764, 2766]], "UAS": [[463, 466], [3314, 3315], [3320, 3321], [3515, 3516], [3528, 3529], [3566, 3567], [4631, 4632]]}, "coref_non_salient": {"0": [[235, 237], [3475, 3477]], "1": [[1122, 1126], [4805, 4808]], "10": [[96, 99], [105, 108], [279, 282], [350, 353], [358, 361], [599, 602], [1287, 1290], [1300, 1303], [1311, 1315], [1366, 1369], [1550, 1553], [1596, 1599], [1626, 1629], [1634, 1637], [1695, 1698], [1740, 1743], [1898, 1901], [1906, 1909], [1988, 1991], [2058, 2061], [2066, 2069], [2146, 2149], [2166, 2169], [2187, 2190], [2223, 2226], [2240, 2243], [2316, 2319], [3056, 3059], [3854, 3857], [3868, 3871], [4052, 4055], [4260, 4263], [4540, 4543], [4635, 4638], [4648, 4651], [4657, 4660], [4675, 4678], [4695, 4698], [4772, 4775], [4867, 4870], [4902, 4905], [5015, 5018]], "11": [[29, 35], [156, 162], [400, 406], [1675, 1677], [2697, 2701], [2701, 2704]], "12": [[2403, 2405], [2415, 2417], [4785, 4786]], "13": [[2501, 2504], [3635, 3638], [4072, 4075]], "14": [[485, 488], [3488, 3491], [3550, 3553], [3573, 3576]], "15": [[514, 517], [3595, 3597]], "16": [[1269, 1272], [2644, 2648]], "17": [[269, 270], [1207, 1208], [2399, 2400], [2424, 2425], [2980, 2981], [4092, 4093], [4116, 4117], [332, 333], [2962, 2963], [4038, 4039], [4090, 4091], [4122, 4123], [4301, 4302], [4347, 4348], [4362, 4363]], "18": [[39, 41], [411, 413], [2518, 2520], [3631, 3633], [3639, 3642], [3665, 3667], [3771, 3773], [3849, 3851], [4473, 4476], [4946, 4948]], "19": [[580, 584], [4600, 4603]], "2": [[570, 572], [1663, 1665], [1671, 1673], [1773, 1775], [1794, 1796], [1914, 1916], [2335, 2337], [2525, 2527], [3040, 3042], [3535, 3537], [3861, 3863], [3919, 3921], [3985, 3987], [4203, 4205], [4483, 4485], [4495, 4497], [4710, 4712], [4721, 4723], [4731, 4733]], "20": [[200, 201], [249, 250], [3128, 3129], [4452, 4456]], "21": [[2, 9], [12, 20], [696, 701], [4814, 4816]], "22": [[865, 868], [869, 874], [3360, 3363]], "23": [[253, 255], [1154, 1156], [1182, 1184], [1372, 1374], [1557, 1559], [3052, 3054], [3864, 3866], [4479, 4481], [4491, 4493], [4639, 4641], [4713, 4715], [4846, 4848]], "24": [[2288, 2289], [4718, 4719]], "25": [[4691, 4693]], "26": [[163, 165], [4163, 4167]], "27": [[91, 94], [275, 278], [343, 346], [556, 559], [1505, 1508], [1571, 1574], [1618, 1621], [1657, 1660], [1875, 1878], [3978, 3981], [4028, 4031], [4568, 4571], [4753, 4756], [4838, 4841], [5010, 5013]], "28": [[307, 309], [1375, 1377]], "29": [[662, 665], [4439, 4442]], "3": [[120, 123], [124, 125], [152, 153], [2711, 2712]], "30": [[70, 72], [4426, 4428], [4979, 4981]], "31": [[2689, 2690], [4298, 4299]], "32": [[2931, 2933], [3413, 3415], [3833, 3835]], "33": [[3010, 3016]], "34": [[4623, 4625], [4767, 4769], [4829, 4831]], "35": [[4432, 4436]], "36": [[134, 139], [4229, 4233], [4912, 4916]], "37": [[288, 290], [2989, 2991], [4486, 4488], [4532, 4534]], "38": [[3112, 3116], [4927, 4929]], "39": [[2510, 2512], [2837, 2839], [3168, 3170], [3776, 3778], [4940, 4942]], "4": [[265, 268], [1203, 1206], [4048, 4051], [4076, 4077]], "40": [[2431, 2434], [2705, 2706], [2768, 2769], [2865, 2866], [3235, 3236], [3240, 3241], [3287, 3288], [3803, 3804]], "41": [[490, 496], [4150, 4154]], "42": [[166, 169], [4127, 4130]], "43": [[1413, 1416], [1488, 1490]], "44": [[321, 324], [4272, 4275], [4359, 4361], [4380, 4383]], "45": [[680, 681]], "46": [[140, 141], [208, 209], [440, 441], [3916, 3917], [4118, 4119], [4983, 4984], [2972, 2973], [3618, 3619], [3908, 3909]], "47": [[4238, 4239]], "48": [[2649, 2650]], "49": [[3927, 3929]], "5": [[3720, 3722], [3895, 3898]], "50": [[1242, 1244]], "51": [[636, 637]], "52": [[2665, 2668]], "53": [[4961, 4966]], "54": [[1485, 1486], [1500, 1501]], "55": [[2377, 2379]], "56": [[645, 647]], "57": [[2692, 2694]], "58": [[3698, 3703]], "59": [[4787, 4789]], "6": [[82, 84], [261, 263], [316, 319], [392, 394], [1113, 1115], [2454, 2456], [3065, 3067], [4197, 4199], [4989, 4991], [5003, 5005]], "60": [[667, 669]], "61": [[1236, 1238], [4131, 4133], [4142, 4144]], "62": [[232, 234], [714, 716], [723, 725], [830, 832], [2840, 2843], [2854, 2856], [3768, 3770]], "63": [[1829, 1830]], "64": [[3745, 3746]], "65": [[1066, 1067]], "66": [[4522, 4524]], "67": [[2983, 2988]], "68": [[1003, 1005]], "69": [[3750, 3751]], "7": [[1560, 1562], [4100, 4103]], "70": [[3964, 3966]], "71": [[2459, 2461]], "72": [[57, 62]], "73": [[1362, 1363]], "74": [[5033, 5040]], "75": [[68, 69], [206, 207], [430, 431], [609, 610], [683, 684], [3097, 3099], [3704, 3708], [4338, 4339], [4701, 4702], [4707, 4708], [4742, 4743]], "76": [[4764, 4766]], "77": [[1095, 1099]], "78": [[4104, 4105]], "79": [[678, 679]], "8": [[48, 53], [194, 198], [1037, 1042], [3123, 3127], [4971, 4977]], "80": [[3743, 3744]], "81": [[2655, 2657]], "82": [[4079, 4085]], "83": [[4955, 4959]], "84": [[4065, 4071]], "85": [[326, 329], [526, 529], [1049, 1051]], "86": [[3075, 3077]], "87": [[4778, 4779]], "88": [[3182, 3183], [3524, 3525]], "9": [[112, 115], [1076, 1078], [1470, 1472], [4040, 4043], [4159, 4162]]}, "doc_id": "0ff9ea8409c932baf3c0302c89ede79add1431aa", "method_subrelations": {"Andor_et_al_": [[[0, 12], "Andor_et_al_"]]}, "n_ary_relations": [{"Material": "Penn_Treebank", "Method": "Andor_et_al_", "Metric": "LAS", "Task": "Dependency_Parsing", "score": "92.79"}, {"Material": "Penn_Treebank", "Method": "Andor_et_al_", "Metric": "POS", "Task": "Dependency_Parsing", "score": "97.44"}, {"Material": "Penn_Treebank", "Method": "Andor_et_al_", "Metric": "UAS", "Task": "Dependency_Parsing", "score": "94.61"}], "ner": [[2, 9, "Method"], [12, 20, "Method"], [29, 35, "Task"], [36, 38, "Task"], [39, 41, "Task"], [48, 53, "Method"], [57, 62, "Method"], [68, 69, "Metric"], [70, 72, "Method"], [82, 84, "Method"], [91, 94, "Task"], [96, 99, "Method"], [105, 108, "Method"], [112, 115, "Method"], [120, 123, "Task"], [124, 125, "Task"], [134, 139, "Method"], [140, 141, "Method"], [156, 162, "Task"], [163, 165, "Task"], [166, 169, "Task"], [194, 198, "Method"], [200, 201, "Method"], [206, 207, "Metric"], [208, 209, "Method"], [232, 234, "Method"], [235, 237, "Method"], [249, 250, "Method"], [253, 255, "Method"], [261, 263, "Method"], [265, 268, "Method"], [269, 270, "Method"], [275, 278, "Task"], [279, 282, "Method"], [288, 290, "Method"], [307, 309, "Method"], [316, 319, "Method"], [321, 324, "Method"], [326, 329, "Method"], [343, 346, "Task"], [350, 353, "Method"], [358, 361, "Method"], [392, 394, "Method"], [400, 406, "Task"], [411, 413, "Task"], [430, 431, "Metric"], [440, 441, "Method"], [449, 451, "Task"], [463, 466, "Metric"], [485, 488, "Method"], [490, 496, "Method"], [514, 517, "Method"], [526, 529, "Method"], [556, 559, "Task"], [570, 572, "Method"], [580, 584, "Method"], [599, 602, "Method"], [609, 610, "Metric"], [636, 637, "Method"], [645, 647, "Method"], [662, 665, "Method"], [667, 669, "Method"], [678, 679, "Metric"], [680, 681, "Metric"], [683, 684, "Metric"], [696, 701, "Method"], [714, 716, "Method"], [723, 725, "Method"], [830, 832, "Method"], [854, 856, "Task"], [865, 868, "Method"], [869, 874, "Method"], [1003, 1005, "Metric"], [1037, 1042, "Method"], [1049, 1051, "Method"], [1066, 1067, "Task"], [1076, 1078, "Method"], [1095, 1099, "Method"], [1113, 1115, "Method"], [1122, 1126, "Method"], [1154, 1156, "Method"], [1182, 1184, "Method"], [1203, 1206, "Method"], [1207, 1208, "Method"], [1236, 1238, "Task"], [1242, 1244, "Task"], [1269, 1272, "Method"], [1287, 1290, "Method"], [1300, 1303, "Method"], [1311, 1315, "Method"], [1362, 1363, "Task"], [1366, 1369, "Method"], [1372, 1374, "Method"], [1375, 1377, "Method"], [1413, 1416, "Method"], [1470, 1472, "Method"], [1485, 1486, "Task"], [1488, 1490, "Method"], [1500, 1501, "Task"], [1505, 1508, "Task"], [1550, 1553, "Method"], [1557, 1559, "Method"], [1560, 1562, "Method"], [1571, 1574, "Task"], [1596, 1599, "Method"], [1618, 1621, "Task"], [1626, 1629, "Method"], [1634, 1637, "Method"], [1657, 1660, "Task"], [1663, 1665, "Method"], [1671, 1673, "Method"], [1675, 1677, "Task"], [1695, 1698, "Method"], [1740, 1743, "Method"], [1773, 1775, "Method"], [1794, 1796, "Method"], [1829, 1830, "Method"], [1875, 1878, "Task"], [1898, 1901, "Method"], [1906, 1909, "Method"], [1914, 1916, "Method"], [1988, 1991, "Method"], [2058, 2061, "Method"], [2066, 2069, "Method"], [2146, 2149, "Method"], [2166, 2169, "Method"], [2187, 2190, "Method"], [2223, 2226, "Method"], [2240, 2243, "Method"], [2288, 2289, "Method"], [2316, 2319, "Method"], [2335, 2337, "Method"], [2377, 2379, "Method"], [2399, 2400, "Method"], [2403, 2405, "Method"], [2415, 2417, "Method"], [2424, 2425, "Method"], [2429, 2430, "Task"], [2431, 2434, "Task"], [2454, 2456, "Method"], [2459, 2461, "Method"], [2501, 2504, "Task"], [2510, 2512, "Method"], [2518, 2520, "Task"], [2525, 2527, "Method"], [2551, 2552, "Metric"], [2644, 2648, "Method"], [2649, 2650, "Method"], [2655, 2657, "Metric"], [2658, 2661, "Metric"], [2665, 2668, "Metric"], [2689, 2690, "Task"], [2692, 2694, "Metric"], [2697, 2701, "Task"], [2701, 2704, "Task"], [2705, 2706, "Task"], [2764, 2766, "Material"], [2837, 2839, "Method"], [2840, 2843, "Method"], [2854, 2856, "Method"], [2931, 2933, "Method"], [2980, 2981, "Method"], [2983, 2988, "Method"], [2989, 2991, "Method"], [3010, 3016, "Method"], [3040, 3042, "Method"], [3052, 3054, "Method"], [3056, 3059, "Method"], [3065, 3067, "Method"], [3075, 3077, "Metric"], [3083, 3085, "Metric"], [3097, 3099, "Metric"], [3112, 3116, "Method"], [3123, 3127, "Method"], [3128, 3129, "Method"], [3132, 3134, "Task"], [3135, 3137, "Task"], [3168, 3170, "Method"], [3182, 3183, "Method"], [3314, 3315, "Metric"], [3316, 3319, "Metric"], [3320, 3321, "Metric"], [3322, 3323, "Metric"], [3413, 3415, "Method"], [3437, 3438, "Task"], [3475, 3477, "Method"], [3488, 3491, "Method"], [3515, 3516, "Metric"], [3524, 3525, "Method"], [3528, 3529, "Metric"], [3532, 3533, "Metric"], [3535, 3537, "Method"], [3550, 3553, "Method"], [3562, 3563, "Metric"], [3566, 3567, "Metric"], [3573, 3576, "Method"], [3595, 3597, "Method"], [3631, 3633, "Task"], [3635, 3638, "Task"], [3639, 3642, "Task"], [3665, 3667, "Task"], [3698, 3703, "Metric"], [3704, 3708, "Metric"], [3720, 3722, "Method"], [3743, 3744, "Metric"], [3745, 3746, "Metric"], [3750, 3751, "Metric"], [3768, 3770, "Method"], [3771, 3773, "Task"], [3776, 3778, "Method"], [3833, 3835, "Method"], [3849, 3851, "Task"], [3854, 3857, "Method"], [3861, 3863, "Method"], [3864, 3866, "Method"], [3868, 3871, "Method"], [3895, 3898, "Method"], [3916, 3917, "Method"], [3919, 3921, "Method"], [3927, 3929, "Metric"], [3964, 3966, "Method"], [3978, 3981, "Task"], [3985, 3987, "Method"], [4028, 4031, "Task"], [4040, 4043, "Method"], [4048, 4051, "Method"], [4052, 4055, "Method"], [4065, 4071, "Method"], [4072, 4075, "Task"], [4076, 4077, "Method"], [4079, 4085, "Method"], [4092, 4093, "Method"], [4100, 4103, "Method"], [4104, 4105, "Method"], [4116, 4117, "Method"], [4118, 4119, "Method"], [4127, 4130, "Task"], [4131, 4133, "Task"], [4142, 4144, "Task"], [4150, 4154, "Method"], [4159, 4162, "Method"], [4163, 4167, "Task"], [4197, 4199, "Method"], [4203, 4205, "Method"], [4229, 4233, "Method"], [4238, 4239, "Method"], [4260, 4263, "Method"], [4272, 4275, "Method"], [4298, 4299, "Task"], [4338, 4339, "Metric"], [4359, 4361, "Method"], [4380, 4383, "Method"], [4417, 4419, "Task"], [4426, 4428, "Method"], [4432, 4436, "Method"], [4439, 4442, "Method"], [4452, 4456, "Method"], [4473, 4476, "Task"], [4479, 4481, "Method"], [4483, 4485, "Method"], [4486, 4488, "Method"], [4491, 4493, "Method"], [4495, 4497, "Method"], [4522, 4524, "Method"], [4532, 4534, "Method"], [4540, 4543, "Method"], [4568, 4571, "Task"], [4574, 4575, "Task"], [4584, 4585, "Task"], [4600, 4603, "Method"], [4623, 4625, "Method"], [4631, 4632, "Metric"], [4635, 4638, "Method"], [4639, 4641, "Method"], [4648, 4651, "Method"], [4657, 4660, "Method"], [4675, 4678, "Method"], [4691, 4693, "Task"], [4695, 4698, "Method"], [4701, 4702, "Metric"], [4707, 4708, "Metric"], [4710, 4712, "Method"], [4713, 4715, "Method"], [4718, 4719, "Method"], [4721, 4723, "Method"], [4731, 4733, "Method"], [4742, 4743, "Metric"], [4753, 4756, "Task"], [4764, 4766, "Method"], [4767, 4769, "Method"], [4772, 4775, "Method"], [4778, 4779, "Method"], [4785, 4786, "Method"], [4787, 4789, "Task"], [4805, 4808, "Method"], [4814, 4816, "Method"], [4829, 4831, "Method"], [4838, 4841, "Task"], [4846, 4848, "Method"], [4867, 4870, "Method"], [4902, 4905, "Method"], [4912, 4916, "Method"], [4927, 4929, "Method"], [4940, 4942, "Method"], [4943, 4945, "Task"], [4946, 4948, "Task"], [4955, 4959, "Method"], [4961, 4966, "Method"], [4971, 4977, "Method"], [4979, 4981, "Method"], [4983, 4984, "Method"], [4989, 4991, "Method"], [5003, 5005, "Method"], [5010, 5013, "Task"], [5015, 5018, "Method"], [5033, 5040, "Method"], [152, 153, "Task"], [332, 333, "Method"], [408, 410, "Task"], [2514, 2516, "Task"], [2711, 2712, "Task"], [2768, 2769, "Task"], [2865, 2866, "Task"], [2962, 2963, "Method"], [2972, 2973, "Method"], [3235, 3236, "Task"], [3240, 3241, "Task"], [3287, 3288, "Task"], [3360, 3363, "Method"], [3618, 3619, "Method"], [3803, 3804, "Task"], [3908, 3909, "Method"], [4038, 4039, "Method"], [4090, 4091, "Method"], [4122, 4123, "Method"], [4301, 4302, "Method"], [4347, 4348, "Method"], [4362, 4363, "Method"]], "sections": [[0, 109], [109, 685], [685, 721], [721, 1109], [1109, 1253], [1253, 1502], [1502, 1661], [1661, 1819], [1819, 2478], [2478, 2695], [2695, 2734], [2734, 2828], [2828, 2946], [2946, 3130], [3130, 3154], [3154, 3325], [3325, 3422], [3422, 3629], [3629, 3643], [3643, 3762], [3762, 3839], [3839, 3969], [3969, 4034], [4034, 4147], [4147, 4457], [4457, 4917], [4917, 5020], [5020, 5075], [5075, 5078]], "sentences": [[0, 9], [9, 43], [43, 73], [73, 109], [109, 112], [112, 129], [129, 170], [170, 187], [187, 218], [218, 244], [244, 285], [285, 310], [310, 335], [335, 340], [340, 362], [362, 386], [386, 420], [420, 446], [446, 456], [456, 470], [470, 497], [497, 505], [505, 538], [538, 550], [550, 575], [575, 622], [622, 648], [648, 685], [685, 688], [688, 702], [702, 721], [721, 725], [725, 742], [742, 747], [747, 755], [755, 766], [766, 782], [782, 798], [798, 824], [824, 853], [853, 892], [892, 911], [911, 922], [922, 956], [956, 970], [970, 1002], [1002, 1014], [1014, 1060], [1060, 1068], [1068, 1081], [1081, 1091], [1091, 1109], [1109, 1115], [1115, 1146], [1146, 1168], [1168, 1177], [1177, 1199], [1199, 1229], [1229, 1235], [1235, 1253], [1253, 1256], [1256, 1267], [1267, 1285], [1285, 1316], [1316, 1336], [1336, 1344], [1344, 1360], [1360, 1378], [1378, 1400], [1400, 1442], [1442, 1454], [1454, 1476], [1476, 1502], [1502, 1508], [1508, 1541], [1541, 1569], [1569, 1610], [1610, 1638], [1638, 1646], [1646, 1661], [1661, 1673], [1673, 1691], [1691, 1715], [1715, 1723], [1723, 1762], [1762, 1781], [1781, 1797], [1797, 1812], [1812, 1819], [1819, 1821], [1821, 1835], [1835, 1844], [1844, 1849], [1849, 1866], [1866, 1879], [1879, 1882], [1882, 1904], [1904, 1912], [1912, 1918], [1918, 1927], [1927, 1953], [1953, 1968], [1968, 1985], [1985, 2000], [2000, 2013], [2013, 2031], [2031, 2050], [2050, 2051], [2051, 2054], [2054, 2072], [2072, 2078], [2078, 2083], [2083, 2106], [2106, 2113], [2113, 2119], [2119, 2135], [2135, 2140], [2140, 2152], [2152, 2179], [2179, 2195], [2195, 2200], [2200, 2234], [2234, 2249], [2249, 2265], [2265, 2287], [2287, 2299], [2299, 2333], [2333, 2359], [2359, 2386], [2386, 2440], [2440, 2469], [2469, 2478], [2478, 2481], [2481, 2505], [2505, 2521], [2521, 2531], [2531, 2568], [2568, 2588], [2588, 2594], [2594, 2606], [2606, 2623], [2623, 2640], [2640, 2679], [2679, 2695], [2695, 2701], [2701, 2734], [2734, 2740], [2740, 2828], [2828, 2833], [2833, 2882], [2882, 2911], [2911, 2923], [2923, 2946], [2946, 2950], [2950, 2979], [2979, 3000], [3000, 3027], [3027, 3039], [3039, 3051], [3051, 3086], [3086, 3109], [3109, 3130], [3130, 3134], [3134, 3154], [3154, 3160], [3160, 3184], [3184, 3203], [3203, 3212], [3212, 3229], [3229, 3259], [3259, 3277], [3277, 3312], [3312, 3325], [3325, 3330], [3330, 3354], [3354, 3410], [3410, 3422], [3422, 3426], [3426, 3450], [3450, 3466], [3466, 3492], [3492, 3532], [3532, 3544], [3544, 3570], [3570, 3577], [3577, 3583], [3583, 3605], [3605, 3612], [3612, 3629], [3629, 3633], [3633, 3643], [3643, 3649], [3649, 3668], [3668, 3696], [3696, 3723], [3723, 3762], [3762, 3767], [3767, 3797], [3797, 3829], [3829, 3839], [3839, 3843], [3843, 3853], [3853, 3864], [3864, 3890], [3890, 3915], [3915, 3944], [3944, 3969], [3969, 3972], [3972, 3988], [3988, 4010], [4010, 4034], [4034, 4040], [4040, 4057], [4057, 4076], [4076, 4104], [4104, 4120], [4120, 4147], [4147, 4155], [4155, 4174], [4174, 4183], [4183, 4186], [4186, 4210], [4210, 4246], [4246, 4264], [4264, 4279], [4279, 4295], [4295, 4320], [4320, 4354], [4354, 4376], [4376, 4402], [4402, 4420], [4420, 4443], [4443, 4457], [4457, 4463], [4463, 4477], [4477, 4509], [4509, 4535], [4535, 4560], [4560, 4576], [4576, 4619], [4619, 4655], [4655, 4686], [4686, 4716], [4716, 4757], [4757, 4776], [4776, 4777], [4777, 4800], [4800, 4822], [4822, 4862], [4862, 4896], [4896, 4917], [4917, 4920], [4920, 4949], [4949, 4967], [4967, 4992], [4992, 5020], [5020, 5023], [5023, 5061], [5061, 5075], [5075, 5078]], "words": ["document", ":", "Globally", "Normalized", "Transition", "-", "Based", "Neural", "Networks", "We", "introduce", "a", "globally", "normalized", "transition", "-", "based", "neural", "network", "model", "that", "achieves", "state", "-", "of", "-", "the", "-", "art", "part", "-", "of", "-", "speech", "tagging", ",", "dependency", "parsing", "and", "sentence", "compression", "results", ".", "Our", "model", "is", "a", "simple", "feed", "-", "forward", "neural", "network", "that", "operates", "on", "a", "task", "-", "specific", "transition", "system", ",", "yet", "achieves", "comparable", "or", "better", "accuracies", "than", "recurrent", "models", ".", "We", "discuss", "the", "importance", "of", "global", "as", "opposed", "to", "local", "normalization", ":", "a", "key", "insight", "is", "that", "the", "label", "bias", "problem", "implies", "that", "globally", "normalized", "models", "can", "be", "strictly", "more", "expressive", "than", "locally", "normalized", "models", ".", "section", ":", "Introduction", "Neural", "network", "approaches", "have", "taken", "the", "field", "of", "natural", "language", "processing", "(", "NLP", ")", "by", "storm", ".", "In", "particular", ",", "variants", "of", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "networks", "have", "produced", "impressive", "results", "on", "some", "of", "the", "classic", "NLP", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", ",", "syntactic", "parsing", "and", "semantic", "role", "labeling", ".", "One", "might", "speculate", "that", "it", "is", "the", "recurrent", "nature", "of", "these", "models", "that", "enables", "these", "results", ".", "In", "this", "work", "we", "demonstrate", "that", "simple", "feed", "-", "forward", "networks", "without", "any", "recurrence", "can", "achieve", "comparable", "or", "better", "accuracies", "than", "LSTMs", ",", "as", "long", "as", "they", "are", "globally", "normalized", ".", "Our", "model", ",", "described", "in", "detail", "in", "Section", "[", "reference", "]", ",", "uses", "a", "transition", "system", "and", "feature", "embeddings", "as", "introduced", "by", "chen", "-", "manning:2014:EMNLP", ".", "We", "do", "not", "use", "any", "recurrence", ",", "but", "perform", "beam", "search", "for", "maintaining", "multiple", "hypotheses", "and", "introduce", "global", "normalization", "with", "a", "conditional", "random", "field", "(", "CRF", ")", "objective", "to", "overcome", "the", "label", "bias", "problem", "that", "locally", "normalized", "models", "suffer", "from", ".", "Since", "we", "use", "beam", "inference", ",", "we", "approximate", "the", "partition", "function", "by", "summing", "over", "the", "elements", "in", "the", "beam", ",", "and", "use", "early", "updates", ".", "We", "compute", "gradients", "based", "on", "this", "approximate", "global", "normalization", "and", "perform", "full", "backpropagation", "training", "of", "all", "neural", "network", "parameters", "based", "on", "the", "CRF", "loss", ".", "In", "Section", "[", "reference", "]", "we", "revisit", "the", "label", "bias", "problem", "and", "the", "implication", "that", "globally", "normalized", "models", "are", "strictly", "more", "expressive", "than", "locally", "normalized", "models", ".", "Lookahead", "features", "can", "partially", "mitigate", "this", "discrepancy", ",", "but", "can", "not", "fully", "compensate", "for", "it", "\u2014", "a", "point", "to", "which", "we", "return", "later", ".", "To", "empirically", "demonstrate", "the", "effectiveness", "of", "global", "normalization", ",", "we", "evaluate", "our", "model", "on", "part", "-", "of", "-", "speech", "tagging", ",", "syntactic", "dependency", "parsing", "and", "sentence", "compression", "(", "Section", "[", "reference", "]", ")", ".", "Our", "model", "achieves", "state", "-", "of", "-", "the", "-", "art", "accuracy", "on", "all", "of", "these", "tasks", ",", "matching", "or", "outperforming", "LSTMs", "while", "being", "significantly", "faster", ".", "In", "particular", "for", "dependency", "parsing", "on", "the", "Wall", "Street", "Journal", "we", "achieve", "the", "best", "-", "ever", "published", "unlabeled", "attachment", "score", "of", "94.61", "%", ".", "As", "discussed", "in", "more", "detail", "in", "Section", "[", "reference", "]", ",", "we", "also", "outperform", "previous", "structured", "training", "approaches", "used", "for", "neural", "network", "transition", "-", "based", "parsing", ".", "Our", "ablation", "experiments", "show", "that", "we", "outperform", "weiss", "-", "etAl:2015:ACL", "and", "alberti", "-", "EtAl:2015:EMNLP", "because", "we", "do", "global", "backpropagation", "training", "of", "all", "model", "parameters", ",", "while", "they", "fix", "the", "neural", "network", "parameters", "when", "training", "the", "global", "part", "of", "their", "model", ".", "We", "also", "outperform", "zhou", "-", "etAl:2015:ACL", "despite", "using", "a", "smaller", "beam", ".", "To", "shed", "additional", "light", "on", "the", "label", "bias", "problem", "in", "practice", ",", "we", "provide", "a", "sentence", "compression", "example", "where", "the", "local", "model", "completely", "fails", ".", "We", "then", "demonstrate", "that", "a", "globally", "normalized", "parsing", "model", "without", "any", "lookahead", "features", "is", "almost", "as", "accurate", "as", "our", "best", "model", ",", "while", "a", "locally", "normalized", "model", "loses", "more", "than", "10", "%", "absolute", "in", "accuracy", "because", "it", "can", "not", "effectively", "incorporate", "evidence", "as", "it", "becomes", "available", ".", "Finally", ",", "we", "provide", "an", "open", "-", "source", "implementation", "of", "our", "method", ",", "called", "SyntaxNet", ",", "which", "we", "have", "integrated", "into", "the", "popular", "TensorFlow", "framework", ".", "We", "also", "provide", "a", "pre", "-", "trained", ",", "state", "-", "of", "-", "the", "art", "English", "dependency", "parser", "called", "\u201c", "Parsey", "McParseface", ",", "\u201d", "which", "we", "tuned", "for", "a", "balance", "of", "speed", ",", "simplicity", ",", "and", "accuracy", ".", "section", ":", "Model", "At", "its", "core", ",", "our", "model", "is", "an", "incremental", "transition", "-", "based", "parser", ".", "To", "apply", "it", "to", "different", "tasks", "we", "only", "need", "to", "adjust", "the", "transition", "system", "and", "the", "input", "features", ".", "subsection", ":", "Transition", "System", "Given", "an", "input", ",", "most", "often", "a", "sentence", ",", "we", "define", ":", "A", "set", "of", "states", ".", "A", "special", "start", "state", ".", "A", "set", "of", "allowed", "decisions", "for", "all", ".", "A", "transition", "function", "returning", "a", "new", "state", "for", "any", "decision", ".", "We", "will", "use", "a", "function", "to", "compute", "the", "score", "of", "decision", "in", "state", "for", "input", ".", "The", "vector", "contains", "the", "model", "parameters", "and", "we", "assume", "that", "is", "differentiable", "with", "respect", "to", ".", "In", "this", "section", ",", "for", "brevity", ",", "we", "will", "drop", "the", "dependence", "of", "in", "the", "functions", "given", "above", ",", "simply", "writing", ",", ",", ",", "and", ".", "Throughout", "this", "work", "we", "will", "use", "transition", "systems", "in", "which", "all", "complete", "structures", "for", "the", "same", "input", "have", "the", "same", "number", "of", "decisions", "(", "or", "for", "brevity", ")", ".", "In", "dependency", "parsing", "for", "example", ",", "this", "is", "true", "for", "both", "the", "arc", "-", "standard", "and", "arc", "-", "eager", "transition", "systems", ",", "where", "for", "a", "sentence", "of", "length", ",", "the", "number", "of", "decisions", "for", "any", "complete", "parse", "is", ".", "A", "complete", "structure", "is", "then", "a", "sequence", "of", "decision", "/", "state", "pairs", "such", "that", ",", "for", ",", "and", ".", "We", "use", "the", "notation", "to", "refer", "to", "a", "decision", "sequence", ".", "We", "assume", "that", "there", "is", "a", "one", "-", "to", "-", "one", "mapping", "between", "decision", "sequences", "and", "states", ":", "that", "is", ",", "we", "essentially", "assume", "that", "a", "state", "encodes", "the", "entire", "history", "of", "decisions", ".", "Thus", ",", "each", "state", "can", "be", "reached", "by", "a", "unique", "decision", "sequence", "from", ".", "We", "will", "use", "decision", "sequences", "and", "states", "interchangeably", ":", "in", "a", "slight", "abuse", "of", "notation", ",", "we", "define", "to", "be", "equal", "to", "where", "is", "the", "state", "reached", "by", "the", "decision", "sequence", ".", "The", "scoring", "function", "can", "be", "defined", "in", "a", "number", "of", "ways", ".", "In", "this", "work", ",", "following", "chen", "-", "manning:2014:EMNLP", ",", "weiss", "-", "etAl:2015:ACL", ",", "and", "zhou", "-", "etAl:2015:ACL", ",", "we", "define", "it", "via", "a", "feed", "-", "forward", "neural", "network", "as", "Here", "are", "the", "parameters", "of", "the", "neural", "network", ",", "excluding", "the", "parameters", "at", "the", "final", "layer", ".", "are", "the", "final", "layer", "parameters", "for", "decision", ".", "is", "the", "representation", "for", "state", "computed", "by", "the", "neural", "network", "under", "parameters", ".", "Note", "that", "the", "score", "is", "linear", "in", "the", "parameters", ".", "We", "next", "describe", "how", "softmax", "-", "style", "normalization", "can", "be", "performed", "at", "the", "local", "or", "global", "level", ".", "subsection", ":", "Global", "vs.", "Local", "Normalization", "In", "the", "chen", "-", "manning:2014:EMNLP", "style", "of", "greedy", "neural", "network", "parsing", ",", "the", "conditional", "probability", "distribution", "over", "decisions", "given", "context", "is", "defined", "as", "where", "Each", "is", "a", "local", "normalization", "term", ".", "The", "probability", "of", "a", "sequence", "of", "decisions", "is", "Beam", "search", "can", "be", "used", "to", "attempt", "to", "find", "the", "maximum", "of", "Eq", ".", "(", "[", "reference", "]", ")", "with", "respect", "to", ".", "The", "additive", "scores", "used", "in", "beam", "search", "are", "the", "log", "-", "softmax", "of", "each", "decision", ",", ",", "not", "the", "raw", "scores", ".", "In", "contrast", ",", "a", "Conditional", "Random", "Field", "(", "CRF", ")", "defines", "a", "distribution", "as", "follows", ":", "where", "and", "is", "the", "set", "of", "all", "valid", "sequences", "of", "decisions", "of", "length", ".", "is", "a", "global", "normalization", "term", ".", "The", "inference", "problem", "is", "now", "to", "find", "Beam", "search", "can", "again", "be", "used", "to", "approximately", "find", "the", ".", "subsection", ":", "Training", "Training", "data", "consists", "of", "inputs", "paired", "with", "gold", "decision", "sequences", ".", "We", "use", "stochastic", "gradient", "descent", "on", "the", "negative", "log", "-", "likelihood", "of", "the", "data", "under", "the", "model", ".", "Under", "a", "locally", "normalized", "model", ",", "the", "negative", "log", "-", "likelihood", "is", "whereas", "under", "a", "globally", "normalized", "model", "it", "is", "A", "significant", "practical", "advantange", "of", "the", "locally", "normalized", "cost", "Eq", ".", "(", "[", "reference", "]", ")", "is", "that", "the", "local", "partition", "function", "and", "its", "derivative", "can", "usually", "be", "computed", "efficiently", ".", "In", "contrast", ",", "the", "term", "in", "Eq", ".", "(", "[", "reference", "]", ")", "contains", "a", "sum", "over", "that", "is", "in", "many", "cases", "intractable", ".", "To", "make", "learning", "tractable", "with", "the", "globally", "normalized", "model", ",", "we", "use", "beam", "search", "and", "early", "updates", ".", "As", "the", "training", "sequence", "is", "being", "decoded", ",", "we", "keep", "track", "of", "the", "location", "of", "the", "gold", "path", "in", "the", "beam", ".", "If", "the", "gold", "path", "falls", "out", "of", "the", "beam", "at", "step", ",", "a", "stochastic", "gradient", "step", "is", "taken", "on", "the", "following", "objective", ":", "Here", "the", "set", "contains", "all", "paths", "in", "the", "beam", "at", "step", ",", "together", "with", "the", "gold", "path", "prefix", ".", "It", "is", "straightforward", "to", "derive", "gradients", "of", "the", "loss", "in", "Eq", ".", "(", "[", "reference", "]", ")", "and", "to", "back", "-", "propagate", "gradients", "to", "all", "levels", "of", "a", "neural", "network", "defining", "the", "score", ".", "If", "the", "gold", "path", "remains", "in", "the", "beam", "throughout", "decoding", ",", "a", "gradient", "step", "is", "performed", "using", ",", "the", "beam", "at", "the", "end", "of", "decoding", ".", "section", ":", "The", "Label", "Bias", "Problem", "Intuitively", ",", "we", "would", "like", "the", "model", "to", "be", "able", "to", "revise", "an", "earlier", "decision", "made", "during", "search", ",", "when", "later", "evidence", "becomes", "available", "that", "rules", "out", "the", "earlier", "decision", "as", "incorrect", ".", "At", "first", "glance", ",", "it", "might", "appear", "that", "a", "locally", "normalized", "model", "used", "in", "conjunction", "with", "beam", "search", "or", "exact", "search", "is", "able", "to", "revise", "earlier", "decisions", ".", "However", "the", "label", "bias", "problem", "(", "see", "bottou", ",", "collins99", "pages", "222", "-", "226", ",", "crf", ",", "bottou", "-", "lecun", "-", "2005", ",", "smithJohnson07", ")", "means", "that", "locally", "normalized", "models", "often", "have", "a", "very", "weak", "ability", "to", "revise", "earlier", "decisions", ".", "This", "section", "gives", "a", "formal", "perspective", "on", "the", "label", "bias", "problem", ",", "through", "a", "proof", "that", "globally", "normalized", "models", "are", "strictly", "more", "expressive", "than", "locally", "normalized", "models", ".", "The", "theorem", "was", "originally", "proved", "by", "smithJohnson07", ".", "The", "example", "underlying", "the", "proof", "gives", "a", "clear", "illustration", "of", "the", "label", "bias", "problem", ".", "paragraph", ":", "Global", "Models", "can", "be", "Strictly", "More", "Expressive", "than", "Local", "Models", "Consider", "a", "tagging", "problem", "where", "the", "task", "is", "to", "map", "an", "input", "sequence", "to", "a", "decision", "sequence", ".", "First", ",", "consider", "a", "locally", "normalized", "model", "where", "we", "restrict", "the", "scoring", "function", "to", "access", "only", "the", "first", "input", "symbols", "when", "scoring", "decision", ".", "We", "will", "return", "to", "this", "restriction", "soon", ".", "The", "scoring", "function", "can", "be", "an", "otherwise", "arbitrary", "function", "of", "the", "tuple", ":", "Second", ",", "consider", "a", "globally", "normalized", "model", "This", "model", "again", "makes", "use", "of", "a", "scoring", "function", "restricted", "to", "the", "first", "input", "symbols", "when", "scoring", "decision", ".", "Define", "to", "be", "the", "set", "of", "all", "possible", "distributions", "under", "the", "local", "model", "obtained", "as", "the", "scores", "vary", ".", "Similarly", ",", "define", "to", "be", "the", "set", "of", "all", "possible", "distributions", "under", "the", "global", "model", ".", "Here", "a", "\u201c", "distribution", "\u201d", "is", "a", "function", "from", "a", "pair", "to", "a", "probability", ".", "Our", "main", "result", "is", "the", "following", ":", "theorem", ":", "See", "also", "PL", "is", "a", "strict", "subset", "of", "PG", ",", "that", "is", "\u228aPLPG", ".", "To", "prove", "this", "we", "will", "first", "prove", "that", ".", "This", "step", "is", "straightforward", ".", "We", "then", "show", "that", ";", "that", "is", ",", "there", "are", "distributions", "in", "that", "are", "not", "in", ".", "The", "proof", "that", "gives", "a", "clear", "illustration", "of", "the", "label", "bias", "problem", ".", "Proof", "that", "\u2286PLPG", ":", "We", "need", "to", "show", "that", "for", "any", "locally", "normalized", "distribution", ",", "we", "can", "construct", "a", "globally", "normalized", "model", "such", "that", ".", "Consider", "a", "locally", "normalized", "model", "with", "scores", ".", "Define", "a", "global", "model", "with", "scores", "Then", "it", "is", "easily", "verified", "that", "for", "all", ".", "In", "proving", "we", "will", "use", "a", "simple", "problem", "where", "every", "example", "seen", "in", "training", "or", "test", "data", "is", "one", "of", "the", "following", "two", "tagged", "sentences", ":", "Note", "that", "the", "input", "is", "ambiguous", ":", "it", "can", "take", "tags", "B", "or", "D", ".", "This", "ambiguity", "is", "resolved", "when", "the", "next", "input", "symbol", ",", "c", "or", "e", ",", "is", "observed", ".", "Now", "consider", "a", "globally", "normalized", "model", ",", "where", "the", "scores", "are", "defined", "as", "follows", ".", "Define", "as", "the", "set", "of", "bigram", "tag", "transitions", "seen", "in", "the", "data", ".", "Similarly", ",", "define", "as", "the", "set", "of", "(", "word", ",", "tag", ")", "pairs", "seen", "in", "the", "data", ".", "We", "define", "where", "is", "the", "single", "scalar", "parameter", "of", "the", "model", ",", "and", "if", "is", "true", ",", "otherwise", ".", "Proof", "that", "\u2288PGPL", ":", "We", "will", "construct", "a", "globally", "normalized", "model", "such", "that", "there", "is", "no", "locally", "normalized", "model", "such", "that", ".", "Under", "the", "definition", "in", "Eq", ".", "(", "[", "reference", "]", ")", ",", "it", "is", "straightforward", "to", "show", "that", "In", "contrast", ",", "under", "any", "definition", "for", ",", "we", "must", "have", "This", "follows", "because", "and", ".", "The", "inequality", "then", "immediately", "implies", "Eq", ".", "(", "[", "reference", "]", ")", ".", "It", "follows", "that", "for", "sufficiently", "large", "values", "of", ",", "we", "have", ",", "and", "given", "Eq", ".", "(", "[", "reference", "]", ")", "it", "is", "impossible", "to", "define", "a", "locally", "normalized", "model", "with", "and", ".", "Under", "the", "restriction", "that", "scores", "depend", "only", "on", "the", "first", "input", "symbols", ",", "the", "globally", "normalized", "model", "is", "still", "able", "to", "model", "the", "data", "in", "Eq", ".", "(", "[", "reference", "]", ")", ",", "while", "the", "locally", "normalized", "model", "fails", "(", "see", "Eq", ".", "[", "reference", "]", ")", ".", "The", "ambiguity", "at", "input", "symbol", "b", "is", "naturally", "resolved", "when", "the", "next", "symbol", "(", "c", "or", "e", ")", "is", "observed", ",", "but", "the", "locally", "normalized", "model", "is", "not", "able", "to", "revise", "its", "prediction", ".", "It", "is", "easy", "to", "fix", "the", "locally", "normalized", "model", "for", "the", "example", "in", "Eq", ".", "(", "[", "reference", "]", ")", "by", "allowing", "scores", "that", "take", "into", "account", "the", "input", "symbol", ".", "More", "generally", "we", "can", "have", "a", "model", "of", "the", "form", "where", "the", "integer", "specifies", "the", "amount", "of", "lookahead", "in", "the", "model", ".", "Such", "lookahead", "is", "common", "in", "practice", ",", "but", "insufficient", "in", "general", ".", "For", "every", "amount", "of", "lookahead", ",", "we", "can", "construct", "examples", "that", "can", "not", "be", "modeled", "with", "a", "locally", "normalized", "model", "by", "duplicating", "the", "middle", "input", "b", "in", "(", "[", "reference", "]", ")", "times", ".", "Only", "a", "local", "model", "with", "scores", "that", "considers", "the", "entire", "input", "can", "capture", "any", "distribution", ":", "in", "this", "case", "the", "decomposition", "makes", "no", "independence", "assumptions", ".", "However", ",", "increasing", "the", "amount", "of", "context", "used", "as", "input", "comes", "at", "a", "cost", ",", "requiring", "more", "powerful", "learning", "algorithms", ",", "and", "potentially", "more", "training", "data", ".", "For", "a", "detailed", "analysis", "of", "the", "trade", "-", "offs", "between", "structural", "features", "in", "CRFs", "and", "more", "powerful", "local", "classifiers", "without", "structural", "constraints", ",", "see", "liang08structure", ";", "in", "these", "experiments", "local", "classifiers", "are", "unable", "to", "reach", "the", "performance", "of", "CRFs", "on", "problems", "such", "as", "parsing", "and", "named", "entity", "recognition", "where", "structural", "constraints", "are", "important", ".", "Note", "that", "there", "is", "nothing", "to", "preclude", "an", "approach", "that", "makes", "use", "of", "both", "global", "normalization", "and", "more", "powerful", "scoring", "functions", ",", "obtaining", "the", "best", "of", "both", "worlds", ".", "The", "experiments", "that", "follow", "make", "use", "of", "both", ".", "section", ":", "Experiments", "To", "demonstrate", "the", "flexibility", "and", "modeling", "power", "of", "our", "approach", ",", "we", "provide", "experimental", "results", "on", "a", "diverse", "set", "of", "structured", "prediction", "tasks", ".", "We", "apply", "our", "approach", "to", "POS", "tagging", ",", "syntactic", "dependency", "parsing", ",", "and", "sentence", "compression", ".", "While", "directly", "optimizing", "the", "global", "model", "defined", "by", "Eq", ".", "(", "[", "reference", "]", ")", "works", "well", ",", "we", "found", "that", "training", "the", "model", "in", "two", "steps", "achieves", "the", "same", "precision", "much", "faster", ":", "we", "first", "pretrain", "the", "network", "using", "the", "local", "objective", "given", "in", "Eq", ".", "(", "[", "reference", "]", ")", ",", "and", "then", "perform", "additional", "training", "steps", "using", "the", "global", "objective", "given", "in", "Eq", ".", "(", "[", "reference", "]", ")", ".", "We", "pretrain", "all", "layers", "except", "the", "softmax", "layer", "in", "this", "way", ".", "We", "purposefully", "abstain", "from", "complicated", "hand", "engineering", "of", "input", "features", ",", "which", "might", "improve", "performance", "further", ".", "We", "use", "the", "training", "recipe", "from", "weiss", "-", "etAl:2015:ACL", "for", "each", "training", "stage", "of", "our", "model", ".", "Specifically", ",", "we", "use", "averaged", "stochastic", "gradient", "descent", "with", "momentum", ",", "and", "we", "tune", "the", "learning", "rate", ",", "learning", "rate", "schedule", ",", "momentum", ",", "and", "early", "stopping", "time", "using", "a", "separate", "held", "-", "out", "corpus", "for", "each", "task", ".", "We", "tune", "again", "with", "a", "different", "set", "of", "hyperparameters", "for", "training", "with", "the", "global", "objective", ".", "subsection", ":", "Part", "of", "Speech", "Tagging", "Part", "of", "speech", "(", "POS", ")", "tagging", "is", "a", "classic", "NLP", "task", ",", "where", "modeling", "the", "structure", "of", "the", "output", "is", "important", "for", "achieving", "state", "-", "of", "-", "the", "-", "art", "performance", ".", "paragraph", ":", "Data", "&", "Evaluation", ".", "We", "conducted", "experiments", "on", "a", "number", "of", "different", "datasets", ":", "(", "1", ")", "the", "English", "Wall", "Street", "Journal", "(", "WSJ", ")", "part", "of", "the", "Penn", "Treebank", "with", "standard", "POS", "tagging", "splits", ";", "(", "2", ")", "the", "English", "\u201c", "Treebank", "Union", "\u201d", "multi", "-", "domain", "corpus", "containing", "data", "from", "the", "OntoNotes", "corpus", "version", "5", ",", "the", "English", "Web", "Treebank", ",", "and", "the", "updated", "and", "corrected", "Question", "Treebank", "with", "identical", "setup", "to", "weiss", "-", "etAl:2015:ACL", ";", "and", "(", "3", ")", "the", "CoNLL", "\u2019", "09", "multi", "-", "lingual", "shared", "task", ".", "paragraph", ":", "Model", "Configuration", ".", "Inspired", "by", "the", "integrated", "POS", "tagging", "and", "parsing", "transition", "system", "of", "bohnet", "-", "nivre:2012:EMNLP", "-", "CoNLL", ",", "we", "employ", "a", "simple", "transition", "system", "that", "uses", "only", "a", "Shift", "action", "and", "predicts", "the", "POS", "tag", "of", "the", "current", "word", "on", "the", "buffer", "as", "it", "gets", "shifted", "to", "the", "stack", ".", "We", "extract", "the", "following", "features", "on", "a", "window", "tokens", "centered", "at", "the", "current", "focus", "token", ":", "word", ",", "cluster", ",", "character", "n", "-", "gram", "up", "to", "length", "3", ".", "We", "also", "extract", "the", "tag", "predicted", "for", "the", "previous", "4", "tokens", ".", "The", "network", "in", "these", "experiments", "has", "a", "single", "hidden", "layer", "with", "256", "units", "on", "WSJ", "and", "Treebank", "Union", "and", "64", "on", "CoNLL\u201909", ".", "paragraph", ":", "Results", ".", "In", "Table", "[", "reference", "]", "we", "compare", "our", "model", "to", "a", "linear", "CRF", "and", "to", "the", "compositional", "character", "-", "to", "-", "word", "LSTM", "model", "of", "ling", "-", "EtAl:2015:EMNLP", ".", "The", "CRF", "is", "a", "first", "-", "order", "linear", "model", "with", "exact", "inference", "and", "the", "same", "emission", "features", "as", "our", "model", ".", "It", "additionally", "also", "has", "transition", "features", "of", "the", "word", ",", "cluster", "and", "character", "n", "-", "gram", "up", "to", "length", "3", "on", "both", "endpoints", "of", "the", "transition", ".", "The", "results", "for", "ling", "-", "EtAl:2015:EMNLP", "were", "solicited", "from", "the", "authors", ".", "Our", "local", "model", "already", "compares", "favorably", "against", "these", "methods", "on", "average", ".", "Using", "beam", "search", "with", "a", "locally", "normalized", "model", "does", "not", "help", ",", "but", "with", "global", "normalization", "it", "leads", "to", "a", "7", "%", "reduction", "in", "relative", "error", ",", "empirically", "demonstrating", "the", "effect", "of", "label", "bias", ".", "The", "set", "of", "character", "ngrams", "feature", "is", "very", "important", ",", "increasing", "average", "accuracy", "on", "the", "CoNLL\u201909", "datasets", "by", "about", "0.5", "%", "absolute", ".", "This", "shows", "that", "character", "-", "level", "modeling", "can", "also", "be", "done", "with", "a", "simple", "feed", "-", "forward", "network", "without", "recurrence", ".", "subsection", ":", "Dependency", "Parsing", "In", "dependency", "parsing", "the", "goal", "is", "to", "produce", "a", "directed", "tree", "representing", "the", "syntactic", "structure", "of", "the", "input", "sentence", ".", "paragraph", ":", "Data", "&", "Evaluation", ".", "We", "use", "the", "same", "corpora", "as", "in", "our", "POS", "tagging", "experiments", ",", "except", "that", "we", "use", "the", "standard", "parsing", "splits", "of", "the", "WSJ", ".", "To", "avoid", "over", "-", "fitting", "to", "the", "development", "set", "(", "Sec", ".", "22", ")", ",", "we", "use", "Sec", ".", "24", "for", "tuning", "the", "hyperparameters", "of", "our", "models", ".", "We", "convert", "the", "English", "constituency", "trees", "to", "Stanford", "style", "dependencies", "using", "version", "3.3.0", "of", "the", "converter", ".", "For", "English", ",", "we", "use", "predicted", "POS", "tags", "(", "the", "same", "POS", "tags", "are", "used", "for", "all", "models", ")", "and", "exclude", "punctuation", "from", "the", "evaluation", ",", "as", "is", "standard", ".", "For", "the", "CoNLL", "\u2019", "09", "datasets", "we", "follow", "standard", "practice", "and", "include", "all", "punctuation", "in", "the", "evaluation", ".", "We", "follow", "alberti", "-", "EtAl:2015:EMNLP", "and", "use", "our", "own", "predicted", "POS", "tags", "so", "that", "we", "can", "include", "a", "k", "-", "best", "tag", "feature", "(", "see", "below", ")", "but", "use", "the", "supplied", "predicted", "morphological", "features", ".", "We", "report", "unlabeled", "and", "labeled", "attachment", "scores", "(", "UAS", "/", "LAS", ")", ".", "paragraph", ":", "Model", "Configuration", ".", "Our", "model", "configuration", "is", "basically", "the", "same", "as", "the", "one", "originally", "proposed", "by", "chen", "-", "manning:2014:EMNLP", "and", "then", "refined", "by", "weiss", "-", "etAl:2015:ACL", ".", "In", "particular", ",", "we", "use", "the", "arc", "-", "standard", "transition", "system", "and", "extract", "the", "same", "set", "of", "features", "as", "prior", "work", ":", "words", ",", "part", "of", "speech", "tags", ",", "and", "dependency", "arcs", "and", "labels", "in", "the", "surrounding", "context", "of", "the", "state", ",", "as", "well", "as", "k", "-", "best", "tags", "as", "proposed", "by", "alberti", "-", "EtAl:2015:EMNLP", ".", "We", "use", "two", "hidden", "layers", "of", "1", ",", "024", "dimensions", "each", ".", "paragraph", ":", "Results", ".", "Tables", "[", "reference", "]", "and", "[", "reference", "]", "show", "our", "final", "parsing", "results", "and", "a", "comparison", "to", "the", "best", "systems", "from", "the", "literature", ".", "We", "obtain", "the", "best", "ever", "published", "results", "on", "almost", "all", "datasets", ",", "including", "the", "WSJ", ".", "Our", "main", "results", "use", "the", "same", "pre", "-", "trained", "word", "embeddings", "as", "weiss", "-", "etAl:2015:ACL", "and", "alberti", "-", "EtAl:2015:EMNLP", ",", "but", "no", "tri", "-", "training", ".", "When", "we", "artificially", "restrict", "ourselves", "to", "not", "use", "pre", "-", "trained", "word", "embeddings", ",", "we", "observe", "only", "a", "modest", "drop", "of", "0.5", "%", "UAS", ";", "for", "example", ",", "training", "only", "on", "the", "WSJ", "yields", "94.08", "%", "UAS", "and", "92.15", "%", "LAS", "for", "our", "global", "model", "with", "a", "beam", "of", "size", "32", ".", "Even", "though", "we", "do", "not", "use", "tri", "-", "training", ",", "our", "model", "compares", "favorably", "to", "the", "94.26", "%", "LAS", "and", "92.41", "%", "UAS", "reported", "by", "weiss", "-", "etAl:2015:ACL", "with", "tri", "-", "training", ".", "As", "we", "show", "in", "Sec", ".", "[", "reference", "]", ",", "these", "gains", "can", "be", "attributed", "to", "the", "full", "backpropagation", "training", "that", "differentiates", "our", "approach", "from", "that", "of", "weiss", "-", "etAl:2015:ACL", "and", "alberti", "-", "EtAl:2015:EMNLP", ".", "Our", "results", "also", "significantly", "outperform", "the", "LSTM", "-", "based", "approaches", "of", "dyer", "-", "etAl:2015:ACL", "and", "BallesterosDS15", ".", "subsection", ":", "Sentence", "Compression", "Our", "final", "structured", "prediction", "task", "is", "extractive", "sentence", "compression", ".", "paragraph", ":", "Data", "&", "Evaluation", ".", "We", "follow", "filippova", "-", "emnlp15", ",", "where", "a", "large", "news", "collection", "is", "used", "to", "heuristically", "generate", "compression", "instances", ".", "Our", "final", "corpus", "contains", "about", "2.3", "M", "compression", "instances", ":", "we", "use", "2", "M", "examples", "for", "training", ",", "130k", "for", "development", "and", "160k", "for", "the", "final", "test", ".", "We", "report", "per", "-", "token", "F1", "score", "and", "per", "-", "sentence", "accuracy", "(", "A", ")", ",", "i.e.", "percentage", "of", "instances", "that", "fully", "match", "the", "golden", "compressions", ".", "Following", "filippova", "-", "emnlp15", "we", "also", "run", "a", "human", "evaluation", "on", "200", "sentences", "where", "we", "ask", "the", "raters", "to", "score", "compressions", "for", "readability", "(", "read", ")", "and", "informativeness", "(", "info", ")", "on", "a", "scale", "from", "0", "to", "5", ".", "paragraph", ":", "Model", "Configuration", ".", "The", "transition", "system", "for", "sentence", "compression", "is", "similar", "to", "POS", "tagging", ":", "we", "scan", "sentences", "from", "left", "-", "to", "-", "right", "and", "label", "each", "token", "as", "keep", "or", "drop", ".", "We", "extract", "features", "from", "words", ",", "POS", "tags", ",", "and", "dependency", "labels", "from", "a", "window", "of", "tokens", "centered", "on", "the", "input", ",", "as", "well", "as", "features", "from", "the", "history", "of", "predictions", ".", "We", "use", "a", "single", "hidden", "layer", "of", "size", "400", ".", "paragraph", ":", "Results", ".", "Table", "[", "reference", "]", "shows", "our", "sentence", "compression", "results", ".", "Our", "globally", "normalized", "model", "again", "significantly", "outperforms", "the", "local", "model", ".", "Beam", "search", "with", "a", "locally", "normalized", "model", "suffers", "from", "severe", "label", "bias", "issues", "that", "we", "discuss", "on", "a", "concrete", "example", "in", "Section", "[", "reference", "]", ".", "We", "also", "compare", "to", "the", "sentence", "compression", "system", "from", "filippova", "-", "emnlp15", ",", "a", "3", "-", "layer", "stacked", "LSTM", "which", "uses", "dependency", "label", "information", ".", "The", "LSTM", "and", "our", "global", "model", "perform", "on", "par", "on", "both", "the", "automatic", "evaluation", "as", "well", "as", "the", "human", "ratings", ",", "but", "our", "model", "is", "roughly", "100", "faster", ".", "All", "compressions", "kept", "approximately", "42", "%", "of", "the", "tokens", "on", "average", "and", "all", "the", "models", "are", "significantly", "better", "than", "the", "automatic", "extractions", "(", ")", ".", "section", ":", "Discussion", "We", "derived", "a", "proof", "for", "the", "label", "bias", "problem", "and", "the", "advantages", "of", "global", "models", ".", "We", "then", "emprirically", "verified", "this", "theoretical", "superiority", "by", "demonstrating", "state", "-", "of", "-", "the", "-", "art", "performance", "on", "three", "different", "tasks", ".", "In", "this", "section", "we", "situate", "and", "compare", "our", "model", "to", "previous", "work", "and", "provide", "two", "examples", "of", "the", "label", "bias", "problem", "in", "practice", ".", "subsection", ":", "Related", "Neural", "CRF", "Work", "Neural", "network", "models", "have", "been", "been", "combined", "with", "conditional", "random", "fields", "and", "globally", "normalized", "models", "before", ".", "bottou", "-", "97", "and", "lecun", "-", "98h", "describe", "global", "training", "of", "neural", "network", "models", "for", "structured", "prediction", "problems", ".", "conditional_neural_fields", "add", "a", "non", "-", "linear", "neural", "network", "layer", "to", "a", "linear", "-", "chain", "CRF", "and", "neural_crf", "apply", "a", "similar", "approach", "to", "more", "general", "Markov", "network", "structures", ".", "recurrentCRF", "and", "Zheng_2015_ICCV", "introduce", "recurrence", "into", "the", "model", "and", "huang2015bidirectional", "finally", "combine", "CRFs", "and", "LSTMs", ".", "These", "neural", "CRF", "models", "are", "limited", "to", "sequence", "labeling", "tasks", "where", "exact", "inference", "is", "possible", ",", "while", "our", "model", "works", "well", "when", "exact", "inference", "is", "intractable", ".", "subsection", ":", "Related", "Transition", "-", "Based", "Parsing", "Work", "For", "early", "work", "on", "neural", "-", "networks", "for", "transition", "-", "based", "parsing", ",", "see", "Henderson", "henderson:2003:NAACL", ",", "henderson:2004:ACL", ".", "Our", "work", "is", "closest", "to", "the", "work", "of", "weiss", "-", "etAl:2015:ACL", ",", "zhou", "-", "etAl:2015:ACL", "and", "watanabe", "-", "sumita:2015:ACL", ";", "in", "these", "approaches", "global", "normalization", "is", "added", "to", "the", "local", "model", "of", "chen", "-", "manning:2014:EMNLP", ".", "Empirically", ",", "weiss", "-", "etAl:2015:ACL", "achieves", "the", "best", "performance", ",", "even", "though", "their", "model", "keeps", "the", "parameters", "of", "the", "locally", "normalized", "neural", "network", "fixed", "and", "only", "trains", "a", "perceptron", "that", "uses", "the", "activations", "as", "features", ".", "Their", "model", "is", "therefore", "limited", "in", "its", "ability", "to", "revise", "the", "predictions", "of", "the", "locally", "normalized", "model", ".", "In", "Table", "[", "reference", "]", "we", "show", "that", "full", "backpropagation", "training", "all", "the", "way", "to", "the", "word", "embeddings", "is", "very", "important", "and", "significantly", "contributes", "to", "the", "performance", "of", "our", "model", ".", "We", "also", "compared", "training", "under", "the", "CRF", "objective", "with", "a", "Perceptron", "-", "like", "hinge", "loss", "between", "the", "gold", "and", "best", "elements", "of", "the", "beam", ".", "When", "we", "limited", "the", "backpropagation", "depth", "to", "training", "only", "the", "top", "layer", ",", "we", "found", "negligible", "differences", "in", "accuracy", ":", "93.20", "%", "and", "93.28", "%", "for", "the", "CRF", "objective", "and", "hinge", "loss", "respectively", ".", "However", ",", "when", "training", "with", "full", "backpropagation", "the", "CRF", "accuracy", "is", "0.2", "%", "higher", "and", "training", "converged", "more", "than", "4", "faster", ".", "zhou", "-", "etAl:2015:ACL", "perform", "full", "backpropagation", "training", "like", "us", ",", "but", "even", "with", "a", "much", "larger", "beam", ",", "their", "performance", "is", "significantly", "lower", "than", "ours", ".", "We", "also", "apply", "our", "model", "to", "two", "additional", "tasks", ",", "while", "they", "experiment", "only", "with", "dependency", "parsing", ".", "Finally", ",", "watanabe", "-", "sumita:2015:ACL", "introduce", "recurrent", "components", "and", "additional", "techniques", "like", "max", "-", "violation", "updates", "for", "a", "corresponding", "constituency", "parsing", "model", ".", "In", "contrast", ",", "our", "model", "does", "not", "require", "any", "recurrence", "or", "specialized", "training", ".", "subsection", ":", "Label", "Bias", "in", "Practice", "We", "observed", "several", "instances", "of", "severe", "label", "bias", "in", "the", "sentence", "compression", "task", ".", "Although", "using", "beam", "search", "with", "the", "local", "model", "outperforms", "greedy", "inference", "on", "average", ",", "beam", "search", "leads", "the", "local", "model", "to", "occasionally", "produce", "empty", "compressions", "(", "Table", "[", "reference", "]", ")", ".", "It", "is", "important", "to", "note", "that", "these", "are", "not", "search", "errors", ":", "the", "empty", "compression", "has", "higher", "probability", "under", "than", "the", "prediction", "from", "greedy", "inference", ".", "However", ",", "the", "more", "expressive", "globally", "normalized", "model", "does", "not", "suffer", "from", "this", "limitation", ",", "and", "correctly", "gives", "the", "empty", "compression", "almost", "zero", "probability", ".", "We", "also", "present", "some", "empirical", "evidence", "that", "the", "label", "bias", "problem", "is", "severe", "in", "parsing", ".", "We", "trained", "models", "where", "the", "scoring", "functions", "in", "parsing", "at", "position", "in", "the", "sentence", "are", "limited", "to", "considering", "only", "tokens", ";", "hence", "unlike", "the", "full", "parsing", "model", ",", "there", "is", "no", "ability", "to", "look", "ahead", "in", "the", "sentence", "when", "making", "a", "decision", ".", "The", "result", "for", "a", "greedy", "model", "under", "this", "constraint", "is", "76.96", "%", "UAS", ";", "for", "a", "locally", "normalized", "model", "with", "beam", "search", "is", "81.35", "%", ";", "and", "for", "a", "globally", "normalized", "model", "is", "93.60", "%", ".", "Thus", "the", "globally", "normalized", "model", "gets", "very", "close", "to", "the", "performance", "of", "a", "model", "with", "full", "lookahead", ",", "while", "the", "locally", "normalized", "model", "with", "a", "beam", "gives", "dramatically", "lower", "performance", ".", "In", "our", "final", "experiments", "with", "full", "lookahead", ",", "the", "globally", "normalized", "model", "achieves", "94.01", "%", "accuracy", ",", "compared", "to", "93.07", "%", "accuracy", "for", "a", "local", "model", "with", "beam", "search", ".", "Thus", "adding", "lookahead", "allows", "the", "local", "model", "to", "close", "the", "gap", "in", "performance", "to", "the", "global", "model", ";", "however", "there", "is", "still", "a", "significant", "difference", "in", "accuracy", ",", "which", "may", "in", "large", "part", "be", "due", "to", "the", "label", "bias", "problem", ".", "A", "number", "of", "authors", "have", "considered", "modified", "training", "procedures", "for", "greedy", "models", ",", "or", "for", "locally", "normalized", "models", ".", "daume09searn", "introduce", "Searn", ",", "an", "algorithm", "that", "allows", "a", "classifier", "making", "greedy", "decisions", "to", "become", "more", "robust", "to", "errors", "made", "in", "previous", "decisions", ".", "goldberg2013training", "describe", "improvements", "to", "a", "greedy", "parsing", "approach", "that", "makes", "use", "of", "methods", "from", "imitation", "learning", "to", "augment", "the", "training", "set", ".", "Note", "that", "these", "methods", "are", "focused", "on", "greedy", "models", ":", "they", "are", "unlikely", "to", "solve", "the", "label", "bias", "problem", "when", "used", "in", "conjunction", "with", "beam", "search", ",", "given", "that", "the", "problem", "is", "one", "of", "expressivity", "of", "the", "underlying", "model", ".", "More", "recent", "work", "has", "augmented", "locally", "normalized", "models", "with", "correctness", "probabilities", "or", "error", "states", ",", "effectively", "adding", "a", "step", "after", "every", "decision", "where", "the", "probability", "of", "correctness", "of", "the", "resulting", "structure", "is", "evaluated", ".", "This", "gives", "considerable", "gains", "over", "a", "locally", "normalized", "model", ",", "although", "performance", "is", "lower", "than", "our", "full", "globally", "normalized", "approach", ".", "section", ":", "Conclusions", "We", "presented", "a", "simple", "and", "yet", "powerful", "model", "architecture", "that", "produces", "state", "-", "of", "-", "the", "-", "art", "results", "for", "POS", "tagging", ",", "dependency", "parsing", "and", "sentence", "compression", ".", "Our", "model", "combines", "the", "flexibility", "of", "transition", "-", "based", "algorithms", "and", "the", "modeling", "power", "of", "neural", "networks", ".", "Our", "results", "demonstrate", "that", "feed", "-", "forward", "network", "without", "recurrence", "can", "outperform", "recurrent", "models", "such", "as", "LSTMs", "when", "they", "are", "trained", "with", "global", "normalization", ".", "We", "further", "support", "our", "empirical", "findings", "with", "a", "proof", "showing", "that", "global", "normalization", "helps", "the", "model", "overcome", "the", "label", "bias", "problem", "from", "which", "locally", "normalized", "models", "suffer", ".", "section", ":", "Acknowledgements", "We", "would", "like", "to", "thank", "Ling", "Wang", "for", "training", "his", "C2W", "part", "-", "of", "-", "speech", "tagger", "on", "our", "setup", ",", "and", "Emily", "Pitler", ",", "Ryan", "McDonald", ",", "Greg", "Coppola", "and", "Fernando", "Pereira", "for", "tremendously", "helpful", "discussions", ".", "Finally", ",", "we", "are", "grateful", "to", "all", "members", "of", "the", "Google", "Parsing", "Team", ".", "bibliography", ":", "References"]}