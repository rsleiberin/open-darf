{"coref": {"AWD-LSTM-MoS": [], "AWD-LSTM-MoS___dynamic_eval": [], "Language_Modelling": [[153, 156], [695, 697]], "Number_of_params": [[526, 527], [587, 588], [630, 631], [2286, 2287], [2295, 2296], [2329, 2330], [2422, 2423], [2459, 2460], [2502, 2503], [2550, 2551], [2565, 2566], [2674, 2675], [2752, 2753], [2981, 2982], [3093, 3094], [3135, 3136], [3246, 3247], [3311, 3312], [3363, 3364], [3422, 3423], [3608, 3609], [3670, 3671], [3834, 3835], [4032, 4033], [4117, 4118], [4132, 4133], [4175, 4176], [4211, 4212], [4258, 4259], [4285, 4286], [4309, 4310], [4345, 4346], [4657, 4658], [5359, 5360], [5468, 5469], [5677, 5678], [5782, 5783], [5827, 5828], [5881, 5882], [5962, 5963], [6059, 6060], [6122, 6123], [6167, 6168], [6579, 6580], [6736, 6737], [6773, 6774], [6824, 6825], [6852, 6853], [550, 551], [2449, 2450], [2897, 2898], [2951, 2952], [3051, 3052], [3296, 3297], [3389, 3390], [3464, 3465], [3469, 3470], [3573, 3574], [3633, 3634], [3883, 3884], [3934, 3935], [4083, 4084], [4205, 4206], [4364, 4365], [4968, 4969], [5377, 5378], [5657, 5658], [6183, 6184], [6214, 6215], [6314, 6315], [6357, 6358], [6415, 6416], [6628, 6629], [6672, 6673]], "Params": [], "Penn_Treebank__Word_Level_": [[105, 107], [618, 620], [2908, 2910], [2911, 2912], [3222, 3223], [3414, 3415], [3532, 3533], [3696, 3699], [3910, 3911], [4305, 4306], [5350, 5351], [5799, 5800], [6372, 6373], [6598, 6599]], "Test_perplexity": [], "Validation_perplexity": [[103, 104], [139, 140], [612, 613], [615, 616], [2924, 2925], [2955, 2957], [3171, 3172], [3253, 3255], [3273, 3274], [3304, 3305], [3904, 3905], [4089, 4091], [4862, 4863], [4928, 4930], [3943, 3944], [4013, 4014], [4122, 4123]], "WikiText-2": [[108, 111], [623, 626], [2915, 2918], [2919, 2920], [3224, 3225], [3416, 3417], [3537, 3538], [5352, 5353], [5997, 5998], [6374, 6375], [6506, 6507]], "dynamic_eval": []}, "coref_non_salient": {"0": [[3032, 3033], [6044, 6051]], "1": [[5408, 5410], [5413, 5416], [5417, 5420], [5432, 5434], [5437, 5440], [5441, 5444], [5445, 5448]], "10": [[5752, 5758], [5831, 5833]], "100": [[2336, 2338]], "101": [[1296, 1298]], "102": [[2698, 2702]], "103": [[1064, 1067]], "104": [[2331, 2333]], "105": [[508, 511]], "106": [[2807, 2809]], "107": [[2569, 2571]], "108": [[4432, 4437]], "109": [[5405, 5406]], "11": [[1873, 1878], [1880, 1885], [1913, 1918], [2268, 2273], [2276, 2281]], "110": [[5979, 5981]], "111": [[798, 803]], "112": [[4594, 4597]], "113": [[199, 204]], "114": [[3195, 3197]], "115": [[4849, 4851]], "116": [[710, 712]], "117": [[2135, 2139]], "118": [[921, 924]], "119": [[4751, 4753]], "12": [[4236, 4238], [6211, 6213], [6231, 6233]], "120": [[4531, 4534]], "13": [[2865, 2866], [6194, 6195], [6493, 6494], [2729, 2730], [2746, 2747], [3341, 3342], [3864, 3865], [6341, 6342], [6469, 6470], [6610, 6611]], "14": [[4216, 4218], [4254, 4257], [6190, 6192], [6353, 6355], [6458, 6460], [6501, 6503], [6520, 6522], [6575, 6577]], "15": [[16, 18], [193, 195], [661, 663], [713, 715], [840, 842], [2234, 2236], [2628, 2630], [2994, 2996], [3218, 3220], [4407, 4409], [5361, 5363], [583, 585], [2354, 2356], [2903, 2905], [4171, 4173], [5857, 5859], [5893, 5895], [6153, 6155], [6581, 6583]], "16": [[496, 498], [1492, 1495], [2291, 2294], [6177, 6180]], "17": [[2415, 2416], [4972, 4973]], "18": [[5400, 5401], [6017, 6018]], "19": [[1372, 1374], [5941, 5943]], "2": [[320, 322], [926, 928], [2591, 2595]], "20": [[4312, 4316], [4368, 4372], [4389, 4393]], "21": [[4295, 4297], [6587, 6589]], "22": [[2934, 2938], [3320, 3322], [4027, 4028], [4059, 4061]], "23": [[163, 166], [2066, 2068], [2070, 2072], [2520, 2521], [2523, 2525], [4500, 4502]], "24": [[2678, 2680], [2960, 2962], [3048, 3050]], "25": [[6631, 6634]], "26": [[522, 525], [529, 532], [2263, 2266], [2282, 2285], [4549, 4552]], "27": [[5803, 5805]], "28": [[2232, 2233], [2242, 2243], [2516, 2517], [2555, 2557], [4472, 4473], [4506, 4507]], "29": [[5896, 5897], [5922, 5923], [5970, 5971], [5976, 5977]], "3": [[387, 393], [405, 411], [904, 909]], "30": [[4915, 4917]], "31": [[6307, 6308], [6484, 6485]], "32": [[6036, 6037], [6126, 6127], [6133, 6134]], "33": [[341, 343], [1382, 1384], [1436, 1438], [5320, 5322]], "34": [[4876, 4879]], "35": [[1092, 1096], [1110, 1115]], "36": [[2366, 2370]], "37": [[20, 23], [421, 424], [665, 668], [698, 700], [1312, 1315]], "38": [[4921, 4923]], "39": [[4713, 4717]], "4": [[39, 42], [167, 170], [3025, 3028], [4450, 4453], [4478, 4483]], "40": [[4, 6], [47, 49], [657, 659], [1501, 1503], [1604, 1606], [1937, 1939], [6160, 6162]], "41": [[4350, 4354], [6709, 6713]], "42": [[3064, 3065], [4064, 4065]], "43": [[4936, 4939]], "44": [[3178, 3183]], "45": [[307, 309], [353, 355], [990, 992]], "46": [[66, 67], [356, 357], [536, 537], [566, 567], [640, 641], [917, 918], [1555, 1556], [1985, 1986], [2419, 2420], [2435, 2436], [2513, 2514], [2682, 2683], [2778, 2779], [3148, 3149], [3298, 3299], [3367, 3368], [3686, 3687], [3814, 3815], [4044, 4045], [4085, 4086], [4156, 4157], [4645, 4646], [4974, 4975], [5087, 5088], [5473, 5474], [5637, 5638], [5786, 5787], [5871, 5872], [5948, 5949], [5968, 5969], [6053, 6054], [6216, 6217], [6596, 6597]], "47": [[2999, 3002], [5384, 5387], [6377, 6378], [127, 128], [4049, 4050], [6510, 6511]], "48": [[3517, 3521], [3569, 3572], [3479, 3482]], "49": [[6304, 6306]], "5": [[249, 252], [283, 285], [1071, 1073]], "50": [[3122, 3124]], "51": [[4833, 4835]], "52": [[6030, 6035]], "53": [[4461, 4467]], "54": [[6534, 6535], [6541, 6542]], "55": [[5701, 5703]], "56": [[3778, 3780]], "57": [[253, 254], [339, 340], [11, 12], [3140, 3141], [4826, 4827]], "58": [[6079, 6081]], "59": [[68, 71], [463, 469]], "6": [[30, 34], [457, 462], [4943, 4948]], "60": [[5034, 5036]], "61": [[4540, 4543]], "62": [[3501, 3503]], "63": [[2774, 2777], [4772, 4775]], "64": [[2059, 2063], [4985, 4989]], "65": [[5759, 5760]], "66": [[4741, 4743]], "67": [[3376, 3378]], "68": [[3757, 3764]], "69": [[4729, 4732]], "7": [[4290, 4292], [4617, 4619]], "70": [[1821, 1823]], "71": [[3765, 3767]], "72": [[4574, 4575], [4588, 4589]], "73": [[189, 192]], "74": [[995, 996]], "75": [[6490, 6492]], "76": [[149, 152]], "77": [[4623, 4626]], "78": [[952, 954]], "79": [[3945, 3946]], "8": [[564, 565], [3939, 3941], [3960, 3961], [5465, 5467]], "80": [[2197, 2200]], "81": [[1819, 1820]], "82": [[1079, 1081]], "83": [[3131, 3132], [3145, 3146]], "84": [[2204, 2208]], "85": [[2766, 2767], [2856, 2857], [3154, 3155], [3365, 3366], [3527, 3528], [3684, 3685], [3816, 3817], [4690, 4691], [5471, 5472], [5639, 5640], [5784, 5785], [6590, 6591], [6608, 6609], [6635, 6636], [6693, 6694], [6801, 6802], [6839, 6840], [2791, 2792], [3425, 3426], [4766, 4767]], "86": [[940, 942], [982, 984]], "87": [[4795, 4797]], "88": [[4891, 4895]], "89": [[867, 870]], "9": [[1990, 1994], [4439, 4445]], "90": [[1431, 1434]], "91": [[2762, 2765]], "92": [[2646, 2648]], "93": [[247, 248], [397, 399]], "94": [[212, 214]], "95": [[3174, 3177]], "96": [[4040, 4042]], "97": [[4886, 4888]], "98": [[4839, 4842]], "99": [[4570, 4573]]}, "doc_id": "303fef411f235e6d1125a40af1e93224f498a4d5", "method_subrelations": {"AWD-LSTM-MoS": [[[0, 12], "AWD-LSTM-MoS"]], "AWD-LSTM-MoS___dynamic_eval": [[[0, 12], "AWD-LSTM-MoS"], [[15, 27], "dynamic_eval"]]}, "n_ary_relations": [{"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-MoS", "Metric": "Params", "Task": "Language_Modelling", "score": "22M"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-MoS___dynamic_eval", "Metric": "Params", "Task": "Language_Modelling", "score": "22M"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-MoS", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "54.44"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-MoS___dynamic_eval", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "47.69"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-MoS", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "56.54"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-MoS___dynamic_eval", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "48.33"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-MoS", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "35M"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-MoS___dynamic_eval", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "35M"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-MoS", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "61.45"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-MoS___dynamic_eval", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "40.68"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-MoS", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "63.88"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-MoS___dynamic_eval", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "42.41"}], "ner": [[4, 6, "Method"], [16, 18, "Method"], [20, 23, "Task"], [30, 34, "Method"], [39, 42, "Method"], [47, 49, "Method"], [66, 67, "Method"], [68, 71, "Method"], [103, 104, "Metric"], [105, 107, "Material"], [108, 111, "Material"], [139, 140, "Metric"], [149, 152, "Task"], [153, 156, "Task"], [163, 166, "Method"], [167, 170, "Method"], [189, 192, "Task"], [193, 195, "Method"], [199, 204, "Method"], [212, 214, "Method"], [247, 248, "Method"], [249, 252, "Method"], [253, 254, "Method"], [283, 285, "Method"], [307, 309, "Method"], [320, 322, "Method"], [339, 340, "Method"], [341, 343, "Method"], [353, 355, "Method"], [356, 357, "Method"], [387, 393, "Method"], [397, 399, "Method"], [405, 411, "Method"], [421, 424, "Task"], [457, 462, "Method"], [463, 469, "Method"], [496, 498, "Task"], [508, 511, "Method"], [522, 525, "Method"], [526, 527, "Method"], [529, 532, "Method"], [536, 537, "Method"], [564, 565, "Metric"], [566, 567, "Method"], [587, 588, "Method"], [612, 613, "Metric"], [615, 616, "Metric"], [618, 620, "Material"], [623, 626, "Material"], [630, 631, "Method"], [640, 641, "Method"], [657, 659, "Method"], [661, 663, "Method"], [665, 668, "Task"], [695, 697, "Task"], [698, 700, "Task"], [710, 712, "Method"], [713, 715, "Method"], [798, 803, "Method"], [840, 842, "Method"], [867, 870, "Method"], [904, 909, "Method"], [917, 918, "Method"], [921, 924, "Method"], [926, 928, "Method"], [940, 942, "Method"], [952, 954, "Method"], [982, 984, "Method"], [990, 992, "Method"], [995, 996, "Method"], [1064, 1067, "Method"], [1071, 1073, "Method"], [1079, 1081, "Method"], [1092, 1096, "Method"], [1110, 1115, "Method"], [1296, 1298, "Task"], [1312, 1315, "Task"], [1372, 1374, "Metric"], [1382, 1384, "Method"], [1431, 1434, "Method"], [1436, 1438, "Method"], [1492, 1495, "Task"], [1501, 1503, "Method"], [1555, 1556, "Method"], [1604, 1606, "Method"], [1819, 1820, "Task"], [1821, 1823, "Task"], [1873, 1878, "Method"], [1880, 1885, "Method"], [1913, 1918, "Method"], [1937, 1939, "Method"], [1985, 1986, "Method"], [1990, 1994, "Method"], [2059, 2063, "Method"], [2066, 2068, "Method"], [2070, 2072, "Method"], [2135, 2139, "Method"], [2197, 2200, "Method"], [2204, 2208, "Method"], [2232, 2233, "Task"], [2234, 2236, "Method"], [2242, 2243, "Task"], [2263, 2266, "Method"], [2268, 2273, "Method"], [2276, 2281, "Method"], [2282, 2285, "Method"], [2286, 2287, "Method"], [2291, 2294, "Task"], [2295, 2296, "Method"], [2329, 2330, "Method"], [2331, 2333, "Method"], [2336, 2338, "Method"], [2366, 2370, "Method"], [2415, 2416, "Metric"], [2419, 2420, "Method"], [2422, 2423, "Method"], [2435, 2436, "Method"], [2459, 2460, "Method"], [2502, 2503, "Method"], [2513, 2514, "Method"], [2516, 2517, "Task"], [2520, 2521, "Method"], [2523, 2525, "Method"], [2550, 2551, "Method"], [2555, 2557, "Task"], [2565, 2566, "Method"], [2569, 2571, "Method"], [2591, 2595, "Method"], [2628, 2630, "Method"], [2646, 2648, "Method"], [2674, 2675, "Method"], [2678, 2680, "Metric"], [2682, 2683, "Method"], [2698, 2702, "Method"], [2752, 2753, "Method"], [2762, 2765, "Method"], [2766, 2767, "Method"], [2774, 2777, "Task"], [2778, 2779, "Method"], [2807, 2809, "Method"], [2856, 2857, "Method"], [2865, 2866, "Method"], [2908, 2910, "Material"], [2911, 2912, "Material"], [2915, 2918, "Material"], [2919, 2920, "Material"], [2924, 2925, "Metric"], [2934, 2938, "Method"], [2955, 2957, "Metric"], [2960, 2962, "Metric"], [2981, 2982, "Method"], [2994, 2996, "Method"], [2999, 3002, "Material"], [3025, 3028, "Method"], [3032, 3033, "Method"], [3048, 3050, "Metric"], [3064, 3065, "Method"], [3093, 3094, "Method"], [3122, 3124, "Material"], [3131, 3132, "Method"], [3135, 3136, "Method"], [3148, 3149, "Method"], [3154, 3155, "Method"], [3171, 3172, "Metric"], [3174, 3177, "Metric"], [3178, 3183, "Metric"], [3195, 3197, "Method"], [3218, 3220, "Method"], [3222, 3223, "Material"], [3224, 3225, "Material"], [3246, 3247, "Method"], [3253, 3255, "Metric"], [3273, 3274, "Metric"], [3298, 3299, "Method"], [3304, 3305, "Metric"], [3311, 3312, "Method"], [3320, 3322, "Method"], [3363, 3364, "Method"], [3365, 3366, "Method"], [3367, 3368, "Method"], [3376, 3378, "Task"], [3414, 3415, "Material"], [3416, 3417, "Material"], [3422, 3423, "Method"], [3501, 3503, "Task"], [3517, 3521, "Method"], [3527, 3528, "Method"], [3532, 3533, "Material"], [3537, 3538, "Material"], [3569, 3572, "Method"], [3608, 3609, "Method"], [3670, 3671, "Method"], [3684, 3685, "Method"], [3686, 3687, "Method"], [3696, 3699, "Material"], [3757, 3764, "Method"], [3765, 3767, "Method"], [3778, 3780, "Metric"], [3814, 3815, "Method"], [3816, 3817, "Method"], [3834, 3835, "Method"], [3904, 3905, "Metric"], [3910, 3911, "Material"], [3939, 3941, "Metric"], [3945, 3946, "Method"], [3960, 3961, "Metric"], [4027, 4028, "Method"], [4032, 4033, "Method"], [4040, 4042, "Metric"], [4044, 4045, "Method"], [4059, 4061, "Method"], [4064, 4065, "Method"], [4085, 4086, "Method"], [4089, 4091, "Metric"], [4117, 4118, "Method"], [4132, 4133, "Method"], [4156, 4157, "Method"], [4175, 4176, "Method"], [4211, 4212, "Method"], [4216, 4218, "Metric"], [4236, 4238, "Metric"], [4254, 4257, "Metric"], [4258, 4259, "Method"], [4285, 4286, "Method"], [4290, 4292, "Method"], [4295, 4297, "Method"], [4305, 4306, "Material"], [4309, 4310, "Method"], [4312, 4316, "Task"], [4345, 4346, "Method"], [4350, 4354, "Task"], [4368, 4372, "Task"], [4389, 4393, "Task"], [4407, 4409, "Method"], [4432, 4437, "Method"], [4439, 4445, "Method"], [4450, 4453, "Method"], [4461, 4467, "Method"], [4472, 4473, "Task"], [4478, 4483, "Method"], [4500, 4502, "Method"], [4506, 4507, "Task"], [4531, 4534, "Method"], [4540, 4543, "Task"], [4549, 4552, "Method"], [4570, 4573, "Method"], [4574, 4575, "Method"], [4588, 4589, "Method"], [4594, 4597, "Method"], [4617, 4619, "Method"], [4623, 4626, "Method"], [4645, 4646, "Method"], [4657, 4658, "Method"], [4690, 4691, "Method"], [4713, 4717, "Method"], [4729, 4732, "Method"], [4741, 4743, "Task"], [4751, 4753, "Method"], [4772, 4775, "Task"], [4795, 4797, "Method"], [4833, 4835, "Method"], [4839, 4842, "Metric"], [4849, 4851, "Task"], [4862, 4863, "Metric"], [4876, 4879, "Method"], [4886, 4888, "Method"], [4891, 4895, "Method"], [4915, 4917, "Method"], [4921, 4923, "Method"], [4928, 4930, "Metric"], [4936, 4939, "Method"], [4943, 4948, "Method"], [4972, 4973, "Metric"], [4974, 4975, "Method"], [4985, 4989, "Method"], [5034, 5036, "Method"], [5087, 5088, "Method"], [5320, 5322, "Method"], [5350, 5351, "Material"], [5352, 5353, "Material"], [5359, 5360, "Method"], [5361, 5363, "Method"], [5384, 5387, "Material"], [5400, 5401, "Task"], [5405, 5406, "Method"], [5408, 5410, "Material"], [5413, 5416, "Material"], [5417, 5420, "Material"], [5432, 5434, "Material"], [5437, 5440, "Material"], [5441, 5444, "Material"], [5445, 5448, "Material"], [5465, 5467, "Metric"], [5468, 5469, "Method"], [5471, 5472, "Method"], [5473, 5474, "Method"], [5637, 5638, "Method"], [5639, 5640, "Method"], [5677, 5678, "Method"], [5701, 5703, "Metric"], [5752, 5758, "Metric"], [5759, 5760, "Method"], [5782, 5783, "Method"], [5784, 5785, "Method"], [5786, 5787, "Method"], [5799, 5800, "Material"], [5803, 5805, "Method"], [5827, 5828, "Method"], [5831, 5833, "Metric"], [5871, 5872, "Method"], [5881, 5882, "Method"], [5896, 5897, "Method"], [5922, 5923, "Method"], [5941, 5943, "Metric"], [5948, 5949, "Method"], [5962, 5963, "Method"], [5968, 5969, "Method"], [5970, 5971, "Method"], [5976, 5977, "Method"], [5979, 5981, "Material"], [5997, 5998, "Material"], [6017, 6018, "Task"], [6030, 6035, "Metric"], [6036, 6037, "Metric"], [6044, 6051, "Method"], [6053, 6054, "Method"], [6059, 6060, "Method"], [6079, 6081, "Metric"], [6122, 6123, "Method"], [6126, 6127, "Metric"], [6133, 6134, "Metric"], [6160, 6162, "Method"], [6167, 6168, "Method"], [6177, 6180, "Task"], [6190, 6192, "Metric"], [6194, 6195, "Method"], [6211, 6213, "Metric"], [6216, 6217, "Method"], [6231, 6233, "Metric"], [6304, 6306, "Method"], [6307, 6308, "Method"], [6353, 6355, "Metric"], [6372, 6373, "Material"], [6374, 6375, "Material"], [6377, 6378, "Material"], [6458, 6460, "Metric"], [6484, 6485, "Method"], [6490, 6492, "Method"], [6493, 6494, "Method"], [6501, 6503, "Metric"], [6506, 6507, "Material"], [6520, 6522, "Metric"], [6534, 6535, "Task"], [6541, 6542, "Task"], [6575, 6577, "Metric"], [6579, 6580, "Method"], [6587, 6589, "Method"], [6590, 6591, "Method"], [6596, 6597, "Method"], [6598, 6599, "Material"], [6608, 6609, "Method"], [6631, 6634, "Metric"], [6635, 6636, "Method"], [6693, 6694, "Method"], [6709, 6713, "Task"], [6736, 6737, "Method"], [6773, 6774, "Method"], [6801, 6802, "Method"], [6824, 6825, "Method"], [6839, 6840, "Method"], [6852, 6853, "Method"], [11, 12, "Method"], [127, 128, "Material"], [550, 551, "Method"], [583, 585, "Method"], [2354, 2356, "Method"], [2449, 2450, "Method"], [2729, 2730, "Method"], [2746, 2747, "Method"], [2791, 2792, "Method"], [2897, 2898, "Method"], [2903, 2905, "Method"], [2951, 2952, "Method"], [3051, 3052, "Method"], [3140, 3141, "Method"], [3145, 3146, "Method"], [3296, 3297, "Method"], [3341, 3342, "Method"], [3389, 3390, "Method"], [3425, 3426, "Method"], [3464, 3465, "Method"], [3469, 3470, "Method"], [3479, 3482, "Method"], [3573, 3574, "Method"], [3633, 3634, "Method"], [3864, 3865, "Method"], [3883, 3884, "Method"], [3934, 3935, "Method"], [3943, 3944, "Metric"], [4013, 4014, "Metric"], [4049, 4050, "Material"], [4083, 4084, "Method"], [4122, 4123, "Metric"], [4171, 4173, "Method"], [4205, 4206, "Method"], [4364, 4365, "Method"], [4766, 4767, "Method"], [4826, 4827, "Method"], [4968, 4969, "Method"], [5377, 5378, "Method"], [5657, 5658, "Method"], [5857, 5859, "Method"], [5893, 5895, "Method"], [6153, 6155, "Method"], [6183, 6184, "Method"], [6214, 6215, "Method"], [6314, 6315, "Method"], [6341, 6342, "Method"], [6357, 6358, "Method"], [6415, 6416, "Method"], [6469, 6470, "Method"], [6510, 6511, "Material"], [6581, 6583, "Method"], [6610, 6611, "Method"], [6628, 6629, "Method"], [6672, 6673, "Method"]], "sections": [[0, 141], [141, 693], [693, 915], [915, 1158], [1158, 1203], [1203, 1264], [1264, 1425], [1425, 1497], [1497, 1622], [1622, 2024], [2024, 2261], [2261, 2691], [2691, 2866], [2866, 2869], [2869, 3374], [3374, 3595], [3595, 4199], [4199, 4203], [4203, 4293], [4293, 4402], [4402, 4931], [4931, 5040], [5040, 5064], [5064, 5067], [5067, 5073], [5073, 5168], [5168, 5257], [5257, 5340], [5340, 5348], [5348, 5382], [5382, 5458], [5458, 5462], [5462, 5848], [5848, 6181], [6181, 6585], [6585, 6863]], "sentences": [[0, 14], [14, 50], [50, 81], [81, 117], [117, 141], [141, 144], [144, 180], [180, 218], [218, 244], [244, 276], [276, 314], [314, 333], [333, 376], [376, 400], [400, 425], [425, 452], [452, 479], [479, 486], [486, 499], [499, 529], [529, 545], [545, 576], [576, 587], [587, 627], [627, 645], [645, 652], [652, 669], [669, 693], [693, 700], [700, 731], [731, 765], [765, 782], [782, 812], [812, 825], [825, 836], [836, 857], [857, 871], [871, 882], [882, 898], [898, 915], [915, 918], [918, 948], [948, 970], [970, 976], [976, 989], [989, 997], [997, 1039], [1039, 1059], [1059, 1082], [1082, 1107], [1107, 1125], [1125, 1132], [1132, 1158], [1158, 1161], [1161, 1175], [1175, 1180], [1180, 1203], [1203, 1206], [1206, 1225], [1225, 1248], [1248, 1264], [1264, 1267], [1267, 1294], [1294, 1316], [1316, 1332], [1332, 1358], [1358, 1375], [1375, 1388], [1388, 1407], [1407, 1425], [1425, 1428], [1428, 1472], [1472, 1497], [1497, 1500], [1500, 1504], [1504, 1543], [1543, 1568], [1568, 1580], [1580, 1598], [1598, 1622], [1622, 1632], [1632, 1651], [1651, 1679], [1679, 1702], [1702, 1707], [1707, 1708], [1708, 1719], [1719, 1724], [1724, 1725], [1725, 1749], [1749, 1765], [1765, 1808], [1808, 1837], [1837, 1870], [1870, 1897], [1897, 1921], [1921, 1946], [1946, 1985], [1985, 2018], [2018, 2024], [2024, 2029], [2029, 2037], [2037, 2039], [2039, 2044], [2044, 2070], [2070, 2092], [2092, 2119], [2119, 2140], [2140, 2157], [2157, 2171], [2171, 2182], [2182, 2194], [2194, 2224], [2224, 2237], [2237, 2244], [2244, 2261], [2261, 2273], [2273, 2295], [2295, 2325], [2325, 2348], [2348, 2381], [2381, 2399], [2399, 2422], [2422, 2441], [2441, 2456], [2456, 2474], [2474, 2498], [2498, 2515], [2515, 2523], [2523, 2533], [2533, 2547], [2547, 2563], [2563, 2596], [2596, 2632], [2632, 2670], [2670, 2691], [2691, 2702], [2702, 2730], [2730, 2780], [2780, 2800], [2800, 2815], [2815, 2850], [2850, 2866], [2866, 2869], [2869, 2873], [2873, 2910], [2910, 2918], [2918, 2921], [2921, 2926], [2926, 2942], [2942, 2975], [2975, 3004], [3004, 3023], [3023, 3045], [3045, 3062], [3062, 3089], [3089, 3119], [3119, 3142], [3142, 3164], [3164, 3189], [3189, 3217], [3217, 3239], [3239, 3275], [3275, 3288], [3288, 3306], [3306, 3325], [3325, 3334], [3334, 3357], [3357, 3374], [3374, 3378], [3378, 3418], [3418, 3450], [3450, 3483], [3483, 3504], [3504, 3514], [3514, 3539], [3539, 3563], [3563, 3595], [3595, 3602], [3602, 3653], [3653, 3665], [3665, 3689], [3689, 3713], [3713, 3735], [3735, 3754], [3754, 3772], [3772, 3791], [3791, 3802], [3802, 3826], [3826, 3849], [3849, 3865], [3865, 3907], [3907, 3947], [3947, 3955], [3955, 3975], [3975, 4016], [4016, 4046], [4046, 4069], [4069, 4106], [4106, 4115], [4115, 4138], [4138, 4150], [4150, 4182], [4182, 4199], [4199, 4203], [4203, 4208], [4208, 4226], [4226, 4249], [4249, 4266], [4266, 4271], [4271, 4293], [4293, 4297], [4297, 4319], [4319, 4336], [4336, 4355], [4355, 4381], [4381, 4402], [4402, 4406], [4406, 4424], [4424, 4446], [4446, 4474], [4474, 4490], [4490, 4511], [4511, 4527], [4527, 4544], [4544, 4578], [4578, 4586], [4586, 4609], [4609, 4622], [4622, 4642], [4642, 4673], [4673, 4703], [4703, 4722], [4722, 4754], [4754, 4783], [4783, 4809], [4809, 4848], [4848, 4869], [4869, 4896], [4896, 4931], [4931, 4934], [4934, 4966], [4966, 4997], [4997, 5040], [5040, 5043], [5043, 5064], [5064, 5067], [5067, 5070], [5070, 5073], [5073, 5077], [5077, 5094], [5094, 5108], [5108, 5115], [5115, 5119], [5119, 5151], [5151, 5164], [5164, 5168], [5168, 5172], [5172, 5190], [5190, 5206], [5206, 5216], [5216, 5243], [5243, 5257], [5257, 5261], [5261, 5281], [5281, 5292], [5292, 5302], [5302, 5317], [5317, 5329], [5329, 5339], [5339, 5340], [5340, 5348], [5348, 5353], [5353, 5368], [5368, 5382], [5382, 5387], [5387, 5399], [5399, 5422], [5422, 5450], [5450, 5458], [5458, 5462], [5462, 5474], [5474, 5489], [5489, 5518], [5518, 5541], [5541, 5562], [5562, 5576], [5576, 5597], [5597, 5626], [5626, 5649], [5649, 5675], [5675, 5694], [5694, 5716], [5716, 5744], [5744, 5789], [5789, 5823], [5823, 5848], [5848, 5859], [5859, 5888], [5888, 5935], [5935, 5958], [5958, 5972], [5972, 5999], [5999, 6026], [6026, 6041], [6041, 6058], [6058, 6082], [6082, 6107], [6107, 6117], [6117, 6145], [6145, 6181], [6181, 6186], [6186, 6195], [6195, 6203], [6203, 6223], [6223, 6271], [6271, 6285], [6285, 6299], [6299, 6322], [6322, 6327], [6327, 6344], [6344, 6345], [6345, 6346], [6346, 6347], [6347, 6380], [6380, 6436], [6436, 6470], [6470, 6494], [6494, 6517], [6517, 6536], [6536, 6564], [6564, 6585], [6585, 6589], [6589, 6640], [6640, 6655], [6655, 6666], [6666, 6667], [6667, 6690], [6690, 6714], [6714, 6736], [6736, 6760], [6760, 6761], [6761, 6766], [6766, 6778], [6778, 6779], [6779, 6798], [6798, 6814], [6814, 6846], [6846, 6863]], "words": ["document", ":", "Breaking", "the", "Softmax", "Bottleneck", ":", "A", "High", "-", "Rank", "RNN", "Language", "Model", "We", "formulate", "language", "modeling", "as", "a", "matrix", "factorization", "problem", ",", "and", "show", "that", "the", "expressiveness", "of", "Softmax", "-", "based", "models", "(", "including", "the", "majority", "of", "neural", "language", "models", ")", "is", "limited", "by", "a", "Softmax", "bottleneck", ".", "Given", "that", "natural", "language", "is", "highly", "context", "-", "dependent", ",", "this", "further", "implies", "that", "in", "practice", "Softmax", "with", "distributed", "word", "embeddings", "does", "not", "have", "enough", "capacity", "to", "model", "natural", "language", ".", "We", "propose", "a", "simple", "and", "effective", "method", "to", "address", "this", "issue", ",", "and", "improve", "the", "state", "-", "of", "-", "the", "-", "art", "perplexities", "on", "Penn", "Treebank", "and", "WikiText", "-", "2", "to", "47.69", "and", "40.68", "respectively", ".", "The", "proposed", "method", "also", "excels", "on", "the", "large", "-", "scale", "1B", "Word", "dataset", ",", "outperforming", "the", "baseline", "by", "over", "5.6", "points", "in", "perplexity", ".", "section", ":", "Introduction", "As", "a", "fundamental", "task", "in", "natural", "language", "processing", ",", "statistical", "language", "modeling", "has", "gone", "through", "significant", "development", "from", "traditional", "Ngram", "language", "models", "to", "neural", "language", "models", "in", "the", "last", "decade", "bengio2003neural", ",", "mnih2007three", ",", "mikolov2010recurrent", ".", "Despite", "the", "huge", "variety", "of", "models", ",", "as", "a", "density", "estimation", "problem", ",", "language", "modeling", "mostly", "relies", "on", "a", "universal", "auto", "-", "regressive", "factorization", "of", "the", "joint", "probability", "and", "then", "models", "each", "conditional", "factor", "using", "different", "approaches", ".", "Specifically", ",", "given", "a", "corpus", "of", "tokens", ",", "the", "joint", "probability", "factorizes", "as", "where", "is", "referred", "to", "as", "the", "context", "of", "the", "conditional", "probability", "hereafter", ".", "Based", "on", "the", "factorization", ",", "recurrent", "neural", "networks", "(", "RNN", ")", "based", "language", "models", "achieve", "state", "-", "of", "-", "the", "-", "art", "results", "on", "various", "benchmarks", "merity2017regularizing", ",", "melis2017state", ",", "krause2017dynamic", ".", "A", "standard", "approach", "is", "to", "use", "a", "recurrent", "network", "to", "encode", "the", "context", "into", "a", "fixed", "size", "vector", ",", "which", "is", "then", "multiplied", "by", "the", "word", "embeddings", "inan2016tying", ",", "press2017using", "using", "dot", "product", "to", "obtain", "the", "logits", ".", "The", "logits", "are", "consumed", "by", "the", "Softmax", "function", "to", "give", "a", "categorical", "probability", "distribution", "over", "the", "next", "token", ".", "In", "spite", "of", "the", "expressiveness", "of", "RNNs", "as", "universal", "approximators", "schafer2006recurrent", ",", "an", "unclear", "question", "is", "whether", "the", "combination", "of", "dot", "product", "and", "Softmax", "is", "capable", "of", "modeling", "the", "conditional", "probability", ",", "which", "can", "vary", "dramatically", "with", "the", "change", "of", "the", "context", ".", "In", "this", "work", ",", "we", "study", "the", "expressiveness", "of", "the", "aforementioned", "Softmax", "-", "based", "recurrent", "language", "models", "from", "a", "perspective", "of", "matrix", "factorization", ".", "We", "show", "that", "learning", "a", "Softmax", "-", "based", "recurrent", "language", "model", "with", "the", "standard", "formulation", "is", "essentially", "equivalent", "to", "solving", "a", "matrix", "factorization", "problem", ".", "More", "importantly", ",", "due", "to", "the", "fact", "that", "natural", "language", "is", "highly", "context", "-", "dependent", ",", "the", "matrix", "to", "be", "factorized", "can", "be", "high", "-", "rank", ".", "This", "further", "implies", "that", "standard", "Softmax", "-", "based", "language", "models", "with", "distributed", "(", "output", ")", "word", "embeddings", "do", "not", "have", "enough", "capacity", "to", "model", "natural", "language", ".", "We", "call", "this", "the", "Softmax", "bottleneck", ".", "We", "propose", "a", "simple", "and", "effective", "method", "to", "address", "the", "Softmax", "bottleneck", ".", "Specifically", ",", "we", "introduce", "discrete", "latent", "variables", "into", "a", "recurrent", "language", "model", ",", "and", "formulate", "the", "next", "-", "token", "probability", "distribution", "as", "a", "Mixture", "of", "Softmaxes", "(", "MoS", ")", ".", "Mixture", "of", "Softmaxes", "is", "more", "expressive", "than", "Softmax", "and", "other", "surrogates", "considered", "in", "prior", "work", ".", "Moreover", ",", "we", "show", "that", "MoS", "learns", "matrices", "that", "have", "much", "larger", "normalized", "singular", "values", "and", "thus", "much", "higher", "rank", "than", "Softmax", "and", "other", "baselines", "on", "real", "-", "world", "datasets", ".", "We", "evaluate", "our", "proposed", "approach", "on", "standard", "language", "modeling", "benchmarks", ".", "MoS", "substantially", "improves", "over", "the", "current", "state", "-", "of", "-", "the", "-", "art", "results", "on", "benchmarks", ",", "by", "up", "to", "3.6", "points", "in", "terms", "of", "perplexity", ",", "reaching", "perplexities", "47.69", "on", "Penn", "Treebank", "and", "40.68", "on", "WikiText", "-", "2", ".", "We", "further", "apply", "MoS", "to", "a", "dialog", "dataset", "and", "show", "improved", "performance", "over", "Softmax", "and", "other", "baselines", ".", "Our", "contribution", "is", "two", "-", "fold", ".", "First", ",", "we", "identify", "the", "Softmax", "bottleneck", "by", "formulating", "language", "modeling", "as", "a", "matrix", "factorization", "problem", ".", "Second", ",", "we", "propose", "a", "simple", "and", "effective", "method", "that", "substantially", "improves", "over", "the", "current", "state", "-", "of", "-", "the", "-", "art", "results", ".", "section", ":", "Language", "Modeling", "as", "Matrix", "Factorization", "As", "discussed", "in", "Section", "[", "reference", "]", ",", "with", "the", "autoregressive", "factorization", ",", "language", "modeling", "can", "be", "reduced", "to", "modeling", "the", "conditional", "distribution", "of", "the", "next", "token", "given", "the", "context", ".", "Though", "one", "might", "argue", "that", "a", "natural", "language", "allows", "an", "infinite", "number", "of", "contexts", "due", "to", "its", "compositionality", "pinker1994language", ",", "we", "proceed", "with", "our", "analysis", "by", "considering", "a", "finite", "set", "of", "possible", "contexts", ".", "The", "unboundedness", "of", "natural", "language", "does", "not", "affect", "our", "conclusions", ",", "which", "will", "be", "discussed", "later", ".", "We", "consider", "a", "natural", "language", "as", "a", "finite", "set", "of", "pairs", "of", "a", "context", "and", "its", "conditional", "next", "-", "token", "distribution", ",", "where", "is", "the", "number", "of", "possible", "contexts", ".", "We", "assume", "everywhere", "to", "account", "for", "errors", "and", "flexibility", "in", "natural", "language", ".", "Let", "denote", "a", "set", "of", "possible", "tokens", "in", "the", "language", ".", "The", "objective", "of", "a", "language", "model", "is", "to", "learn", "a", "model", "distribution", "parameterized", "by", "to", "match", "the", "true", "data", "distribution", ".", "In", "this", "work", ",", "we", "study", "the", "expressiveness", "of", "the", "parametric", "model", "class", ".", "In", "other", "words", ",", "we", "are", "asking", "the", "following", "question", ":", "given", "a", "natural", "language", ",", "does", "there", "exist", "a", "parameter", "such", "that", "for", "all", "in", "?", "We", "start", "by", "looking", "at", "a", "Softmax", "-", "based", "model", "class", "since", "it", "is", "widely", "used", ".", "subsection", ":", "Softmax", "The", "majority", "of", "parametric", "language", "models", "use", "a", "Softmax", "function", "operating", "on", "a", "context", "vector", "(", "or", "hidden", "state", ")", "and", "a", "word", "embedding", "to", "define", "the", "conditional", "distribution", ".", "More", "specifically", ",", "the", "model", "distribution", "is", "usually", "written", "as", "where", "is", "a", "function", "of", ",", "and", "is", "a", "function", "of", ".", "Both", "functions", "are", "parameterized", "by", ".", "Both", "the", "context", "vector", "and", "the", "word", "embedding", "have", "the", "same", "dimension", ".", "The", "dot", "product", "is", "called", "a", "logit", ".", "To", "help", "discuss", "the", "expressiveness", "of", "Softmax", ",", "we", "define", "three", "matrices", ":", "where", ",", ",", ",", "and", "the", "rows", "of", ",", ",", "and", "correspond", "to", "context", "vectors", ",", "word", "embeddings", ",", "and", "log", "probabilities", "of", "the", "true", "data", "distribution", "respectively", ".", "We", "use", "the", "subscript", "because", "is", "effectively", "a", "function", "indexed", "by", "the", "parameter", ",", "from", "the", "joint", "function", "family", ".", "Concretely", ",", "is", "implemented", "as", "deep", "neural", "networks", ",", "such", "as", "a", "recurrent", "network", ",", "while", "is", "instantiated", "as", "an", "embedding", "lookup", ".", "We", "further", "specify", "a", "set", "of", "matrices", "formed", "by", "applying", "row", "-", "wise", "shift", "to", "where", "is", "an", "all", "-", "ones", "matrix", "with", "size", ".", "Essentially", ",", "the", "row", "-", "wise", "shift", "operation", "adds", "an", "arbitrary", "real", "number", "to", "each", "row", "of", ".", "Thus", ",", "is", "an", "infinite", "set", ".", "Notably", ",", "the", "set", "has", "two", "important", "properties", "(", "see", "Appendix", "[", "reference", "]", "for", "the", "proof", ")", ",", "which", "are", "key", "to", "our", "analysis", ".", "theorem", ":", ".", "For", "any", "matrix", "A\u2032", ",", "\u2208A\u2032\u2062F", "(", "A", ")", "if", "and", "only", "if", "=", "\u2062Softmax", "(", "A\u2032", ")", "P*.", "In", "other", "words", ",", "\u2062F", "(", "A", ")", "defines", "the", "set", "of", "all", "possible", "logits", "that", "correspond", "to", "the", "true", "data", "distribution", ".", "theorem", ":", ".", "For", "any", "A1\u2260A2\u2208\u2062F", "(", "A", ")", ",", "\u2264|", "-", "\u2062rank", "(", "A1", ")", "\u2062rank", "(", "A2", ")", "|1", ".", "In", "other", "words", ",", "all", "matrices", "in", "\u2062F", "(", "A", ")", "have", "similar", "ranks", ",", "with", "the", "maximum", "rank", "difference", "being", "1", ".", "Based", "on", "the", "Property", "[", "reference", "]", "of", ",", "we", "immediately", "have", "the", "following", "Lemma", ".", "theorem", ":", ".", "Given", "a", "model", "parameter", "\u03b8", ",", "\u2208\u2062H\u03b8W\u22a4\u03b8\u2062F", "(", "A", ")", "if", "and", "only", "if", "P\u03b8", "(", "X|c", ")", "=P*", "(", "X|c", ")", "for", "all", "c", "in", "L.", "Now", "the", "expressiveness", "question", "becomes", ":", "does", "there", "exist", "a", "parameter", "and", "such", "that", "This", "is", "essentially", "a", "matrix", "factorization", "problem", ".", "We", "want", "the", "model", "to", "learn", "matrices", "and", "that", "are", "able", "to", "factorize", "some", "matrix", ".", "First", ",", "note", "that", "for", "a", "valid", "factorization", "to", "exist", ",", "the", "rank", "of", "has", "to", "be", "at", "least", "as", "large", "as", "the", "rank", "of", ".", "Further", ",", "since", "and", ",", "the", "rank", "of", "is", "strictly", "upper", "bounded", "by", "the", "embedding", "size", ".", "As", "a", "result", ",", "if", ",", "a", "universal", "approximator", "can", "theoretically", "recover", ".", "However", ",", "if", ",", "no", "matter", "how", "expressive", "the", "function", "family", "is", ",", "no", "can", "even", "theoretically", "recover", ".", "We", "summarize", "the", "reasoning", "above", "as", "follows", "(", "see", "Appendix", "[", "reference", "]", "for", "the", "proof", ")", ".", "theorem", ":", ".", "Given", "that", "the", "function", "family", "U", "is", "a", "universal", "approximator", ",", "there", "exists", "a", "parameter", "\u03b8", "such", "that", "P\u03b8", "(", "X|c", ")", "=P*", "(", "X|c", ")", "for", "all", "c", "in", "L", "if", "and", "only", "if", "\u2265d\u2062min\u2208A\u2032\u2062F", "(", "A", ")", "rank", "(", "A\u2032", ")", ".", "Combining", "Proposition", "[", "reference", "]", "with", "the", "Property", "[", "reference", "]", "of", ",", "we", "are", "now", "able", "to", "state", "the", "Softmax", "Bottleneck", "problem", "formally", ".", "theorem", ":", ".", "(", "Softmax", "Bottleneck", ")", "If", "<", "d", "-", "\u2062rank", "(", "A", ")", "1", ",", "for", "any", "function", "family", "U", "and", "any", "model", "parameter", "\u03b8", ",", "there", "exists", "a", "context", "c", "in", "L", "such", "that", "P\u03b8", "(", "X|c", ")", "\u2260P*", "(", "X|c", ")", ".", "The", "above", "corollary", "indicates", "that", "when", "the", "dimension", "is", "too", "small", ",", "Softmax", "does", "not", "have", "the", "capacity", "to", "express", "the", "true", "data", "distribution", ".", "Clearly", ",", "this", "conclusion", "is", "not", "restricted", "to", "a", "finite", "language", ".", "When", "is", "infinite", ",", "one", "can", "always", "take", "a", "finite", "subset", "and", "the", "Softmax", "bottleneck", "still", "exists", ".", "Next", ",", "we", "discuss", "why", "the", "Softmax", "bottleneck", "is", "an", "issue", "by", "presenting", "our", "hypothesis", "that", "is", "high", "-", "rank", "for", "natural", "language", ".", "subsection", ":", "Hypothesis", ":", "Natural", "Language", "is", "High", "-", "Rank", "We", "hypothesize", "that", "for", "a", "natural", "language", ",", "the", "log", "probability", "matrix", "is", "a", "high", "-", "rank", "matrix", ".", "It", "is", "difficult", "(", "if", "possible", ")", "to", "rigorously", "prove", "this", "hypothesis", "since", "we", "do", "not", "have", "access", "to", "the", "true", "data", "distribution", "of", "a", "natural", "language", ".", "However", ",", "it", "is", "suggested", "by", "the", "following", "intuitive", "reasoning", "and", "empirical", "observations", ":", "Natural", "language", "is", "highly", "context", "-", "dependent", "mikolov2012context", ".", "For", "example", ",", "the", "token", "\u2018", "\u2018", "north", "\u2019", "\u2019", "is", "likely", "to", "be", "followed", "by", "\u2018", "\u2018", "korea", "\u2019", "\u2019", "or", "\u2018", "\u2018", "korean", "\u2019", "\u2019", "in", "a", "news", "article", "on", "international", "politics", ",", "which", "however", "is", "unlikely", "in", "a", "textbook", "on", "U.S.", "domestic", "history", ".", "We", "hypothesize", "that", "such", "subtle", "context", "dependency", "should", "result", "in", "a", "high", "-", "rank", "matrix", ".", "If", "is", "low", "-", "rank", ",", "it", "means", "humans", "only", "need", "a", "limited", "number", "(", "e.g.", "a", "few", "hundred", ")", "of", "bases", ",", "and", "all", "semantic", "meanings", "can", "be", "created", "by", "(", "potentially", ")", "negating", "and", "(", "weighted", ")", "averaging", "these", "bases", ".", "However", ",", "it", "is", "hard", "to", "find", "a", "natural", "concept", "in", "linguistics", "and", "cognitive", "science", "that", "corresponds", "to", "such", "bases", ",", "which", "questions", "the", "existence", "of", "such", "bases", ".", "For", "example", ",", "semantic", "meanings", "might", "not", "be", "those", "bases", "since", "a", "few", "hundred", "meanings", "may", "not", "be", "enough", "to", "cover", "everyday", "meanings", ",", "not", "to", "mention", "niche", "meanings", "in", "specialized", "domains", ".", "Empirically", ",", "our", "high", "-", "rank", "language", "model", "outperforms", "conventional", "low", "-", "rank", "language", "models", "on", "several", "benchmarks", ",", "as", "shown", "in", "Section", "[", "reference", "]", ".", "We", "also", "provide", "evidences", "in", "Section", "[", "reference", "]", "to", "support", "our", "hypothesis", "that", "learning", "a", "high", "-", "rank", "language", "model", "is", "important", ".", "Given", "the", "hypothesis", "that", "natural", "language", "is", "high", "-", "rank", ",", "it", "is", "clear", "that", "the", "Softmax", "bottleneck", "limits", "the", "expressiveness", "of", "the", "models", ".", "In", "practice", ",", "the", "embedding", "dimension", "is", "usually", "set", "at", "the", "scale", "of", ",", "while", "the", "rank", "of", "can", "possibly", "be", "as", "high", "as", "(", "at", "the", "scale", "of", ")", ",", "which", "is", "orders", "of", "magnitude", "larger", "than", ".", "Softmax", "is", "effectively", "learning", "a", "low", "-", "rank", "approximation", "to", ",", "and", "our", "experiments", "suggest", "that", "such", "approximation", "loses", "the", "ability", "to", "model", "context", "dependency", ",", "both", "qualitatively", "and", "quantitatively", "(", "Cf", ".", "Section", "[", "reference", "]", ")", ".", "subsection", ":", "Easy", "Fixes", "?", "Identifying", "the", "Softmax", "bottleneck", "immediately", "suggests", "some", "possible", "\u2018", "\u2018", "easy", "fixes", "\u2019", "\u2019", ".", "First", ",", "as", "considered", "by", "a", "lot", "of", "prior", "work", ",", "one", "can", "employ", "a", "non", "-", "parametric", "model", ",", "namely", "an", "Ngram", "model", "kneser1995improved", ".", "Ngram", "models", "are", "not", "constrained", "by", "any", "parametric", "forms", "so", "it", "can", "universally", "approximate", "any", "natural", "language", ",", "given", "enough", "parameters", ".", "Second", ",", "it", "is", "possible", "to", "increase", "the", "dimension", "(", "e.g.", ",", "to", "match", ")", "so", "that", "the", "model", "can", "express", "a", "high", "-", "rank", "matrix", ".", "However", ",", "these", "two", "methods", "increase", "the", "number", "of", "parameters", "dramatically", ",", "compared", "to", "using", "a", "low", "-", "dimensional", "Softmax", ".", "More", "specifically", ",", "an", "Ngram", "needs", "parameters", "in", "order", "to", "express", ",", "where", "is", "potentially", "unbounded", ".", "Similarly", ",", "a", "high", "-", "dimensional", "Softmax", "requires", "parameters", "for", "the", "word", "embeddings", ".", "Increasing", "the", "number", "of", "model", "parameters", "easily", "leads", "to", "overfitting", ".", "In", "past", "work", ",", "used", "back", "-", "off", "to", "alleviate", "overfitting", ".", "Moreover", ",", "as", "deep", "learning", "models", "were", "tuned", "by", "extensive", "hyper", "-", "parameter", "search", ",", "increasing", "the", "dimension", "beyond", "several", "hundred", "is", "not", "helpful", "merity2017regularizing", ",", "melis2017state", ",", "krause2017dynamic", ".", "Clearly", "there", "is", "a", "tradeoff", "between", "expressiveness", "and", "generalization", "on", "language", "modeling", ".", "Naively", "increasing", "the", "expressiveness", "hurts", "generalization", ".", "Below", ",", "we", "introduce", "an", "alternative", "approach", "that", "increases", "the", "expressiveness", "without", "exploding", "the", "parametric", "space", ".", "subsection", ":", "Mixture", "of", "Softmaxes", ":", "A", "High", "-", "Rank", "Language", "Model", "We", "propose", "a", "high", "-", "rank", "language", "model", "called", "Mixture", "of", "Softmaxes", "(", "MoS", ")", "to", "alleviate", "the", "Softmax", "bottleneck", "issue", ".", "MoS", "formulates", "the", "conditional", "distribution", "as", "where", "is", "the", "prior", "or", "mixture", "weight", "of", "the", "-", "th", "component", ",", "and", "is", "the", "-", "th", "context", "vector", "associated", "with", "context", ".", "In", "other", "words", ",", "MoS", "computes", "Softmax", "distributions", "and", "uses", "a", "weighted", "average", "of", "them", "as", "the", "next", "-", "token", "probability", "distribution", ".", "Similar", "to", "prior", "work", "on", "recurrent", "language", "modeling", "merity2017regularizing", ",", "melis2017state", ",", "krause2017dynamic", ",", "we", "first", "apply", "a", "stack", "of", "recurrent", "layers", "on", "top", "of", "to", "obtain", "a", "sequence", "of", "hidden", "states", ".", "The", "prior", "and", "the", "context", "vector", "for", "context", "are", "parameterized", "as", "and", "where", "and", "are", "model", "parameters", ".", "Our", "method", "is", "simple", "and", "easy", "to", "implement", ",", "and", "has", "the", "following", "advantages", ":", "Improved", "expressiveness", "(", "compared", "to", "Softmax", ")", ".", "MoS", "is", "theoretically", "more", "(", "or", "at", "least", "equally", ")", "expressive", "compared", "to", "Softmax", "given", "the", "same", "dimension", ".", "This", "can", "be", "seen", "by", "the", "fact", "that", "MoS", "with", "is", "reduced", "to", "Softmax", ".", "More", "importantly", ",", "MoS", "effectively", "approximates", "by", "where", "is", "an", "diagonal", "matrix", "with", "elements", "being", "the", "prior", ".", "Because", "is", "a", "nonlinear", "function", "(", "log_sum_exp", ")", "of", "the", "context", "vectors", "and", "the", "word", "embeddings", ",", "can", "be", "arbitrarily", "high", "-", "rank", ".", "As", "a", "result", ",", "MoS", "does", "not", "suffer", "from", "the", "rank", "limitation", ",", "compared", "to", "Softmax", ".", "Improved", "generalization", "(", "compared", "to", "Ngram", ")", ".", "Ngram", "models", "and", "high", "-", "dimensional", "Softmax", "(", "Cf", ".", "Section", "[", "reference", "]", ")", "improve", "the", "expressiveness", "but", "do", "not", "generalize", "well", ".", "In", "contrast", ",", "MoS", "does", "not", "have", "a", "generalization", "issue", "due", "to", "the", "following", "reasons", ".", "First", ",", "MoS", "defines", "the", "following", "generative", "process", ":", "a", "discrete", "latent", "variable", "is", "first", "sampled", "from", ",", "and", "then", "the", "next", "token", "is", "sampled", "based", "on", "the", "-", "th", "Softmax", "component", ".", "By", "doing", "so", "we", "introduce", "an", "inductive", "bias", "that", "the", "next", "token", "is", "generated", "based", "on", "a", "latent", "discrete", "decision", "(", "e.g.", ",", "a", "topic", ")", ",", "which", "is", "often", "safe", "in", "language", "modeling", "blei2003latent", ".", "Second", ",", "since", "is", "defined", "by", "a", "nonlinear", "function", "and", "not", "restricted", "by", "the", "rank", "bottleneck", ",", "in", "practice", "it", "is", "possible", "to", "reduce", "to", "compensate", "for", "the", "increase", "of", "model", "parameters", "brought", "by", "the", "mixture", "structure", ".", "As", "a", "result", ",", "MoS", "has", "a", "similar", "model", "size", "compared", "to", "Softmax", "and", "thus", "is", "not", "prone", "to", "overfitting", ".", "subsection", ":", "Mixture", "of", "Contexts", ":", "A", "Low", "-", "Rank", "Baseline", "Another", "possible", "approach", "is", "to", "directly", "mix", "the", "context", "vectors", "(", "or", "logits", ")", "before", "taking", "the", "Softmax", ",", "rather", "than", "mixing", "the", "probabilities", "afterwards", "as", "in", "MoS.", "Specifically", ",", "the", "conditional", "distribution", "is", "parameterized", "as", "where", "and", "share", "the", "same", "parameterization", "as", "in", "MoS.", "Despite", "its", "superficial", "similarity", "to", "MoS", ",", "this", "model", ",", "which", "we", "refer", "to", "as", "mixture", "of", "contexts", "(", "MoC", ")", ",", "actually", "suffers", "from", "the", "same", "rank", "limitation", "problem", "as", "Softmax", ".", "This", "can", "be", "easily", "seen", "by", "defining", ",", "which", "turns", "the", "MoC", "parameterization", "(", "[", "reference", "]", ")", "into", ".", "Note", "that", "this", "is", "equivalent", "to", "the", "Softmax", "parameterization", "(", "[", "reference", "]", ")", ".", "Thus", ",", "performing", "mixture", "in", "the", "feature", "space", "can", "only", "make", "the", "function", "family", "more", "expressive", ",", "but", "does", "not", "change", "the", "fact", "that", "the", "rank", "of", "is", "upper", "bounded", "by", "the", "embedding", "dimension", ".", "In", "our", "experiments", ",", "we", "implement", "MoC", "as", "a", "baseline", "and", "compare", "it", "experimentally", "to", "MoS.", "section", ":", "Experiments", "subsection", ":", "Main", "Results", "We", "conduct", "a", "series", "of", "experiments", "with", "the", "following", "settings", ":", "Following", "previous", "work", "krause2017dynamic", ",", "merity2017regularizing", ",", "melis2017state", ",", "we", "evaluate", "the", "proposed", "MoS", "model", "on", "two", "widely", "used", "language", "modeling", "datasets", ",", "namely", "Penn", "Treebank", "(", "PTB", ")", "mikolov2010recurrent", "and", "WikiText", "-", "2", "(", "WT2", ")", "merity2016pointer", "based", "on", "perplexity", ".", "For", "fair", "comparison", ",", "we", "closely", "follow", "the", "regularization", "and", "optimization", "techniques", "introduced", "by", "merity2017regularizing", ".", "We", "heuristically", "and", "manually", "search", "hyper", "-", "parameters", "for", "MoS", "based", "on", "the", "validation", "performance", "while", "limiting", "the", "model", "size", "(", "see", "Appendix", "[", "reference", "]", "for", "our", "hyper", "-", "parameters", ")", ".", "To", "investigate", "whether", "the", "effectiveness", "of", "MoS", "can", "be", "extended", "to", "even", "larger", "datasets", ",", "we", "conduct", "an", "additional", "language", "modeling", "experiment", "on", "the", "1B", "Word", "dataset", "chelba2013one", ".", "Specifically", ",", "we", "lower", "-", "case", "the", "text", "and", "choose", "the", "top", "100", "K", "tokens", "as", "the", "vocabulary", ".", "A", "standard", "neural", "language", "model", "with", "2", "layers", "of", "LSTMs", "followed", "by", "a", "Softmax", "output", "layer", "is", "used", "as", "the", "baseline", ".", "Again", ",", "the", "network", "size", "of", "MoS", "is", "adjusted", "to", "ensure", "a", "comparable", "number", "of", "parameters", ".", "Notably", ",", "dropout", "was", "not", "used", ",", "since", "we", "found", "it", "not", "helpful", "to", "either", "model", "(", "see", "Appendix", "[", "reference", "]", "for", "more", "details", ")", ".", "To", "show", "that", "the", "MoS", "is", "a", "generic", "structure", "that", "can", "be", "used", "to", "model", "other", "context", "-", "dependent", "distributions", ",", "we", "additionally", "conduct", "experiments", "in", "the", "dialog", "domain", ".", "We", "use", "the", "Switchboard", "dataset", "godfrey1997switchboard", "preprocessed", "by", "zhao2017learning", "to", "train", "a", "Seq2Seq", "sutskever2014sequence", "model", "with", "MoS", "added", "to", "the", "decoder", "RNN", ".", "Then", ",", "a", "Seq2Seq", "model", "using", "Softmax", "and", "another", "one", "augmented", "by", "MoC", "with", "comparable", "parameter", "sizes", "are", "used", "as", "baselines", ".", "For", "evaluation", ",", "we", "include", "both", "the", "perplexity", "and", "the", "precision", "/", "recall", "of", "Smoothed", "Sentence", "-", "level", "BLEU", ",", "as", "suggested", "by", "zhao2017learning", ".", "When", "generating", "responses", ",", "we", "use", "beam", "search", "with", "beam", "size", "10", ",", "restrict", "the", "maximum", "length", "to", "30", ",", "and", "retain", "the", "top", "-", "5", "responses", ".", "The", "language", "modeling", "results", "on", "PTB", "and", "WT2", "are", "presented", "in", "Table", "[", "reference", "]", "and", "Table", "[", "reference", "]", "respectively", ".", "With", "a", "comparable", "number", "of", "parameters", ",", "MoS", "outperforms", "all", "baselines", "with", "or", "without", "dynamic", "evaluation", ",", "and", "substantially", "improves", "over", "the", "current", "state", "of", "the", "art", ",", "by", "up", "to", "3.6", "points", "in", "perplexity", ".", "The", "improvement", "on", "the", "large", "-", "scale", "dataset", "is", "even", "more", "significant", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "MoS", "outperforms", "Softmax", "by", "over", "5.6", "points", "in", "perplexity", ".", "It", "suggests", "the", "effectiveness", "of", "MoS", "is", "not", "limited", "to", "small", "datasets", "where", "many", "regularization", "techniques", "are", "used", ".", "Note", "that", "with", "limited", "computational", "resources", ",", "we", "did", "n\u2019t", "tune", "the", "hyper", "-", "parameters", "for", "MoS.", "Further", ",", "the", "experimental", "results", "on", "Switchboard", "are", "summarized", "in", "Table", "[", "reference", "]", ".", "Clearly", ",", "on", "all", "metrics", ",", "MoS", "outperforms", "MoC", "and", "Softmax", ",", "showing", "its", "general", "effectiveness", ".", "subsection", ":", "Ablation", "Study", "To", "further", "verify", "the", "improvement", "shown", "above", "does", "come", "from", "the", "MoS", "structure", "rather", "than", "adding", "another", "hidden", "layer", "or", "finding", "a", "particular", "set", "of", "hyper", "-", "parameters", ",", "we", "conduct", "an", "ablation", "study", "on", "both", "PTB", "and", "WT2", ".", "Firstly", ",", "we", "compare", "MoS", "with", "an", "MoC", "architecture", "with", "the", "same", "number", "of", "layers", ",", "hidden", "sizes", ",", "and", "embedding", "sizes", ",", "which", "thus", "has", "the", "same", "number", "of", "parameters", ".", "In", "addition", ",", "we", "adopt", "the", "hyper", "-", "parameters", "used", "to", "obtain", "the", "best", "MoS", "model", "(", "denoted", "as", "MoS", "hyper", "-", "parameters", ")", ",", "and", "train", "a", "baseline", "AWD", "-", "LSTM", ".", "To", "avoid", "distractive", "factors", "and", "save", "computational", "resources", ",", "all", "ablative", "experiments", "excluded", "the", "use", "of", "finetuing", "and", "dynamic", "evaluation", ".", "The", "results", "are", "shown", "in", "Table", "[", "reference", "]", ".", "Compared", "to", "the", "vanilla", "AWD", "-", "LSTM", ",", "though", "being", "more", "expressive", ",", "MoC", "performs", "only", "better", "on", "PTB", ",", "but", "worse", "on", "WT2", ".", "It", "suggests", "that", "simply", "adding", "another", "hidden", "layer", "or", "employing", "a", "mixture", "structure", "in", "the", "feature", "space", "does", "not", "guarantee", "a", "better", "performance", ".", "On", "the", "other", "hand", ",", "training", "AWD", "-", "LSTM", "using", "MoS", "hyper", "-", "parameters", "severely", "hurts", "the", "performance", ",", "which", "rules", "out", "hyper", "-", "parameters", "as", "the", "main", "source", "of", "improvement", ".", "subsection", ":", "Verify", "the", "Role", "of", "Rank", "While", "the", "study", "above", "verifies", "that", "MoS", "is", "the", "key", "to", "achieving", "the", "state", "-", "of", "-", "the", "-", "art", "performance", ",", "it", "is", "still", "not", "clear", "whether", "the", "superiority", "of", "MoS", "comes", "from", "its", "potential", "high", "rank", ",", "as", "suggested", "by", "our", "theoretical", "analysis", "in", "Section", "[", "reference", "]", ".", "In", "the", "sequel", ",", "we", "take", "steps", "to", "verify", "this", "hypothesis", ".", "Firstly", ",", "we", "verify", "that", "MoS", "does", "induce", "a", "high", "-", "rank", "log", "-", "probability", "matrix", "empirically", ",", "while", "MoC", "and", "Softmax", "fail", ".", "On", "the", "validation", "or", "test", "set", "of", "PTB", "with", "tokens", ",", "we", "compute", "the", "log", "probabilities", "for", "each", "token", "using", "all", "three", "models", ".", "Then", ",", "for", "each", "model", ",", "we", "stack", "all", "log", "-", "probability", "vectors", "into", "a", "matrix", ",", "resulting", "in", ",", "and", ".", "Theoretically", ",", "the", "number", "of", "non", "-", "zero", "singular", "values", "of", "a", "matrix", "is", "equal", "to", "its", "rank", ".", "However", ",", "performing", "singular", "value", "decomposition", "of", "real", "valued", "matrices", "using", "numerical", "approaches", "often", "encounter", "roundoff", "errors", ".", "Hence", ",", "we", "adopt", "the", "expected", "roundoff", "error", "suggested", "by", "press2007numerical", "when", "estimating", "the", "ranks", "of", ",", "and", ".", "The", "estimated", "ranks", "are", "shown", "in", "Table", "[", "reference", "]", ".", "As", "predicted", "by", "our", "theoretical", "analysis", ",", "the", "matrix", "ranks", "induced", "by", "Softmax", "and", "MoC", "are", "both", "limited", "by", "the", "corresponding", "embedding", "sizes", ".", "By", "contrast", ",", "the", "matrix", "rank", "obtained", "from", "MoS", "does", "not", "suffer", "from", "this", "constraint", ",", "almost", "reaching", "full", "rank", "(", ")", ".", "In", "appendix", "[", "reference", "]", ",", "we", "give", "additional", "evidences", "for", "the", "higher", "rank", "of", "MoS.", "Secondly", ",", "we", "show", "that", ",", "before", "reaching", "full", "rank", ",", "increasing", "the", "number", "of", "mixture", "components", "in", "MoS", "also", "increases", "the", "rank", "of", "the", "log", "-", "probability", "matrix", ",", "which", "in", "turn", "leads", "to", "improved", "performance", "(", "lower", "perplexity", ")", ".", "Specifically", ",", "on", "PTB", ",", "with", "other", "hyper", "-", "parameters", "fixed", "as", "used", "in", "section", "[", "reference", "]", ",", "we", "vary", "the", "number", "of", "mixtures", "used", "in", "MoS", "and", "compare", "the", "corresponding", "empirical", "rank", "and", "test", "perplexity", "without", "finetuning", ".", "Table", "[", "reference", "]", "summarizes", "the", "results", ".", "This", "clear", "positive", "correlation", "between", "rank", "and", "performance", "strongly", "supports", "the", "our", "theoretical", "analysis", "in", "section", "[", "reference", "]", ".", "Moreover", ",", "note", "that", "after", "reaching", "almost", "full", "rank", "(", "i.e.", ",", "using", "15", "mixture", "components", ")", ",", "further", "increasing", "the", "number", "of", "components", "degrades", "the", "performance", "due", "to", "overfitting", "(", "as", "we", "inspected", "the", "training", "and", "test", "perplexities", ")", ".", "In", "addition", ",", "as", "performance", "improvement", "can", "often", "come", "from", "better", "regularization", ",", "we", "investigate", "whether", "MoS", "has", "a", "better", ",", "though", "unexpected", ",", "regularization", "effect", "compared", "to", "Softmax", ".", "We", "consider", "the", "1B", "word", "dataset", "where", "overfitting", "is", "unlikely", "and", "no", "explicit", "regularization", "technique", "(", "e.g.", ",", "dropout", ")", "is", "employed", ".", "As", "we", "can", "see", "from", "the", "left", "part", "of", "Table", "[", "reference", "]", ",", "MoS", "and", "Softmax", "achieve", "a", "similar", "generalization", "gap", ",", "i.e.", ",", "the", "performance", "gap", "between", "the", "test", "set", "and", "the", "training", "set", ".", "It", "suggests", "both", "models", "have", "similar", "regularization", "effects", ".", "Meanwhile", ",", "MoS", "has", "a", "lower", "training", "perplexity", "compared", "to", "Softmax", ",", "indicating", "that", "the", "improvement", "of", "MoS", "results", "from", "improved", "expressiveness", ".", "The", "last", "evidence", "we", "provide", "is", "based", "on", "an", "inverse", "experiment", ".", "Empirically", ",", "we", "find", "that", "when", "Softmax", "does", "not", "suffer", "from", "a", "rank", "limitation", ",", "e.g.", ",", "in", "character", "-", "level", "language", "modeling", ",", "using", "MoS", "will", "not", "improve", "the", "performance", ".", "Due", "to", "lack", "of", "space", ",", "we", "refer", "readers", "to", "Appendix", "[", "reference", "]", "for", "details", ".", "subsection", ":", "Additional", "analysis", "paragraph", ":", "MoS", "computational", "time", "The", "expressiveness", "of", "MoS", "does", "come", "with", "a", "computational", "cost", "\u2014", "computing", "a", "-", "times", "larger", "Softmax", ".", "To", "give", "readers", "a", "concrete", "idea", "of", "the", "influence", "on", "training", "time", ",", "we", "perform", "detailed", "analysis", "in", "Appendix", "[", "reference", "]", ".", "As", "we", "will", "see", ",", "computational", "wall", "time", "of", "MoS", "is", "actually", "sub", "-", "linear", "w.r.t", ".", "the", "number", "of", "Softmaxes", ".", "In", "most", "settings", ",", "we", "observe", "a", "two", "to", "three", "times", "slowdown", "when", "using", "MoS", "with", "up", "to", "15", "mixture", "components", ".", "paragraph", ":", "Qualitative", "analysis", "Finally", ",", "we", "conduct", "a", "case", "study", "on", "PTB", "to", "see", "how", "MoS", "improves", "the", "next", "-", "token", "prediction", "in", "detail", ".", "Due", "to", "lack", "of", "space", ",", "we", "refer", "readers", "to", "Appendix", "[", "reference", "]", "for", "details", ".", "The", "key", "insight", "from", "the", "case", "study", "is", "that", "MoS", "is", "better", "at", "making", "context", "-", "dependent", "predictions", ".", "Specifically", ",", "given", "the", "same", "immediate", "preceding", "word", ",", "MoS", "will", "produce", "distinct", "next", "-", "step", "prediction", "based", "on", "long", "-", "term", "context", "in", "history", ".", "By", "contrast", ",", "the", "baseline", "often", "yields", "similar", "next", "-", "step", "prediction", ",", "independent", "of", "the", "long", "-", "term", "context", ".", "section", ":", "Related", "work", "In", "language", "modeling", ",", "hutchinson2011low", ",", "hutchinson2012sparse", "have", "previously", "considered", "the", "problem", "from", "a", "matrix", "rank", "perspective", ".", "However", ",", "their", "focus", "was", "to", "improve", "the", "generalization", "of", "Ngram", "language", "models", "via", "a", "sparse", "plus", "low", "-", "rank", "approximation", ".", "By", "contrast", ",", "as", "neural", "language", "models", "already", "generalize", "well", ",", "we", "focus", "on", "a", "high", "-", "rank", "neural", "language", "model", "that", "improves", "expressiveness", "without", "sacrificing", "generalization", ".", "neubig2016generalizing", "proposed", "to", "mix", "Ngram", "and", "neural", "language", "models", "to", "unify", "and", "benefit", "from", "both", ".", "However", ",", "this", "mixture", "might", "not", "generalize", "well", "since", "an", "Ngram", "model", ",", "which", "has", "poor", "generalization", ",", "is", "included", ".", "Moreover", ",", "the", "fact", "that", "the", "two", "components", "are", "separately", "trained", "can", "limit", "its", "expressiveness", ".", "levy2014neural", "also", "considered", "the", "matrix", "factorization", "perspective", ",", "but", "in", "the", "context", "of", "learning", "word", "embeddings", ".", "In", "a", "general", "sense", ",", "Mixture", "of", "Softmaxes", "proposed", "in", "this", "work", "can", "be", "seen", "as", "a", "particular", "instantiation", "of", "the", "long", "-", "existing", "idea", "called", "Mixture", "of", "Experts", "(", "MoE", ")", "jacobs1991adaptive", ".", "However", ",", "there", "are", "two", "core", "differences", ".", "Firstly", ",", "MoE", "has", "usually", "been", "instantiated", "as", "mixture", "of", "Gaussians", "to", "model", "data", "in", "continuous", "domains", "jacobs1991adaptive", ",", "graves2013generating", ",", "bazzani2016recurrent", ".", "More", "importantly", ",", "the", "motivation", "of", "using", "the", "mixture", "structure", "is", "distinct", ".", "For", "Gaussian", "mixture", "models", ",", "the", "mixture", "structure", "is", "employed", "to", "allow", "for", "a", "parameterized", "multi", "-", "modal", "distribution", ".", "By", "contrast", ",", "Softmax", "by", "itself", "can", "parameterize", "a", "multi", "-", "modal", "distribution", ",", "and", "MoS", "is", "introduced", "to", "break", "the", "Softmax", "bottleneck", "as", "discussed", "in", "Section", "[", "reference", "]", ".", "There", "has", "been", "previous", "work", "eigen2013learning", ",", "shazeer2017outrageously", "proposing", "architectures", "that", "can", "be", "categorized", "as", "instantiations", "of", "MoC", ",", "since", "the", "mixture", "structure", "is", "employed", "in", "the", "feature", "space", ".", "The", "target", "of", "eigen2013learning", "is", "to", "create", "a", "more", "expressive", "feed", "-", "forward", "layer", "through", "the", "mixture", "structure", ".", "In", "comparison", ",", "shazeer2017outrageously", "focuses", "on", "a", "sparse", "gating", "mechanism", "also", "on", "the", "feature", "level", ",", "which", "enables", "efficient", "conditional", "computation", "and", "allows", "the", "training", "of", "a", "very", "large", "neural", "architecture", ".", "In", "addition", "to", "having", "different", "motivations", "from", "our", "work", ",", "all", "these", "MoC", "variants", "suffer", "from", "the", "same", "rank", "limitation", "problem", "as", "discussed", "in", "Section", "[", "reference", "]", ".", "Finally", ",", "several", "previous", "works", "have", "tried", "to", "introduce", "latent", "variables", "into", "sequence", "modeling", "bayer2014learning", ",", "gregor2015draw", ",", "chung2015recurrent", ",", "gan2015deep", ",", "fraccaro2016sequential", ",", "chung2016hierarchical", ".", "Except", "for", "chung2016hierarchical", ",", "these", "structures", "all", "define", "a", "continuous", "latent", "variable", "for", "each", "step", "of", "the", "RNN", "computation", ",", "and", "rely", "on", "the", "SGVB", "estimator", "kingma2013auto", "to", "optimize", "a", "variational", "lower", "bound", "of", "the", "log", "-", "likelihood", ".", "Since", "exact", "integration", "is", "infeasible", ",", "these", "models", "can", "not", "estimate", "the", "likelihood", "(", "perplexity", ")", "exactly", "at", "test", "time", ".", "Moreover", ",", "for", "discrete", "data", ",", "the", "variational", "lower", "bound", "is", "usually", "too", "loose", "to", "yield", "a", "competitive", "approximation", "compared", "to", "standard", "auto", "-", "regressive", "models", ".", "As", "an", "exception", ",", "chung2016hierarchical", "utilizes", "Bernoulli", "latent", "variables", "to", "model", "the", "hierarchical", "structure", "in", "language", ",", "where", "the", "Bernoulli", "sampling", "is", "replaced", "by", "a", "thresholding", "operation", "at", "test", "time", "to", "give", "perplexity", "estimation", ".", "section", ":", "Conclusions", "Under", "the", "matrix", "factorization", "framework", ",", "the", "expressiveness", "of", "Softmax", "-", "based", "language", "models", "is", "limited", "by", "the", "dimension", "of", "the", "word", "embeddings", ",", "which", "is", "termed", "as", "the", "Softmax", "bottleneck", ".", "Our", "proposed", "MoS", "model", "improves", "the", "expressiveness", "over", "Softmax", ",", "and", "at", "the", "same", "time", "avoids", "overfitting", "compared", "to", "non", "-", "parametric", "models", "and", "naively", "increasing", "the", "word", "embedding", "dimensions", ".", "Our", "method", "improves", "the", "current", "state", "-", "of", "-", "the", "-", "art", "results", "on", "standard", "benchmarks", "by", "a", "large", "margin", ",", "which", "in", "turn", "justifies", "our", "theoretical", "reasoning", ":", "it", "is", "important", "to", "have", "a", "high", "-", "rank", "model", "for", "natural", "language", ".", "subsubsection", ":", "Acknowledgments", "This", "work", "was", "supported", "by", "the", "DARPA", "award", "D17AP00001", ",", "the", "Google", "focused", "award", ",", "and", "the", "Nvidia", "NVAIL", "award", ".", "bibliography", ":", "References", "appendix", ":", "Proofs", "Proof", "of", "Property", "proof", ":", "Proof", ".", "For", "any", ",", "let", "denote", "the", "distribution", "defined", "by", "applying", "Softmax", "on", "the", "logits", "given", "by", ".", "Consider", "row", "column", ",", "by", "definition", "any", "entry", "in", "can", "be", "expressed", "as", ".", "It", "follows", "For", "any", ",", "for", "any", "and", ",", "we", "have", "It", "follows", "that", "for", "any", ",", ",", "and", ",", "As", "a", "result", ",", "This", "means", "each", "row", "in", "can", "be", "obtained", "by", "adding", "a", "real", "number", "to", "the", "corresponding", "row", "in", ".", "Therefore", ",", "there", "exists", "a", "diagonal", "matrix", "such", "that", "It", "follows", "that", ".", "\u220e", "Proof", "of", "Property", "proof", ":", "Proof", ".", "For", "any", "and", "in", ",", "by", "definition", "we", "have", ",", "and", "where", "and", "are", "two", "diagonal", "matrices", ".", "It", "can", "be", "rewritten", "as", "Let", "be", "a", "maximum", "set", "of", "linearly", "independent", "rows", "in", ".", "Let", "be", "an", "all", "-", "ones", "vector", "with", "dimension", ".", "The", "-", "th", "row", "vector", "in", "can", "be", "written", "as", "Because", "is", "a", "linear", "combination", "of", "vectors", "in", ",", "is", "a", "linear", "combination", "of", "vectors", "in", ".", "It", "follows", "that", "Similarly", ",", "we", "can", "derive", "Therefore", ",", "\u220e", "Proof", "of", "Proposition", "proof", ":", "Proof", ".", "If", "there", "exists", "a", "parameter", "such", "that", "for", "all", "in", ",", "by", "Lemma", "[", "reference", "]", ",", "we", "have", ".", "As", "a", "result", ",", "there", "exists", "a", "matrix", "such", "that", ".", "Because", "and", "are", "of", "dimensions", "and", "respectively", ",", "we", "have", "If", ",", "there", "exist", "matrices", ",", "and", ",", "such", "that", "can", "be", "factorized", "as", ".", "Because", "is", "a", "universal", "approximator", ",", "there", "exists", "such", "that", "and", ".", "By", "Lemma", "[", "reference", "]", ",", "for", "all", "in", ".", "\u220e", "appendix", ":", "Experiment", "setting", "and", "Hyper", "-", "parameters", "subsection", ":", "PTB", "and", "WT2", "The", "hyper", "-", "parameters", "used", "for", "MoS", "in", "language", "modeling", "experiment", "is", "summarized", "below", ".", "The", "hyper", "-", "parameters", "used", "for", "dynamic", "evaluation", "of", "MoS", "is", "summarized", "below", ".", "subsection", ":", "1B", "Word", "Dataset", "For", "training", ",", "we", "use", "all", "of", "the", "100", "training", "shards", ".", "For", "validation", ",", "we", "use", "two", "shards", "from", "the", "heldout", "set", ",", "namely", "[", "heldout", "-", "00", ",", "heldout", "-", "10", "]", ".", "For", "test", ",", "we", "use", "another", "three", "shards", "from", "the", "heldout", "set", ",", "namely", "[", "heldout", "-", "20", ",", "heldout", "-", "30", ",", "heldout", "-", "40", "]", ".", "The", "hyper", "-", "parameters", "are", "listed", "below", ".", "appendix", ":", "Additional", "experiments", "subsection", ":", "Higher", "empirical", "rank", "of", "MoS", "compared", "to", "MoC", "and", "Softmax", "Cumulative", "percentage", "of", "normalized", "singulars", "given", "a", "value", "in", "[", "0", ",", "1", "]", ".", "In", "section", "[", "reference", "]", ",", "we", "compute", "the", "rank", "of", "different", "models", "based", "on", "the", "non", "-", "zero", "singular", "values", "of", "the", "empirical", "log", "-", "likelihood", "matrix", ".", "Since", "there", "can", "be", "roundoff", "mistakes", ",", "a", "less", "error", "-", "prone", "approach", "is", "to", "directly", "study", "the", "distribution", "of", "singular", "values", ".", "Specifically", ",", "if", "more", "singular", "values", "have", "relatively", "larger", "magnitude", ",", "the", "rank", "of", "the", "matrix", "tends", "to", "be", "higher", ".", "Motivated", "from", "this", "intuition", ",", "we", "visualize", "the", "distribution", "of", "the", "singular", "values", ".", "To", "account", "for", "the", "different", "magnitudes", "of", "singular", "values", "from", "different", "models", ",", "we", "first", "normalize", "all", "singular", "values", "to", ".", "Then", ",", "we", "plot", "the", "cumulative", "percentage", "of", "normalized", "singular", "values", ",", "i.e.", ",", "percentage", "of", "normalized", "singular", "values", "below", "a", "threshold", ",", "in", "Figure", "[", "reference", "]", ".", "As", "we", "can", "see", ",", "most", "of", "the", "singular", "values", "of", "Softmax", "and", "MoC", "concentrate", "on", "an", "area", "with", "very", "low", "values", ".", "In", "comparison", ",", "the", "concentration", "area", "of", "the", "MoS", "singular", "values", "is", "not", "only", "several", "orders", "larger", ",", "but", "also", "spans", "a", "much", "wider", "region", ".", "Intuitively", ",", "MoS", "utilizes", "the", "corresponding", "singular", "vectors", "to", "capture", "a", "larger", "and", "more", "diverse", "set", "of", "contexts", ".", "What", "\u2019s", "more", ",", "another", "indicator", "of", "high", "rank", "is", "that", "the", "model", "can", "precisely", "capture", "the", "nuance", "of", "difference", "contexts", ".", "If", "a", "model", "can", "better", "capture", "the", "distinctions", "among", "contexts", ",", "we", "expect", "the", "next", "-", "step", "conditional", "distributions", "to", "be", "less", "similar", "to", "each", "on", "average", ".", "Based", "on", "this", "intuition", ",", "we", "use", "the", "expected", "pairwise", "Kullback", "\u2013", "Leibler", "divergence", "(", "KLD", ")", ",", "i.e.", ",", "where", "denotes", "all", "possible", "contexts", ",", "as", "another", "metric", "to", "evaluate", "the", "ranks", "of", "the", "three", "models", "(", "MoS", ",", "MoC", "and", "Softmax", ")", ".", "Practically", ",", "we", "sample", "from", "validation", "or", "test", "data", "of", "PTB", "to", "get", "the", "empirical", "estimations", "for", "the", "three", "models", ",", "which", "are", "shown", "in", "the", "right", "half", "of", "Table", "[", "reference", "]", ".", "As", "we", "expected", ",", "MoS", "achieves", "higher", "expected", "pairwise", "KLD", ",", "indicating", "its", "superiority", "in", "covering", "more", "contexts", "of", "the", "next", "-", "token", "distribution", ".", "subsection", ":", "An", "inverse", "experiment", "on", "character", "-", "level", "language", "modeling", "Here", ",", "we", "detail", "the", "inverse", "experiment", ",", "which", "shows", "that", "when", "Softmax", "does", "not", "suffer", "from", "a", "rank", "limitation", ",", "using", "MoS", "will", "not", "improve", "the", "performance", ".", "Notice", "that", "character", "-", "level", "language", "modeling", "(", "CharLM", ")", "is", "exactly", "such", "a", "problem", ",", "because", "the", "rank", "of", "the", "log", "-", "likelihood", "matrix", "is", "upper", "bounded", "by", "the", "vocabulary", "size", ",", "and", "CharLM", "usually", "has", "a", "very", "limited", "vocabulary", "(", "tens", "of", "characters", ")", ".", "In", "this", "case", ",", "with", "the", "embedding", "size", "being", "hundreds", "in", "practice", ",", "Softmax", "is", "no", "longer", "a", "bottleneck", "in", "this", "task", ".", "Hence", ",", "we", "expect", "MoS", "to", "yield", "similar", "performance", "to", "Softmax", "on", "CharLM", ".", "We", "conduct", "experiments", "of", "CharLM", "using", "the", "text8", "dataset", "mahoney2011large", ",", "which", "consists", "of", "100", "M", "characters", "including", "only", "alphabetical", "characters", "and", "spaces", "derived", "from", "Wikipedia", ".", "We", "follow", "mikolov2012subword", "and", "use", "the", "first", "90", "M", "characters", "for", "training", ",", "the", "next", "5", "M", "for", "validation", "and", "the", "final", "5", "M", "for", "testing", ".", "The", "standard", "evaluation", "metric", "bit", "-", "per", "-", "character", "(", "BPC", ")", "is", "employed", ".", "We", "employ", "a", "1", "-", "layer", "1024", "-", "unit", "LSTM", "followed", "by", "Softmax", "as", "the", "baseline", ".", "For", "MoS", ",", "we", "consider", "7", "or", "10", "mixtures", "and", "reduce", "the", "hidden", "and", "/", "or", "embedding", "size", "to", "match", "the", "baseline", "capacity", ".", "When", "decreasing", "the", "hidden", "and", "/", "or", "embedding", "size", ",", "we", "either", "keep", "both", "the", "same", ",", "or", "make", "the", "hidden", "size", "relatively", "larger", ".", "The", "results", "are", "summarized", "in", "Table", "[", "reference", "]", ".", "Clearly", ",", "the", "Softmax", "and", "MoS", "obtain", "the", "same", "BPC", "on", "the", "test", "set", "and", "comparable", "BPC", "on", "the", "validation", "set", ",", "which", "well", "match", "our", "hypothesis", ".", "Since", "the", "only", "difference", "in", "word", "-", "level", "language", "modeling", "is", "the", "existence", "of", "the", "Softmax", "bottleneck", ",", "the", "distinct", "behavior", "of", "MoS", "again", "supports", "our", "hypothesis", "that", "it", "is", "solving", "the", "Softmax", "bottleneck", "problem", ".", "subsection", ":", "MoS", "Computational", "Time", "We", "evaluate", "the", "additional", "computational", "cost", "introduced", "by", "MoS.", "We", "consider", "two", "sets", "of", "controlled", "experiments", ".", "In", "the", "first", "set", ",", "we", "compare", "the", "training", "time", "of", "MoS", "and", "Softmax", "using", "the", "same", "batch", "sizes", ".", "In", "the", "second", "set", ",", "we", "compare", "the", "training", "time", "of", "two", "methods", "using", "the", "hyper", "-", "parameter", "settings", "that", "achieve", "the", "best", "performance", "for", "each", "model", "(", "i.e.", ",", "the", "settings", "in", "Tables", "[", "reference", "]", ",", "[", "reference", "]", ",", "and", "[", "reference", "]", ")", ".", "In", "both", "sets", ",", "we", "control", "two", "models", "to", "have", "comparable", "model", "sizes", ".", "The", "results", "on", "the", "three", "datasets", "are", "shown", "in", "Table", "[", "reference", "]", ".", "Thanks", "to", "the", "efficiency", "of", "matrix", "multiplication", "on", "GPU", ",", "the", "computational", "wall", "time", "of", "MoS", "is", "actually", "sub", "-", "linear", "w.r.t", ".", "the", "number", "of", "Softmaxes", ".", "In", "most", "settings", ",", "we", "observe", "a", "two", "to", "three", "times", "slowdown", "when", "using", "MoS.", "Specifically", ",", "the", "\u2018", "\u2018", "bs", "\u2019", "\u2019", "setting", "measures", "the", "computational", "cost", "introduced", "by", "MoS", "given", "enough", "memory", ",", "which", "is", "1.9x", ",", "2.5x", ",", "and", "3.8x", "slowdown", "on", "PTB", ",", "WT2", ",", "and", "1B", "respectively", ".", "The", "\u2018", "\u2018", "best", "-", "1", "\u2019", "\u2019", "setting", "is", "usually", "slower", "compared", "to", "\u2018", "\u2018", "bs", "\u2019", "\u2019", ",", "because", "a", "single", "batch", "does", "not", "fit", "into", "the", "memory", "of", "a", "single", "GPU", "using", "MoS", ",", "in", "which", "case", "we", "have", "to", "split", "one", "batch", "into", "multiple", "small", "ones", ",", "resulting", "in", "further", "slowdown", ".", "In", "this", "sense", ",", "the", "gap", "between", "\u2018", "\u2018", "best", "-", "1", "\u2019", "\u2019", "and", "\u2018", "\u2018", "bs", "\u2019", "\u2019", "measures", "the", "computational", "cost", "introduced", "due", "to", "the", "increase", "of", "memory", "consumed", "by", "MoS.", "The", "\u2018", "\u2018", "best", "-", "3", "\u2019", "\u2019", "alleviates", "this", "issue", "by", "using", "three", "GPUs", ",", "which", "allows", "larger", "-", "batch", "training", "for", "MoS.", "In", "this", "case", ",", "we", "reduce", "the", "computational", "cost", "to", "2.9x", "on", "WT2", "and", "2.1x", "on", "1B", "with", "our", "best", "performing", "model", ".", "Note", "that", "the", "computational", "cost", "is", "closely", "related", "to", "the", "batch", "size", ",", "which", "is", "interleaved", "with", "optimization", ".", "Though", "how", "batch", "sizes", "affect", "optimization", "remains", "an", "open", "question", "and", "might", "be", "task", "dependent", ",", "we", "believe", "the", "\u2018", "\u2018", "best", "-", "1", "\u2019", "\u2019", "and", "\u2018", "\u2018", "best", "-", "3", "\u2019", "\u2019", "settings", "well", "reflect", "the", "actual", "computational", "cost", "brought", "by", "MoS", "on", "language", "modeling", "tasks", ".", "subsection", ":", "Qualitative", "Analysis", "Since", "MoC", "shows", "a", "stronger", "performance", "than", "Softmax", "on", "PTB", ",", "the", "qualitative", "study", "focuses", "on", "the", "comparison", "between", "MoC", "and", "MoS.", "Concretely", ",", "given", "the", "same", "context", "(", "previous", "tokens", ")", ",", "we", "search", "for", "prediction", "steps", "where", "MoS", "achieves", "lower", "negative", "log", "loss", "than", "MoC", "by", "a", "margin", ".", "We", "show", "some", "representative", "cases", "in", "Table", "[", "reference", "]", "with", "the", "following", "observations", ":", "Comparing", "the", "first", "two", "cases", ",", "given", "the", "same", "preceding", "word", "\u2018", "\u2018", "N", "\u2019", "\u2019", ",", "MoS", "flexibly", "adjusts", "its", "top", "predictions", "based", "on", "the", "different", "topic", "quantities", "being", "discussed", "in", "the", "context", ".", "In", "comparison", ",", "MoC", "emits", "quite", "similar", "top", "choices", "regardless", "of", "the", "context", ",", "suggesting", "its", "inferiority", "in", "make", "context", "-", "dependent", "predictions", ".", "In", "the", "3rd", "case", ",", "the", "context", "is", "about", "international", "politics", ",", "where", "country", "/", "region", "names", "are", "likely", "to", "appear", ".", "MoS", "captures", "this", "nuance", "well", ",", "and", "yields", "top", "choices", "that", "can", "be", "used", "to", "complete", "a", "country", "name", "given", "the", "immediate", "preceding", "word", "\u2018", "\u2018", "south", "\u2019", "\u2019", ".", "Similarly", ",", "in", "the", "4th", "case", ",", "MoS", "is", "able", "to", "include", "\u2018", "\u2018", "ual", "\u2019", "\u2019", ",", "a", "core", "entity", "of", "discussion", "in", "the", "context", ",", "in", "its", "top", "predictions", ".", "In", "contrast", ",", "MoC", "gives", "rather", "generic", "predictions", "irrieselevant", "to", "the", "context", "in", "both", "cases", ".", "For", "the", "5th", "and", "the", "6th", "example", ",", "we", "see", "MoS", "is", "able", "to", "exploit", "less", "common", "words", "accurately", "according", "to", "the", "context", ",", "while", "MoC", "fails", "to", "yield", "such", "choices", ".", "This", "well", "matches", "our", "analysis", "that", "MoS", "has", "the", "capacity", "of", "modeling", "context", "-", "dependent", "language", "."]}