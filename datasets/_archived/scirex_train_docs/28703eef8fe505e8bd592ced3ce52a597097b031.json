{"coref": {"BLEU_score": [[474, 475], [2608, 2609], [4543, 4544], [5290, 5291], [5308, 5309], [5320, 5321], [5388, 5389], [5414, 5415]], "IWSLT2015_German-English": [], "Machine_Translation": [[167, 169], [215, 217], [694, 696], [1486, 1488], [2590, 2592], [4289, 4291], [5068, 5069], [5270, 5271], [3796, 3798], [5077, 5079], [5138, 5140]], "Word-level_CNN_w_attn__input_feeding": [[5264, 5266]]}, "coref_non_salient": {"0": [[170, 173], [466, 471], [5038, 5039]], "1": [[374, 375], [649, 651], [1938, 1941], [2277, 2279]], "10": [[38, 46], [245, 248], [1223, 1227]], "100": [[183, 186]], "101": [[1806, 1807]], "102": [[2737, 2739]], "103": [[527, 531]], "104": [[907, 909]], "105": [[231, 233]], "106": [[30, 32]], "107": [[60, 67]], "108": [[794, 798]], "109": [[4453, 4454], [3669, 3670], [3723, 3724], [3822, 3823], [4316, 4317], [5344, 5345], [5451, 5452]], "11": [[787, 789], [807, 809]], "110": [[5250, 5252]], "111": [[556, 560]], "112": [[1140, 1146]], "113": [[237, 239]], "114": [[338, 342]], "115": [[88, 89]], "116": [[2015, 2016]], "117": [[445, 449]], "118": [[1676, 1679], [1690, 1693]], "119": [[5061, 5065]], "12": [[1968, 1970], [2006, 2008]], "13": [[107, 109], [827, 829]], "14": [[385, 387], [578, 580], [594, 598], [959, 961], [1134, 1137], [1988, 1991], [2743, 2745], [2849, 2852], [3057, 3060], [3148, 3150], [4126, 4128]], "15": [[895, 896], [915, 916], [1358, 1359], [1407, 1410], [1680, 1681], [1731, 1732], [2119, 2120], [2355, 2356], [3243, 3244], [1170, 1171], [1341, 1342], [1750, 1751], [1808, 1809], [1976, 1977], [2325, 2326], [2422, 2423]], "16": [[4900, 4902], [4912, 4914]], "17": [[623, 625], [5429, 5431], [5571, 5573]], "18": [[112, 114], [1260, 1264]], "19": [[1041, 1047], [1097, 1101]], "2": [[2759, 2764], [3178, 3179], [3323, 3328]], "20": [[1109, 1113], [1244, 1247], [2020, 2026], [3829, 3831]], "21": [[161, 163], [687, 689], [4282, 4284], [4326, 4328], [4387, 4389], [4553, 4556], [4930, 4934], [5674, 5677]], "22": [[3708, 3710], [3712, 3714], [5190, 5195], [5221, 5223], [5244, 5246]], "23": [[47, 48], [1508, 1510]], "24": [[19, 20], [92, 93], [128, 129], [189, 190], [1355, 1356], [3222, 3223], [3645, 3650], [3824, 3825], [5014, 5015], [5592, 5593], [148, 149], [262, 263], [331, 332], [347, 348], [533, 534], [599, 600], [627, 628], [671, 672], [704, 705], [1092, 1093], [1174, 1175], [1265, 1266], [1400, 1401], [1454, 1455], [1589, 1590], [1672, 1673], [2060, 2061], [2587, 2588], [2724, 2725], [2729, 2730], [2958, 2959], [3013, 3014], [3205, 3206], [3404, 3405], [3668, 3669], [3687, 3688], [3724, 3725], [3800, 3801], [4138, 4139], [4263, 4264], [4317, 4318], [4365, 4366], [4729, 4730], [5110, 5111], [5450, 5451], [5657, 5658]], "25": [[5028, 5029]], "26": [[740, 742], [743, 745], [772, 774], [2164, 2167]], "27": [[296, 297], [450, 451], [2570, 2571], [3989, 3992], [4060, 4061], [5546, 5547]], "28": [[50, 54], [326, 329], [1895, 1898]], "29": [[1000, 1002], [1048, 1051], [1118, 1121], [2817, 2820], [5357, 5358]], "3": [[4392, 4394], [5021, 5022]], "30": [[132, 134], [550, 552], [2257, 2259], [2274, 2276], [3129, 3131]], "31": [[1858, 1859], [2143, 2145]], "32": [[58, 59], [5352, 5353]], "33": [[1257, 1259]], "34": [[5088, 5090]], "35": [[2309, 2313], [2320, 2324]], "36": [[1542, 1543], [1545, 1548]], "37": [[2242, 2243], [2446, 2448], [3416, 3417]], "38": [[75, 82], [5596, 5600]], "39": [[1190, 1192], [1250, 1251]], "4": [[1518, 1523], [1549, 1550], [4559, 4563], [4854, 4858]], "40": [[13, 18], [156, 160], [2, 7], [176, 181]], "41": [[2233, 2234], [3813, 3814], [4455, 4457], [4474, 4476], [4627, 4628], [5006, 5007], [5024, 5025], [4140, 4142]], "42": [[561, 562], [1003, 1004], [1021, 1022], [1197, 1198], [2216, 2217], [2813, 2814], [973, 974], [1074, 1075], [3432, 3433]], "43": [[4581, 4583], [4875, 4877], [5210, 5212]], "44": [[1127, 1129], [1147, 1149]], "45": [[9, 13], [982, 984], [2048, 2051]], "46": [[777, 778], [821, 822], [897, 898], [964, 965]], "47": [[4669, 4671]], "48": [[1597, 1598], [1684, 1686], [1887, 1888], [1934, 1935]], "49": [[125, 127], [620, 622]], "5": [[1124, 1126], [2998, 2999], [4696, 4698]], "50": [[3971, 3978]], "51": [[4055, 4059]], "52": [[4789, 4791]], "53": [[1578, 1581]], "54": [[267, 270], [353, 356], [1816, 1819]], "55": [[4005, 4010]], "56": [[4175, 4178]], "57": [[2292, 2298]], "58": [[1422, 1423], [4118, 4119], [4564, 4566], [4190, 4192], [4859, 4861]], "59": [[5444, 5446]], "6": [[2688, 2690], [3247, 3250]], "60": [[2350, 2352]], "61": [[4143, 4147]], "62": [[1528, 1530], [1537, 1539], [1571, 1573]], "63": [[853, 854], [899, 900], [966, 967]], "64": [[674, 677], [712, 715], [2229, 2232], [3815, 3818], [4258, 4261]], "65": [[1057, 1061]], "66": [[5196, 5198]], "67": [[116, 120]], "68": [[1242, 1243]], "69": [[4203, 4205]], "7": [[1499, 1501], [1505, 1507], [4513, 4515], [4799, 4803], [4804, 4807], [5128, 5134]], "70": [[1304, 1306]], "71": [[1471, 1472]], "72": [[4351, 4354]], "73": [[164, 165], [234, 235], [690, 692], [2961, 2962], [4285, 4287], [4704, 4706], [4720, 4722]], "74": [[730, 732], [998, 999], [1442, 1443], [4413, 4414], [4716, 4717]], "75": [[253, 255]], "76": [[3181, 3183]], "77": [[3061, 3062], [3847, 3848], [5008, 5009], [5017, 5018]], "78": [[707, 708], [5260, 5262]], "79": [[3953, 3955]], "8": [[1695, 1700], [3109, 3111], [3698, 3701], [3716, 3719]], "80": [[4213, 4216]], "81": [[5359, 5360]], "82": [[4112, 4114], [4129, 4130], [4574, 4575], [4868, 4869], [5202, 5203]], "83": [[837, 841]], "84": [[1427, 1428]], "85": [[1161, 1163], [1201, 1203]], "86": [[204, 207]], "87": [[3339, 3342]], "88": [[4636, 4638]], "89": [[4355, 4359]], "9": [[251, 252], [1429, 1431]], "90": [[1902, 1906]], "91": [[753, 756]], "92": [[3336, 3338]], "93": [[1971, 1973]], "94": [[336, 337]], "95": [[3412, 3415]], "96": [[399, 403]], "97": [[2765, 2766], [3329, 3330], [3384, 3385]], "98": [[1853, 1855]], "99": [[780, 784]]}, "doc_id": "28703eef8fe505e8bd592ced3ce52a597097b031", "method_subrelations": {"Word-level_CNN_w_attn__input_feeding": [[[0, 36], "Word-level_CNN_w_attn__input_feeding"]]}, "n_ary_relations": [{"Material": "IWSLT2015_German-English", "Method": "Word-level_CNN_w_attn__input_feeding", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "24.0"}], "ner": [[9, 13, "Task"], [13, 18, "Task"], [19, 20, "Task"], [30, 32, "Method"], [38, 46, "Task"], [47, 48, "Method"], [50, 54, "Method"], [58, 59, "Metric"], [60, 67, "Task"], [75, 82, "Method"], [88, 89, "Method"], [92, 93, "Task"], [107, 109, "Method"], [112, 114, "Metric"], [116, 120, "Metric"], [125, 127, "Method"], [128, 129, "Task"], [132, 134, "Method"], [156, 160, "Task"], [161, 163, "Task"], [164, 165, "Task"], [167, 169, "Task"], [170, 173, "Metric"], [183, 186, "Method"], [189, 190, "Task"], [204, 207, "Task"], [215, 217, "Task"], [231, 233, "Task"], [234, 235, "Task"], [237, 239, "Task"], [245, 248, "Task"], [251, 252, "Task"], [253, 255, "Task"], [267, 270, "Method"], [296, 297, "Task"], [326, 329, "Method"], [336, 337, "Metric"], [338, 342, "Task"], [353, 356, "Method"], [374, 375, "Task"], [385, 387, "Method"], [399, 403, "Method"], [445, 449, "Task"], [450, 451, "Task"], [466, 471, "Metric"], [474, 475, "Metric"], [527, 531, "Method"], [550, 552, "Method"], [556, 560, "Method"], [561, 562, "Method"], [578, 580, "Method"], [594, 598, "Method"], [620, 622, "Method"], [623, 625, "Metric"], [649, 651, "Task"], [674, 677, "Method"], [687, 689, "Task"], [690, 692, "Task"], [694, 696, "Task"], [707, 708, "Method"], [712, 715, "Method"], [730, 732, "Task"], [740, 742, "Task"], [743, 745, "Task"], [753, 756, "Task"], [772, 774, "Task"], [777, 778, "Method"], [780, 784, "Method"], [787, 789, "Method"], [794, 798, "Method"], [807, 809, "Method"], [821, 822, "Method"], [827, 829, "Method"], [837, 841, "Method"], [853, 854, "Method"], [895, 896, "Method"], [897, 898, "Method"], [899, 900, "Method"], [907, 909, "Task"], [915, 916, "Method"], [959, 961, "Method"], [964, 965, "Method"], [966, 967, "Method"], [982, 984, "Task"], [998, 999, "Task"], [1000, 1002, "Method"], [1003, 1004, "Method"], [1021, 1022, "Method"], [1041, 1047, "Method"], [1048, 1051, "Method"], [1057, 1061, "Method"], [1097, 1101, "Method"], [1109, 1113, "Method"], [1118, 1121, "Method"], [1124, 1126, "Method"], [1127, 1129, "Method"], [1134, 1137, "Method"], [1140, 1146, "Method"], [1147, 1149, "Method"], [1161, 1163, "Method"], [1190, 1192, "Method"], [1197, 1198, "Method"], [1201, 1203, "Method"], [1223, 1227, "Task"], [1242, 1243, "Method"], [1244, 1247, "Method"], [1250, 1251, "Method"], [1257, 1259, "Metric"], [1260, 1264, "Metric"], [1304, 1306, "Method"], [1355, 1356, "Task"], [1358, 1359, "Method"], [1407, 1410, "Method"], [1422, 1423, "Method"], [1427, 1428, "Method"], [1429, 1431, "Task"], [1442, 1443, "Task"], [1471, 1472, "Task"], [1486, 1488, "Task"], [1499, 1501, "Material"], [1505, 1507, "Material"], [1508, 1510, "Method"], [1518, 1523, "Method"], [1528, 1530, "Method"], [1537, 1539, "Method"], [1542, 1543, "Method"], [1545, 1548, "Method"], [1549, 1550, "Method"], [1571, 1573, "Method"], [1578, 1581, "Method"], [1597, 1598, "Method"], [1676, 1679, "Method"], [1680, 1681, "Method"], [1684, 1686, "Method"], [1690, 1693, "Method"], [1695, 1700, "Method"], [1731, 1732, "Method"], [1806, 1807, "Task"], [1816, 1819, "Method"], [1853, 1855, "Method"], [1858, 1859, "Method"], [1887, 1888, "Method"], [1895, 1898, "Method"], [1902, 1906, "Metric"], [1934, 1935, "Method"], [1938, 1941, "Task"], [1968, 1970, "Method"], [1971, 1973, "Method"], [1988, 1991, "Method"], [2006, 2008, "Method"], [2015, 2016, "Task"], [2020, 2026, "Method"], [2048, 2051, "Task"], [2119, 2120, "Method"], [2143, 2145, "Method"], [2164, 2167, "Task"], [2216, 2217, "Method"], [2229, 2232, "Method"], [2233, 2234, "Method"], [2242, 2243, "Method"], [2257, 2259, "Method"], [2274, 2276, "Method"], [2277, 2279, "Task"], [2292, 2298, "Metric"], [2309, 2313, "Method"], [2320, 2324, "Method"], [2350, 2352, "Method"], [2355, 2356, "Method"], [2446, 2448, "Method"], [2570, 2571, "Task"], [2590, 2592, "Task"], [2688, 2690, "Method"], [2737, 2739, "Method"], [2743, 2745, "Method"], [2759, 2764, "Method"], [2765, 2766, "Method"], [2813, 2814, "Method"], [2817, 2820, "Method"], [2849, 2852, "Method"], [2961, 2962, "Task"], [2998, 2999, "Method"], [3057, 3060, "Method"], [3061, 3062, "Method"], [3109, 3111, "Method"], [3129, 3131, "Method"], [3148, 3150, "Method"], [3178, 3179, "Method"], [3181, 3183, "Task"], [3222, 3223, "Task"], [3243, 3244, "Method"], [3247, 3250, "Method"], [3323, 3328, "Method"], [3329, 3330, "Method"], [3336, 3338, "Metric"], [3339, 3342, "Task"], [3384, 3385, "Method"], [3412, 3415, "Method"], [3416, 3417, "Method"], [3645, 3650, "Task"], [3698, 3701, "Method"], [3708, 3710, "Method"], [3712, 3714, "Method"], [3716, 3719, "Method"], [3813, 3814, "Method"], [3815, 3818, "Method"], [3824, 3825, "Task"], [3829, 3831, "Method"], [3847, 3848, "Method"], [3953, 3955, "Method"], [3971, 3978, "Method"], [3989, 3992, "Task"], [4005, 4010, "Method"], [4055, 4059, "Method"], [4060, 4061, "Task"], [4112, 4114, "Method"], [4118, 4119, "Method"], [4126, 4128, "Method"], [4129, 4130, "Method"], [4143, 4147, "Method"], [4175, 4178, "Metric"], [4203, 4205, "Method"], [4213, 4216, "Task"], [4258, 4261, "Method"], [4282, 4284, "Task"], [4285, 4287, "Task"], [4289, 4291, "Task"], [4326, 4328, "Task"], [4351, 4354, "Task"], [4355, 4359, "Method"], [4387, 4389, "Task"], [4392, 4394, "Task"], [4413, 4414, "Task"], [4453, 4454, "Method"], [4455, 4457, "Method"], [4474, 4476, "Method"], [4513, 4515, "Material"], [4553, 4556, "Task"], [4559, 4563, "Method"], [4564, 4566, "Method"], [4574, 4575, "Method"], [4581, 4583, "Method"], [4627, 4628, "Method"], [4636, 4638, "Method"], [4669, 4671, "Task"], [4696, 4698, "Method"], [4704, 4706, "Task"], [4716, 4717, "Task"], [4720, 4722, "Task"], [4789, 4791, "Task"], [4799, 4803, "Material"], [4804, 4807, "Material"], [4868, 4869, "Method"], [4875, 4877, "Method"], [4900, 4902, "Method"], [4912, 4914, "Method"], [4930, 4934, "Task"], [5006, 5007, "Method"], [5008, 5009, "Method"], [5014, 5015, "Task"], [5017, 5018, "Method"], [5021, 5022, "Task"], [5024, 5025, "Method"], [5028, 5029, "Task"], [5038, 5039, "Metric"], [5061, 5065, "Method"], [5068, 5069, "Task"], [5088, 5090, "Method"], [5128, 5134, "Material"], [5190, 5195, "Method"], [5196, 5198, "Method"], [5202, 5203, "Method"], [5210, 5212, "Method"], [5221, 5223, "Method"], [5244, 5246, "Method"], [5250, 5252, "Method"], [5260, 5262, "Method"], [5264, 5266, "Method"], [5270, 5271, "Task"], [5352, 5353, "Metric"], [5357, 5358, "Method"], [5359, 5360, "Method"], [5429, 5431, "Metric"], [5444, 5446, "Task"], [5546, 5547, "Task"], [5571, 5573, "Metric"], [5592, 5593, "Task"], [5596, 5600, "Method"], [5674, 5677, "Task"], [2, 7, "Task"], [148, 149, "Task"], [176, 181, "Task"], [262, 263, "Task"], [331, 332, "Task"], [347, 348, "Task"], [533, 534, "Task"], [599, 600, "Task"], [627, 628, "Task"], [671, 672, "Task"], [704, 705, "Task"], [973, 974, "Method"], [1074, 1075, "Method"], [1092, 1093, "Task"], [1170, 1171, "Method"], [1174, 1175, "Task"], [1265, 1266, "Task"], [1341, 1342, "Method"], [1400, 1401, "Task"], [1454, 1455, "Task"], [1589, 1590, "Task"], [1672, 1673, "Task"], [1750, 1751, "Method"], [1808, 1809, "Method"], [1976, 1977, "Method"], [2060, 2061, "Task"], [2325, 2326, "Method"], [2422, 2423, "Method"], [2587, 2588, "Task"], [2608, 2609, "Metric"], [2724, 2725, "Task"], [2729, 2730, "Task"], [2958, 2959, "Task"], [3013, 3014, "Task"], [3205, 3206, "Task"], [3404, 3405, "Task"], [3432, 3433, "Method"], [3668, 3669, "Task"], [3669, 3670, "Method"], [3687, 3688, "Task"], [3723, 3724, "Method"], [3724, 3725, "Task"], [3796, 3798, "Task"], [3800, 3801, "Task"], [3822, 3823, "Method"], [4138, 4139, "Task"], [4140, 4142, "Method"], [4190, 4192, "Method"], [4263, 4264, "Task"], [4316, 4317, "Method"], [4317, 4318, "Task"], [4365, 4366, "Task"], [4543, 4544, "Metric"], [4729, 4730, "Task"], [4854, 4858, "Method"], [4859, 4861, "Method"], [5077, 5079, "Task"], [5110, 5111, "Task"], [5138, 5140, "Task"], [5290, 5291, "Metric"], [5308, 5309, "Metric"], [5320, 5321, "Metric"], [5344, 5345, "Method"], [5388, 5389, "Metric"], [5414, 5415, "Metric"], [5450, 5451, "Task"], [5451, 5452, "Method"], [5657, 5658, "Task"]], "sections": [[0, 173], [173, 733], [733, 1446], [1446, 2046], [2046, 2307], [2307, 2777], [2777, 3163], [3163, 3650], [3650, 3677], [3677, 3921], [3921, 4243], [4243, 4324], [4324, 4694], [4694, 5066], [5066, 5417], [5417, 5583], [5583, 5642], [5642, 5690], [5690, 5693]], "sentences": [[0, 13], [13, 47], [47, 68], [68, 99], [99, 135], [135, 170], [170, 173], [173, 176], [176, 208], [208, 256], [256, 294], [294, 314], [314, 343], [343, 371], [371, 388], [388, 413], [413, 445], [445, 476], [476, 521], [521, 581], [581, 602], [602, 630], [630, 655], [655, 678], [678, 709], [709, 733], [733, 737], [737, 767], [767, 812], [812, 819], [819, 853], [853, 890], [890, 950], [950, 968], [968, 1000], [1000, 1037], [1037, 1066], [1066, 1103], [1103, 1150], [1150, 1164], [1164, 1165], [1165, 1193], [1193, 1217], [1217, 1242], [1242, 1274], [1274, 1295], [1295, 1318], [1318, 1368], [1368, 1390], [1390, 1446], [1446, 1451], [1451, 1480], [1480, 1508], [1508, 1540], [1540, 1560], [1560, 1582], [1582, 1599], [1599, 1615], [1615, 1637], [1637, 1671], [1671, 1687], [1687, 1714], [1714, 1729], [1729, 1748], [1748, 1751], [1751, 1776], [1776, 1808], [1808, 1820], [1820, 1860], [1860, 1882], [1882, 1922], [1922, 1932], [1932, 1983], [1983, 2009], [2009, 2046], [2046, 2051], [2051, 2064], [2064, 2089], [2089, 2129], [2129, 2168], [2168, 2180], [2180, 2197], [2197, 2236], [2236, 2255], [2255, 2280], [2280, 2307], [2307, 2313], [2313, 2328], [2328, 2339], [2339, 2376], [2376, 2390], [2390, 2407], [2407, 2408], [2408, 2442], [2442, 2472], [2472, 2474], [2474, 2487], [2487, 2496], [2496, 2505], [2505, 2552], [2552, 2581], [2581, 2624], [2624, 2671], [2671, 2727], [2727, 2751], [2751, 2768], [2768, 2777], [2777, 2783], [2783, 2811], [2811, 2835], [2835, 2853], [2853, 2875], [2875, 2904], [2904, 2923], [2923, 2949], [2949, 3000], [3000, 3063], [3063, 3089], [3089, 3108], [3108, 3123], [3123, 3163], [3163, 3169], [3169, 3184], [3184, 3202], [3202, 3234], [3234, 3244], [3244, 3276], [3276, 3279], [3279, 3280], [3280, 3281], [3281, 3283], [3283, 3293], [3293, 3298], [3298, 3315], [3315, 3332], [3332, 3369], [3369, 3407], [3407, 3425], [3425, 3444], [3444, 3455], [3455, 3486], [3486, 3487], [3487, 3517], [3517, 3555], [3555, 3586], [3586, 3615], [3615, 3619], [3619, 3631], [3631, 3645], [3645, 3650], [3650, 3655], [3655, 3677], [3677, 3680], [3680, 3752], [3752, 3784], [3784, 3803], [3803, 3826], [3826, 3849], [3849, 3878], [3878, 3921], [3921, 3924], [3924, 3939], [3939, 3956], [3956, 3986], [3986, 4011], [4011, 4044], [4044, 4074], [4074, 4105], [4105, 4131], [4131, 4170], [4170, 4232], [4232, 4243], [4243, 4248], [4248, 4266], [4266, 4292], [4292, 4324], [4324, 4328], [4328, 4360], [4360, 4386], [4386, 4404], [4404, 4450], [4450, 4466], [4466, 4506], [4506, 4551], [4551, 4584], [4584, 4596], [4596, 4609], [4609, 4639], [4639, 4664], [4664, 4694], [4694, 4698], [4698, 4718], [4718, 4761], [4761, 4771], [4771, 4783], [4783, 4792], [4792, 4793], [4793, 4794], [4794, 4795], [4795, 4823], [4823, 4852], [4852, 4878], [4878, 4915], [4915, 4927], [4927, 4962], [4962, 4992], [4992, 5006], [5006, 5030], [5030, 5066], [5066, 5069], [5069, 5113], [5113, 5143], [5143, 5172], [5172, 5199], [5199, 5213], [5213, 5267], [5267, 5292], [5292, 5323], [5323, 5338], [5338, 5365], [5365, 5399], [5399, 5417], [5417, 5420], [5420, 5441], [5441, 5476], [5476, 5523], [5523, 5555], [5555, 5583], [5583, 5586], [5586, 5630], [5630, 5642], [5642, 5645], [5645, 5666], [5666, 5678], [5678, 5690], [5690, 5693]], "words": ["document", ":", "Sequence", "-", "to", "-", "Sequence", "Learning", "as", "Beam", "-", "Search", "Optimization", "Sequence", "-", "to", "-", "Sequence", "(", "seq2seq", ")", "modeling", "has", "rapidly", "become", "an", "important", "general", "-", "purpose", "NLP", "tool", "that", "has", "proven", "effective", "for", "many", "text", "-", "generation", "and", "sequence", "-", "labeling", "tasks", ".", "Seq2seq", "builds", "on", "deep", "neural", "language", "modeling", "and", "inherits", "its", "remarkable", "accuracy", "in", "estimating", "local", ",", "next", "-", "word", "distributions", ".", "In", "this", "work", ",", "we", "introduce", "a", "model", "and", "beam", "-", "search", "training", "scheme", ",", "based", "on", "the", "work", "of", "daume05learning", ",", "that", "extends", "seq2seq", "to", "learn", "global", "sequence", "scores", ".", "This", "structured", "approach", "avoids", "classical", "biases", "associated", "with", "local", "training", "and", "unifies", "the", "training", "loss", "with", "the", "test", "-", "time", "usage", ",", "while", "preserving", "the", "proven", "model", "architecture", "of", "seq2seq", "and", "its", "efficient", "training", "approach", ".", "We", "show", "that", "our", "system", "outperforms", "a", "highly", "-", "optimized", "attention", "-", "based", "seq2seq", "system", "and", "other", "baselines", "on", "three", "different", "sequence", "to", "sequence", "tasks", ":", "word", "ordering", ",", "parsing", ",", "and", "machine", "translation", ".", "succ", "topK", "score", "section", ":", "Introduction", "Sequence", "-", "to", "-", "Sequence", "learning", "with", "deep", "neural", "networks", "(", "herein", ",", "seq2seq", ")", "has", "rapidly", "become", "a", "very", "useful", "and", "surprisingly", "general", "-", "purpose", "tool", "for", "natural", "language", "processing", ".", "In", "addition", "to", "demonstrating", "impressive", "results", "for", "machine", "translation", ",", "roughly", "the", "same", "model", "and", "training", "have", "also", "proven", "to", "be", "useful", "for", "sentence", "compression", ",", "parsing", ",", "and", "dialogue", "systems", ",", "and", "they", "additionally", "underlie", "other", "text", "generation", "applications", ",", "such", "as", "image", "or", "video", "captioning", ".", "The", "dominant", "approach", "to", "training", "a", "seq2seq", "system", "is", "as", "a", "conditional", "language", "model", ",", "with", "training", "maximizing", "the", "likelihood", "of", "each", "successive", "target", "word", "conditioned", "on", "the", "input", "sequence", "and", "the", "gold", "history", "of", "target", "words", ".", "Thus", ",", "training", "uses", "a", "strictly", "word", "-", "level", "loss", ",", "usually", "cross", "-", "entropy", "over", "the", "target", "vocabulary", ".", "This", "approach", "has", "proven", "to", "be", "very", "effective", "and", "efficient", "for", "training", "neural", "language", "models", ",", "and", "seq2seq", "models", "similarly", "obtain", "impressive", "perplexities", "for", "word", "-", "generation", "tasks", ".", "Notably", ",", "however", ",", "seq2seq", "models", "are", "not", "used", "as", "conditional", "language", "models", "at", "test", "-", "time", ";", "they", "must", "instead", "generate", "fully", "-", "formed", "word", "sequences", ".", "In", "practice", ",", "generation", "is", "accomplished", "by", "searching", "over", "output", "sequences", "greedily", "or", "with", "beam", "search", ".", "In", "this", "context", ",", "ranzato16sequence", "note", "that", "the", "combination", "of", "the", "training", "and", "generation", "scheme", "just", "described", "leads", "to", "at", "least", "two", "major", "issues", ":", "Exposure", "Bias", ":", "the", "model", "is", "never", "exposed", "to", "its", "own", "errors", "during", "training", ",", "and", "so", "the", "inferred", "histories", "at", "test", "-", "time", "do", "not", "resemble", "the", "gold", "training", "histories", ".", "Loss", "-", "Evaluation", "Mismatch", ":", "training", "uses", "a", "word", "-", "level", "loss", ",", "while", "at", "test", "-", "time", "we", "target", "improving", "sequence", "-", "level", "evaluation", "metrics", ",", "such", "as", "BLEU", ".", "We", "might", "additionally", "add", "the", "concern", "of", "label", "bias", "to", "the", "list", ",", "since", "word", "-", "probabilities", "at", "each", "time", "-", "step", "are", "locally", "normalized", ",", "guaranteeing", "that", "successors", "of", "incorrect", "histories", "receive", "the", "same", "mass", "as", "do", "the", "successors", "of", "the", "true", "history", ".", "In", "this", "work", "we", "develop", "a", "non", "-", "probabilistic", "variant", "of", "the", "seq2seq", "model", "that", "can", "assign", "a", "score", "to", "any", "possible", "target", "sequence", ",", "and", "we", "propose", "a", "training", "procedure", ",", "inspired", "by", "the", "learning", "as", "search", "optimization", "(", "LaSO", ")", "framework", "of", "daume05learning", ",", "that", "defines", "a", "loss", "function", "in", "terms", "of", "errors", "made", "during", "beam", "search", ".", "Furthermore", ",", "we", "provide", "an", "efficient", "algorithm", "to", "back", "-", "propagate", "through", "the", "beam", "-", "search", "procedure", "during", "seq2seq", "training", ".", "This", "approach", "offers", "a", "possible", "solution", "to", "each", "of", "the", "three", "aforementioned", "issues", ",", "while", "largely", "maintaining", "the", "model", "architecture", "and", "training", "efficiency", "of", "standard", "seq2seq", "learning", ".", "Moreover", ",", "by", "scoring", "sequences", "rather", "than", "words", ",", "our", "approach", "also", "allows", "for", "enforcing", "hard", "-", "constraints", "on", "sequence", "generation", "at", "training", "time", ".", "To", "test", "out", "the", "effectiveness", "of", "the", "proposed", "approach", ",", "we", "develop", "a", "general", "-", "purpose", "seq2seq", "system", "with", "beam", "search", "optimization", ".", "We", "run", "experiments", "on", "three", "very", "different", "problems", ":", "word", "ordering", ",", "syntactic", "parsing", ",", "and", "machine", "translation", ",", "and", "compare", "to", "a", "highly", "-", "tuned", "seq2seq", "system", "with", "attention", ".", "The", "version", "with", "beam", "search", "optimization", "shows", "significant", "improvements", "on", "all", "three", "tasks", ",", "and", "particular", "improvements", "on", "tasks", "that", "require", "difficult", "search", ".", "section", ":", "Related", "Work", "The", "issues", "of", "exposure", "bias", "and", "label", "bias", "have", "received", "much", "attention", "from", "authors", "in", "the", "structured", "prediction", "community", ",", "and", "we", "briefly", "review", "some", "of", "this", "work", "here", ".", "One", "prominent", "approach", "to", "combating", "exposure", "bias", "is", "that", "of", "SEARN", ",", "a", "meta", "-", "training", "algorithm", "that", "learns", "a", "search", "policy", "in", "the", "form", "of", "a", "cost", "-", "sensitive", "classifier", "trained", "on", "examples", "generated", "from", "an", "interpolation", "of", "an", "oracle", "policy", "and", "the", "model", "\u2019s", "current", "(", "learned", ")", "policy", ".", "Thus", ",", "SEARN", "explicitly", "targets", "the", "mismatch", "between", "oracular", "training", "and", "non", "-", "oracular", "(", "often", "greedy", ")", "test", "-", "time", "inference", "by", "training", "on", "the", "output", "of", "the", "model", "\u2019s", "own", "policy", ".", "DAgger", "is", "a", "similar", "approach", ",", "which", "differs", "in", "terms", "of", "how", "training", "examples", "are", "generated", "and", "aggregated", ",", "and", "there", "have", "additionally", "been", "important", "refinements", "to", "this", "style", "of", "training", "over", "the", "past", "several", "years", ".", "When", "it", "comes", "to", "training", "RNNs", ",", "SEARN", "/", "DAgger", "has", "been", "applied", "under", "the", "name", "\u201c", "scheduled", "sampling", "\u201d", ",", "which", "involves", "training", "an", "RNN", "to", "generate", "the", "\u2019", "st", "token", "in", "a", "target", "sequence", "after", "consuming", "either", "the", "true", "\u2019", "th", "token", ",", "or", ",", "with", "probability", "that", "increases", "throughout", "training", ",", "the", "predicted", "\u2019", "th", "token", ".", "Though", "technically", "possible", ",", "it", "is", "uncommon", "to", "use", "beam", "search", "when", "training", "with", "SEARN", "/", "DAgger", ".", "The", "early", "-", "update", "and", "LaSO", "training", "strategies", ",", "however", ",", "explicitly", "account", "for", "beam", "search", ",", "and", "describe", "strategies", "for", "updating", "parameters", "when", "the", "gold", "structure", "becomes", "unreachable", "during", "search", ".", "Early", "update", "and", "LaSO", "differ", "primarily", "in", "that", "the", "former", "discards", "a", "training", "example", "after", "the", "first", "search", "error", ",", "whereas", "LaSO", "resumes", "searching", "after", "an", "error", "from", "a", "state", "that", "includes", "the", "gold", "partial", "structure", ".", "In", "the", "context", "of", "feed", "-", "forward", "neural", "network", "training", ",", "early", "update", "training", "has", "been", "recently", "explored", "in", "a", "feed", "-", "forward", "setting", "by", "zhou15a", "and", "andor16globally", ".", "Our", "work", "differs", "in", "that", "we", "adopt", "a", "LaSO", "-", "like", "paradigm", "(", "with", "some", "minor", "modifications", ")", ",", "and", "apply", "it", "to", "the", "training", "of", "seq2seq", "RNNs", "(", "rather", "than", "feed", "-", "forward", "networks", ")", ".", "We", "also", "note", "that", "watanabe15transition", "apply", "maximum", "-", "violation", "training", ",", "which", "is", "similar", "to", "early", "-", "update", ",", "to", "a", "parsing", "model", "with", "recurrent", "components", ",", "and", "that", "yazdani15incremental", "use", "beam", "-", "search", "in", "training", "a", "discriminative", ",", "locally", "normalized", "dependency", "parser", "with", "recurrent", "components", ".", "Recently", "authors", "have", "also", "proposed", "alleviating", "exposure", "bias", "using", "techniques", "from", "reinforcement", "learning", ".", "ranzato16sequence", "follow", "this", "approach", "to", "train", "RNN", "decoders", "in", "a", "seq2seq", "model", ",", "and", "they", "obtain", "consistent", "improvements", "in", "performance", ",", "even", "over", "models", "trained", "with", "scheduled", "sampling", ".", "As", "daume05learning", "note", ",", "LaSO", "is", "similar", "to", "reinforcement", "learning", ",", "except", "it", "does", "not", "require", "\u201c", "exploration", "\u201d", "in", "the", "same", "way", ".", "Such", "exploration", "may", "be", "unnecessary", "in", "supervised", "text", "-", "generation", ",", "since", "we", "typically", "know", "the", "gold", "partial", "sequences", "at", "each", "time", "-", "step", ".", "shen16mrt", "use", "minimum", "risk", "training", "(", "approximated", "by", "sampling", ")", "to", "address", "the", "issues", "of", "exposure", "bias", "and", "loss", "-", "evaluation", "mismatch", "for", "seq2seq", "MT", ",", "and", "show", "impressive", "performance", "gains", ".", "Whereas", "exposure", "bias", "results", "from", "training", "in", "a", "certain", "way", ",", "label", "bias", "results", "from", "properties", "of", "the", "model", "itself", ".", "In", "particular", ",", "label", "bias", "is", "likely", "to", "affect", "structured", "models", "that", "make", "sub", "-", "structure", "predictions", "using", "locally", "-", "normalized", "scores", ".", "Because", "the", "neural", "and", "non", "-", "neural", "literature", "on", "this", "point", "has", "recently", "been", "reviewed", "by", "andor16globally", ",", "we", "simply", "note", "here", "that", "RNN", "models", "are", "typically", "locally", "normalized", ",", "and", "we", "are", "unaware", "of", "any", "specifically", "seq2seq", "work", "with", "RNNs", "that", "does", "not", "use", "locally", "-", "normalized", "scores", ".", "The", "model", "we", "introduce", "here", ",", "however", ",", "is", "not", "locally", "normalized", ",", "and", "so", "should", "not", "suffer", "from", "label", "bias", ".", "We", "also", "note", "that", "there", "are", "some", "(", "non", "-", "seq2seq", ")", "exceptions", "to", "the", "trend", "of", "locally", "normalized", "RNNs", ",", "such", "as", "the", "work", "of", "sak14sequence", "and", "voigtlaender15sequence", ",", "who", "train", "LSTMs", "in", "the", "context", "of", "HMMs", "for", "speech", "recognition", "using", "sequence", "-", "level", "objectives", ";", "their", "work", "does", "not", "consider", "search", ",", "however", ".", "section", ":", "Background", "and", "Notation", "In", "the", "simplest", "seq2seq", "scenario", ",", "we", "are", "given", "a", "collection", "of", "source", "-", "target", "sequence", "pairs", "and", "tasked", "with", "learning", "to", "generate", "target", "sequences", "from", "source", "sequences", ".", "For", "instance", ",", "we", "might", "view", "machine", "translation", "in", "this", "way", ",", "where", "in", "particular", "we", "attempt", "to", "generate", "English", "sentences", "from", "(", "corresponding", ")", "French", "sentences", ".", "Seq2seq", "models", "are", "part", "of", "the", "broader", "class", "of", "\u201c", "encoder", "-", "decoder", "\u201d", "models", ",", "which", "first", "use", "an", "encoding", "model", "to", "transform", "a", "source", "object", "into", "an", "encoded", "representation", ".", "Many", "different", "sequential", "(", "and", "non", "-", "sequential", ")", "encoders", "have", "proven", "to", "be", "effective", "for", "different", "source", "domains", ".", "In", "this", "work", "we", "are", "agnostic", "to", "the", "form", "of", "the", "encoding", "model", ",", "and", "simply", "assume", "an", "abstract", "source", "representation", ".", "Once", "the", "input", "sequence", "is", "encoded", ",", "seq2seq", "models", "generate", "a", "target", "sequence", "using", "a", "decoder", ".", "The", "decoder", "is", "tasked", "with", "generating", "a", "target", "sequence", "of", "words", "from", "a", "target", "vocabulary", ".", "In", "particular", ",", "words", "are", "generated", "sequentially", "by", "conditioning", "on", "the", "input", "representation", "and", "on", "the", "previously", "generated", "words", "or", "history", ".", "We", "use", "the", "notation", "to", "refer", "to", "an", "arbitrary", "word", "sequence", "of", "length", ",", "and", "the", "notation", "to", "refer", "to", "the", "gold", "(", "i.e.", ",", "correct", ")", "target", "word", "sequence", "for", "an", "input", ".", "Most", "seq2seq", "systems", "utilize", "a", "recurrent", "neural", "network", "(", "RNN", ")", "for", "the", "decoder", "model", ".", "Formally", ",", "a", "recurrent", "neural", "network", "is", "a", "parameterized", "non", "-", "linear", "function", "that", "recursively", "maps", "a", "sequence", "of", "vectors", "to", "a", "sequence", "of", "hidden", "states", ".", "Let", "be", "a", "sequence", "of", "vectors", ",", "and", "let", "be", "some", "initial", "state", "vector", ".", "Applying", "an", "RNN", "to", "any", "such", "sequence", "yields", "hidden", "states", "at", "each", "time", "-", "step", ",", "as", "follows", ":", "_", "t", "RNN", "(", "_", "t", ",", "_", "t", "-", "1", ";", ")", ",", "where", "is", "the", "set", "of", "model", "parameters", ",", "which", "are", "shared", "over", "time", ".", "In", "this", "work", ",", "the", "vectors", "will", "always", "correspond", "to", "the", "embeddings", "of", "a", "target", "word", "sequence", ",", "and", "so", "we", "will", "also", "write", ",", "with", "standing", "in", "for", "its", "embedding", ".", "RNN", "decoders", "are", "typically", "trained", "to", "act", "as", "conditional", "language", "models", ".", "That", "is", ",", "one", "attempts", "to", "model", "the", "probability", "of", "the", "\u2019", "th", "target", "word", "conditioned", "on", "and", "the", "target", "history", "by", "stipulating", "that", ",", "for", "some", "parameterized", "function", "typically", "computed", "with", "an", "affine", "layer", "followed", "by", "a", "softmax", ".", "In", "computing", "these", "probabilities", ",", "the", "state", "represents", "the", "target", "history", ",", "and", "is", "typically", "set", "to", "be", "some", "function", "of", ".", "The", "complete", "model", "(", "including", "encoder", ")", "is", "trained", ",", "analogously", "to", "a", "neural", "language", "model", ",", "to", "minimize", "the", "cross", "-", "entropy", "loss", "at", "each", "time", "-", "step", "while", "conditioning", "on", "the", "gold", "history", "in", "the", "training", "data", ".", "That", "is", ",", "the", "model", "is", "trained", "to", "minimize", ".", "Once", "the", "decoder", "is", "trained", ",", "discrete", "sequence", "generation", "can", "be", "performed", "by", "approximately", "maximizing", "the", "probability", "of", "the", "target", "sequence", "under", "the", "conditional", "distribution", ",", ",", "where", "we", "use", "the", "notation", "to", "emphasize", "that", "the", "decoding", "process", "requires", "heuristic", "search", ",", "since", "the", "RNN", "model", "is", "non", "-", "Markovian", ".", "In", "practice", ",", "a", "simple", "beam", "search", "procedure", "that", "explores", "prospective", "histories", "at", "each", "time", "-", "step", "has", "proven", "to", "be", "an", "effective", "decoding", "approach", ".", "However", ",", "as", "noted", "above", ",", "decoding", "in", "this", "manner", "after", "conditional", "language", "-", "model", "style", "training", "potentially", "suffers", "from", "the", "issues", "of", "exposure", "bias", "and", "label", "bias", ",", "which", "motivates", "the", "work", "of", "this", "paper", ".", "section", ":", "Beam", "Search", "Optimization", "We", "begin", "by", "making", "one", "small", "change", "to", "the", "seq2seq", "modeling", "framework", ".", "Instead", "of", "predicting", "the", "probability", "of", "the", "next", "word", ",", "we", "instead", "learn", "to", "produce", "(", "non", "-", "probabilistic", ")", "scores", "for", "ranking", "sequences", ".", "Define", "the", "score", "of", "a", "sequence", "consisting", "of", "history", "followed", "by", "a", "single", "word", "as", ",", "where", "is", "a", "parameterized", "function", "examining", "the", "current", "hidden", "-", "state", "of", "the", "relevant", "RNN", "at", "time", "as", "well", "as", "the", "input", "representation", ".", "In", "experiments", ",", "our", "will", "have", "an", "identical", "form", "to", "but", "without", "the", "final", "softmax", "transformation", "(", "which", "transforms", "unnormalized", "scores", "into", "probabilities", ")", ",", "thereby", "allowing", "the", "model", "to", "avoid", "issues", "associated", "with", "the", "label", "bias", "problem", ".", "More", "importantly", ",", "we", "also", "modify", "how", "this", "model", "is", "trained", ".", "Ideally", "we", "would", "train", "by", "comparing", "the", "gold", "sequence", "to", "the", "highest", "-", "scoring", "complete", "sequence", ".", "However", ",", "because", "finding", "the", "argmax", "sequence", "according", "to", "this", "model", "is", "intractable", ",", "we", "propose", "to", "adopt", "a", "LaSO", "-", "like", "scheme", "to", "train", ",", "which", "we", "will", "refer", "to", "as", "beam", "search", "optimization", "(", "BSO", ")", ".", "In", "particular", ",", "we", "define", "a", "loss", "that", "penalizes", "the", "gold", "sequence", "falling", "off", "the", "beam", "during", "training", ".", "The", "proposed", "training", "approach", "is", "a", "simple", "way", "to", "expose", "the", "model", "to", "incorrect", "histories", "and", "to", "match", "the", "training", "procedure", "to", "test", "generation", ".", "Furthermore", "we", "show", "that", "it", "can", "be", "implemented", "efficiently", "without", "changing", "the", "asymptotic", "run", "-", "time", "of", "training", ",", "beyond", "a", "factor", "of", "the", "beam", "size", ".", "subsection", ":", "Search", "-", "Based", "Loss", "We", "now", "formalize", "this", "notion", "of", "a", "search", "-", "based", "loss", "for", "RNN", "training", ".", "Assume", "we", "have", "a", "set", "of", "candidate", "sequences", "of", "length", ".", "We", "can", "calculate", "a", "score", "for", "each", "sequence", "in", "using", "a", "scoring", "function", "parameterized", "with", "an", "RNN", ",", "as", "above", ",", "and", "we", "define", "the", "sequence", "to", "be", "the", "\u2019", "th", "ranked", "sequence", "in", "according", "to", ".", "That", "is", ",", "assuming", "distinct", "scores", ",", "\u2014", "{", "^y:1t", "(", "k", ")", "\u2208S_t", "\u2223f", "(", "^y_t^", "(", "k", ")", ",", "^", "\u00bf", "f", "(", "^y_t^", "(", "K", ")", ",", "^", "=", "K", "-", "1", ",", "where", "is", "the", "\u2019", "th", "token", "in", ",", "is", "the", "RNN", "state", "corresponding", "to", "its", "\u2019", "st", "step", ",", "and", "where", "we", "have", "omitted", "the", "argument", "to", "for", "brevity", ".", "We", "now", "define", "a", "loss", "function", "that", "gives", "loss", "each", "time", "the", "score", "of", "the", "gold", "prefix", "does", "not", "exceed", "that", "of", "by", "a", "margin", ":", "(", "f", ")", "=", "_", "t=1^T", "(", ")", "1", "-", "f", "(", "y_t", ",", "_", "t", "-", "1", ")", "+", "f", "(", "_", "t^", "(", "K", ")", ",", "_", "t", "-", "1^", "(", "K", ")", ")", ".", "Above", ",", "the", "term", "denotes", "a", "mistake", "-", "specific", "cost", "-", "function", ",", "which", "allows", "us", "to", "scale", "the", "loss", "depending", "on", "the", "severity", "of", "erroneously", "predicting", ";", "it", "is", "assumed", "to", "return", "0", "when", "the", "margin", "requirement", "is", "satisfied", ",", "and", "a", "positive", "number", "otherwise", ".", "It", "is", "this", "term", "that", "allows", "us", "to", "use", "sequence", "-", "rather", "than", "word", "-", "level", "costs", "in", "training", "(", "addressing", "the", "2nd", "issue", "in", "the", "introduction", ")", ".", "For", "instance", ",", "when", "training", "a", "seq2seq", "model", "for", "machine", "translation", ",", "it", "may", "be", "desirable", "to", "have", "be", "inversely", "related", "to", "the", "partial", "sentence", "-", "level", "BLEU", "score", "of", "with", ";", "we", "experiment", "along", "these", "lines", "in", "Section", "[", "reference", "]", ".", "Finally", ",", "because", "we", "want", "the", "full", "gold", "sequence", "to", "be", "at", "the", "top", "of", "the", "beam", "at", "the", "end", "of", "search", ",", "when", "we", "modify", "the", "loss", "to", "require", "the", "score", "of", "to", "exceed", "the", "score", "of", "the", "highest", "ranked", "incorrect", "prediction", "by", "a", "margin", ".", "We", "can", "optimize", "the", "loss", "using", "a", "two", "-", "step", "process", ":", "(", "1", ")", "in", "a", "forward", "pass", ",", "we", "compute", "candidate", "sets", "and", "record", "margin", "violations", "(", "sequences", "with", "non", "-", "zero", "loss", ")", ";", "(", "2", ")", "in", "a", "backward", "pass", ",", "we", "back", "-", "propagate", "the", "errors", "through", "the", "seq2seq", "RNNs", ".", "Unlike", "standard", "seq2seq", "training", ",", "the", "first", "-", "step", "requires", "running", "search", "(", "in", "our", "case", "beam", "search", ")", "to", "find", "margin", "violations", ".", "The", "second", "step", "can", "be", "done", "by", "adapting", "back", "-", "propagation", "through", "time", "(", "BPTT", ")", ".", "We", "next", "discuss", "the", "details", "of", "this", "process", ".", "subsection", ":", "Forward", ":", "Find", "Violations", "In", "order", "to", "minimize", "this", "loss", ",", "we", "need", "to", "specify", "a", "procedure", "for", "constructing", "candidate", "sequences", "at", "each", "time", "step", "so", "that", "we", "find", "margin", "violations", ".", "We", "follow", "LaSO", "(", "rather", "than", "early", "-", "update", ";", "see", "Section", "[", "reference", "]", ")", "and", "build", "candidates", "in", "a", "recursive", "manner", ".", "If", "there", "was", "no", "margin", "violation", "at", ",", "then", "is", "constructed", "using", "a", "standard", "beam", "search", "update", ".", "If", "there", "was", "a", "margin", "violation", ",", "is", "constructed", "as", "the", "best", "sequences", "assuming", "the", "gold", "history", "through", "time", "-", "step", ".", "Formally", ",", "assume", "the", "function", "maps", "a", "sequence", "to", "the", "set", "of", "all", "valid", "sequences", "of", "length", "that", "can", "be", "formed", "by", "appending", "to", "it", "a", "valid", "word", ".", "In", "the", "simplest", ",", "unconstrained", "case", ",", "we", "will", "have", "(", ")", "=", "{", ",", "w", "w", "}", ".", "As", "an", "important", "aside", ",", "note", "that", "for", "some", "problems", "it", "may", "be", "preferable", "to", "define", "a", "function", "which", "imposes", "hard", "constraints", "on", "successor", "sequences", ".", "For", "instance", ",", "if", "we", "would", "like", "to", "use", "seq2seq", "models", "for", "parsing", "(", "by", "emitting", "a", "constituency", "or", "dependency", "structure", "encoded", "into", "a", "sequence", "in", "some", "way", ")", ",", "we", "will", "have", "hard", "constraints", "on", "the", "sequences", "the", "model", "can", "output", ",", "namely", ",", "that", "they", "represent", "valid", "parses", ".", "While", "hard", "constraints", "such", "as", "these", "would", "be", "difficult", "to", "add", "to", "standard", "seq2seq", "at", "training", "time", ",", "in", "our", "framework", "they", "can", "naturally", "be", "added", "to", "the", "function", ",", "allowing", "us", "to", "train", "with", "hard", "constraints", ";", "we", "experiment", "along", "these", "lines", "in", "Section", "[", "reference", "]", ",", "where", "we", "refer", "to", "a", "model", "trained", "with", "constrained", "beam", "search", "as", "ConBSO", ".", "Having", "defined", "an", "appropriate", "function", ",", "we", "specify", "the", "candidate", "set", "as", ":", "S_t", "=", "(", ")", "violation", "at", "t", "-", "1", "_", "k=1^K", "(", ")", "otherwise", ",", "where", "we", "have", "a", "margin", "violation", "at", "iff", ",", "and", "where", "considers", "the", "scores", "given", "by", ".", "This", "search", "procedure", "is", "illustrated", "in", "the", "top", "portion", "of", "Figure", "[", "reference", "]", ".", "In", "the", "forward", "pass", "of", "our", "training", "algorithm", ",", "shown", "as", "the", "first", "part", "of", "Algorithm", "[", "reference", "]", ",", "we", "run", "this", "version", "of", "beam", "search", "and", "collect", "all", "sequences", "and", "their", "hidden", "states", "that", "lead", "to", "losses", ".", "subsection", ":", "Backward", ":", "Merge", "Sequences", "Once", "we", "have", "collected", "margin", "violations", "we", "can", "run", "backpropagation", "to", "compute", "parameter", "updates", ".", "Assume", "a", "margin", "violation", "occurs", "at", "time", "-", "step", "between", "the", "predicted", "history", "and", "the", "gold", "history", ".", "As", "in", "standard", "seq2seq", "training", "we", "must", "back", "-", "propagate", "this", "error", "through", "the", "gold", "history", ";", "however", ",", "unlike", "seq2seq", "we", "also", "have", "a", "gradient", "for", "the", "wrongly", "predicted", "history", ".", "Recall", "that", "to", "back", "-", "propagate", "errors", "through", "an", "RNN", "we", "run", "a", "recursive", "backward", "procedure", "\u2014", "denoted", "below", "by", "\u2014", "at", "each", "time", "-", "step", ",", "which", "accumulates", "the", "gradients", "of", "next", "-", "step", "and", "future", "losses", "with", "respect", "to", ".", "We", "have", ":", "_", "_", "t", "BRNN", "(", "_", "_", "t", "_", "t", "+", "1", ",", "_", "_", "t", "+", "1", ")", ",", "where", "is", "the", "loss", "at", "step", ",", "deriving", ",", "for", "instance", ",", "from", "the", "score", ".", "Running", "this", "procedure", "from", "to", "is", "known", "as", "back", "-", "propagation", "through", "time", "(", "BPTT", ")", ".", "In", "determining", "the", "total", "computational", "cost", "of", "back", "-", "propagation", "here", ",", "first", "note", "that", "in", "the", "worst", "case", "there", "is", "one", "violation", "at", "each", "time", "-", "step", ",", "which", "leads", "to", "independent", ",", "incorrect", "sequences", ".", "Since", "we", "need", "to", "call", "times", "for", "each", "sequence", ",", "a", "naive", "strategy", "of", "running", "BPTT", "for", "each", "incorrect", "sequence", "would", "lead", "to", "an", "backward", "pass", ",", "rather", "than", "the", "time", "required", "for", "the", "standard", "seq2seq", "approach", ".", "Fortunately", ",", "our", "combination", "of", "search", "-", "strategy", "and", "loss", "make", "it", "possible", "to", "efficiently", "share", "operations", ".", "This", "shared", "structure", "comes", "naturally", "from", "the", "LaSO", "update", ",", "which", "resets", "the", "beam", "in", "a", "convenient", "way", ".", "We", "informally", "illustrate", "the", "process", "in", "Figure", "[", "reference", "]", ".", "The", "top", "of", "the", "diagram", "shows", "a", "possible", "sequence", "of", "formed", "during", "search", "with", "a", "beam", "of", "size", "3", "for", "the", "target", "sequence", "\u201c", "a", "red", "dog", "runs", "quickly", "today", ".", "\u201d", "When", "the", "gold", "sequence", "falls", "off", "the", "beam", "at", ",", "search", "resumes", "with", ",", "and", "so", "all", "subsequent", "predicted", "sequences", "have", "as", "a", "prefix", "and", "are", "thus", "functions", "of", ".", "Moreover", ",", "because", "our", "loss", "function", "only", "involves", "the", "scores", "of", "the", "gold", "prefix", "and", "the", "violating", "prefix", ",", "we", "end", "up", "with", "the", "relatively", "simple", "computation", "tree", "shown", "at", "the", "bottom", "of", "Figure", "[", "reference", "]", ".", "It", "is", "evident", "that", "we", "can", "backpropagate", "in", "a", "single", "pass", ",", "accumulating", "gradients", "from", "sequences", "that", "diverge", "from", "the", "gold", "at", "the", "time", "-", "step", "that", "precedes", "their", "divergence", ".", "The", "second", "half", "of", "Algorithm", "[", "reference", "]", "shows", "this", "explicitly", "for", "a", "single", "sequence", ",", "though", "it", "is", "straightforward", "to", "extend", "the", "algorithm", "to", "operate", "in", "batch", ".", "[", "t", "!", "]", "{", "algorithmic}", "[", "1", "]", "empty", "storage", "^y:1", "T", "and", "^", "init", "S1", "\u2190\u2062violations{0", "}", "if", "\u2260tT", "else", "\u2062", "t", "to", "\u2062violations", "/", "*Backward*", "/", "\u2190\u2062grad_^", "Seq2seq", "Beam", "-", "Search", "Optimization", "section", ":", "Data", "and", "Methods", "We", "run", "experiments", "on", "three", "different", "tasks", ",", "comparing", "our", "approach", "to", "the", "seq2seq", "baseline", ",", "and", "to", "other", "relevant", "baselines", ".", "subsection", ":", "Model", "While", "the", "method", "we", "describe", "applies", "to", "seq2seq", "RNNs", "in", "general", ",", "for", "all", "experiments", "we", "use", "the", "global", "attention", "model", "of", "luong15effective", "\u2014", "which", "consists", "of", "an", "LSTM", "encoder", "and", "an", "LSTM", "decoder", "with", "a", "global", "attention", "model", "\u2014", "as", "both", "the", "baseline", "seq2seq", "model", "(", "i.e.", ",", "as", "the", "model", "that", "computes", "the", "in", "Section", "[", "reference", "]", ")", "and", "as", "the", "model", "that", "computes", "our", "sequence", "-", "scores", ".", "As", "in", "luong15effective", ",", "we", "also", "use", "\u201c", "input", "feeding", ",", "\u201d", "which", "involves", "feeding", "the", "attention", "distribution", "from", "the", "previous", "time", "-", "step", "into", "the", "decoder", "at", "the", "current", "step", ".", "This", "model", "architecture", "has", "been", "found", "to", "be", "highly", "performant", "for", "neural", "machine", "translation", "and", "other", "seq2seq", "tasks", ".", "To", "distinguish", "the", "models", "we", "refer", "to", "our", "system", "as", "BSO", "(", "beam", "search", "optimization", ")", "and", "to", "the", "baseline", "as", "seq2seq", ".", "When", "we", "apply", "constrained", "training", "(", "as", "discussed", "in", "Section", "[", "reference", "]", ")", ",", "we", "refer", "to", "the", "model", "as", "ConBSO", ".", "In", "providing", "results", "we", "also", "distinguish", "between", "the", "beam", "size", "with", "which", "the", "model", "is", "trained", ",", "and", "the", "beam", "size", "which", "is", "used", "at", "test", "-", "time", ".", "In", "general", ",", "if", "we", "plan", "on", "evaluating", "with", "a", "beam", "of", "size", "it", "makes", "sense", "to", "train", "with", "a", "beam", "of", "size", ",", "since", "our", "objective", "requires", "the", "gold", "sequence", "to", "be", "scored", "higher", "than", "the", "last", "sequence", "on", "the", "beam", ".", "subsection", ":", "Methodology", "Here", "we", "detail", "additional", "techniques", "we", "found", "necessary", "to", "ensure", "the", "model", "learned", "effectively", ".", "First", ",", "we", "found", "that", "the", "model", "failed", "to", "learn", "when", "trained", "from", "a", "random", "initialization", ".", "We", "therefore", "found", "it", "necessary", "to", "pre", "-", "train", "the", "model", "using", "a", "standard", ",", "word", "-", "level", "cross", "-", "entropy", "loss", "as", "described", "in", "Section", "[", "reference", "]", ".", "The", "necessity", "of", "pre", "-", "training", "in", "this", "instance", "is", "consistent", "with", "the", "findings", "of", "other", "authors", "who", "train", "non", "-", "local", "neural", "models", ".", "Similarly", ",", "it", "is", "clear", "that", "the", "smaller", "the", "beam", "used", "in", "training", "is", ",", "the", "less", "room", "the", "model", "has", "to", "make", "erroneous", "predictions", "without", "running", "afoul", "of", "the", "margin", "loss", ".", "Accordingly", ",", "we", "also", "found", "it", "useful", "to", "use", "a", "\u201c", "curriculum", "beam", "\u201d", "strategy", "in", "training", ",", "whereby", "the", "size", "of", "the", "beam", "is", "increased", "gradually", "during", "training", ".", "In", "particular", ",", "given", "a", "desired", "training", "beam", "size", ",", "we", "began", "training", "with", "a", "beam", "of", "size", "2", ",", "and", "increased", "it", "by", "1", "every", "2", "epochs", "until", "reaching", ".", "Finally", ",", "it", "has", "been", "established", "that", "dropout", "regularization", "improves", "the", "performance", "of", "LSTMs", ",", "and", "in", "our", "experiments", "we", "run", "beam", "search", "under", "dropout", ".", "For", "all", "experiments", ",", "we", "trained", "both", "seq2seq", "and", "BSO", "models", "with", "mini", "-", "batch", "Adagrad", "(", "using", "batches", "of", "size", "64", ")", ",", "and", "we", "renormalized", "all", "gradients", "so", "they", "did", "not", "exceed", "5", "before", "updating", "parameters", ".", "We", "did", "not", "extensively", "tune", "learning", "-", "rates", ",", "but", "we", "found", "initial", "rates", "of", "0.02", "for", "the", "encoder", "and", "decoder", "LSTMs", ",", "and", "a", "rate", "of", "0.1", "or", "0.2", "for", "the", "final", "linear", "layer", "(", "i.e.", ",", "the", "layer", "tasked", "with", "making", "word", "-", "predictions", "at", "each", "time", "-", "step", ")", "to", "work", "well", "across", "all", "the", "tasks", "we", "considered", ".", "Code", "implementing", "the", "experiments", "described", "below", "can", "be", "found", "at", ".", "subsection", ":", "Tasks", "and", "Results", "Our", "experiments", "are", "primarily", "intended", "to", "evaluate", "the", "effectiveness", "of", "beam", "search", "optimization", "over", "standard", "seq2seq", "training", ".", "As", "such", ",", "we", "run", "experiments", "with", "the", "same", "model", "across", "three", "very", "different", "problems", ":", "word", "ordering", ",", "dependency", "parsing", ",", "and", "machine", "translation", ".", "While", "we", "do", "not", "include", "all", "the", "features", "and", "extensions", "necessary", "to", "reach", "state", "-", "of", "-", "the", "-", "art", "performance", ",", "even", "the", "baseline", "seq2seq", "model", "is", "generally", "quite", "performant", ".", "paragraph", ":", "Word", "Ordering", "The", "task", "of", "correctly", "ordering", "the", "words", "in", "a", "shuffled", "sentence", "has", "recently", "gained", "some", "attention", "as", "a", "way", "to", "test", "the", "(", "syntactic", ")", "capabilities", "of", "text", "-", "generation", "systems", ".", "We", "cast", "this", "task", "as", "seq2seq", "problem", "by", "viewing", "a", "shuffled", "sentence", "as", "a", "source", "sentence", ",", "and", "the", "correctly", "ordered", "sentence", "as", "the", "target", ".", "While", "word", "ordering", "is", "a", "somewhat", "synthetic", "task", ",", "it", "has", "two", "interesting", "properties", "for", "our", "purposes", ".", "First", ",", "it", "is", "a", "task", "which", "plausibly", "requires", "search", "(", "due", "to", "the", "exponentially", "many", "possible", "orderings", ")", ",", "and", ",", "second", ",", "there", "is", "a", "clear", "hard", "constraint", "on", "output", "sequences", ",", "namely", ",", "that", "they", "be", "a", "permutation", "of", "the", "source", "sequence", ".", "For", "both", "the", "baseline", "and", "BSO", "models", "we", "enforce", "this", "constraint", "at", "test", "-", "time", ".", "However", ",", "we", "also", "experiment", "with", "constraining", "the", "BSO", "model", "during", "training", ",", "as", "described", "in", "Section", "[", "reference", "]", ",", "by", "defining", "the", "function", "to", "only", "allow", "successor", "sequences", "containing", "un", "-", "used", "words", "in", "the", "source", "sentence", ".", "For", "experiments", ",", "we", "use", "the", "same", "PTB", "dataset", "(", "with", "the", "standard", "training", ",", "development", ",", "and", "test", "splits", ")", "and", "evaluation", "procedure", "as", "in", "zhang15discriminative", "and", "later", "work", ",", "with", "performance", "reported", "in", "terms", "of", "BLEU", "score", "with", "the", "correctly", "ordered", "sentences", ".", "For", "all", "word", "-", "ordering", "experiments", "we", "use", "2", "-", "layer", "encoder", "and", "decoder", "LSTMs", ",", "each", "with", "256", "hidden", "units", ",", "and", "dropout", "with", "a", "rate", "of", "0.2", "between", "LSTM", "layers", ".", "We", "use", "simple", "0", "/", "1", "costs", "in", "defining", "the", "function", ".", "We", "show", "our", "test", "-", "set", "results", "in", "Table", "[", "reference", "]", ".", "We", "see", "that", "on", "this", "task", "there", "is", "a", "large", "improvement", "at", "each", "beam", "size", "from", "switching", "to", "BSO", ",", "and", "a", "further", "improvement", "from", "using", "the", "constrained", "model", ".", "Inspired", "by", "a", "similar", "analysis", "in", "daume05learning", ",", "we", "further", "examine", "the", "relationship", "between", "and", "when", "training", "with", "ConBSO", "in", "Table", "[", "reference", "]", ".", "We", "see", "that", "larger", "hurt", "greedy", "inference", ",", "but", "that", "results", "continue", "to", "improve", ",", "at", "least", "initially", ",", "when", "using", "a", "that", "is", "(", "somewhat", ")", "bigger", "than", ".", "paragraph", ":", "Dependency", "Parsing", "We", "next", "apply", "our", "model", "to", "dependency", "parsing", ",", "which", "also", "has", "hard", "constraints", "and", "plausibly", "benefits", "from", "search", ".", "We", "treat", "dependency", "parsing", "with", "arc", "-", "standard", "transitions", "as", "a", "seq2seq", "task", "by", "attempting", "to", "map", "from", "a", "source", "sentence", "to", "a", "target", "sequence", "of", "source", "sentence", "words", "interleaved", "with", "the", "arc", "-", "standard", ",", "reduce", "-", "actions", "in", "its", "parse", ".", "For", "example", ",", "we", "attempt", "to", "map", "the", "source", "sentence", "But", "it", "was", "the", "Quotron", "problems", "that", "\u2026", "to", "the", "target", "sequence", "But", "it", "was", "@L_SBJ", "@L_DEP", "the", "Quotron", "problems", "@L_NMOD", "@L_NMOD", "that", "\u2026", "We", "use", "the", "standard", "Penn", "Treebank", "dataset", "splits", "with", "Stanford", "dependency", "labels", ",", "and", "the", "standard", "UAS", "/", "LAS", "evaluation", "metric", "(", "excluding", "punctuation", ")", "following", "chen14fast", ".", "All", "models", "thus", "see", "only", "the", "words", "in", "the", "source", "and", ",", "when", "decoding", ",", "the", "actions", "it", "has", "emitted", "so", "far", ";", "no", "other", "features", "are", "used", ".", "We", "use", "2", "-", "layer", "encoder", "and", "decoder", "LSTMs", "with", "300", "hidden", "units", "per", "layer", "and", "dropout", "with", "a", "rate", "of", "0.3", "between", "LSTM", "layers", ".", "We", "replace", "singleton", "words", "in", "the", "training", "set", "with", "an", "UNK", "token", ",", "normalize", "digits", "to", "a", "single", "symbol", ",", "and", "initialize", "word", "embeddings", "for", "both", "source", "and", "target", "words", "from", "the", "publicly", "available", "word2vec", "embeddings", ".", "We", "use", "simple", "0", "/", "1", "costs", "in", "defining", "the", "function", ".", "As", "in", "the", "word", "-", "ordering", "case", ",", "we", "also", "experiment", "with", "modifying", "the", "function", "in", "order", "to", "train", "under", "hard", "constraints", ",", "namely", ",", "that", "the", "emitted", "target", "sequence", "be", "a", "valid", "parse", ".", "In", "particular", ",", "we", "constrain", "the", "output", "at", "each", "time", "-", "step", "to", "obey", "the", "stack", "constraint", ",", "and", "we", "ensure", "words", "in", "the", "source", "are", "emitted", "in", "order", ".", "We", "show", "results", "on", "the", "test", "-", "set", "in", "Table", "[", "reference", "]", ".", "BSO", "and", "ConBSO", "both", "show", "significant", "improvements", "over", "seq2seq", ",", "with", "ConBSO", "improving", "most", "on", "UAS", ",", "and", "BSO", "improving", "most", "on", "LAS", ".", "We", "achieve", "a", "reasonable", "final", "score", "of", "91.57", "UAS", ",", "which", "lags", "behind", "the", "state", "-", "of", "-", "the", "-", "art", ",", "but", "is", "promising", "for", "a", "general", "-", "purpose", ",", "word", "-", "only", "model", ".", "paragraph", ":", "Translation", "We", "finally", "evaluate", "our", "model", "on", "a", "small", "machine", "translation", "dataset", ",", "which", "allows", "us", "to", "experiment", "with", "a", "cost", "function", "that", "is", "not", "0", "/", "1", ",", "and", "to", "consider", "other", "baselines", "that", "attempt", "to", "mitigate", "exposure", "bias", "in", "the", "seq2seq", "setting", ".", "We", "use", "the", "dataset", "from", "the", "work", "of", "ranzato16sequence", ",", "which", "uses", "data", "from", "the", "German", "-", "to", "-", "English", "portion", "of", "the", "IWSLT", "2014", "machine", "translation", "evaluation", "campaign", ".", "The", "data", "comes", "from", "translated", "TED", "talks", ",", "and", "the", "dataset", "contains", "roughly", "153", "K", "training", "sentences", ",", "7", "K", "development", "sentences", ",", "and", "7", "K", "test", "sentences", ".", "We", "use", "the", "same", "preprocessing", "and", "dataset", "splits", "as", "ranzato16sequence", ",", "and", "like", "them", "we", "also", "use", "a", "single", "-", "layer", "LSTM", "decoder", "with", "256", "units", ".", "We", "also", "use", "dropout", "with", "a", "rate", "of", "0.2", "between", "each", "LSTM", "layer", ".", "We", "emphasize", ",", "however", ",", "that", "while", "our", "decoder", "LSTM", "is", "of", "the", "same", "size", "as", "that", "of", "ranzato16sequence", ",", "our", "results", "are", "not", "directly", "comparable", ",", "because", "we", "use", "an", "LSTM", "encoder", "(", "rather", "than", "a", "convolutional", "encoder", "as", "they", "do", ")", ",", "a", "slightly", "different", "attention", "mechanism", ",", "and", "input", "feeding", ".", "For", "our", "main", "MT", "results", ",", "we", "set", "to", ",", "where", "is", "the", "last", "margin", "violation", "and", "denotes", "smoothed", ",", "sentence", "-", "level", "BLEU", ".", "This", "setting", "of", "should", "act", "to", "penalize", "erroneous", "predictions", "with", "a", "relatively", "low", "sentence", "-", "level", "BLEU", "score", "more", "than", "those", "with", "a", "relatively", "high", "sentence", "-", "level", "BLEU", "score", ".", "In", "Table", "[", "reference", "]", "we", "show", "our", "final", "results", "and", "those", "from", "ranzato16sequence", ".", "While", "we", "start", "with", "an", "improved", "baseline", ",", "we", "see", "similarly", "large", "increases", "in", "accuracy", "as", "those", "obtained", "by", "DAD", "and", "MIXER", ",", "in", "particular", "when", ".", "We", "further", "investigate", "the", "utility", "of", "these", "sequence", "-", "level", "costs", "in", "Table", "[", "reference", "]", ",", "which", "compares", "using", "sentence", "-", "level", "BLEU", "costs", "in", "defining", "with", "using", "0", "/", "1", "costs", ".", "We", "see", "that", "the", "more", "sophisticated", "sequence", "-", "level", "costs", "have", "a", "moderate", "effect", "on", "BLEU", "score", ".", "paragraph", ":", "Timing", "Given", "Algorithm", "[", "reference", "]", ",", "we", "would", "expect", "training", "time", "to", "increase", "linearly", "with", "the", "size", "of", "the", "beam", ".", "On", "the", "above", "MT", "task", ",", "our", "highly", "tuned", "seq2seq", "baseline", "processes", "an", "average", "of", "13", ",", "038", "tokens", "/", "second", "(", "including", "both", "source", "and", "target", "tokens", ")", "on", "a", "GTX", "970", "GPU", ".", "For", "beams", "of", "size", "=", "2", ",", "3", ",", "4", ",", "5", ",", "and", "6", ",", "our", "implementation", "processes", "on", "average", "1", ",", "985", ",", "1", ",", "768", ",", "1", ",", "709", ",", "1", ",", "521", ",", "and", "1", ",", "458", "tokens", "/", "second", ",", "respectively", ".", "Thus", ",", "we", "appear", "to", "pay", "an", "initial", "constant", "factor", "of", "due", "to", "the", "more", "complicated", "forward", "and", "backward", "passes", ",", "and", "then", "training", "scales", "with", "the", "size", "of", "the", "beam", ".", "Because", "we", "batch", "beam", "predictions", "on", "a", "GPU", ",", "however", ",", "we", "find", "that", "in", "practice", "training", "time", "scales", "sub", "-", "linearly", "with", "the", "beam", "-", "size", ".", "section", ":", "Conclusion", "We", "have", "introduced", "a", "variant", "of", "seq2seq", "and", "an", "associated", "beam", "search", "training", "scheme", ",", "which", "addresses", "exposure", "bias", "as", "well", "as", "label", "bias", ",", "and", "moreover", "allows", "for", "both", "training", "with", "sequence", "-", "level", "cost", "functions", "as", "well", "as", "with", "hard", "constraints", ".", "Future", "work", "will", "examine", "scaling", "this", "approach", "to", "much", "larger", "datasets", ".", "section", ":", "Acknowledgments", "We", "thank", "Yoon", "Kim", "for", "helpful", "discussions", "and", "for", "providing", "the", "initial", "seq2seq", "code", "on", "which", "our", "implementations", "are", "based", ".", "We", "thank", "Allen", "Schmaltz", "for", "help", "with", "the", "word", "ordering", "experiments", ".", "We", "also", "gratefully", "acknowledge", "the", "support", "of", "a", "Google", "Research", "Award", ".", "bibliography", ":", "References"]}