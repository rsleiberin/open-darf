{"coref": {"BLEU_score": [[142, 144], [191, 193], [218, 220], [904, 906], [965, 967], [1069, 1071], [1080, 1081], [2475, 2478], [3294, 3296], [3305, 3307], [3331, 3333], [3479, 3481], [156, 158], [979, 981], [3281, 3283]], "LSTM": [[82, 87], [88, 89], [139, 140], [171, 172], [204, 205], [237, 238], [286, 287], [614, 619], [620, 621], [637, 638], [664, 665], [683, 684], [703, 704], [917, 919], [985, 986], [1043, 1044], [1103, 1104], [1192, 1193], [1228, 1229], [1266, 1267], [1507, 1512], [1513, 1514], [1528, 1529], [1539, 1540], [1564, 1565], [1589, 1590], [1692, 1693], [1761, 1762], [1794, 1795], [1806, 1808], [1810, 1812], [1817, 1818], [1855, 1856], [1919, 1920], [2204, 2205], [2369, 2370], [2401, 2402], [2424, 2425], [2439, 2440], [2688, 2689], [2701, 2702], [2726, 2727], [2750, 2752], [2800, 2802], [2805, 2807], [2844, 2845], [2967, 2968], [3174, 3175], [3204, 3205], [3400, 3403], [3441, 3444], [3475, 3476], [3515, 3516], [4257, 4258], [4271, 4272], [4302, 4303], [107, 108], [154, 155], [1600, 1601], [1637, 1638], [2153, 2154], [2463, 2464], [2740, 2741], [2788, 2789], [2865, 2866], [2874, 2875], [2890, 2891], [3126, 3127], [3423, 3424], [3863, 3864], [4077, 4078], [4118, 4119]], "LSTM5": [], "Machine_Translation": [[527, 529], [844, 846], [954, 956], [2039, 2041], [3643, 3645], [3677, 3679], [3897, 3899], [4123, 4124], [1934, 1936], [3455, 3457]], "SMT": [[214, 216], [969, 971], [1055, 1057], [1910, 1912], [1953, 1955], [1967, 1969], [1976, 1978], [2068, 2069], [3446, 3451], [3888, 3890], [4097, 4101], [4333, 4335], [187, 189], [1035, 1037]], "SMT_LSTM5": [], "WMT2014_English-French": [[131, 133], [1997, 2002], [2023, 2025]]}, "coref_non_salient": {"0": [[59, 65], [853, 856]], "1": [[337, 339], [340, 343], [524, 526], [861, 863]], "10": [[535, 537], [4107, 4112]], "11": [[1326, 1332], [1351, 1352], [1364, 1365], [1395, 1396], [1436, 1437], [1450, 1451], [1473, 1474], [1489, 1490], [3658, 3662], [3663, 3664], [4204, 4205], [4227, 4228]], "12": [[817, 820], [3906, 3908]], "13": [[3736, 3738]], "14": [[2, 6], [66, 68], [757, 762], [1419, 1422], [4133, 4136], [4188, 4190]], "15": [[4172, 4174]], "16": [[309, 311], [1168, 1170]], "17": [[13, 14], [29, 30], [320, 321], [344, 345], [368, 369], [407, 408], [440, 441], [476, 477], [585, 586]], "18": [[3690, 3692], [3767, 3769]], "19": [[3407, 3409]], "2": [[7, 9], [9, 12], [316, 319], [389, 391], [763, 765], [822, 824], [866, 868], [957, 960], [3640, 3642], [3840, 3843], [3884, 3886], [3901, 3903]], "20": [[808, 812], [3965, 3969]], "21": [[935, 944], [2216, 2224], [3764, 3765]], "22": [[803, 805], [2142, 2143], [3745, 3746]], "23": [[2974, 2977]], "24": [[1189, 1190], [1893, 1894]], "25": [[3352, 3354], [3430, 3432]], "26": [[2832, 2833]], "27": [[2140, 2141]], "28": [[3320, 3322]], "29": [[2913, 2915], [2926, 2928]], "3": [[1973, 1974], [3286, 3287], [3696, 3698], [4345, 4347]], "30": [[2908, 2909]], "31": [[2178, 2180]], "32": [[3939, 3941]], "33": [[2356, 2358]], "34": [[3315, 3317]], "35": [[3263, 3264], [3988, 3994]], "36": [[4012, 4014]], "37": [[3087, 3088]], "38": [[3624, 3625]], "39": [[325, 328]], "4": [[124, 129], [894, 900], [4207, 4212]], "40": [[3121, 3124]], "41": [[1785, 1787]], "42": [[241, 245], [1269, 1271], [1987, 1989]], "43": [[3673, 3674], [3718, 3719], [3733, 3734], [3761, 3762], [3779, 3780]], "44": [[2813, 2814]], "45": [[2904, 2907]], "46": [[653, 658], [1243, 1248], [1573, 1577]], "47": [[352, 354]], "48": [[24, 27]], "49": [[3323, 3325]], "5": [[687, 692], [1335, 1340], [2072, 2075], [3667, 3672]], "50": [[2078, 2080]], "51": [[395, 397]], "52": [[2412, 2415]], "6": [[565, 569], [1017, 1024]], "7": [[530, 532], [625, 630], [4361, 4365]], "8": [[412, 414], [459, 461], [2624, 2625]], "9": [[3137, 3138], [3165, 3169]]}, "doc_id": "39dba6f22d72853561a4ed684be265e179a39e4f", "method_subrelations": {"LSTM": [[[0, 4], "LSTM"]], "SMT_LSTM5": [[[0, 3], "SMT"], [[4, 9], "LSTM5"]]}, "n_ary_relations": [{"Material": "WMT2014_English-French", "Method": "LSTM", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": 34.81}, {"Material": "WMT2014_English-French", "Method": "SMT_LSTM5", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": 36.5}], "ner": [[2, 6, "Task"], [7, 9, "Method"], [9, 12, "Method"], [13, 14, "Method"], [24, 27, "Task"], [29, 30, "Method"], [59, 65, "Method"], [66, 68, "Task"], [82, 87, "Method"], [88, 89, "Method"], [124, 129, "Task"], [131, 133, "Material"], [139, 140, "Method"], [142, 144, "Metric"], [171, 172, "Method"], [191, 193, "Metric"], [204, 205, "Method"], [214, 216, "Method"], [218, 220, "Metric"], [237, 238, "Method"], [241, 245, "Method"], [286, 287, "Method"], [309, 311, "Task"], [316, 319, "Method"], [320, 321, "Method"], [325, 328, "Method"], [337, 339, "Task"], [340, 343, "Task"], [344, 345, "Method"], [352, 354, "Task"], [368, 369, "Method"], [389, 391, "Method"], [395, 397, "Method"], [407, 408, "Method"], [412, 414, "Method"], [440, 441, "Method"], [459, 461, "Method"], [476, 477, "Method"], [524, 526, "Task"], [527, 529, "Task"], [530, 532, "Task"], [535, 537, "Task"], [565, 569, "Method"], [585, 586, "Method"], [614, 619, "Method"], [620, 621, "Method"], [625, 630, "Task"], [637, 638, "Method"], [653, 658, "Method"], [664, 665, "Method"], [683, 684, "Method"], [687, 692, "Method"], [703, 704, "Method"], [757, 762, "Task"], [763, 765, "Method"], [803, 805, "Task"], [808, 812, "Method"], [817, 820, "Method"], [822, 824, "Method"], [844, 846, "Task"], [853, 856, "Method"], [861, 863, "Task"], [866, 868, "Method"], [894, 900, "Task"], [904, 906, "Metric"], [917, 919, "Method"], [935, 944, "Method"], [954, 956, "Task"], [957, 960, "Method"], [965, 967, "Metric"], [969, 971, "Method"], [985, 986, "Method"], [1017, 1024, "Method"], [1043, 1044, "Method"], [1055, 1057, "Method"], [1069, 1071, "Metric"], [1080, 1081, "Metric"], [1103, 1104, "Method"], [1168, 1170, "Task"], [1189, 1190, "Method"], [1192, 1193, "Method"], [1228, 1229, "Method"], [1243, 1248, "Method"], [1266, 1267, "Method"], [1269, 1271, "Method"], [1326, 1332, "Method"], [1335, 1340, "Method"], [1351, 1352, "Method"], [1364, 1365, "Method"], [1395, 1396, "Method"], [1419, 1422, "Task"], [1436, 1437, "Method"], [1450, 1451, "Method"], [1473, 1474, "Method"], [1489, 1490, "Method"], [1507, 1512, "Method"], [1513, 1514, "Method"], [1528, 1529, "Method"], [1539, 1540, "Method"], [1564, 1565, "Method"], [1573, 1577, "Method"], [1589, 1590, "Method"], [1692, 1693, "Method"], [1761, 1762, "Method"], [1785, 1787, "Metric"], [1794, 1795, "Method"], [1806, 1808, "Method"], [1810, 1812, "Method"], [1817, 1818, "Method"], [1855, 1856, "Method"], [1893, 1894, "Method"], [1910, 1912, "Method"], [1919, 1920, "Method"], [1953, 1955, "Method"], [1967, 1969, "Method"], [1973, 1974, "Metric"], [1976, 1978, "Method"], [1987, 1989, "Method"], [1997, 2002, "Material"], [2023, 2025, "Material"], [2039, 2041, "Task"], [2068, 2069, "Method"], [2072, 2075, "Method"], [2078, 2080, "Method"], [2140, 2141, "Task"], [2142, 2143, "Task"], [2178, 2180, "Metric"], [2204, 2205, "Method"], [2216, 2224, "Method"], [2356, 2358, "Method"], [2369, 2370, "Method"], [2401, 2402, "Method"], [2412, 2415, "Metric"], [2424, 2425, "Method"], [2439, 2440, "Method"], [2475, 2478, "Metric"], [2624, 2625, "Method"], [2688, 2689, "Method"], [2701, 2702, "Method"], [2726, 2727, "Method"], [2750, 2752, "Method"], [2800, 2802, "Method"], [2805, 2807, "Method"], [2813, 2814, "Metric"], [2832, 2833, "Method"], [2844, 2845, "Method"], [2904, 2907, "Method"], [2908, 2909, "Method"], [2913, 2915, "Metric"], [2926, 2928, "Metric"], [2967, 2968, "Method"], [2974, 2977, "Task"], [3087, 3088, "Method"], [3121, 3124, "Method"], [3137, 3138, "Method"], [3165, 3169, "Method"], [3174, 3175, "Method"], [3204, 3205, "Method"], [3263, 3264, "Task"], [3286, 3287, "Metric"], [3294, 3296, "Metric"], [3305, 3307, "Metric"], [3315, 3317, "Method"], [3320, 3322, "Task"], [3323, 3325, "Metric"], [3331, 3333, "Metric"], [3352, 3354, "Method"], [3400, 3403, "Method"], [3407, 3409, "Method"], [3430, 3432, "Method"], [3441, 3444, "Method"], [3446, 3451, "Method"], [3475, 3476, "Method"], [3479, 3481, "Metric"], [3515, 3516, "Method"], [3624, 3625, "Method"], [3640, 3642, "Method"], [3643, 3645, "Task"], [3658, 3662, "Method"], [3663, 3664, "Method"], [3667, 3672, "Method"], [3673, 3674, "Method"], [3677, 3679, "Task"], [3690, 3692, "Method"], [3696, 3698, "Metric"], [3718, 3719, "Method"], [3733, 3734, "Method"], [3736, 3738, "Method"], [3745, 3746, "Task"], [3761, 3762, "Method"], [3764, 3765, "Method"], [3767, 3769, "Method"], [3779, 3780, "Method"], [3840, 3843, "Method"], [3884, 3886, "Method"], [3888, 3890, "Method"], [3897, 3899, "Task"], [3901, 3903, "Method"], [3906, 3908, "Method"], [3939, 3941, "Task"], [3965, 3969, "Method"], [3988, 3994, "Task"], [4012, 4014, "Method"], [4097, 4101, "Method"], [4107, 4112, "Task"], [4123, 4124, "Task"], [4133, 4136, "Task"], [4172, 4174, "Method"], [4188, 4190, "Task"], [4204, 4205, "Method"], [4207, 4212, "Task"], [4227, 4228, "Method"], [4257, 4258, "Method"], [4271, 4272, "Method"], [4302, 4303, "Method"], [4333, 4335, "Method"], [4345, 4347, "Metric"], [4361, 4365, "Task"], [107, 108, "Method"], [154, 155, "Method"], [156, 158, "Metric"], [187, 189, "Method"], [979, 981, "Metric"], [1035, 1037, "Method"], [1600, 1601, "Method"], [1637, 1638, "Method"], [1934, 1936, "Task"], [2153, 2154, "Method"], [2463, 2464, "Method"], [2740, 2741, "Method"], [2788, 2789, "Method"], [2865, 2866, "Method"], [2874, 2875, "Method"], [2890, 2891, "Method"], [3126, 3127, "Method"], [3281, 3283, "Metric"], [3423, 3424, "Method"], [3455, 3457, "Task"], [3863, 3864, "Method"], [4077, 4078, "Method"], [4118, 4119, "Method"]], "sections": [[0, 313], [313, 1321], [1321, 1921], [1921, 1990], [1990, 2138], [2138, 2416], [2416, 2732], [2732, 3117], [3117, 3273], [3273, 3502], [3502, 3546], [3546, 3626], [3626, 4064], [4064, 4366], [4366, 4415], [4415, 4418]], "sentences": [[0, 9], [9, 28], [28, 51], [51, 77], [77, 117], [117, 168], [168, 180], [180, 200], [200, 236], [236, 263], [263, 313], [313, 316], [316, 344], [344, 361], [361, 386], [386, 404], [404, 429], [429, 470], [470, 497], [497, 521], [521, 533], [533, 559], [559, 580], [580, 602], [602, 631], [631, 676], [676, 681], [681, 702], [702, 740], [740, 745], [745, 766], [766, 796], [796, 813], [813, 852], [852, 882], [882, 892], [892, 945], [945, 961], [961, 977], [977, 1011], [1011, 1038], [1038, 1062], [1062, 1100], [1100, 1123], [1123, 1155], [1155, 1176], [1176, 1185], [1185, 1201], [1201, 1223], [1223, 1249], [1249, 1294], [1294, 1321], [1321, 1325], [1325, 1343], [1343, 1385], [1385, 1415], [1415, 1465], [1465, 1504], [1504, 1535], [1535, 1563], [1563, 1634], [1634, 1642], [1642, 1678], [1678, 1742], [1742, 1755], [1755, 1801], [1801, 1822], [1822, 1841], [1841, 1868], [1868, 1906], [1906, 1921], [1921, 1924], [1924, 1940], [1940, 1970], [1970, 1990], [1990, 1994], [1994, 2003], [2003, 2036], [2036, 2070], [2070, 2093], [2093, 2121], [2121, 2138], [2138, 2143], [2143, 2159], [2159, 2187], [2187, 2244], [2244, 2263], [2263, 2275], [2275, 2288], [2288, 2318], [2318, 2330], [2330, 2365], [2365, 2383], [2383, 2416], [2416, 2422], [2422, 2458], [2458, 2488], [2488, 2518], [2518, 2548], [2548, 2563], [2563, 2587], [2587, 2622], [2622, 2652], [2652, 2686], [2686, 2712], [2712, 2732], [2732, 2736], [2736, 2748], [2748, 2785], [2785, 2798], [2798, 2828], [2828, 2842], [2842, 2877], [2877, 2902], [2902, 2918], [2918, 2932], [2932, 2943], [2943, 2966], [2966, 2984], [2984, 3006], [3006, 3022], [3022, 3027], [3027, 3033], [3033, 3091], [3091, 3117], [3117, 3120], [3120, 3150], [3150, 3170], [3170, 3198], [3198, 3215], [3215, 3237], [3237, 3263], [3263, 3273], [3273, 3277], [3277, 3291], [3291, 3300], [3300, 3318], [3318, 3326], [3326, 3345], [3345, 3379], [3379, 3393], [3393, 3417], [3417, 3474], [3474, 3502], [3502, 3508], [3508, 3532], [3532, 3546], [3546, 3550], [3550, 3574], [3574, 3585], [3585, 3616], [3616, 3626], [3626, 3630], [3630, 3646], [3646, 3699], [3699, 3720], [3720, 3748], [3748, 3790], [3790, 3804], [3804, 3852], [3852, 3891], [3891, 3927], [3927, 3970], [3970, 3988], [3988, 4024], [4024, 4064], [4064, 4067], [4067, 4113], [4113, 4144], [4144, 4163], [4163, 4193], [4193, 4217], [4217, 4221], [4221, 4248], [4248, 4265], [4265, 4299], [4299, 4308], [4308, 4315], [4315, 4348], [4348, 4366], [4366, 4369], [4369, 4415], [4415, 4418]], "words": ["document", ":", "Sequence", "to", "Sequence", "Learning", "with", "Neural", "Networks", "Deep", "Neural", "Networks", "(", "DNNs", ")", "are", "powerful", "models", "that", "have", "achieved", "excellent", "performance", "on", "difficult", "learning", "tasks", ".", "Although", "DNNs", "work", "well", "whenever", "large", "labeled", "training", "sets", "are", "available", ",", "they", "can", "not", "be", "used", "to", "map", "sequences", "to", "sequences", ".", "In", "this", "paper", ",", "we", "present", "a", "general", "end", "-", "to", "-", "end", "approach", "to", "sequence", "learning", "that", "makes", "minimal", "assumptions", "on", "the", "sequence", "structure", ".", "Our", "method", "uses", "a", "multilayered", "Long", "Short", "-", "Term", "Memory", "(", "LSTM", ")", "to", "map", "the", "input", "sequence", "to", "a", "vector", "of", "a", "fixed", "dimensionality", ",", "and", "then", "another", "deep", "LSTM", "to", "decode", "the", "target", "sequence", "from", "the", "vector", ".", "Our", "main", "result", "is", "that", "on", "an", "English", "to", "French", "translation", "task", "from", "the", "WMT\u201914", "dataset", ",", "the", "translations", "produced", "by", "the", "LSTM", "achieve", "a", "BLEU", "score", "of", "34.8", "on", "the", "entire", "test", "set", ",", "where", "the", "LSTM", "\u2019s", "BLEU", "score", "was", "penalized", "on", "out", "-", "of", "-", "vocabulary", "words", ".", "Additionally", ",", "the", "LSTM", "did", "not", "have", "difficulty", "on", "long", "sentences", ".", "For", "comparison", ",", "a", "phrase", "-", "based", "SMT", "system", "achieves", "a", "BLEU", "score", "of", "33.3", "on", "the", "same", "dataset", ".", "When", "we", "used", "the", "LSTM", "to", "rerank", "the", "1000", "hypotheses", "produced", "by", "the", "aforementioned", "SMT", "system", ",", "its", "BLEU", "score", "increases", "to", "36.5", ",", "which", "is", "close", "to", "the", "previous", "best", "result", "on", "this", "task", ".", "The", "LSTM", "also", "learned", "sensible", "phrase", "and", "sentence", "representations", "that", "are", "sensitive", "to", "word", "order", "and", "are", "relatively", "invariant", "to", "the", "active", "and", "the", "passive", "voice", ".", "Finally", ",", "we", "found", "that", "reversing", "the", "order", "of", "the", "words", "in", "all", "source", "sentences", "(", "but", "not", "target", "sentences", ")", "improved", "the", "LSTM", "\u2019s", "performance", "markedly", ",", "because", "doing", "so", "introduced", "many", "short", "term", "dependencies", "between", "the", "source", "and", "the", "target", "sentence", "which", "made", "the", "optimization", "problem", "easier", ".", "section", ":", "Introduction", "Deep", "Neural", "Networks", "(", "DNNs", ")", "are", "extremely", "powerful", "machine", "learning", "models", "that", "achieve", "excellent", "performance", "on", "difficult", "problems", "such", "as", "speech", "recognition", "and", "visual", "object", "recognition", ".", "DNNs", "are", "powerful", "because", "they", "can", "perform", "arbitrary", "parallel", "computation", "for", "a", "modest", "number", "of", "steps", ".", "A", "surprising", "example", "of", "the", "power", "of", "DNNs", "is", "their", "ability", "to", "sort", "-", "bit", "numbers", "using", "only", "2", "hidden", "layers", "of", "quadratic", "size", ".", "So", ",", "while", "neural", "networks", "are", "related", "to", "conventional", "statistical", "models", ",", "they", "learn", "an", "intricate", "computation", ".", "Furthermore", ",", "large", "DNNs", "can", "be", "trained", "with", "supervised", "backpropagation", "whenever", "the", "labeled", "training", "set", "has", "enough", "information", "to", "specify", "the", "network", "\u2019s", "parameters", ".", "Thus", ",", "if", "there", "exists", "a", "parameter", "setting", "of", "a", "large", "DNN", "that", "achieves", "good", "results", "(", "for", "example", ",", "because", "humans", "can", "solve", "the", "task", "very", "rapidly", ")", ",", "supervised", "backpropagation", "will", "find", "these", "parameters", "and", "solve", "the", "problem", ".", "Despite", "their", "flexibility", "and", "power", ",", "DNNs", "can", "only", "be", "applied", "to", "problems", "whose", "inputs", "and", "targets", "can", "be", "sensibly", "encoded", "with", "vectors", "of", "fixed", "dimensionality", ".", "It", "is", "a", "significant", "limitation", ",", "since", "many", "important", "problems", "are", "best", "expressed", "with", "sequences", "whose", "lengths", "are", "not", "known", "a", "-", "priori", ".", "For", "example", ",", "speech", "recognition", "and", "machine", "translation", "are", "sequential", "problems", ".", "Likewise", ",", "question", "answering", "can", "also", "be", "seen", "as", "mapping", "a", "sequence", "of", "words", "representing", "the", "question", "to", "a", "sequence", "of", "words", "representing", "the", "answer", ".", "It", "is", "therefore", "clear", "that", "a", "domain", "-", "independent", "method", "that", "learns", "to", "map", "sequences", "to", "sequences", "would", "be", "useful", ".", "Sequences", "pose", "a", "challenge", "for", "DNNs", "because", "they", "require", "that", "the", "dimensionality", "of", "the", "inputs", "and", "outputs", "is", "known", "and", "fixed", ".", "In", "this", "paper", ",", "we", "show", "that", "a", "straightforward", "application", "of", "the", "Long", "Short", "-", "Term", "Memory", "(", "LSTM", ")", "architecture", "can", "solve", "general", "sequence", "to", "sequence", "problems", ".", "The", "idea", "is", "to", "use", "one", "LSTM", "to", "read", "the", "input", "sequence", ",", "one", "timestep", "at", "a", "time", ",", "to", "obtain", "large", "fixed", "-", "dimensional", "vector", "representation", ",", "and", "then", "to", "use", "another", "LSTM", "to", "extract", "the", "output", "sequence", "from", "that", "vector", "(", "fig", ".", "[", "reference", "]", ")", ".", "The", "second", "LSTM", "is", "essentially", "a", "recurrent", "neural", "network", "language", "model", "except", "that", "it", "is", "conditioned", "on", "the", "input", "sequence", ".", "The", "LSTM", "\u2019s", "ability", "to", "successfully", "learn", "on", "data", "with", "long", "range", "temporal", "dependencies", "makes", "it", "a", "natural", "choice", "for", "this", "application", "due", "to", "the", "considerable", "time", "lag", "between", "the", "inputs", "and", "their", "corresponding", "outputs", "(", "fig", ".", "[", "reference", "]", ")", ".", "There", "have", "been", "a", "number", "of", "related", "attempts", "to", "address", "the", "general", "sequence", "to", "sequence", "learning", "problem", "with", "neural", "networks", ".", "Our", "approach", "is", "closely", "related", "to", "Kalchbrenner", "and", "Blunsom", "who", "were", "the", "first", "to", "map", "the", "entire", "input", "sentence", "to", "vector", ",", "and", "is", "related", "to", "Cho", "et", "al", ".", "although", "the", "latter", "was", "used", "only", "for", "rescoring", "hypotheses", "produced", "by", "a", "phrase", "-", "based", "system", ".", "Graves", "introduced", "a", "novel", "differentiable", "attention", "mechanism", "that", "allows", "neural", "networks", "to", "focus", "on", "different", "parts", "of", "their", "input", ",", "and", "an", "elegant", "variant", "of", "this", "idea", "was", "successfully", "applied", "to", "machine", "translation", "by", "Bahdanau", "et", "al", ".", ".", "The", "Connectionist", "Sequence", "Classification", "is", "another", "popular", "technique", "for", "mapping", "sequences", "to", "sequences", "with", "neural", "networks", ",", "but", "it", "assumes", "a", "monotonic", "alignment", "between", "the", "inputs", "and", "the", "outputs", ".", "The", "main", "result", "of", "this", "work", "is", "the", "following", ".", "On", "the", "WMT\u201914", "English", "to", "French", "translation", "task", ",", "we", "obtained", "a", "BLEU", "score", "of", "34.81", "by", "directly", "extracting", "translations", "from", "an", "ensemble", "of", "5", "deep", "LSTMs", "(", "with", "384", "M", "parameters", "and", "8", ",", "000", "dimensional", "state", "each", ")", "using", "a", "simple", "left", "-", "to", "-", "right", "beam", "-", "search", "decoder", ".", "This", "is", "by", "far", "the", "best", "result", "achieved", "by", "direct", "translation", "with", "large", "neural", "networks", ".", "For", "comparison", ",", "the", "BLEU", "score", "of", "an", "SMT", "baseline", "on", "this", "dataset", "is", "33.30", ".", "The", "34.81", "BLEU", "score", "was", "achieved", "by", "an", "LSTM", "with", "a", "vocabulary", "of", "80k", "words", ",", "so", "the", "score", "was", "penalized", "whenever", "the", "reference", "translation", "contained", "a", "word", "not", "covered", "by", "these", "80k", ".", "This", "result", "shows", "that", "a", "relatively", "unoptimized", "small", "-", "vocabulary", "neural", "network", "architecture", "which", "has", "much", "room", "for", "improvement", "outperforms", "a", "phrase", "-", "based", "SMT", "system", ".", "Finally", ",", "we", "used", "the", "LSTM", "to", "rescore", "the", "publicly", "available", "1000", "-", "best", "lists", "of", "the", "SMT", "baseline", "on", "the", "same", "task", ".", "By", "doing", "so", ",", "we", "obtained", "a", "BLEU", "score", "of", "36.5", ",", "which", "improves", "the", "baseline", "by", "3.2", "BLEU", "points", "and", "is", "close", "to", "the", "previous", "best", "published", "result", "on", "this", "task", "(", "which", "is", "37.0", ")", ".", "Surprisingly", ",", "the", "LSTM", "did", "not", "suffer", "on", "very", "long", "sentences", ",", "despite", "the", "recent", "experience", "of", "other", "researchers", "with", "related", "architectures", ".", "We", "were", "able", "to", "do", "well", "on", "long", "sentences", "because", "we", "reversed", "the", "order", "of", "words", "in", "the", "source", "sentence", "but", "not", "the", "target", "sentences", "in", "the", "training", "and", "test", "set", ".", "By", "doing", "so", ",", "we", "introduced", "many", "short", "term", "dependencies", "that", "made", "the", "optimization", "problem", "much", "simpler", "(", "see", "sec", ".", "[", "reference", "]", "and", "[", "reference", "]", ")", ".", "As", "a", "result", ",", "SGD", "could", "learn", "LSTMs", "that", "had", "no", "trouble", "with", "long", "sentences", ".", "The", "simple", "trick", "of", "reversing", "the", "words", "in", "the", "source", "sentence", "is", "one", "of", "the", "key", "technical", "contributions", "of", "this", "work", ".", "A", "useful", "property", "of", "the", "LSTM", "is", "that", "it", "learns", "to", "map", "an", "input", "sentence", "of", "variable", "length", "into", "a", "fixed", "-", "dimensional", "vector", "representation", ".", "Given", "that", "translations", "tend", "to", "be", "paraphrases", "of", "the", "source", "sentences", ",", "the", "translation", "objective", "encourages", "the", "LSTM", "to", "find", "sentence", "representations", "that", "capture", "their", "meaning", ",", "as", "sentences", "with", "similar", "meanings", "are", "close", "to", "each", "other", "while", "different", "sentences", "meanings", "will", "be", "far", ".", "A", "qualitative", "evaluation", "supports", "this", "claim", ",", "showing", "that", "our", "model", "is", "aware", "of", "word", "order", "and", "is", "fairly", "invariant", "to", "the", "active", "and", "passive", "voice", ".", "section", ":", "The", "model", "The", "Recurrent", "Neural", "Network", "(", "RNN", ")", "is", "a", "natural", "generalization", "of", "feedforward", "neural", "networks", "to", "sequences", ".", "Given", "a", "sequence", "of", "inputs", ",", "a", "standard", "RNN", "computes", "a", "sequence", "of", "outputs", "by", "iterating", "the", "following", "equation", ":", "The", "RNN", "can", "easily", "map", "sequences", "to", "sequences", "whenever", "the", "alignment", "between", "the", "inputs", "the", "outputs", "is", "known", "ahead", "of", "time", ".", "However", ",", "it", "is", "not", "clear", "how", "to", "apply", "an", "RNN", "to", "problems", "whose", "input", "and", "the", "output", "sequences", "have", "different", "lengths", "with", "complicated", "and", "non", "-", "monotonic", "relationships", ".", "The", "simplest", "strategy", "for", "general", "sequence", "learning", "is", "to", "map", "the", "input", "sequence", "to", "a", "fixed", "-", "sized", "vector", "using", "one", "RNN", ",", "and", "then", "to", "map", "the", "vector", "to", "the", "target", "sequence", "with", "another", "RNN", "(", "this", "approach", "has", "also", "been", "taken", "by", "Cho", "et", "al", ".", ")", ".", "While", "it", "could", "work", "in", "principle", "since", "the", "RNN", "is", "provided", "with", "all", "the", "relevant", "information", ",", "it", "would", "be", "difficult", "to", "train", "the", "RNNs", "due", "to", "the", "resulting", "long", "term", "dependencies", "(", "figure", "[", "reference", "]", ")", ".", "However", ",", "the", "Long", "Short", "-", "Term", "Memory", "(", "LSTM", ")", "is", "known", "to", "learn", "problems", "with", "long", "range", "temporal", "dependencies", ",", "so", "an", "LSTM", "may", "succeed", "in", "this", "setting", ".", "The", "goal", "of", "the", "LSTM", "is", "to", "estimate", "the", "conditional", "probability", "where", "is", "an", "input", "sequence", "and", "is", "its", "corresponding", "output", "sequence", "whose", "length", "may", "differ", "from", ".", "The", "LSTM", "computes", "this", "conditional", "probability", "by", "first", "obtaining", "the", "fixed", "-", "dimensional", "representation", "of", "the", "input", "sequence", "given", "by", "the", "last", "hidden", "state", "of", "the", "LSTM", ",", "and", "then", "computing", "the", "probability", "of", "with", "a", "standard", "LSTM", "-", "LM", "formulation", "whose", "initial", "hidden", "state", "is", "set", "to", "the", "representation", "of", ":", "In", "this", "equation", ",", "each", "distribution", "is", "represented", "with", "a", "softmax", "over", "all", "the", "words", "in", "the", "vocabulary", ".", "We", "use", "the", "LSTM", "formulation", "from", "Graves", ".", "Note", "that", "we", "require", "that", "each", "sentence", "ends", "with", "a", "special", "end", "-", "of", "-", "sentence", "symbol", "\u201c", "EOS", "\u201d", ",", "which", "enables", "the", "model", "to", "define", "a", "distribution", "over", "sequences", "of", "all", "possible", "lengths", ".", "The", "overall", "scheme", "is", "outlined", "in", "figure", "[", "reference", "]", ",", "where", "the", "shown", "LSTM", "computes", "the", "representation", "of", "\u201c", "A", "\u201d", ",", "\u201c", "B", "\u201d", ",", "\u201c", "C", "\u201d", ",", "\u201c", "EOS", "\u201d", "and", "then", "uses", "this", "representation", "to", "compute", "the", "probability", "of", "\u201c", "W", "\u201d", ",", "\u201c", "X", "\u201d", ",", "\u201c", "Y", "\u201d", ",", "\u201c", "Z", "\u201d", ",", "\u201c", "EOS", "\u201d", ".", "Our", "actual", "models", "differ", "from", "the", "above", "description", "in", "three", "important", "ways", ".", "First", ",", "we", "used", "two", "different", "LSTMs", ":", "one", "for", "the", "input", "sequence", "and", "another", "for", "the", "output", "sequence", ",", "because", "doing", "so", "increases", "the", "number", "model", "parameters", "at", "negligible", "computational", "cost", "and", "makes", "it", "natural", "to", "train", "the", "LSTM", "on", "multiple", "language", "pairs", "simultaneously", ".", "Second", ",", "we", "found", "that", "deep", "LSTMs", "significantly", "outperformed", "shallow", "LSTMs", ",", "so", "we", "chose", "an", "LSTM", "with", "four", "layers", ".", "Third", ",", "we", "found", "it", "extremely", "valuable", "to", "reverse", "the", "order", "of", "the", "words", "of", "the", "input", "sentence", ".", "So", "for", "example", ",", "instead", "of", "mapping", "the", "sentence", "to", "the", "sentence", ",", "the", "LSTM", "is", "asked", "to", "map", "to", ",", "where", "is", "the", "translation", "of", ".", "This", "way", ",", "is", "in", "close", "proximity", "to", ",", "is", "fairly", "close", "to", ",", "and", "so", "on", ",", "a", "fact", "that", "makes", "it", "easy", "for", "SGD", "to", "\u201c", "establish", "communication", "\u201d", "between", "the", "input", "and", "the", "output", ".", "We", "found", "this", "simple", "data", "transformation", "to", "greatly", "improve", "the", "performance", "of", "the", "LSTM", ".", "section", ":", "Experiments", "We", "applied", "our", "method", "to", "the", "WMT\u201914", "English", "to", "French", "MT", "task", "in", "two", "ways", ".", "We", "used", "it", "to", "directly", "translate", "the", "input", "sentence", "without", "using", "a", "reference", "SMT", "system", "and", "we", "it", "to", "rescore", "the", "n", "-", "best", "lists", "of", "an", "SMT", "baseline", ".", "We", "report", "the", "accuracy", "of", "these", "translation", "methods", ",", "present", "sample", "translations", ",", "and", "visualize", "the", "resulting", "sentence", "representation", ".", "subsection", ":", "Dataset", "details", "We", "used", "the", "WMT\u201914", "English", "to", "French", "dataset", ".", "We", "trained", "our", "models", "on", "a", "subset", "of", "12", "M", "sentences", "consisting", "of", "348", "M", "French", "words", "and", "304", "M", "English", "words", ",", "which", "is", "a", "clean", "\u201c", "selected", "\u201d", "subset", "from", ".", "We", "chose", "this", "translation", "task", "and", "this", "specific", "training", "set", "subset", "because", "of", "the", "public", "availability", "of", "a", "tokenized", "training", "and", "test", "set", "together", "with", "1000", "-", "best", "lists", "from", "the", "baseline", "SMT", ".", "As", "typical", "neural", "language", "models", "rely", "on", "a", "vector", "representation", "for", "each", "word", ",", "we", "used", "a", "fixed", "vocabulary", "for", "both", "languages", ".", "We", "used", "160", ",", "000", "of", "the", "most", "frequent", "words", "for", "the", "source", "language", "and", "80", ",", "000", "of", "the", "most", "frequent", "words", "for", "the", "target", "language", ".", "Every", "out", "-", "of", "-", "vocabulary", "word", "was", "replaced", "with", "a", "special", "\u201c", "UNK", "\u201d", "token", ".", "subsection", ":", "Decoding", "and", "Rescoring", "The", "core", "of", "our", "experiments", "involved", "training", "a", "large", "deep", "LSTM", "on", "many", "sentence", "pairs", ".", "We", "trained", "it", "by", "maximizing", "the", "log", "probability", "of", "a", "correct", "translation", "given", "the", "source", "sentence", ",", "so", "the", "training", "objective", "is", "where", "is", "the", "training", "set", ".", "Once", "training", "is", "complete", ",", "we", "produce", "translations", "by", "finding", "the", "most", "likely", "translation", "according", "to", "the", "LSTM", ":", "We", "search", "for", "the", "most", "likely", "translation", "using", "a", "simple", "left", "-", "to", "-", "right", "beam", "search", "decoder", "which", "maintains", "a", "small", "number", "of", "partial", "hypotheses", ",", "where", "a", "partial", "hypothesis", "is", "a", "prefix", "of", "some", "translation", ".", "At", "each", "timestep", "we", "extend", "each", "partial", "hypothesis", "in", "the", "beam", "with", "every", "possible", "word", "in", "the", "vocabulary", ".", "This", "greatly", "increases", "the", "number", "of", "the", "hypotheses", "so", "we", "discard", "all", "but", "the", "most", "likely", "hypotheses", "according", "to", "the", "model", "\u2019s", "log", "probability", ".", "As", "soon", "as", "the", "\u201c", "EOS", "\u201d", "symbol", "is", "appended", "to", "a", "hypothesis", ",", "it", "is", "removed", "from", "the", "beam", "and", "is", "added", "to", "the", "set", "of", "complete", "hypotheses", ".", "While", "this", "decoder", "is", "approximate", ",", "it", "is", "simple", "to", "implement", ".", "Interestingly", ",", "our", "system", "performs", "well", "even", "with", "a", "beam", "size", "of", "1", ",", "and", "a", "beam", "of", "size", "2", "provides", "most", "of", "the", "benefits", "of", "beam", "search", "(", "Table", "[", "reference", "]", ")", ".", "We", "also", "used", "the", "LSTM", "to", "rescore", "the", "1000", "-", "best", "lists", "produced", "by", "the", "baseline", "system", ".", "To", "rescore", "an", "n", "-", "best", "list", ",", "we", "computed", "the", "log", "probability", "of", "every", "hypothesis", "with", "our", "LSTM", "and", "took", "an", "even", "average", "with", "their", "score", "and", "the", "LSTM", "\u2019s", "score", ".", "subsection", ":", "Reversing", "the", "Source", "Sentences", "While", "the", "LSTM", "is", "capable", "of", "solving", "problems", "with", "long", "term", "dependencies", ",", "we", "discovered", "that", "the", "LSTM", "learns", "much", "better", "when", "the", "source", "sentences", "are", "reversed", "(", "the", "target", "sentences", "are", "not", "reversed", ")", ".", "By", "doing", "so", ",", "the", "LSTM", "\u2019s", "test", "perplexity", "dropped", "from", "5.8", "to", "4.7", ",", "and", "the", "test", "BLEU", "scores", "of", "its", "decoded", "translations", "increased", "from", "25.9", "to", "30.6", ".", "While", "we", "do", "not", "have", "a", "complete", "explanation", "to", "this", "phenomenon", ",", "we", "believe", "that", "it", "is", "caused", "by", "the", "introduction", "of", "many", "short", "term", "dependencies", "to", "the", "dataset", ".", "Normally", ",", "when", "we", "concatenate", "a", "source", "sentence", "with", "a", "target", "sentence", ",", "each", "word", "in", "the", "source", "sentence", "is", "far", "from", "its", "corresponding", "word", "in", "the", "target", "sentence", ".", "As", "a", "result", ",", "the", "problem", "has", "a", "large", "\u201c", "minimal", "time", "lag", "\u201d", ".", "By", "reversing", "the", "words", "in", "the", "source", "sentence", ",", "the", "average", "distance", "between", "corresponding", "words", "in", "the", "source", "and", "target", "language", "is", "unchanged", ".", "However", ",", "the", "first", "few", "words", "in", "the", "source", "language", "are", "now", "very", "close", "to", "the", "first", "few", "words", "in", "the", "target", "language", ",", "so", "the", "problem", "\u2019s", "minimal", "time", "lag", "is", "greatly", "reduced", ".", "Thus", ",", "backpropagation", "has", "an", "easier", "time", "\u201c", "establishing", "communication", "\u201d", "between", "the", "source", "sentence", "and", "the", "target", "sentence", ",", "which", "in", "turn", "results", "in", "substantially", "improved", "overall", "performance", ".", "Initially", ",", "we", "believed", "that", "reversing", "the", "input", "sentences", "would", "only", "lead", "to", "more", "confident", "predictions", "in", "the", "early", "parts", "of", "the", "target", "sentence", "and", "to", "less", "confident", "predictions", "in", "the", "later", "parts", ".", "However", ",", "LSTMs", "trained", "on", "reversed", "source", "sentences", "did", "much", "better", "on", "long", "sentences", "than", "LSTMs", "trained", "on", "the", "raw", "source", "sentences", "(", "see", "sec", ".", "[", "reference", "]", ")", ",", "which", "suggests", "that", "reversing", "the", "input", "sentences", "results", "in", "LSTMs", "with", "better", "memory", "utilization", ".", "subsection", ":", "Training", "details", "We", "found", "that", "the", "LSTM", "models", "are", "fairly", "easy", "to", "train", ".", "We", "used", "deep", "LSTMs", "with", "4", "layers", ",", "with", "1000", "cells", "at", "each", "layer", "and", "1000", "dimensional", "word", "embeddings", ",", "with", "an", "input", "vocabulary", "of", "160", ",", "000", "and", "an", "output", "vocabulary", "of", "80", ",", "000", ".", "Thus", "the", "deep", "LSTM", "uses", "8000", "real", "numbers", "to", "represent", "a", "sentence", ".", "We", "found", "deep", "LSTMs", "to", "significantly", "outperform", "shallow", "LSTMs", ",", "where", "each", "additional", "layer", "reduced", "perplexity", "by", "nearly", "10", "%", ",", "possibly", "due", "to", "their", "much", "larger", "hidden", "state", ".", "We", "used", "a", "naive", "softmax", "over", "80", ",", "000", "words", "at", "each", "output", ".", "The", "resulting", "LSTM", "has", "384", "M", "parameters", "of", "which", "64", "M", "are", "pure", "recurrent", "connections", "(", "32", "M", "for", "the", "\u201c", "encoder", "\u201d", "LSTM", "and", "32", "M", "for", "the", "\u201c", "decoder", "\u201d", "LSTM", ")", ".", "The", "complete", "training", "details", "are", "given", "below", ":", "We", "initialized", "all", "of", "the", "LSTM", "\u2019s", "parameters", "with", "the", "uniform", "distribution", "between", "-", "0.08", "and", "0.08", "We", "used", "stochastic", "gradient", "descent", "without", "momentum", ",", "with", "a", "fixed", "learning", "rate", "of", "0.7", ".", "After", "5", "epochs", ",", "we", "begun", "halving", "the", "learning", "rate", "every", "half", "epoch", ".", "We", "trained", "our", "models", "for", "a", "total", "of", "7.5", "epochs", ".", "We", "used", "batches", "of", "128", "sequences", "for", "the", "gradient", "and", "divided", "it", "the", "size", "of", "the", "batch", "(", "namely", ",", "128", ")", ".", "Although", "LSTMs", "tend", "to", "not", "suffer", "from", "the", "vanishing", "gradient", "problem", ",", "they", "can", "have", "exploding", "gradients", ".", "Thus", "we", "enforced", "a", "hard", "constraint", "on", "the", "norm", "of", "the", "gradient", "by", "scaling", "it", "when", "its", "norm", "exceeded", "a", "threshold", ".", "For", "each", "training", "batch", ",", "we", "compute", ",", "where", "is", "the", "gradient", "divided", "by", "128", ".", "If", ",", "we", "set", ".", "Different", "sentences", "have", "different", "lengths", ".", "Most", "sentences", "are", "short", "(", "e.g.", ",", "length", "20", "-", "30", ")", "but", "some", "sentences", "are", "long", "(", "e.g.", ",", "length", "100", ")", ",", "so", "a", "minibatch", "of", "128", "randomly", "chosen", "training", "sentences", "will", "have", "many", "short", "sentences", "and", "few", "long", "sentences", ",", "and", "as", "a", "result", ",", "much", "of", "the", "computation", "in", "the", "minibatch", "is", "wasted", ".", "To", "address", "this", "problem", ",", "we", "made", "sure", "that", "all", "sentences", "in", "a", "minibatch", "are", "roughly", "of", "the", "same", "length", ",", "yielding", "a", "2x", "speedup", ".", "subsection", ":", "Parallelization", "A", "C", "++", "implementation", "of", "deep", "LSTM", "with", "the", "configuration", "from", "the", "previous", "section", "on", "a", "single", "GPU", "processes", "a", "speed", "of", "approximately", "1", ",", "700", "words", "per", "second", ".", "This", "was", "too", "slow", "for", "our", "purposes", ",", "so", "we", "parallelized", "our", "model", "using", "an", "8", "-", "GPU", "machine", ".", "Each", "layer", "of", "the", "LSTM", "was", "executed", "on", "a", "different", "GPU", "and", "communicated", "its", "activations", "to", "the", "next", "GPU", "/", "layer", "as", "soon", "as", "they", "were", "computed", ".", "Our", "models", "have", "4", "layers", "of", "LSTMs", ",", "each", "of", "which", "resides", "on", "a", "separate", "GPU", ".", "The", "remaining", "4", "GPUs", "were", "used", "to", "parallelize", "the", "softmax", ",", "so", "each", "GPU", "was", "responsible", "for", "multiplying", "by", "a", "matrix", ".", "The", "resulting", "implementation", "achieved", "a", "speed", "of", "6", ",", "300", "(", "both", "English", "and", "French", ")", "words", "per", "second", "with", "a", "minibatch", "size", "of", "128", ".", "Training", "took", "about", "a", "ten", "days", "with", "this", "implementation", ".", "subsection", ":", "Experimental", "Results", "We", "used", "the", "cased", "BLEU", "score", "to", "evaluate", "the", "quality", "of", "our", "translations", ".", "We", "computed", "our", "BLEU", "scores", "using", "multi", "-", "bleu.pl", "There", "several", "variants", "of", "the", "BLEU", "score", ",", "and", "each", "variant", "is", "defined", "with", "a", "perl", "script", ".", "on", "the", "tokenized", "predictions", "and", "ground", "truth", ".", "This", "way", "of", "evaluating", "the", "BELU", "score", "is", "consistent", "with", "and", ",", "and", "reproduces", "the", "33.3", "score", "of", ".", "However", ",", "if", "we", "evaluate", "the", "best", "WMT\u201914", "system", "(", "whose", "predictions", "can", "be", "downloaded", "from", ")", "in", "this", "manner", ",", "we", "get", "37.0", ",", "which", "is", "greater", "than", "the", "35.8", "reported", "by", ".", "The", "results", "are", "presented", "in", "tables", "[", "reference", "]", "and", "[", "reference", "]", ".", "Our", "best", "results", "are", "obtained", "with", "an", "ensemble", "of", "LSTMs", "that", "differ", "in", "their", "random", "initializations", "and", "in", "the", "random", "order", "of", "minibatches", ".", "While", "the", "decoded", "translations", "of", "the", "LSTM", "ensemble", "do", "not", "outperform", "the", "best", "WMT\u201914", "system", ",", "it", "is", "the", "first", "time", "that", "a", "pure", "neural", "translation", "system", "outperforms", "a", "phrase", "-", "based", "SMT", "baseline", "on", "a", "large", "scale", "MT", "task", "by", "a", "sizeable", "margin", ",", "despite", "its", "inability", "to", "handle", "out", "-", "of", "-", "vocabulary", "words", ".", "The", "LSTM", "is", "within", "0.5", "BLEU", "points", "of", "the", "best", "WMT\u201914", "result", "if", "it", "is", "used", "to", "rescore", "the", "1000", "-", "best", "list", "of", "the", "baseline", "system", ".", "subsection", ":", "Performance", "on", "long", "sentences", "We", "were", "surprised", "to", "discover", "that", "the", "LSTM", "did", "well", "on", "long", "sentences", ",", "which", "is", "shown", "quantitatively", "in", "figure", "[", "reference", "]", ".", "Table", "[", "reference", "]", "presents", "several", "examples", "of", "long", "sentences", "and", "their", "translations", ".", "subsection", ":", "Model", "Analysis", "One", "of", "the", "attractive", "features", "of", "our", "model", "is", "its", "ability", "to", "turn", "a", "sequence", "of", "words", "into", "a", "vector", "of", "fixed", "dimensionality", ".", "Figure", "[", "reference", "]", "visualizes", "some", "of", "the", "learned", "representations", ".", "The", "figure", "clearly", "shows", "that", "the", "representations", "are", "sensitive", "to", "the", "order", "of", "words", ",", "while", "being", "fairly", "insensitive", "to", "the", "replacement", "of", "an", "active", "voice", "with", "a", "passive", "voice", ".", "The", "two", "-", "dimensional", "projections", "are", "obtained", "using", "PCA", ".", "section", ":", "Related", "work", "There", "is", "a", "large", "body", "of", "work", "on", "applications", "of", "neural", "networks", "to", "machine", "translation", ".", "So", "far", ",", "the", "simplest", "and", "most", "effective", "way", "of", "applying", "an", "RNN", "-", "Language", "Model", "(", "RNNLM", ")", "or", "a", "Feedforward", "Neural", "Network", "Language", "Model", "(", "NNLM", ")", "to", "an", "MT", "task", "is", "by", "rescoring", "the", "n", "-", "best", "lists", "of", "a", "strong", "MT", "baseline", ",", "which", "reliably", "improves", "translation", "quality", ".", "More", "recently", ",", "researchers", "have", "begun", "to", "look", "into", "ways", "of", "including", "information", "about", "the", "source", "language", "into", "the", "NNLM", ".", "Examples", "of", "this", "work", "include", "Auli", "et", "al", ".", ",", "who", "combine", "an", "NNLM", "with", "a", "topic", "model", "of", "the", "input", "sentence", ",", "which", "improves", "rescoring", "performance", ".", "Devlin", "et", "al", ".", "followed", "a", "similar", "approach", ",", "but", "they", "incorporated", "their", "NNLM", "into", "the", "decoder", "of", "an", "MT", "system", "and", "used", "the", "decoder", "\u2019s", "alignment", "information", "to", "provide", "the", "NNLM", "with", "the", "most", "useful", "words", "in", "the", "input", "sentence", ".", "Their", "approach", "was", "highly", "successful", "and", "it", "achieved", "large", "improvements", "over", "their", "baseline", ".", "Our", "work", "is", "closely", "related", "to", "Kalchbrenner", "and", "Blunsom", ",", "who", "were", "the", "first", "to", "map", "the", "input", "sentence", "into", "a", "vector", "and", "then", "back", "to", "a", "sentence", ",", "although", "they", "map", "sentences", "to", "vectors", "using", "convolutional", "neural", "networks", ",", "which", "lose", "the", "ordering", "of", "the", "words", ".", "Similarly", "to", "this", "work", ",", "Cho", "et", "al", ".", "used", "an", "LSTM", "-", "like", "RNN", "architecture", "to", "map", "sentences", "into", "vectors", "and", "back", ",", "although", "their", "primary", "focus", "was", "on", "integrating", "their", "neural", "network", "into", "an", "SMT", "system", ".", "Bahdanau", "et", "al", ".", "also", "attempted", "direct", "translations", "with", "a", "neural", "network", "that", "used", "an", "attention", "mechanism", "to", "overcome", "the", "poor", "performance", "on", "long", "sentences", "experienced", "by", "Cho", "et", "al", ".", "and", "achieved", "encouraging", "results", ".", "Likewise", ",", "Pouget", "-", "Abadie", "et", "al", ".", "attempted", "to", "address", "the", "memory", "problem", "of", "Cho", "et", "al", ".", "by", "translating", "pieces", "of", "the", "source", "sentence", "in", "way", "that", "produces", "smooth", "translations", ",", "which", "is", "similar", "to", "a", "phrase", "-", "based", "approach", ".", "We", "suspect", "that", "they", "could", "achieve", "similar", "improvements", "by", "simply", "training", "their", "networks", "on", "reversed", "source", "sentences", ".", "End", "-", "to", "-", "end", "training", "is", "also", "the", "focus", "of", "Hermann", "et", "al", ".", ",", "whose", "model", "represents", "the", "inputs", "and", "outputs", "by", "feedforward", "networks", ",", "and", "map", "them", "to", "similar", "points", "in", "space", ".", "However", ",", "their", "approach", "can", "not", "generate", "translations", "directly", ":", "to", "get", "a", "translation", ",", "they", "need", "to", "do", "a", "look", "up", "for", "closest", "vector", "in", "the", "pre", "-", "computed", "database", "of", "sentences", ",", "or", "to", "rescore", "a", "sentence", ".", "section", ":", "Conclusion", "In", "this", "work", ",", "we", "showed", "that", "a", "large", "deep", "LSTM", ",", "that", "has", "a", "limited", "vocabulary", "and", "that", "makes", "almost", "no", "assumption", "about", "problem", "structure", "can", "outperform", "a", "standard", "SMT", "-", "based", "system", "whose", "vocabulary", "is", "unlimited", "on", "a", "large", "-", "scale", "MT", "task", ".", "The", "success", "of", "our", "simple", "LSTM", "-", "based", "approach", "on", "MT", "suggests", "that", "it", "should", "do", "well", "on", "many", "other", "sequence", "learning", "problems", ",", "provided", "they", "have", "enough", "training", "data", ".", "We", "were", "surprised", "by", "the", "extent", "of", "the", "improvement", "obtained", "by", "reversing", "the", "words", "in", "the", "source", "sentences", ".", "We", "conclude", "that", "it", "is", "important", "to", "find", "a", "problem", "encoding", "that", "has", "the", "greatest", "number", "of", "short", "term", "dependencies", ",", "as", "they", "make", "the", "learning", "problem", "much", "simpler", ".", "In", "particular", ",", "while", "we", "were", "unable", "to", "train", "a", "standard", "RNN", "on", "the", "non", "-", "reversed", "translation", "problem", "(", "shown", "in", "fig", ".", "[", "reference", "]", ")", ",", "we", "believe", "that", "a", "standard", "RNN", "should", "be", "easily", "trainable", "when", "the", "source", "sentences", "are", "reversed", "(", "although", "we", "did", "not", "verify", "it", "experimentally", ")", ".", "We", "were", "also", "surprised", "by", "the", "ability", "of", "the", "LSTM", "to", "correctly", "translate", "very", "long", "sentences", ".", "We", "were", "initially", "convinced", "that", "the", "LSTM", "would", "fail", "on", "long", "sentences", "due", "to", "its", "limited", "memory", ",", "and", "other", "researchers", "reported", "poor", "performance", "on", "long", "sentences", "with", "a", "model", "similar", "to", "ours", ".", "And", "yet", ",", "LSTMs", "trained", "on", "the", "reversed", "dataset", "had", "little", "difficulty", "translating", "long", "sentences", ".", "Most", "importantly", ",", "we", "demonstrated", "that", "a", "simple", ",", "straightforward", "and", "a", "relatively", "unoptimized", "approach", "can", "outperform", "an", "SMT", "system", ",", "so", "further", "work", "will", "likely", "lead", "to", "even", "greater", "translation", "accuracies", ".", "These", "results", "suggest", "that", "our", "approach", "will", "likely", "do", "well", "on", "other", "challenging", "sequence", "to", "sequence", "problems", ".", "section", ":", "Acknowledgments", "We", "thank", "Samy", "Bengio", ",", "Jeff", "Dean", ",", "Matthieu", "Devin", ",", "Geoffrey", "Hinton", ",", "Nal", "Kalchbrenner", ",", "Thang", "Luong", ",", "Wolfgang", "Macherey", ",", "Rajat", "Monga", ",", "Vincent", "Vanhoucke", ",", "Peng", "Xu", ",", "Wojciech", "Zaremba", ",", "and", "the", "Google", "Brain", "team", "for", "useful", "comments", "and", "discussions", ".", "bibliography", ":", "References"]}