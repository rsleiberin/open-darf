{"coref": {"Language_Modelling": [[11, 13], [32, 34], [112, 114], [283, 285], [347, 349], [424, 426], [1505, 1507], [2532, 2534], [4531, 4533], [4697, 4699]], "Params": [], "Penn_Treebank__Word_Level_": [[152, 154], [577, 580], [2657, 2660], [2661, 2662], [2677, 2678], [2756, 2758], [2789, 2790], [3549, 3550], [3722, 3723], [3929, 3930], [3966, 3967], [4190, 4191], [4273, 4275], [4646, 4649], [4823, 4824], [4988, 4989], [3033, 3034], [3674, 3675], [3831, 3832], [4093, 4095], [4467, 4468], [5083, 5084]], "Test_perplexity": [[4035, 4038], [4155, 4156]], "Tied_Variational_LSTM": [[2927, 2930]], "Tied_Variational_LSTM___augmented_loss": [], "Validation_perplexity": [[3664, 3666], [3712, 3716]], "augmented_loss": [[1721, 1723], [2204, 2206], [2418, 2419], [2958, 2961], [3053, 3055], [3341, 3343], [3461, 3462], [3608, 3610], [3651, 3652], [3692, 3693], [3737, 3738], [3750, 3751], [3775, 3776], [3836, 3837], [3855, 3856], [3995, 3996], [5031, 5033], [5034, 5035], [1248, 1250], [1806, 1808], [2019, 2021], [2303, 2305], [3069, 3071], [3185, 3187], [3315, 3317], [3360, 3362], [3376, 3378], [3425, 3427], [3615, 3616], [3818, 3820], [3976, 3978], [5057, 5059]]}, "coref_non_salient": {"0": [[550, 552], [2454, 2456], [4517, 4519], [4683, 4685], [4793, 4800], [4809, 4813], [4873, 4878]], "1": [[304, 306], [1528, 1532]], "10": [[4766, 4768], [4803, 4805]], "11": [[1691, 1697], [2946, 2949]], "12": [[230, 232], [664, 666], [1012, 1013]], "13": [[200, 202], [4713, 4716]], "14": [[16, 19], [212, 215], [743, 746], [831, 834], [2351, 2354]], "15": [[2744, 2747], [3551, 3554], [3724, 3727], [4650, 4653], [4825, 4829], [5008, 5011], [621, 624], [2669, 2672], [3848, 3851], [3909, 3912], [4177, 4180], [5093, 5096]], "16": [[1824, 1826], [3059, 3060], [3114, 3116]], "17": [[3585, 3589], [3600, 3604], [3620, 3624], [3640, 3644]], "18": [[5, 6], [45, 47], [237, 239]], "19": [[3792, 3794], [4167, 4169]], "2": [[2328, 2330], [2896, 2898], [3151, 3152], [3590, 3592], [3605, 3607], [3625, 3627], [3645, 3647], [4889, 4891]], "20": [[193, 195], [4721, 4723]], "21": [[24, 28], [4234, 4236], [4473, 4474], [4499, 4500]], "22": [[864, 865], [879, 880], [2818, 2819], [2841, 2842], [3043, 3044], [3613, 3614], [3633, 3634], [3655, 3656], [4099, 4100], [4104, 4105], [4300, 4301], [4480, 4481], [4486, 4487], [4504, 4505], [4512, 4513]], "23": [[1005, 1006], [1214, 1215]], "24": [[79, 80], [110, 111], [264, 265], [1708, 1709], [2452, 2453], [2468, 2469], [3570, 3571], [3805, 3806], [4573, 4574]], "25": [[5239, 5241]], "26": [[158, 160], [164, 167]], "27": [[2441, 2443], [3697, 3699], [3898, 3900]], "28": [[1189, 1191], [3199, 3200]], "29": [[3141, 3142], [3496, 3497]], "3": [[858, 863], [3649, 3650], [3694, 3695], [3739, 3740], [3748, 3749], [3840, 3841], [3992, 3993], [4044, 4045], [3635, 3636], [3951, 3952], [4140, 4141], [4506, 4507]], "30": [[4310, 4312]], "31": [[224, 229], [649, 654], [658, 663]], "32": [[3488, 3490], [2099, 2101], [2243, 2245], [3431, 3433], [3471, 3473]], "33": [[3132, 3133]], "34": [[8, 10], [421, 423], [2387, 2389], [4528, 4530]], "35": [[2147, 2150], [4599, 4602], [4677, 4680]], "36": [[2163, 2165]], "37": [[1434, 1435]], "38": [[939, 942], [1022, 1025], [1196, 1198], [1485, 1488], [2423, 2426]], "39": [[803, 806]], "4": [[1935, 1941], [4911, 4913]], "40": [[843, 845], [2337, 2339]], "41": [[4838, 4839]], "42": [[186, 188]], "43": [[587, 589], [1630, 1632], [1711, 1713], [2585, 2587], [2686, 2688], [4709, 4711]], "44": [[1604, 1605]], "45": [[4124, 4127]], "46": [[3628, 3630], [4133, 4135]], "47": [[1261, 1264], [2885, 2888]], "48": [[1273, 1275], [1918, 1920], [2158, 2160], [2593, 2595], [3144, 3146]], "49": [[851, 852]], "5": [[1141, 1143], [1585, 1588]], "50": [[912, 915]], "51": [[217, 222]], "52": [[5129, 5132]], "53": [[4842, 4844]], "54": [[449, 453]], "55": [[372, 374]], "56": [[176, 178]], "57": [[2437, 2439]], "58": [[2932, 2935], [3593, 3596], [4313, 4316]], "59": [[4212, 4215]], "6": [[29, 30], [180, 182], [4717, 4719]], "60": [[4970, 4973]], "61": [[4118, 4120]], "62": [[4150, 4153]], "63": [[1607, 1610]], "64": [[3876, 3878]], "65": [[907, 908], [2324, 2325]], "66": [[1116, 1120]], "67": [[105, 107]], "68": [[2346, 2348]], "69": [[2528, 2530]], "7": [[2270, 2272], [3958, 3960]], "70": [[847, 850]], "71": [[2117, 2119]], "8": [[1008, 1010], [2290, 2292]], "9": [[1656, 1657], [3177, 3178], [3191, 3192]]}, "doc_id": "424aef7340ee618132cc3314669400e23ad910ba", "method_subrelations": {"Tied_Variational_LSTM___augmented_loss": [[[0, 21], "Tied_Variational_LSTM"], [[24, 38], "augmented_loss"]]}, "n_ary_relations": [{"Material": "Penn_Treebank__Word_Level_", "Method": "Tied_Variational_LSTM___augmented_loss", "Metric": "Params", "Task": "Language_Modelling", "score": "24M"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "Tied_Variational_LSTM___augmented_loss", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "73.2"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "Tied_Variational_LSTM___augmented_loss", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "75.7"}], "ner": [[5, 6, "Method"], [8, 10, "Method"], [11, 13, "Task"], [16, 19, "Method"], [24, 28, "Task"], [29, 30, "Task"], [32, 34, "Task"], [45, 47, "Method"], [79, 80, "Task"], [105, 107, "Method"], [110, 111, "Task"], [112, 114, "Task"], [152, 154, "Material"], [158, 160, "Method"], [164, 167, "Method"], [176, 178, "Task"], [180, 182, "Task"], [186, 188, "Task"], [193, 195, "Task"], [200, 202, "Task"], [212, 215, "Method"], [217, 222, "Task"], [224, 229, "Method"], [230, 232, "Method"], [237, 239, "Method"], [264, 265, "Task"], [283, 285, "Task"], [304, 306, "Method"], [347, 349, "Task"], [372, 374, "Method"], [421, 423, "Method"], [424, 426, "Task"], [449, 453, "Method"], [550, 552, "Task"], [577, 580, "Material"], [587, 589, "Method"], [649, 654, "Method"], [658, 663, "Method"], [664, 666, "Method"], [743, 746, "Method"], [803, 806, "Method"], [831, 834, "Method"], [843, 845, "Method"], [847, 850, "Method"], [851, 852, "Method"], [858, 863, "Method"], [864, 865, "Method"], [907, 908, "Method"], [912, 915, "Metric"], [939, 942, "Method"], [1005, 1006, "Task"], [1008, 1010, "Task"], [1012, 1013, "Method"], [1022, 1025, "Method"], [1116, 1120, "Method"], [1141, 1143, "Method"], [1189, 1191, "Metric"], [1196, 1198, "Method"], [1214, 1215, "Task"], [1261, 1264, "Method"], [1273, 1275, "Method"], [1434, 1435, "Method"], [1485, 1488, "Method"], [1505, 1507, "Task"], [1528, 1532, "Method"], [1585, 1588, "Method"], [1604, 1605, "Method"], [1607, 1610, "Method"], [1630, 1632, "Method"], [1656, 1657, "Task"], [1691, 1697, "Task"], [1708, 1709, "Task"], [1711, 1713, "Method"], [1721, 1723, "Method"], [1824, 1826, "Metric"], [1918, 1920, "Method"], [1935, 1941, "Method"], [2117, 2119, "Method"], [2147, 2150, "Method"], [2158, 2160, "Method"], [2163, 2165, "Method"], [2204, 2206, "Method"], [2270, 2272, "Metric"], [2290, 2292, "Task"], [2324, 2325, "Method"], [2328, 2330, "Method"], [2337, 2339, "Method"], [2346, 2348, "Method"], [2351, 2354, "Method"], [2387, 2389, "Method"], [2418, 2419, "Method"], [2423, 2426, "Method"], [2437, 2439, "Method"], [2441, 2443, "Method"], [2452, 2453, "Task"], [2454, 2456, "Task"], [2468, 2469, "Task"], [2528, 2530, "Method"], [2532, 2534, "Task"], [2585, 2587, "Method"], [2593, 2595, "Method"], [2657, 2660, "Material"], [2661, 2662, "Material"], [2677, 2678, "Material"], [2686, 2688, "Method"], [2744, 2747, "Material"], [2756, 2758, "Material"], [2789, 2790, "Material"], [2885, 2888, "Method"], [2896, 2898, "Method"], [2927, 2930, "Method"], [2932, 2935, "Method"], [2946, 2949, "Task"], [2958, 2961, "Method"], [3053, 3055, "Method"], [3059, 3060, "Metric"], [3114, 3116, "Metric"], [3132, 3133, "Task"], [3141, 3142, "Method"], [3144, 3146, "Method"], [3151, 3152, "Method"], [3177, 3178, "Task"], [3191, 3192, "Task"], [3199, 3200, "Metric"], [3341, 3343, "Method"], [3461, 3462, "Method"], [3488, 3490, "Method"], [3496, 3497, "Method"], [3549, 3550, "Material"], [3551, 3554, "Material"], [3570, 3571, "Task"], [3585, 3589, "Method"], [3590, 3592, "Method"], [3593, 3596, "Method"], [3600, 3604, "Method"], [3605, 3607, "Method"], [3608, 3610, "Method"], [3620, 3624, "Method"], [3625, 3627, "Method"], [3628, 3630, "Method"], [3640, 3644, "Method"], [3645, 3647, "Method"], [3649, 3650, "Method"], [3651, 3652, "Method"], [3664, 3666, "Metric"], [3692, 3693, "Method"], [3694, 3695, "Method"], [3697, 3699, "Method"], [3712, 3716, "Metric"], [3722, 3723, "Material"], [3724, 3727, "Material"], [3737, 3738, "Method"], [3739, 3740, "Method"], [3748, 3749, "Method"], [3750, 3751, "Method"], [3775, 3776, "Method"], [3792, 3794, "Method"], [3805, 3806, "Task"], [3836, 3837, "Method"], [3840, 3841, "Method"], [3855, 3856, "Method"], [3876, 3878, "Task"], [3898, 3900, "Method"], [3929, 3930, "Material"], [3958, 3960, "Metric"], [3966, 3967, "Material"], [3992, 3993, "Method"], [3995, 3996, "Method"], [4035, 4038, "Metric"], [4044, 4045, "Method"], [4118, 4120, "Method"], [4124, 4127, "Method"], [4133, 4135, "Method"], [4150, 4153, "Method"], [4155, 4156, "Metric"], [4167, 4169, "Method"], [4190, 4191, "Material"], [4212, 4215, "Metric"], [4234, 4236, "Task"], [4273, 4275, "Material"], [4310, 4312, "Method"], [4313, 4316, "Method"], [4473, 4474, "Task"], [4499, 4500, "Task"], [4517, 4519, "Task"], [4528, 4530, "Method"], [4531, 4533, "Task"], [4573, 4574, "Task"], [4599, 4602, "Method"], [4646, 4649, "Material"], [4650, 4653, "Material"], [4677, 4680, "Method"], [4683, 4685, "Task"], [4709, 4711, "Method"], [4713, 4716, "Task"], [4717, 4719, "Task"], [4721, 4723, "Task"], [4766, 4768, "Metric"], [4793, 4800, "Task"], [4803, 4805, "Metric"], [4809, 4813, "Task"], [4823, 4824, "Material"], [4825, 4829, "Material"], [4838, 4839, "Method"], [4842, 4844, "Method"], [4873, 4878, "Task"], [4889, 4891, "Method"], [4911, 4913, "Method"], [4970, 4973, "Method"], [4988, 4989, "Material"], [5008, 5011, "Material"], [5031, 5033, "Method"], [5034, 5035, "Method"], [5129, 5132, "Task"], [5239, 5241, "Method"], [621, 624, "Material"], [879, 880, "Method"], [1248, 1250, "Method"], [1806, 1808, "Method"], [2019, 2021, "Method"], [2099, 2101, "Method"], [2243, 2245, "Method"], [2303, 2305, "Method"], [2669, 2672, "Material"], [2818, 2819, "Method"], [2841, 2842, "Method"], [3033, 3034, "Material"], [3043, 3044, "Method"], [3069, 3071, "Method"], [3185, 3187, "Method"], [3315, 3317, "Method"], [3360, 3362, "Method"], [3376, 3378, "Method"], [3425, 3427, "Method"], [3431, 3433, "Method"], [3471, 3473, "Method"], [3613, 3614, "Method"], [3615, 3616, "Method"], [3633, 3634, "Method"], [3635, 3636, "Method"], [3655, 3656, "Method"], [3674, 3675, "Material"], [3818, 3820, "Method"], [3831, 3832, "Material"], [3848, 3851, "Material"], [3909, 3912, "Material"], [3951, 3952, "Method"], [3976, 3978, "Method"], [4093, 4095, "Material"], [4099, 4100, "Method"], [4104, 4105, "Method"], [4140, 4141, "Method"], [4177, 4180, "Material"], [4300, 4301, "Method"], [4467, 4468, "Material"], [4480, 4481, "Method"], [4486, 4487, "Method"], [4504, 4505, "Method"], [4506, 4507, "Method"], [4512, 4513, "Method"], [4697, 4699, "Task"], [5057, 5059, "Method"], [5083, 5084, "Material"], [5093, 5096, "Material"]], "sections": [[0, 13], [13, 161], [161, 645], [645, 1689], [1689, 2306], [2306, 2647], [2647, 2808], [2808, 2938], [2938, 3545], [3545, 4220], [4220, 4753], [4753, 5124], [5124, 5408], [5408, 5510], [5510, 5512]], "sentences": [[0, 5], [5, 7], [7, 13], [13, 16], [16, 35], [35, 75], [75, 101], [101, 141], [141, 161], [161, 164], [164, 206], [206, 282], [282, 312], [312, 340], [340, 361], [361, 413], [413, 433], [433, 443], [443, 480], [480, 490], [490, 525], [525, 544], [544, 567], [567, 595], [595, 613], [613, 645], [645, 654], [654, 695], [695, 696], [696, 702], [702, 733], [733, 764], [764, 775], [775, 800], [800, 816], [816, 873], [873, 885], [885, 891], [891, 902], [902, 931], [931, 969], [969, 995], [995, 998], [998, 1057], [1057, 1098], [1098, 1136], [1136, 1161], [1161, 1162], [1162, 1165], [1165, 1192], [1192, 1209], [1209, 1235], [1235, 1237], [1237, 1239], [1239, 1240], [1240, 1254], [1254, 1276], [1276, 1290], [1290, 1291], [1291, 1296], [1296, 1299], [1299, 1300], [1300, 1315], [1315, 1323], [1323, 1335], [1335, 1399], [1399, 1417], [1417, 1446], [1446, 1491], [1491, 1533], [1533, 1541], [1541, 1589], [1589, 1606], [1606, 1635], [1635, 1662], [1662, 1689], [1689, 1697], [1697, 1714], [1714, 1738], [1738, 1749], [1749, 1759], [1759, 1783], [1783, 1797], [1797, 1802], [1802, 1813], [1813, 1827], [1827, 1839], [1839, 1878], [1878, 1897], [1897, 1921], [1921, 1926], [1926, 1927], [1927, 1929], [1929, 1933], [1933, 1957], [1957, 1971], [1971, 1992], [1992, 2013], [2013, 2036], [2036, 2048], [2048, 2063], [2063, 2112], [2112, 2119], [2119, 2129], [2129, 2136], [2136, 2166], [2166, 2167], [2167, 2183], [2183, 2230], [2230, 2258], [2258, 2306], [2306, 2310], [2310, 2354], [2354, 2390], [2390, 2432], [2432, 2457], [2457, 2486], [2486, 2507], [2507, 2535], [2535, 2555], [2555, 2601], [2601, 2607], [2607, 2631], [2631, 2647], [2647, 2650], [2650, 2689], [2689, 2703], [2703, 2744], [2744, 2759], [2759, 2808], [2808, 2814], [2814, 2833], [2833, 2880], [2880, 2905], [2905, 2920], [2920, 2938], [2938, 2949], [2949, 2988], [2988, 3002], [3002, 3020], [3020, 3061], [3061, 3086], [3086, 3107], [3107, 3154], [3154, 3190], [3190, 3229], [3229, 3254], [3254, 3275], [3275, 3292], [3292, 3301], [3301, 3339], [3339, 3381], [3381, 3422], [3422, 3452], [3452, 3487], [3487, 3488], [3488, 3509], [3509, 3545], [3545, 3555], [3555, 3582], [3582, 3597], [3597, 3617], [3617, 3637], [3637, 3660], [3660, 3690], [3690, 3707], [3707, 3732], [3732, 3758], [3758, 3774], [3774, 3775], [3775, 3784], [3784, 3821], [3821, 3828], [3828, 3842], [3842, 3860], [3860, 3879], [3879, 3915], [3915, 3938], [3938, 3968], [3968, 3991], [3991, 3992], [3992, 4000], [4000, 4033], [4033, 4076], [4076, 4097], [4097, 4121], [4121, 4159], [4159, 4162], [4162, 4183], [4183, 4220], [4220, 4224], [4224, 4263], [4263, 4321], [4321, 4363], [4363, 4401], [4401, 4432], [4432, 4460], [4460, 4493], [4493, 4520], [4520, 4534], [4534, 4575], [4575, 4603], [4603, 4619], [4619, 4641], [4641, 4686], [4686, 4724], [4724, 4753], [4753, 4761], [4761, 4783], [4783, 4802], [4802, 4821], [4821, 4840], [4840, 4864], [4864, 4886], [4886, 4914], [4914, 4963], [4963, 4987], [4987, 5007], [5007, 5025], [5025, 5045], [5045, 5072], [5072, 5098], [5098, 5124], [5124, 5132], [5132, 5150], [5150, 5171], [5171, 5188], [5188, 5195], [5195, 5225], [5225, 5231], [5231, 5242], [5242, 5263], [5263, 5278], [5278, 5286], [5286, 5289], [5289, 5323], [5323, 5345], [5345, 5354], [5354, 5388], [5388, 5408], [5408, 5412], [5412, 5454], [5454, 5487], [5487, 5489], [5489, 5510], [5510, 5512]], "words": ["TYING", "WORD", "VECTORS", "AND", "WORD", "CLASSIFIERS", ":", "A", "LOSS", "FRAMEWORK", "FOR", "LANGUAGE", "MODELING", "section", ":", "ABSTRACT", "Recurrent", "neural", "networks", "have", "been", "very", "successful", "at", "predicting", "sequences", "of", "words", "in", "tasks", "such", "as", "language", "modeling", ".", "However", ",", "all", "such", "models", "are", "based", "on", "the", "conventional", "classification", "framework", ",", "where", "the", "model", "is", "trained", "against", "one", "-", "hot", "targets", ",", "and", "each", "word", "is", "represented", "both", "as", "an", "input", "and", "as", "an", "output", "in", "isolation", ".", "This", "causes", "inefficiencies", "in", "learning", "both", "in", "terms", "of", "utilizing", "all", "of", "the", "information", "and", "in", "terms", "of", "the", "number", "of", "parameters", "needed", "to", "train", ".", "We", "introduce", "a", "novel", "theoretical", "framework", "that", "facilitates", "better", "learning", "in", "language", "modeling", ",", "and", "show", "that", "our", "framework", "leads", "to", "tying", "together", "the", "input", "embedding", "and", "the", "output", "projection", "matrices", ",", "greatly", "reducing", "the", "number", "of", "trainable", "variables", ".", "Our", "framework", "leads", "to", "state", "of", "the", "art", "performance", "on", "the", "Penn", "Treebank", "with", "a", "variety", "of", "network", "models", ".", "section", ":", "INTRODUCTION", "Neural", "network", "models", "have", "recently", "made", "tremendous", "progress", "in", "a", "variety", "of", "NLP", "applications", "such", "as", "speech", "recognition", "[", "reference", "]", ",", "sentiment", "analysis", "[", "reference", "]", ")", ",", "text", "summarization", "[", "reference", "]", ",", "and", "machine", "translation", "[", "reference", "]", ".", "Despite", "the", "overwhelming", "success", "achieved", "by", "recurrent", "neural", "networks", "in", "modeling", "long", "range", "dependencies", "between", "words", ",", "current", "recurrent", "neural", "network", "language", "models", "(", "RNNLM", ")", "are", "based", "on", "the", "conventional", "classification", "framework", ",", "which", "has", "two", "major", "drawbacks", ":", "First", ",", "there", "is", "no", "assumed", "metric", "on", "the", "output", "classes", ",", "whereas", "there", "is", "evidence", "suggesting", "that", "learning", "is", "improved", "when", "one", "can", "define", "a", "natural", "metric", "on", "the", "output", "space", "[", "reference", "]", ".", "In", "language", "modeling", ",", "there", "is", "a", "well", "established", "metric", "space", "for", "the", "outputs", "(", "words", "in", "the", "language", ")", "based", "on", "word", "embeddings", ",", "with", "meaningful", "distances", "between", "words", "[", "reference", "][", "reference", "]", ".", "Second", ",", "in", "the", "classical", "framework", ",", "inputs", "and", "outputs", "are", "considered", "as", "isolated", "entities", "with", "no", "semantic", "link", "between", "them", ".", "This", "is", "clearly", "not", "the", "case", "for", "language", "modeling", ",", "where", "inputs", "and", "outputs", "in", "fact", "live", "in", "identical", "spaces", ".", "Therefore", ",", "even", "for", "models", "with", "moderately", "sized", "vocabularies", ",", "the", "classical", "framework", "could", "be", "a", "vast", "source", "of", "inefficiency", "in", "terms", "of", "the", "number", "of", "variables", "in", "the", "model", ",", "and", "in", "terms", "of", "utilizing", "the", "information", "gathered", "by", "different", "parts", "of", "the", "model", "(", "e.g.", "inputs", "and", "outputs", ")", ".", "In", "this", "work", ",", "we", "introduce", "a", "novel", "loss", "framework", "for", "language", "modeling", "to", "remedy", "the", "above", "two", "problems", ".", "Our", "framework", "is", "comprised", "of", "two", "closely", "linked", "improvements", ".", "First", ",", "we", "augment", "the", "classical", "cross", "-", "entropy", "loss", "with", "an", "additional", "term", "which", "minimizes", "the", "KL", "-", "divergence", "between", "the", "model", "'s", "prediction", "and", "an", "estimated", "target", "distribution", "based", "on", "the", "word", "embeddings", "space", ".", "This", "estimated", "distribution", "uses", "knowledge", "of", "word", "vector", "similarity", ".", "We", "then", "theoretically", "analyze", "this", "loss", ",", "and", "this", "leads", "to", "a", "second", "and", "synergistic", "improvement", ":", "tying", "together", "two", "large", "matrices", "by", "reusing", "the", "input", "word", "embedding", "matrix", "as", "the", "output", "classification", "matrix", ".", "We", "empirically", "validate", "our", "theory", "in", "a", "practical", "setting", ",", "with", "much", "milder", "assumptions", "than", "those", "in", "theory", ".", "We", "also", "find", "empirically", "that", "for", "large", "networks", ",", "most", "of", "the", "improvement", "could", "be", "achieved", "by", "only", "reusing", "the", "word", "embeddings", ".", "We", "test", "our", "framework", "by", "performing", "extensive", "experiments", "on", "the", "Penn", "Treebank", "corpus", ",", "a", "dataset", "widely", "used", "for", "benchmarking", "language", "models", "[", "reference", "][", "reference", "]", ".", "We", "demonstrate", "that", "models", "trained", "using", "our", "proposed", "framework", "significantly", "outperform", "models", "trained", "using", "the", "conventional", "framework", ".", "We", "also", "perform", "experiments", "on", "the", "newly", "introduced", "Wikitext", "-", "2", "dataset", "[", "reference", "]", ",", "and", "verify", "that", "the", "empirical", "performance", "of", "our", "proposed", "framework", "is", "consistent", "across", "different", "datasets", ".", "section", ":", "BACKGROUND", ":", "RECURRENT", "NEURAL", "NETWORK", "LANGUAGE", "MODEL", "In", "any", "variant", "of", "recurrent", "neural", "network", "language", "model", "(", "RNNLM", ")", ",", "the", "goal", "is", "to", "predict", "the", "next", "word", "indexed", "by", "t", "in", "a", "sequence", "of", "one", "-", "hot", "word", "tokens", "(", "y", "*", "1", ",", ".", ".", ".", "y", "*", "N", ")", "as", "follows", ":", "The", "matrix", "L", "\u2208", "R", "dx\u00d7|V", "|", "is", "the", "word", "embedding", "matrix", ",", "where", "d", "x", "is", "the", "word", "embedding", "dimension", "and", "|V", "|", "is", "the", "size", "of", "the", "vocabulary", ".", "The", "function", "f", "(", ".", ",", ".", ")", "represents", "the", "recurrent", "neural", "network", "which", "takes", "in", "the", "current", "input", "and", "the", "previous", "hidden", "state", "and", "produces", "the", "next", "hidden", "state", ".", "W", "\u2208", "R", "|V", "|\u00d7d", "h", "and", "b", "\u2208", "R", "|V", "|", "are", "the", "the", "output", "projection", "matrix", "and", "the", "bias", ",", "respectively", ",", "and", "d", "h", "is", "the", "size", "of", "the", "RNN", "hidden", "state", ".", "The", "|V", "|", "dimensional", "y", "t", "models", "the", "discrete", "probability", "distribution", "for", "the", "next", "word", ".", "Note", "that", "the", "above", "formulation", "does", "not", "make", "any", "assumptions", "about", "the", "specifics", "of", "the", "recurrent", "neural", "units", ",", "and", "f", "could", "be", "replaced", "with", "a", "standard", "recurrent", "unit", ",", "a", "gated", "recurrent", "unit", "(", "GRU", ")", "[", "reference", "]", ",", "a", "long", "-", "short", "term", "memory", "(", "LSTM", ")", "unit", "[", "reference", "]", ",", "etc", ".", "For", "our", "experiments", ",", "we", "use", "LSTM", "units", "with", "two", "layers", ".", "Given", "y", "t", "for", "the", "t", "th", "example", ",", "a", "loss", "is", "calculated", "for", "that", "example", ".", "The", "loss", "used", "in", "the", "RNNLMs", "is", "almost", "exclusively", "the", "cross", "-", "entropy", "between", "y", "t", "and", "the", "observed", "one", "-", "hot", "word", "token", ",", "y", "*", "t", ":", "We", "shall", "refer", "to", "y", "t", "as", "the", "model", "prediction", "distribution", "for", "the", "t", "th", "example", ",", "and", "y", "*", "t", "as", "the", "empirical", "target", "distribution", "(", "both", "are", "in", "fact", "conditional", "distributions", "given", "the", "history", ")", ".", "Since", "crossentropy", "and", "Kullback", "-", "Leibler", "divergence", "are", "equivalent", "when", "the", "target", "distribution", "is", "one", "-", "hot", ",", "we", "can", "rewrite", "the", "loss", "for", "the", "t", "th", "example", "as", "Therefore", ",", "we", "can", "think", "of", "the", "optimization", "of", "the", "conventional", "loss", "in", "an", "RNNLM", "as", "trying", "to", "minimize", "the", "distance", "1", "between", "the", "model", "prediction", "distribution", "(", "y", ")", "and", "the", "empirical", "target", "distribution", "(", "y", "*", ")", ",", "which", ",", "with", "many", "training", "examples", ",", "will", "get", "close", "to", "minimizing", "distance", "to", "the", "actual", "target", "distribution", ".", "In", "the", "framework", "which", "we", "will", "introduce", ",", "we", "utilize", "Kullback", "-", "Leibler", "divergence", "as", "opposed", "to", "cross", "-", "entropy", "due", "to", "its", "intuitive", "interpretation", "as", "a", "distance", "between", "distributions", ",", "although", "the", "two", "are", "not", "equivalent", "in", "our", "framework", ".", "In", "above", ",", "\u03b1", "is", "a", "hyperparameter", "to", "be", "adjusted", ",", "and\u0177", "t", "is", "almost", "identical", "to", "the", "regular", "model", "prediction", "distribution", "y", "t", "with", "the", "exception", "that", "the", "logits", "are", "divided", "by", "a", "temperature", "parameter", "\u03c4", ".", "We", "define\u1ef9", "t", "as", "some", "probability", "distribution", "that", "estimates", "the", "true", "data", "distribution", "(", "conditioned", "on", "the", "word", "history", ")", "which", "satisfies", "E\u1ef9", "t", "=", "Ey", "*", "t", ".", "The", "goal", "of", "this", "framework", "is", "to", "minimize", "the", "1", "We", "note", ",", "however", ",", "that", "Kullback", "-", "Leibler", "divergence", "is", "not", "a", "valid", "distance", "metric", ".", "distribution", "distance", "between", "the", "prediction", "distribution", "and", "a", "more", "accurate", "estimate", "of", "the", "true", "data", "distribution", ".", "To", "understand", "the", "effect", "of", "optimizing", "in", "this", "setting", ",", "let", "'s", "focus", "on", "an", "ideal", "case", "in", "which", "we", "are", "given", "the", "true", "data", "distribution", "so", "that\u1ef9", "t", "=", "Ey", "*", "t", ",", "and", "we", "only", "use", "the", "augmented", "loss", ",", "J", "aug", ".", "We", "will", "carry", "out", "our", "investigation", "through", "stochastic", "gradient", "descent", ",", "which", "is", "the", "technique", "dominantly", "used", "for", "training", "neural", "networks", ".", "The", "gradient", "of", "J", "aug", "t", "with", "respect", "to", "the", "logits", "W", "h", "t", "is", "Let", "'s", "denote", "by", "e", "j", "\u2208", "R", "|V", "|", "the", "vector", "whose", "j", "th", "entry", "is", "1", ",", "and", "others", "are", "zero", ".", "We", "can", "then", "rewrite", "(", "3.4", ")", "as", "(", "3.5", ")", "Implication", "of", "(", "3.5", ")", "is", "the", "following", ":", "Every", "time", "the", "optimizer", "sees", "one", "training", "example", ",", "it", "takes", "a", "step", "not", "only", "on", "account", "of", "the", "label", "seen", ",", "but", "it", "proceeds", "taking", "into", "account", "all", "the", "class", "labels", "for", "which", "the", "conditional", "probability", "is", "not", "zero", ",", "and", "the", "relative", "step", "size", "for", "each", "step", "is", "given", "by", "the", "conditional", "probability", "for", "that", "label", ",", "\u1ef9", "t", ",", "i", ".", "Furthermore", ",", "this", "is", "a", "much", "less", "noisy", "update", "since", "the", "target", "distribution", "is", "exact", "and", "deterministic", ".", "Therefore", ",", "unless", "all", "the", "examples", "exclusively", "belong", "to", "a", "specific", "class", "with", "probability", "1", ",", "the", "optimization", "will", "act", "much", "differently", "and", "train", "with", "greatly", "improved", "supervision", ".", "The", "idea", "proposed", "in", "the", "recent", "work", "by", "[", "reference", "]", "might", "be", "considered", "as", "an", "application", "of", "this", "framework", ",", "where", "they", "try", "to", "obtain", "a", "good", "set", "of\u1ef9", "'s", "by", "training", "very", "large", "models", "and", "using", "the", "model", "prediction", "distributions", "of", "those", ".", "Although", "finding", "a", "good\u1ef9", "in", "general", "is", "rather", "nontrivial", ",", "in", "the", "context", "of", "language", "modeling", "we", "can", "hope", "to", "achieve", "this", "by", "exploiting", "the", "inherent", "metric", "space", "of", "classes", "encoded", "into", "the", "model", ",", "namely", "the", "space", "of", "word", "embeddings", ".", "Specifically", ",", "we", "propose", "the", "following", "for\u1ef9", ":", "In", "words", ",", "we", "first", "find", "the", "target", "word", "vector", "which", "corresponds", "to", "the", "target", "word", "token", "(", "resulting", "in", "u", "t", ")", ",", "and", "then", "take", "the", "inner", "product", "of", "the", "target", "word", "vector", "with", "all", "the", "other", "word", "vectors", "to", "get", "an", "unnormalized", "probability", "distribution", ".", "We", "adjust", "this", "with", "the", "same", "temperature", "parameter", "\u03c4", "used", "for", "obtaining\u0177", "t", "and", "apply", "softmax", ".", "The", "target", "distribution", "estimate", ",", "\u1ef9", ",", "therefore", "measures", "the", "similarity", "between", "the", "word", "vectors", "and", "assigns", "similar", "probability", "masses", "to", "words", "that", "the", "language", "model", "deems", "close", ".", "Note", "that", "the", "estimation", "of\u1ef9", "with", "this", "procedure", "is", "iterative", ",", "and", "the", "estimates", "of\u1ef9", "in", "the", "initial", "phase", "of", "the", "training", "are", "not", "necessarily", "informative", ".", "However", ",", "as", "training", "procedes", ",", "we", "expect\u1ef9", "to", "capture", "the", "word", "statistics", "better", "and", "yield", "a", "consistently", "more", "accurate", "estimate", "of", "the", "true", "data", "distribution", ".", "section", ":", "THEORETICALLY", "DRIVEN", "REUSE", "OF", "WORD", "EMBEDDINGS", "We", "now", "theoretically", "motivate", "and", "introduce", "a", "second", "modification", "to", "improve", "learning", "in", "the", "language", "model", ".", "We", "do", "this", "by", "analyzing", "the", "proposed", "augmented", "loss", "in", "a", "particular", "setting", ",", "and", "observe", "an", "implicit", "core", "mechanism", "of", "this", "loss", ".", "We", "then", "make", "our", "proposition", "by", "making", "this", "mechanism", "explicit", ".", "We", "start", "by", "introducing", "our", "setting", "for", "the", "analysis", ".", "We", "restrict", "our", "attention", "to", "the", "case", "where", "the", "input", "embedding", "dimension", "is", "equal", "to", "the", "dimension", "of", "the", "RNN", "hidden", "state", ",", "i.e.", "We", "also", "set", "b", "=", "0", "in", "(", "2.3", ")", "so", "that", "y", "t", "=", "W", "h", "t", ".", "We", "only", "use", "the", "augmented", "loss", ",", "i.e.", "J", "tot", "=", "J", "aug", ",", "and", "we", "assume", "that", "we", "can", "achieve", "zero", "training", "loss", ".", "Finally", ",", "we", "set", "the", "temperature", "parameter", "\u03c4", "to", "be", "large", ".", "We", "first", "show", "that", "when", "the", "temperature", "parameter", ",", "\u03c4", ",", "is", "high", "enough", ",", "J", "aug", "t", "acts", "to", "match", "the", "logits", "of", "the", "prediction", "distribution", "to", "the", "logits", "of", "the", "the", "more", "informative", "labels", ",", "\u1ef9", ".", "We", "proceed", "in", "the", "same", "way", "as", "was", "done", "in", "[", "reference", "]", "to", "make", "an", "identical", "argument", ".", "Particularly", ",", "we", "consider", "the", "derivative", "of", "J", "aug", "t", "with", "respect", "to", "the", "entries", "of", "the", "logits", "produced", "by", "the", "neural", "network", ".", "Let", "'s", "denote", "by", "l", "i", "the", "i", "th", "column", "of", "L.", "Using", "the", "first", "order", "approximation", "of", "exponential", "function", "around", "zero", "(", "exp", "(", "x", ")", "\u2248", "1", "+", "x", ")", ",", "we", "can", "approximate\u1ef9", "t", "(", "same", "holds", "for\u0177", "t", ")", "at", "high", "temperatures", "as", "follows", ":", "\u1ef9", "We", "can", "further", "simplify", "(", "4.1", ")", "if", "we", "assume", "that", "u", "t", ",", "l", "j", "=", "0", "on", "average", ":", "By", "replacing\u1ef9", "t", "and\u0177", "t", "in", "(", "3.4", ")", "with", "their", "simplified", "forms", "according", "to", "(", "4.2", ")", ",", "we", "get", "which", "is", "the", "desired", "result", "that", "augmented", "loss", "tries", "to", "match", "the", "logits", "of", "the", "model", "to", "the", "logits", "of", "y", "'s", ".", "Since", "the", "training", "loss", "is", "zero", "by", "assumption", ",", "we", "necessarily", "have", "for", "each", "training", "example", ",", "i.e.", ",", "gradient", "contributed", "by", "each", "example", "is", "zero", ".", "Provided", "that", "W", "and", "L", "are", "full", "rank", "matrices", "and", "there", "are", "more", "linearly", "independent", "examples", "of", "h", "t", "'s", "than", "the", "embedding", "dimension", "d", ",", "we", "get", "that", "the", "space", "spanned", "by", "the", "columns", "of", "L", "T", "is", "equivalent", "to", "that", "spanned", "by", "the", "columns", "of", "W", ".", "Let", "'s", "now", "introduce", "a", "square", "matrix", "T", "and", "W", "span", "the", "same", "column", "space", ")", ".", "In", "this", "case", ",", "we", "can", "rewrite", "In", "other", "words", ",", "by", "reusing", "the", "embedding", "matrix", "in", "the", "output", "projection", "layer", "(", "with", "a", "transpose", ")", "and", "letting", "the", "neural", "network", "do", "the", "necessary", "linear", "mapping", "h", "\u2192", "Ah", ",", "we", "get", "the", "same", "result", "as", "we", "would", "have", "in", "the", "first", "place", ".", "Although", "the", "above", "scenario", "could", "be", "difficult", "to", "exactly", "replicate", "in", "practice", ",", "it", "uncovers", "a", "mechanism", "through", "which", "our", "proposed", "loss", "augmentation", "acts", ",", "which", "is", "trying", "to", "constrain", "the", "output", "(", "unnormalized", ")", "probability", "space", "to", "a", "small", "subspace", "governed", "by", "the", "embedding", "matrix", ".", "This", "suggests", "that", "we", "can", "make", "this", "mechanism", "explicit", "and", "constrain", "W", "=", "L", "T", "during", "training", "while", "setting", "the", "output", "bias", ",", "b", ",", "to", "zero", ".", "Doing", "so", "would", "not", "only", "eliminate", "a", "big", "matrix", "which", "dominates", "the", "network", "size", "for", "models", "with", "even", "moderately", "sized", "vocabularies", ",", "but", "it", "would", "also", "be", "optimal", "in", "our", "setting", "of", "loss", "augmentation", "as", "it", "would", "eliminate", "much", "work", "to", "be", "done", "by", "the", "augmented", "loss", ".", "section", ":", "RELATED", "WORK", "Since", "their", "introduction", "in", "[", "reference", "]", ",", "many", "improvements", "have", "been", "proposed", "for", "RNNLMs", ",", "including", "different", "dropout", "methods", "[", "reference", "][", "reference", "]", ",", "novel", "recurrent", "units", "[", "reference", "]", ",", "and", "use", "of", "pointer", "networks", "to", "complement", "the", "recurrent", "neural", "network", "[", "reference", "]", ".", "However", ",", "none", "of", "the", "improvements", "dealt", "with", "the", "loss", "structure", ",", "and", "to", "the", "best", "of", "our", "knowledge", ",", "our", "work", "is", "the", "first", "to", "offer", "a", "new", "loss", "framework", ".", "Our", "technique", "is", "closely", "related", "to", "the", "one", "in", "[", "reference", "]", ",", "where", "they", "also", "try", "to", "estimate", "a", "more", "informed", "data", "distribution", "and", "augment", "the", "conventional", "loss", "with", "KL", "divergence", "between", "model", "prediction", "distribution", "and", "the", "estimated", "data", "distribution", ".", "However", ",", "they", "estimate", "their", "data", "distribution", "by", "training", "large", "networks", "on", "the", "data", "and", "then", "use", "it", "to", "improve", "learning", "in", "smaller", "networks", ".", "This", "is", "fundamentally", "different", "from", "our", "approach", ",", "where", "we", "improve", "learning", "by", "transferring", "knowledge", "between", "different", "parts", "of", "the", "same", "network", ",", "in", "a", "self", "contained", "manner", ".", "The", "work", "we", "present", "in", "this", "paper", "is", "based", "on", "a", "report", "which", "was", "made", "public", "in", "[", "reference", "]", ".", "We", "have", "recently", "come", "across", "a", "concurrent", "preprint", "[", "reference", "]", "where", "the", "authors", "reuse", "the", "word", "embedding", "matrix", "in", "the", "output", "projection", "to", "improve", "language", "modeling", ".", "However", ",", "their", "work", "is", "purely", "empirical", ",", "and", "they", "do", "not", "provide", "any", "theoretical", "justification", "for", "their", "approach", ".", "Finally", ",", "we", "would", "like", "to", "note", "that", "the", "idea", "of", "using", "the", "same", "representation", "for", "input", "and", "output", "words", "has", "been", "explored", "in", "the", "past", ",", "and", "there", "exists", "language", "models", "which", "could", "be", "interpreted", "as", "simple", "neural", "networks", "with", "shared", "input", "and", "output", "embeddings", "[", "reference", "][", "reference", "]", ".", "However", ",", "shared", "input", "and", "output", "representations", "were", "implicitly", "built", "into", "these", "models", ",", "rather", "than", "proposed", "as", "a", "supplement", "to", "a", "baseline", ".", "Consequently", ",", "possibility", "of", "improvement", "was", "not", "particularly", "pursued", "by", "sharing", "input", "and", "output", "representations", ".", "section", ":", "EXPERIMENTS", "In", "our", "experiments", ",", "we", "use", "the", "Penn", "Treebank", "corpus", "(", "PTB", ")", "[", "reference", "]", ",", "and", "the", "Wikitext", "-", "2", "dataset", "[", "reference", "]", ".", "PTB", "has", "been", "a", "standard", "dataset", "used", "for", "benchmarking", "language", "models", ".", "It", "consists", "of", "923k", "training", ",", "73k", "validation", ",", "and", "82k", "test", "words", ".", "The", "version", "of", "this", "dataset", "which", "we", "use", "is", "the", "one", "processed", "in", "[", "reference", "]", ",", "with", "the", "most", "frequent", "10k", "words", "selected", "to", "be", "in", "the", "vocabulary", "and", "rest", "replaced", "with", "a", "an", "<", "unk", ">", "token", "2", ".", "Wikitext", "-", "2", "is", "a", "dataset", "released", "recently", "as", "an", "alternative", "to", "PTB", "3", ".", "It", "contains", "2", ",", "088k", "training", ",", "217k", "validation", ",", "and", "245k", "test", "tokens", ",", "and", "has", "a", "vocabulary", "of", "33", ",", "278", "words", ";", "therefore", ",", "in", "comparison", "to", "PTB", ",", "it", "is", "roughly", "2", "times", "larger", "in", "dataset", "size", ",", "and", "3", "times", "larger", "in", "vocabulary", ".", "section", ":", "MODEL", "AND", "TRAINING", "HIGHLIGHTS", "We", "closely", "follow", "the", "LSTM", "based", "language", "model", "proposed", "in", "[", "reference", "]", "for", "constructing", "our", "baseline", "model", ".", "Specifically", ",", "we", "use", "a", "2", "-", "layer", "LSTM", "with", "the", "same", "number", "of", "hidden", "units", "in", "each", "layer", ",", "and", "we", "use", "3", "different", "network", "sizes", ":", "small", "(", "200", "units", ")", ",", "medium", "(", "650", "units", ")", ",", "and", "large", "(", "1500", "units", ")", ".", "We", "train", "our", "models", "using", "stochastic", "gradient", "descent", ",", "and", "we", "use", "a", "variant", "of", "the", "dropout", "method", "proposed", "in", "Gal", "(", "2015", ")", ".", "We", "defer", "further", "details", "regarding", "training", "the", "models", "to", "section", "A", "of", "the", "appendix", ".", "We", "refer", "to", "our", "baseline", "network", "as", "variational", "dropout", "LSTM", ",", "or", "VD", "-", "LSTM", "in", "short", ".", "section", ":", "EMPIRICAL", "VALIDATION", "FOR", "THE", "THEORY", "OF", "REUSING", "WORD", "EMBEDDINGS", "In", "Section", "4", ",", "we", "showed", "that", "the", "particular", "loss", "augmentation", "scheme", "we", "choose", "constrains", "the", "output", "projection", "matrix", "to", "be", "close", "to", "the", "input", "embedding", "matrix", ",", "without", "explicitly", "doing", "so", "by", "reusing", "the", "input", "embedding", "matrix", ".", "As", "a", "first", "experiment", ",", "we", "set", "out", "to", "validate", "this", "theoretical", "result", ".", "To", "do", "this", ",", "we", "try", "to", "simulate", "the", "setting", "in", "Section", "4", "by", "doing", "the", "following", ":", "We", "select", "a", "randomly", "chosen", "20", ",", "000", "contiguous", "word", "sequence", "in", "the", "PTB", "training", "set", ",", "and", "train", "a", "2", "-", "layer", "LSTM", "language", "model", "with", "300", "units", "in", "each", "layer", "with", "loss", "augmentation", "by", "minimizing", "the", "following", "loss", ":", "Here", ",", "\u03b2", "is", "the", "proportion", "of", "the", "augmented", "loss", "used", "in", "the", "total", "loss", ",", "and", "J", "aug", "is", "scaled", "by", "\u03c4", "2", "|V", "|", "to", "approximately", "match", "the", "magnitudes", "of", "the", "derivatives", "of", "J", "and", "J", "aug", "(", "see", "(", "4.3", ")", ")", ".", "Since", "we", "aim", "to", "achieve", "the", "minimum", "training", "loss", "possible", ",", "and", "the", "goal", "is", "to", "show", "a", "particular", "result", "rather", "than", "to", "achieve", "good", "generalization", ",", "we", "do", "not", "use", "any", "kind", "of", "regularization", "in", "the", "neural", "network", "(", "e.g.", "weight", "decay", ",", "dropout", ")", ".", "For", "this", "set", "of", "experiments", ",", "we", "also", "constrain", "each", "row", "of", "the", "input", "embedding", "matrix", "to", "have", "a", "norm", "of", "1", "because", "training", "becomes", "difficult", "without", "this", "constraint", "when", "only", "augmented", "loss", "is", "used", ".", "After", "training", ",", "we", "compute", "a", "metric", "that", "measures", "distance", "between", "the", "subspace", "spanned", "by", "the", "rows", "of", "the", "input", "embedding", "matrix", ",", "L", ",", "and", "that", "spanned", "by", "the", "columns", "of", "the", "output", "projection", "matrix", ",", "W", ".", "For", "this", ",", "we", "use", "a", "common", "metric", "based", "on", "the", "relative", "residual", "norm", "from", "projection", "of", "one", "matrix", "onto", "another", "[", "reference", "]", ".", "The", "computed", "distance", "between", "the", "subspaces", "is", "1", "when", "they", "are", "orthogonal", ",", "and", "0", "when", "they", "are", "the", "same", ".", "Interested", "reader", "may", "refer", "to", "section", "B", "in", "the", "appendix", "for", "the", "details", "of", "this", "metric", ".", "Figure", "1", "shows", "the", "results", "from", "two", "tests", ".", "In", "one", "(", "panel", "a", ")", ",", "we", "test", "the", "effect", "of", "using", "the", "augmented", "loss", "by", "sweeping", "\u03b2", "in", "(", "6.1", ")", "from", "0", "to", "1", "at", "a", "reasonably", "high", "temperature", "(", "\u03c4", "=", "10", ")", ".", "With", "no", "loss", "augmentation", "(", "\u03b2", "=", "0", ")", ",", "the", "distance", "is", "almost", "1", ",", "and", "as", "more", "and", "more", "augmented", "loss", "is", "used", "the", "distance", "decreases", "rapidly", ",", "and", "eventually", "reaches", "around", "0.06", "when", "only", "augmented", "loss", "is", "used", ".", "In", "the", "second", "test", "(", "panel", "b", ")", ",", "we", "set", "\u03b2", "=", "1", ",", "and", "try", "to", "see", "the", "effect", "of", "the", "temperature", "on", "the", "subspace", "distance", "(", "remember", "the", "theory", "predicts", "low", "distance", "when", "\u03c4", "\u2192", "\u221e", ")", ".", "Notably", ",", "the", "augmented", "loss", "causes", "W", "to", "approach", "L", "T", "sufficiently", "even", "at", "temperatures", "as", "low", "as", "2", ",", "although", "higher", "temperatures", "still", "lead", "to", "smaller", "subspace", "distances", ".", "These", "results", "confirm", "the", "mechanism", "through", "which", "our", "proposed", "loss", "pushes", "W", "to", "learn", "the", "same", "column", "space", "as", "L", "T", ",", "and", "it", "suggests", "that", "reusing", "the", "input", "embedding", "matrix", "by", "explicitly", "constraining", "W", "=", "L", "T", "is", "not", "simply", "a", "kind", "of", "regularization", ",", "but", "is", "in", "fact", "an", "optimal", "choice", "in", "our", "framework", ".", "What", "can", "be", "achieved", "separately", "with", "each", "of", "the", "two", "proposed", "improvements", "as", "well", "as", "with", "the", "two", "of", "them", "combined", "is", "a", "question", "of", "empirical", "nature", ",", "which", "we", "investigate", "in", "the", "next", "section", ".", "section", ":", "RESULTS", "ON", "PTB", "AND", "WIKITEXT", "-", "2", "DATASETS", "In", "order", "to", "investigate", "the", "extent", "to", "which", "each", "of", "our", "proposed", "improvements", "helps", "with", "learning", ",", "we", "train", "4", "different", "models", "for", "each", "network", "size", ":", "(", "1", ")", "2", "-", "Layer", "LSTM", "with", "variational", "dropout", "(", "VD", "-", "LSTM", ")", "(", "2", ")", "2", "-", "Layer", "LSTM", "with", "variational", "dropout", "and", "augmented", "loss", "(", "VD", "-", "LSTM", "+", "AL", ")", "(", "3", ")", "2", "-", "Layer", "LSTM", "with", "variational", "dropout", "and", "reused", "embeddings", "(", "VD", "-", "LSTM", "+", "RE", ")", "(", "4", ")", "2", "-", "Layer", "LSTM", "with", "variational", "dropout", "and", "both", "RE", "and", "AL", "(", "VD", "-", "LSTM", "+", "REAL", ")", ".", "Figure", "2", "shows", "the", "validation", "perplexities", "of", "the", "four", "models", "during", "training", "on", "the", "PTB", "corpus", "for", "small", "(", "panel", "a", ")", "and", "large", "(", "panel", "b", ")", "networks", ".", "All", "of", "AL", ",", "RE", ",", "and", "REAL", "networks", "significantly", "outperform", "the", "baseline", "in", "both", "cases", ".", "Table", "1", "compares", "the", "final", "validation", "and", "test", "perplexities", "of", "the", "four", "models", "on", "both", "PTB", "and", "Wikitext", "-", "2", "for", "each", "network", "size", ".", "In", "both", "datasets", ",", "both", "AL", "and", "RE", "improve", "upon", "the", "baseline", "individually", ",", "and", "using", "RE", "and", "AL", "together", "leads", "to", "the", "best", "performance", ".", "Based", "on", "performance", "comparisons", ",", "we", "make", "the", "following", "notes", "on", "the", "two", "proposed", "improvements", ":", "\u2022", "AL", "provides", "better", "performance", "gains", "for", "smaller", "networks", ".", "This", "is", "not", "surprising", "given", "the", "fact", "that", "small", "models", "are", "rather", "inflexible", ",", "and", "one", "would", "expect", "to", "see", "improved", "learning", "by", "training", "against", "a", "more", "informative", "data", "distribution", "(", "contributed", "by", "the", "augmented", "loss", ")", "(", "see", "[", "reference", "]", ")", ".", "For", "the", "smaller", "PTB", "dataset", ",", "performance", "with", "AL", "surpasses", "that", "with", "RE", ".", "In", "comparison", ",", "for", "the", "larger", "Wikitext", "-", "2", "dataset", ",", "improvement", "by", "AL", "is", "more", "limited", ".", "This", "is", "expected", "given", "larger", "training", "sets", "better", "represent", "the", "true", "data", "distribution", ",", "mitigating", "the", "supervision", "problem", ".", "In", "fact", ",", "we", "set", "out", "to", "validate", "this", "reasoning", "in", "a", "direct", "manner", ",", "and", "additionally", "train", "the", "small", "networks", "separately", "on", "the", "first", "and", "second", "halves", "of", "the", "Wikitext", "-", "2", "training", "set", ".", "This", "results", "in", "two", "distinct", "datasets", "which", "are", "each", "about", "the", "same", "size", "as", "PTB", "(", "1044", "K", "vs", "929", "K", ")", ".", "As", "can", "be", "seen", "in", "Table", "2", "has", "significantly", "improved", "competitive", "performance", "against", "RE", "and", "REAL", "despite", "the", "fact", "that", "embedding", "size", "is", "3", "times", "larger", "compared", "to", "PTB", ".", "These", "results", "support", "our", "argument", "that", "the", "proposed", "augmented", "loss", "term", "acts", "to", "improve", "the", "amount", "of", "information", "gathered", "from", "the", "dataset", ".", "\u2022", "RE", "significantly", "outperforms", "AL", "for", "larger", "networks", ".", "This", "indicates", "that", ",", "for", "large", "models", ",", "the", "more", "effective", "mechanism", "of", "our", "proposed", "framework", "is", "the", "one", "which", "enforces", "proximity", "between", "the", "output", "projection", "space", "and", "the", "input", "embedding", "space", ".", "From", "a", "model", "complexity", "perspective", ",", "the", "nontrivial", "gains", "offered", "by", "RE", "for", "all", "network", "sizes", "and", "for", "both", "datasets", "could", "be", "largely", "attributed", "to", "its", "explicit", "function", "to", "reduce", "the", "model", "size", "while", "preserving", "the", "representational", "power", "according", "to", "our", "framework", ".", "We", "list", "in", "Table", "3", "the", "comparison", "of", "models", "with", "and", "without", "our", "proposed", "modifications", "on", "the", "Penn", "Treebank", "Corpus", ".", "The", "best", "LSTM", "model", "(", "VD", "-", "LSTM", "+", "REAL", ")", "outperforms", "all", "previous", "work", "which", "uses", "conventional", "framework", ",", "including", "large", "ensembles", ".", "The", "recently", "proposed", "recurrent", "highway", "networks", "[", "reference", "]", "when", "trained", "with", "reused", "embeddings", "(", "VD", "-", "RHN", "+", "RE", ")", "achieves", "the", "best", "overall", "performance", ",", "improving", "on", "VD", "-", "RHN", "by", "a", "perplexity", "of", "2.5", ".", "Table", "2", ":", "Performance", "of", "the", "four", "different", "small", "models", "trained", "on", "the", "equally", "sized", "two", "partitions", "of", "Wikitext", "-", "2", "training", "set", ".", "These", "results", "are", "consistent", "with", "those", "on", "PTB", "(", "see", "Table", "1", ")", ",", "which", "has", "a", "similar", "training", "set", "size", "with", "each", "of", "these", "partitions", ",", "although", "its", "word", "embedding", "dimension", "is", "three", "times", "smaller", ".", "section", ":", "QUALITATIVE", "RESULTS", "One", "important", "feature", "of", "our", "framework", "that", "leads", "to", "better", "word", "predictions", "is", "the", "explicit", "mechanism", "to", "assign", "probabilities", "to", "words", "not", "merely", "according", "to", "the", "observed", "output", "statistics", ",", "but", "also", "considering", "the", "metric", "similarity", "between", "words", ".", "We", "observe", "direct", "consequences", "of", "this", "mechanism", "qualitatively", "in", "the", "Penn", "Treebank", "in", "different", "ways", ":", "First", ",", "we", "notice", "that", "the", "probability", "of", "generating", "the", "<", "unk", ">", "token", "with", "our", "proposed", "network", "(", "VD", "-", "LSTM", "+", "REAL", ")", "is", "significantly", "lower", "compared", "to", "the", "baseline", "network", "(", "VD", "-", "LSTM", ")", "across", "many", "words", ".", "This", "could", "be", "explained", "by", "noting", "the", "fact", "that", "the", "<", "unk", ">", "token", "is", "an", "aggregated", "token", "rather", "than", "a", "specific", "word", ",", "and", "it", "is", "often", "not", "expected", "to", "be", "close", "to", "specific", "words", "in", "the", "word", "embedding", "space", ".", "We", "observe", "the", "same", "behavior", "with", "very", "frequent", "words", "such", "as", "\"", "a", "\"", ",", "\"", "an", "\"", ",", "and", "\"", "the", "\"", ",", "owing", "to", "the", "same", "fact", "that", "they", "are", "not", "correlated", "with", "particular", "words", ".", "Second", ",", "we", "not", "only", "observe", "better", "probability", "assignments", "for", "the", "target", "words", ",", "but", "we", "also", "observe", "relatively", "higher", "probability", "weights", "associated", "with", "the", "words", "close", "to", "the", "targets", ".", "Sometimes", "this", "happens", "in", "the", "form", "of", "predicting", "words", "semantically", "close", "together", "which", "are", "plausible", "even", "when", "the", "target", "word", "is", "not", "successfully", "captured", "by", "the", "model", ".", "We", "provide", "a", "few", "examples", "from", "the", "PTB", "test", "set", "which", "compare", "the", "prediction", "performance", "of", "1500", "unit", "VD", "-", "LSTM", "and", "1500", "unit", "VD", "-", "LSTM", "+", "REAL", "in", "table", "4", ".", "We", "would", "like", "to", "note", "that", "prediction", "performance", "of", "VD", "-", "LSTM", "+", "RE", "is", "similar", "to", "VD", "-", "LSTM", "+", "REAL", "for", "the", "large", "network", ".", "In", "this", "work", ",", "we", "introduced", "a", "novel", "loss", "framework", "for", "language", "modeling", ".", "Particularly", ",", "we", "showed", "that", "the", "metric", "encoded", "into", "the", "space", "of", "word", "embeddings", "could", "be", "used", "to", "generate", "a", "more", "informed", "data", "distribution", "than", "the", "one", "-", "hot", "targets", ",", "and", "that", "additionally", "training", "against", "this", "distribution", "improves", "learning", ".", "We", "also", "showed", "theoretically", "that", "this", "approach", "lends", "itself", "to", "a", "second", "improvement", ",", "which", "is", "simply", "reusing", "the", "input", "embedding", "matrix", "in", "the", "output", "projection", "layer", ".", "This", "has", "an", "additional", "benefit", "of", "reducing", "the", "number", "of", "trainable", "variables", "in", "the", "model", ".", "We", "empirically", "validated", "the", "theoretical", "link", ",", "and", "verified", "that", "both", "proposed", "changes", "do", "in", "fact", "belong", "to", "the", "same", "framework", ".", "In", "our", "experiments", "on", "the", "Penn", "Treebank", "corpus", "and", "Wikitext", "-", "2", ",", "we", "showed", "that", "our", "framework", "outperforms", "the", "conventional", "one", ",", "and", "that", "even", "the", "simple", "modification", "of", "reusing", "the", "word", "embedding", "in", "the", "output", "projection", "layer", "is", "sufficient", "for", "large", "networks", ".", "The", "improvements", "achieved", "by", "our", "framework", "are", "not", "unique", "to", "vanilla", "language", "modeling", ",", "and", "are", "readily", "applicable", "to", "other", "tasks", "which", "utilize", "language", "models", "such", "as", "neural", "machine", "translation", ",", "speech", "recognition", ",", "and", "text", "summarization", ".", "This", "could", "lead", "to", "significant", "improvements", "in", "such", "models", "especially", "with", "large", "vocabularies", ",", "with", "the", "additional", "benefit", "of", "greatly", "reducing", "the", "number", "of", "parameters", "to", "be", "trained", ".", "section", ":", "APPENDIX", "A", "MODEL", "AND", "TRAINING", "DETAILS", "We", "begin", "training", "with", "a", "learning", "rate", "of", "1", "and", "start", "decaying", "it", "with", "a", "constant", "rate", "after", "a", "certain", "epoch", ".", "This", "is", "5", ",", "10", ",", "and", "1", "for", "the", "small", ",", "medium", ",", "and", "large", "networks", "respectively", ".", "The", "decay", "rate", "is", "0.9", "for", "the", "small", "and", "medium", "networks", ",", "and", "0.97", "for", "the", "large", "network", ".", "For", "both", "PTB", "and", "Wikitext", "-", "2", "datasets", ",", "we", "unroll", "the", "network", "for", "35", "steps", "for", "backpropagation", ".", "We", "use", "gradient", "clipping", "[", "reference", "]", ";", "i.e.", "we", "rescale", "the", "gradients", "using", "the", "global", "norm", "if", "it", "exceeds", "a", "certain", "value", ".", "For", "both", "datasets", ",", "this", "is", "5", "for", "the", "small", "and", "the", "medium", "network", ",", "and", "6", "for", "the", "large", "network", ".", "We", "use", "the", "dropout", "method", "introduced", "in", "Gal", "(", "2015", ")", ";", "particularly", ",", "we", "use", "the", "same", "dropout", "mask", "for", "each", "example", "through", "the", "unrolled", "network", ".", "Differently", "from", "what", "was", "proposed", "in", "Gal", "(", "2015", ")", ",", "we", "tie", "the", "dropout", "weights", "for", "hidden", "states", "further", ",", "and", "we", "use", "the", "same", "mask", "when", "they", "are", "propagated", "as", "states", "in", "the", "current", "layer", "and", "when", "they", "are", "used", "as", "inputs", "for", "the", "next", "layer", ".", "We", "do", "n't", "use", "dropout", "in", "the", "input", "embedding", "layer", ",", "and", "we", "use", "the", "same", "dropout", "probability", "for", "inputs", "and", "hidden", "states", ".", "For", "PTB", ",", "dropout", "probabilities", "are", "0.7", ",", "0.5", "and", "0.35", "for", "small", ",", "medium", "and", "large", "networks", "respectively", ".", "For", "Wikitext", "-", "2", ",", "probabilities", "are", "0.8", "for", "the", "small", "and", "0.6", "for", "the", "medium", "networks", ".", "When", "training", "the", "networks", "with", "the", "augmented", "loss", "(", "AL", ")", ",", "we", "use", "a", "temperature", "\u03c4", "=", "20", ".", "We", "have", "empirically", "observed", "that", "setting", "\u03b1", ",", "the", "weight", "of", "the", "augmented", "loss", ",", "according", "to", "\u03b1", "=", "\u03b3\u03c4", "for", "all", "the", "networks", "works", "satisfactorily", ".", "We", "set", "\u03b3", "to", "values", "between", "0.5", "and", "0.8", "for", "the", "PTB", "dataset", ",", "and", "between", "1.0", "and", "1.5", "for", "the", "Wikitext", "-", "2", "dataset", ".", "We", "would", "like", "to", "note", "that", "we", "have", "not", "observed", "sudden", "deteriorations", "in", "the", "performance", "with", "respect", "to", "moderate", "variations", "in", "either", "\u03c4", "or", "\u03b1", ".", "section", ":", "B", "METRIC", "FOR", "CALCULATING", "SUBSPACE", "DISTANCES", "In", "this", "section", ",", "we", "detail", "the", "metric", "used", "for", "computing", "the", "subspace", "distance", "between", "two", "matrices", ".", "The", "computed", "metric", "is", "closely", "related", "with", "the", "principle", "angles", "between", "subspaces", ",", "first", "defined", "in", "Jordan", "(", "1875", ")", ".", "Our", "aim", "is", "to", "compute", "a", "metric", "distance", "between", "two", "given", "matrices", ",", "X", "and", "Y", ".", "We", "do", "this", "in", "three", "steps", ":", "(", "1", ")", "Obtain", "two", "matrices", "with", "orthonormal", "columns", ",", "U", "and", "V", ",", "such", "that", "span", "(", "U", ")", "=", "span", "(", "X", ")", "and", "span", "(", "V", ")", "=", "span", "(", "Y", ")", ".", "U", "and", "V", "could", "be", "obtained", "with", "a", "QR", "decomposition", ".", "(", "2", ")", "Calculate", "the", "projection", "of", "either", "one", "of", "U", "and", "V", "onto", "the", "other", ";", "e.g.", "do", "S", "=", "U", "U", "T", "V", ",", "where", "S", "is", "the", "projection", "of", "V", "onto", "U", ".", "Then", "calculate", "the", "residual", "matrix", "as", "R", "=", "V", "\u2212", "S.", "We", "note", "that", "d", "as", "calculated", "above", "is", "a", "valid", "metric", "up", "to", "the", "equivalence", "set", "of", "matrices", "which", "span", "the", "same", "column", "space", ",", "although", "we", "are", "not", "going", "to", "show", "it", ".", "Instead", ",", "we", "will", "mention", "some", "metric", "properties", "of", "d", ",", "and", "relate", "it", "to", "the", "principal", "angles", "between", "the", "subspaces", ".", "We", "first", "work", "out", "an", "expression", "for", "d", ":", "where", "\u03c1", "i", "is", "the", "i", "th", "singular", "value", "of", "U", "T", "V", ",", "commonly", "referred", "to", "as", "the", "i", "th", "principle", "angle", "between", "the", "subspaces", "of", "X", "and", "Y", ",", "\u03b8", "i", ".", "In", "above", ",", "we", "used", "the", "cyclic", "permutation", "property", "of", "the", "trace", "in", "the", "third", "and", "the", "fourth", "lines", ".", "section", ":", "Since", "d", "2", "is", "1", "C", "Trace", "(", "R", "T", "R", ")", ",", "it", "is", "always", "nonnegative", ",", "and", "it", "is", "only", "zero", "when", "the", "residual", "is", "zero", ",", "which", "is", "the", "case", "when", "span", "(", "X", ")", "=", "span", "(", "Y", ")", ".", "Further", ",", "it", "is", "symmetric", "between", "U", "and", "V", "due", "to", "the", "form", "of", "(", "B.1", ")", "(", "singular", "values", "of", "V", "T", "U", "and", "V", "T", "U", "are", "the", "same", ")", ".", "Also", ",", ",", "namely", "the", "average", "of", "the", "sines", "of", "the", "principle", "angles", ",", "which", "is", "a", "quantity", "between", "0", "and", "1", ".", "section", ":"]}