{"coref": {"Accuracy": [[891, 892], [1348, 1349], [2837, 2838], [3057, 3058], [3108, 3109], [3204, 3205], [4588, 4589], [4592, 4593], [4606, 4607]], "MT-DNN": [[2, 8], [19, 25], [26, 29], [43, 46], [79, 82], [104, 107], [163, 166], [257, 263], [264, 267], [749, 752], [769, 772], [791, 794], [848, 851], [862, 865], [940, 943], [965, 968], [1635, 1638], [2157, 2160], [2645, 2648], [2669, 2672], [3472, 3475], [3602, 3605], [3650, 3653], [3747, 3750], [3896, 3899], [3936, 3939], [3981, 3984], [4187, 4190], [4354, 4357], [4383, 4386], [4557, 4560], [4581, 4584], [4634, 4637], [4659, 4662], [4679, 4682], [4708, 4711], [4731, 4734], [670, 673], [982, 985], [1357, 1360], [1365, 1368], [2425, 2428], [4322, 4325]], "Matched": [[3189, 3190]], "Mismatched": [[3196, 3197]], "MultiNLI": [[1177, 1178], [3120, 3121], [3231, 3232], [3732, 3733], [4064, 4065]], "Natural_Language_Inference": [[9, 12], [625, 628], [1179, 1182], [1805, 1808], [1816, 1818], [2772, 2774], [3121, 3127], [3254, 3257], [3727, 3729], [4306, 4308]], "Parameters": [], "Paraphrase_Identification": [], "Quora_Question_Pairs": [[1206, 1207], [3011, 3012], [3013, 3017], [3026, 3032], [3105, 3106], [3742, 3743], [4066, 4067]], "SNLI": [[123, 124], [154, 155], [853, 854], [896, 897], [2656, 2660], [2701, 2702], [2715, 2716], [2768, 2769], [3298, 3299], [3300, 3304], [3305, 3306], [4149, 4150], [4211, 4212], [4309, 4310], [4340, 4341], [4413, 4414], [4429, 4430], [4488, 4489], [4510, 4511], [4701, 4702], [945, 946], [2661, 2662], [4577, 4578]], "SST-2_Binary_classification": [[2808, 2811], [2812, 2815], [1071, 1074], [1684, 1687]], "SciTail": [[125, 126], [156, 158], [855, 856], [901, 902], [2665, 2666], [2703, 2704], [2718, 2720], [2770, 2771], [3402, 3403], [3449, 3450], [4151, 4152], [4222, 4223], [4311, 4312], [4342, 4343], [4415, 4416], [4431, 4432], [4464, 4465], [4512, 4513], [4624, 4625], [4703, 4704], [947, 948], [3359, 3360]], "Sentiment_Analysis": [[1809, 1810], [2744, 2746], [3343, 3344]], "__Test_Accuracy": [], "__Train_Accuracy": []}, "coref_non_salient": {"0": [[2101, 2103], [2544, 2552]], "1": [[1310, 1313], [2412, 2413]], "10": [[736, 738], [2493, 2496]], "100": [[2066, 2068]], "101": [[4248, 4250]], "102": [[1095, 1097], [1725, 1727]], "103": [[368, 371]], "104": [[683, 687]], "105": [[1469, 1473]], "106": [[1064, 1066]], "107": [[2877, 2879]], "108": [[528, 533]], "109": [[2121, 2123]], "11": [[732, 735], [998, 1001], [1142, 1145], [2437, 2444], [4026, 4030]], "110": [[2851, 2855]], "111": [[2078, 2079]], "112": [[3554, 3557]], "113": [[31, 33]], "114": [[91, 98]], "115": [[1101, 1103]], "116": [[497, 498]], "117": [[814, 818]], "118": [[1853, 1856]], "119": [[3486, 3487]], "12": [[3245, 3246], [3250, 3251], [3614, 3615], [4332, 4333]], "120": [[2398, 2400]], "121": [[1002, 1005]], "122": [[2798, 2801]], "123": [[1845, 1847]], "124": [[1947, 1948]], "125": [[4144, 4146]], "126": [[3247, 3249]], "127": [[3369, 3372]], "128": [[3489, 3490]], "13": [[819, 820], [1138, 1139], [1300, 1301], [2654, 2655], [2696, 2697], [2713, 2714], [2941, 2942], [3593, 3594], [3825, 3826], [4535, 4536], [4706, 4707], [132, 133], [137, 138], [826, 827], [1023, 1024], [2279, 2280], [2730, 2731], [3587, 3588], [3628, 3629], [3932, 3933], [3955, 3956], [4328, 4329]], "14": [[1296, 1299], [1350, 1352], [2937, 2940], [3821, 3824], [4119, 4121]], "15": [[1263, 1264], [1291, 1292], [2087, 2088], [2560, 2561], [2920, 2921], [2976, 2977], [4099, 4100], [4117, 4118], [3814, 3815]], "16": [[2803, 2805], [2842, 2844], [3064, 3066], [3115, 3117], [3201, 3203]], "17": [[4395, 4397]], "18": [[762, 765], [2691, 2695], [3672, 3675], [4019, 4022]], "19": [[35, 38], [218, 221]], "2": [[1461, 1464], [1676, 1678], [1757, 1759], [2106, 2108]], "20": [[274, 276], [363, 364], [409, 410], [457, 458], [773, 774], [2689, 2690], [3670, 3671], [3676, 3677], [3929, 3930], [3998, 3999], [4744, 4745], [632, 633]], "21": [[101, 102], [503, 504], [523, 524], [595, 596], [680, 681], [746, 748], [767, 768], [840, 841], [1612, 1614], [2683, 2684], [3708, 3709], [3867, 3868], [3871, 3873], [4097, 4098], [4115, 4116], [4138, 4139], [4402, 4404], [4562, 4563], [4614, 4615], [4646, 4647], [181, 182], [880, 881], [2181, 2182], [3482, 3483], [3646, 3647], [3654, 3655], [3914, 3915], [3940, 3941], [4040, 4041], [4075, 4076], [4410, 4411], [4421, 4422], [4543, 4544], [4590, 4591]], "22": [[3964, 3967], [4093, 4096], [4102, 4105], [4133, 4136]], "23": [[3764, 3769], [4345, 4349], [4378, 4382]], "24": [[2055, 2056], [2165, 2166]], "25": [[39, 40], [222, 223], [119, 120], [559, 560], [617, 618], [660, 661], [723, 724], [809, 810], [990, 991], [1018, 1019], [2651, 2652], [2737, 2738], [2764, 2765], [4694, 4695]], "26": [[1731, 1735], [2499, 2502]], "27": [[1208, 1209], [3069, 3070], [3740, 3741], [4068, 4069]], "28": [[167, 169], [866, 868], [3351, 3353], [3459, 3461], [4234, 4236], [4301, 4303], [4641, 4643], [4718, 4720]], "29": [[2154, 2156], [3770, 3772], [4749, 4751]], "3": [[1435, 1437], [1570, 1572], [1578, 1584], [1935, 1937], [2192, 2194], [2334, 2336]], "30": [[740, 742], [1007, 1009], [1235, 1237], [2539, 2542], [2979, 2982]], "31": [[230, 234], [269, 273], [4664, 4668]], "32": [[4740, 4743]], "33": [[3059, 3061], [3110, 3112]], "34": [[2748, 2750], [3223, 3225]], "35": [[4245, 4247]], "36": [[2760, 2763]], "37": [[3756, 3758]], "38": [[1268, 1272], [2926, 2930]], "39": [[2779, 2783]], "4": [[235, 240], [459, 464], [553, 557], [758, 761], [1620, 1623]], "40": [[4669, 4674]], "41": [[2912, 2917]], "42": [[3778, 3782]], "43": [[1857, 1858], [1873, 1874], [1904, 1905], [3796, 3797], [4083, 4084]], "44": [[4047, 4052]], "45": [[3492, 3494]], "46": [[2519, 2522]], "47": [[2449, 2453], [4128, 4131]], "48": [[3326, 3328]], "49": [[3373, 3374]], "5": [[1878, 1882], [1956, 1959]], "50": [[1776, 1778], [2888, 2890]], "51": [[2405, 2407]], "52": [[3546, 3548]], "53": [[3970, 3974]], "54": [[788, 790], [3978, 3980]], "55": [[622, 624], [2741, 2743]], "56": [[1712, 1713]], "57": [[1488, 1490], [1539, 1541], [2189, 2191], [2330, 2332]], "58": [[2324, 2326]], "59": [[372, 373], [392, 393], [3847, 3848], [4005, 4006], [4038, 4039], [4072, 4073]], "6": [[2390, 2391], [3809, 3812], [4111, 4114]], "60": [[4715, 4717]], "61": [[428, 432], [973, 977], [4537, 4541]], "62": [[2167, 2173], [2212, 2219]], "63": [[365, 367], [4675, 4678]], "64": [[2222, 2229]], "65": [[1057, 1059]], "66": [[3129, 3139]], "67": [[1709, 1711]], "68": [[1175, 1176], [3730, 3731]], "69": [[3801, 3806]], "7": [[545, 549], [2206, 2209]], "70": [[3261, 3264]], "71": [[2070, 2071]], "72": [[3737, 3739]], "73": [[970, 972]], "74": [[2082, 2084]], "75": [[500, 501]], "76": [[201, 207]], "77": [[508, 512]], "78": [[2420, 2422]], "79": [[1783, 1785]], "8": [[727, 731], [993, 997], [1032, 1034], [1649, 1653], [2432, 2434]], "80": [[279, 282]], "81": [[650, 654]], "82": [[3516, 3521]], "83": [[2374, 2375]], "84": [[2059, 2062]], "85": [[3071, 3075]], "86": [[2198, 2201]], "87": [[3268, 3271]], "88": [[1849, 1851]], "89": [[2698, 2700]], "9": [[776, 780], [1631, 1634]], "90": [[3210, 3214]], "91": [[2016, 2020]], "92": [[541, 544]], "93": [[2202, 2205]], "94": [[471, 474]], "95": [[2230, 2231]], "96": [[2847, 2850]], "97": [[1799, 1802]], "98": [[1127, 1130]], "99": [[1869, 1872]]}, "doc_id": "34f63959ea4a13a05948274a1558c6854a051150", "method_subrelations": {"MT-DNN": [[[0, 6], "MT-DNN"]]}, "n_ary_relations": [{"Material": "MultiNLI", "Method": "MT-DNN", "Metric": "Matched", "Task": "Natural_Language_Inference", "score": "86.7"}, {"Material": "MultiNLI", "Method": "MT-DNN", "Metric": "Mismatched", "Task": "Natural_Language_Inference", "score": "86.0"}, {"Material": "Quora_Question_Pairs", "Method": "MT-DNN", "Metric": "Accuracy", "Task": "Paraphrase_Identification", "score": "89.6"}, {"Material": "SNLI", "Method": "MT-DNN", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "91.1"}, {"Material": "SNLI", "Method": "MT-DNN", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "96.8"}, {"Material": "SNLI", "Method": "MT-DNN", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "110m"}, {"Material": "SST-2_Binary_classification", "Method": "MT-DNN", "Metric": "Accuracy", "Task": "Sentiment_Analysis", "score": "95.6"}, {"Material": "SciTail", "Method": "MT-DNN", "Metric": "Accuracy", "Task": "Natural_Language_Inference", "score": "94.1"}], "ner": [[2, 8, "Method"], [9, 12, "Task"], [19, 25, "Method"], [26, 29, "Method"], [31, 33, "Task"], [35, 38, "Task"], [39, 40, "Task"], [43, 46, "Method"], [79, 82, "Method"], [91, 98, "Method"], [101, 102, "Method"], [104, 107, "Method"], [123, 124, "Material"], [125, 126, "Material"], [154, 155, "Material"], [156, 158, "Material"], [163, 166, "Method"], [167, 169, "Task"], [201, 207, "Method"], [218, 221, "Task"], [222, 223, "Task"], [230, 234, "Task"], [235, 240, "Method"], [257, 263, "Method"], [264, 267, "Method"], [269, 273, "Task"], [274, 276, "Task"], [279, 282, "Task"], [363, 364, "Task"], [365, 367, "Task"], [368, 371, "Method"], [372, 373, "Method"], [409, 410, "Task"], [428, 432, "Method"], [457, 458, "Task"], [459, 464, "Method"], [471, 474, "Task"], [497, 498, "Method"], [500, 501, "Method"], [503, 504, "Method"], [508, 512, "Method"], [523, 524, "Method"], [528, 533, "Method"], [541, 544, "Task"], [545, 549, "Task"], [553, 557, "Method"], [595, 596, "Method"], [622, 624, "Task"], [625, 628, "Task"], [650, 654, "Task"], [680, 681, "Method"], [683, 687, "Method"], [727, 731, "Task"], [732, 735, "Task"], [736, 738, "Task"], [740, 742, "Task"], [746, 748, "Method"], [749, 752, "Method"], [758, 761, "Method"], [762, 765, "Task"], [767, 768, "Method"], [769, 772, "Method"], [773, 774, "Task"], [776, 780, "Method"], [788, 790, "Method"], [791, 794, "Method"], [814, 818, "Task"], [819, 820, "Material"], [840, 841, "Method"], [848, 851, "Method"], [853, 854, "Material"], [855, 856, "Material"], [862, 865, "Method"], [866, 868, "Task"], [891, 892, "Metric"], [896, 897, "Material"], [901, 902, "Material"], [940, 943, "Method"], [965, 968, "Method"], [970, 972, "Method"], [973, 977, "Method"], [993, 997, "Task"], [998, 1001, "Task"], [1002, 1005, "Task"], [1007, 1009, "Task"], [1032, 1034, "Task"], [1057, 1059, "Task"], [1064, 1066, "Material"], [1095, 1097, "Metric"], [1101, 1103, "Task"], [1127, 1130, "Method"], [1138, 1139, "Material"], [1142, 1145, "Task"], [1175, 1176, "Method"], [1177, 1178, "Material"], [1179, 1182, "Task"], [1206, 1207, "Material"], [1208, 1209, "Material"], [1235, 1237, "Task"], [1263, 1264, "Material"], [1268, 1272, "Material"], [1291, 1292, "Material"], [1296, 1299, "Task"], [1300, 1301, "Material"], [1310, 1313, "Task"], [1348, 1349, "Metric"], [1350, 1352, "Task"], [1435, 1437, "Method"], [1461, 1464, "Method"], [1469, 1473, "Task"], [1488, 1490, "Method"], [1539, 1541, "Method"], [1570, 1572, "Method"], [1578, 1584, "Method"], [1612, 1614, "Method"], [1620, 1623, "Method"], [1631, 1634, "Method"], [1635, 1638, "Method"], [1649, 1653, "Task"], [1676, 1678, "Method"], [1709, 1711, "Method"], [1712, 1713, "Method"], [1725, 1727, "Metric"], [1731, 1735, "Task"], [1757, 1759, "Method"], [1776, 1778, "Metric"], [1783, 1785, "Method"], [1799, 1802, "Metric"], [1805, 1808, "Task"], [1809, 1810, "Task"], [1816, 1818, "Task"], [1845, 1847, "Method"], [1849, 1851, "Method"], [1853, 1856, "Method"], [1857, 1858, "Method"], [1869, 1872, "Method"], [1878, 1882, "Method"], [1935, 1937, "Method"], [1947, 1948, "Method"], [1956, 1959, "Method"], [2016, 2020, "Method"], [2055, 2056, "Task"], [2059, 2062, "Method"], [2066, 2068, "Method"], [2070, 2071, "Task"], [2078, 2079, "Metric"], [2082, 2084, "Metric"], [2087, 2088, "Material"], [2101, 2103, "Method"], [2106, 2108, "Method"], [2121, 2123, "Metric"], [2154, 2156, "Method"], [2157, 2160, "Method"], [2165, 2166, "Task"], [2167, 2173, "Task"], [2189, 2191, "Method"], [2192, 2194, "Method"], [2198, 2201, "Task"], [2202, 2205, "Task"], [2206, 2209, "Task"], [2212, 2219, "Task"], [2222, 2229, "Method"], [2230, 2231, "Method"], [2324, 2326, "Method"], [2330, 2332, "Method"], [2334, 2336, "Method"], [2374, 2375, "Method"], [2390, 2391, "Metric"], [2398, 2400, "Task"], [2405, 2407, "Task"], [2412, 2413, "Task"], [2420, 2422, "Method"], [2432, 2434, "Task"], [2437, 2444, "Task"], [2449, 2453, "Metric"], [2493, 2496, "Task"], [2499, 2502, "Task"], [2519, 2522, "Metric"], [2539, 2542, "Task"], [2544, 2552, "Method"], [2560, 2561, "Material"], [2645, 2648, "Method"], [2654, 2655, "Material"], [2656, 2660, "Material"], [2665, 2666, "Material"], [2669, 2672, "Method"], [2683, 2684, "Method"], [2689, 2690, "Task"], [2691, 2695, "Task"], [2696, 2697, "Material"], [2698, 2700, "Method"], [2701, 2702, "Material"], [2703, 2704, "Material"], [2713, 2714, "Material"], [2715, 2716, "Material"], [2718, 2720, "Material"], [2741, 2743, "Task"], [2744, 2746, "Task"], [2748, 2750, "Task"], [2760, 2763, "Metric"], [2768, 2769, "Material"], [2770, 2771, "Material"], [2772, 2774, "Task"], [2779, 2783, "Task"], [2798, 2801, "Metric"], [2803, 2805, "Metric"], [2808, 2811, "Material"], [2812, 2815, "Material"], [2837, 2838, "Metric"], [2842, 2844, "Metric"], [2847, 2850, "Material"], [2851, 2855, "Task"], [2877, 2879, "Material"], [2888, 2890, "Metric"], [2912, 2917, "Metric"], [2920, 2921, "Material"], [2926, 2930, "Material"], [2937, 2940, "Task"], [2941, 2942, "Material"], [2976, 2977, "Material"], [2979, 2982, "Task"], [3011, 3012, "Material"], [3013, 3017, "Material"], [3026, 3032, "Material"], [3057, 3058, "Metric"], [3059, 3061, "Metric"], [3064, 3066, "Metric"], [3069, 3070, "Material"], [3071, 3075, "Material"], [3105, 3106, "Material"], [3108, 3109, "Metric"], [3110, 3112, "Metric"], [3115, 3117, "Metric"], [3120, 3121, "Material"], [3121, 3127, "Task"], [3129, 3139, "Task"], [3189, 3190, "Metric"], [3196, 3197, "Metric"], [3201, 3203, "Metric"], [3204, 3205, "Metric"], [3210, 3214, "Material"], [3223, 3225, "Task"], [3231, 3232, "Material"], [3245, 3246, "Material"], [3247, 3249, "Material"], [3250, 3251, "Material"], [3254, 3257, "Task"], [3261, 3264, "Material"], [3268, 3271, "Task"], [3298, 3299, "Material"], [3300, 3304, "Material"], [3305, 3306, "Material"], [3326, 3328, "Material"], [3343, 3344, "Task"], [3351, 3353, "Task"], [3369, 3372, "Material"], [3373, 3374, "Material"], [3402, 3403, "Material"], [3449, 3450, "Material"], [3459, 3461, "Task"], [3472, 3475, "Method"], [3486, 3487, "Method"], [3489, 3490, "Method"], [3492, 3494, "Metric"], [3516, 3521, "Method"], [3546, 3548, "Metric"], [3554, 3557, "Task"], [3593, 3594, "Material"], [3602, 3605, "Method"], [3614, 3615, "Material"], [3650, 3653, "Method"], [3670, 3671, "Task"], [3672, 3675, "Task"], [3676, 3677, "Task"], [3708, 3709, "Method"], [3727, 3729, "Task"], [3730, 3731, "Method"], [3732, 3733, "Material"], [3737, 3739, "Method"], [3740, 3741, "Material"], [3742, 3743, "Material"], [3747, 3750, "Method"], [3756, 3758, "Method"], [3764, 3769, "Method"], [3770, 3772, "Method"], [3778, 3782, "Task"], [3801, 3806, "Method"], [3809, 3812, "Metric"], [3821, 3824, "Task"], [3825, 3826, "Material"], [3867, 3868, "Method"], [3871, 3873, "Method"], [3896, 3899, "Method"], [3929, 3930, "Task"], [3936, 3939, "Method"], [3964, 3967, "Method"], [3970, 3974, "Task"], [3978, 3980, "Method"], [3981, 3984, "Method"], [3998, 3999, "Task"], [4019, 4022, "Task"], [4026, 4030, "Task"], [4047, 4052, "Method"], [4064, 4065, "Material"], [4066, 4067, "Material"], [4068, 4069, "Material"], [4093, 4096, "Method"], [4097, 4098, "Method"], [4099, 4100, "Material"], [4102, 4105, "Method"], [4111, 4114, "Metric"], [4115, 4116, "Method"], [4117, 4118, "Material"], [4119, 4121, "Task"], [4128, 4131, "Metric"], [4133, 4136, "Method"], [4138, 4139, "Method"], [4144, 4146, "Task"], [4149, 4150, "Material"], [4151, 4152, "Material"], [4187, 4190, "Method"], [4211, 4212, "Material"], [4222, 4223, "Material"], [4234, 4236, "Task"], [4245, 4247, "Method"], [4248, 4250, "Task"], [4301, 4303, "Task"], [4306, 4308, "Task"], [4309, 4310, "Material"], [4311, 4312, "Material"], [4332, 4333, "Material"], [4340, 4341, "Material"], [4342, 4343, "Material"], [4345, 4349, "Method"], [4354, 4357, "Method"], [4378, 4382, "Method"], [4383, 4386, "Method"], [4395, 4397, "Method"], [4402, 4404, "Method"], [4413, 4414, "Material"], [4415, 4416, "Material"], [4429, 4430, "Material"], [4431, 4432, "Material"], [4464, 4465, "Material"], [4488, 4489, "Material"], [4510, 4511, "Material"], [4512, 4513, "Material"], [4535, 4536, "Material"], [4537, 4541, "Method"], [4557, 4560, "Method"], [4562, 4563, "Method"], [4581, 4584, "Method"], [4588, 4589, "Metric"], [4592, 4593, "Metric"], [4606, 4607, "Metric"], [4614, 4615, "Method"], [4624, 4625, "Material"], [4634, 4637, "Method"], [4641, 4643, "Task"], [4646, 4647, "Method"], [4659, 4662, "Method"], [4664, 4668, "Task"], [4669, 4674, "Task"], [4675, 4678, "Task"], [4679, 4682, "Method"], [4701, 4702, "Material"], [4703, 4704, "Material"], [4706, 4707, "Material"], [4708, 4711, "Method"], [4715, 4717, "Metric"], [4718, 4720, "Task"], [4731, 4734, "Method"], [4740, 4743, "Task"], [4744, 4745, "Task"], [4749, 4751, "Method"], [119, 120, "Task"], [132, 133, "Material"], [137, 138, "Material"], [181, 182, "Method"], [392, 393, "Method"], [559, 560, "Task"], [617, 618, "Task"], [632, 633, "Task"], [660, 661, "Task"], [670, 673, "Method"], [723, 724, "Task"], [809, 810, "Task"], [826, 827, "Material"], [880, 881, "Method"], [945, 946, "Material"], [947, 948, "Material"], [982, 985, "Method"], [990, 991, "Task"], [1018, 1019, "Task"], [1023, 1024, "Material"], [1071, 1074, "Material"], [1357, 1360, "Method"], [1365, 1368, "Method"], [1684, 1687, "Material"], [1873, 1874, "Method"], [1904, 1905, "Method"], [2181, 2182, "Method"], [2279, 2280, "Material"], [2425, 2428, "Method"], [2651, 2652, "Task"], [2661, 2662, "Material"], [2730, 2731, "Material"], [2737, 2738, "Task"], [2764, 2765, "Task"], [3359, 3360, "Material"], [3482, 3483, "Method"], [3587, 3588, "Material"], [3628, 3629, "Material"], [3646, 3647, "Method"], [3654, 3655, "Method"], [3796, 3797, "Method"], [3814, 3815, "Material"], [3847, 3848, "Method"], [3914, 3915, "Method"], [3932, 3933, "Material"], [3940, 3941, "Method"], [3955, 3956, "Material"], [4005, 4006, "Method"], [4038, 4039, "Method"], [4040, 4041, "Method"], [4072, 4073, "Method"], [4075, 4076, "Method"], [4083, 4084, "Method"], [4322, 4325, "Method"], [4328, 4329, "Material"], [4410, 4411, "Method"], [4421, 4422, "Method"], [4543, 4544, "Method"], [4577, 4578, "Material"], [4590, 4591, "Method"], [4694, 4695, "Task"]], "sections": [[0, 197], [197, 978], [978, 1028], [1028, 1093], [1093, 1140], [1140, 1233], [1233, 1353], [1353, 1486], [1486, 1568], [1568, 1647], [1647, 1723], [1723, 1797], [1797, 2080], [2080, 2148], [2148, 2638], [2638, 2705], [2705, 2775], [2775, 2806], [2806, 2845], [2845, 2918], [2918, 3009], [3009, 3067], [3067, 3118], [3118, 3206], [3206, 3243], [3243, 3296], [3296, 3357], [3357, 3465], [3465, 3585], [3585, 4147], [4147, 4232], [4232, 4648], [4648, 4775], [4775, 4795], [4795, 4798]], "sentences": [[0, 12], [12, 43], [43, 79], [79, 104], [104, 149], [149, 184], [184, 197], [197, 200], [200, 226], [226, 241], [241, 269], [269, 302], [302, 326], [326, 354], [354, 387], [387, 409], [409, 423], [423, 454], [454, 482], [482, 490], [490, 506], [506, 520], [520, 550], [550, 589], [589, 629], [629, 663], [663, 688], [688, 743], [743, 766], [766, 791], [791, 842], [842, 858], [858, 883], [883, 923], [923, 960], [960, 978], [978, 981], [981, 1010], [1010, 1028], [1028, 1035], [1035, 1053], [1053, 1070], [1070, 1093], [1093, 1098], [1098, 1104], [1104, 1127], [1127, 1140], [1140, 1146], [1146, 1171], [1171, 1206], [1206, 1218], [1218, 1233], [1233, 1238], [1238, 1263], [1263, 1274], [1274, 1290], [1290, 1336], [1336, 1353], [1353, 1361], [1361, 1377], [1377, 1396], [1396, 1433], [1433, 1458], [1458, 1474], [1474, 1486], [1486, 1493], [1493, 1503], [1503, 1517], [1517, 1538], [1538, 1568], [1568, 1575], [1575, 1601], [1601, 1610], [1610, 1647], [1647, 1655], [1655, 1682], [1682, 1692], [1692, 1723], [1723, 1729], [1729, 1739], [1739, 1765], [1765, 1797], [1797, 1804], [1804, 1815], [1815, 1841], [1841, 1873], [1873, 1883], [1883, 1903], [1903, 1911], [1911, 1952], [1952, 1973], [1973, 1988], [1988, 2002], [2002, 2015], [2015, 2054], [2054, 2069], [2069, 2080], [2080, 2086], [2086, 2092], [2092, 2118], [2118, 2148], [2148, 2153], [2153, 2174], [2174, 2185], [2185, 2210], [2210, 2262], [2262, 2299], [2299, 2311], [2311, 2315], [2315, 2320], [2320, 2338], [2338, 2346], [2346, 2348], [2348, 2354], [2354, 2364], [2364, 2367], [2367, 2374], [2374, 2386], [2386, 2389], [2389, 2394], [2394, 2401], [2401, 2408], [2408, 2415], [2415, 2420], [2420, 2423], [2423, 2430], [2430, 2491], [2491, 2535], [2535, 2559], [2559, 2565], [2565, 2591], [2591, 2628], [2628, 2638], [2638, 2641], [2641, 2667], [2667, 2705], [2705, 2708], [2708, 2729], [2729, 2767], [2767, 2775], [2775, 2778], [2778, 2796], [2796, 2806], [2806, 2811], [2811, 2823], [2823, 2837], [2837, 2845], [2845, 2850], [2850, 2880], [2880, 2903], [2903, 2918], [2918, 2921], [2921, 2943], [2943, 2968], [2968, 3009], [3009, 3012], [3012, 3033], [3033, 3045], [3045, 3067], [3067, 3070], [3070, 3103], [3103, 3118], [3118, 3121], [3121, 3140], [3140, 3177], [3177, 3200], [3200, 3206], [3206, 3209], [3209, 3226], [3226, 3243], [3243, 3246], [3246, 3265], [3265, 3272], [3272, 3296], [3296, 3299], [3299, 3334], [3334, 3345], [3345, 3357], [3357, 3360], [3360, 3377], [3377, 3390], [3390, 3426], [3426, 3453], [3453, 3465], [3465, 3469], [3469, 3484], [3484, 3505], [3505, 3515], [3515, 3534], [3534, 3551], [3551, 3566], [3566, 3585], [3585, 3589], [3589, 3602], [3602, 3649], [3649, 3676], [3676, 3691], [3691, 3744], [3744, 3791], [3791, 3827], [3827, 3857], [3857, 3867], [3867, 3885], [3885, 3896], [3896, 3921], [3921, 3935], [3935, 3964], [3964, 3985], [3985, 4023], [4023, 4053], [4053, 4087], [4087, 4101], [4101, 4132], [4132, 4147], [4147, 4153], [4153, 4184], [4184, 4232], [4232, 4237], [4237, 4256], [4256, 4274], [4274, 4290], [4290, 4374], [4374, 4387], [4387, 4423], [4423, 4452], [4452, 4478], [4478, 4502], [4502, 4526], [4526, 4547], [4547, 4564], [4564, 4619], [4619, 4626], [4626, 4648], [4648, 4651], [4651, 4679], [4679, 4708], [4708, 4722], [4722, 4775], [4775, 4778], [4778, 4795], [4795, 4798]], "words": ["document", ":", "Multi", "-", "Task", "Deep", "Neural", "Networks", "for", "Natural", "Language", "Understanding", "In", "this", "paper", ",", "we", "present", "a", "Multi", "-", "Task", "Deep", "Neural", "Network", "(", "MT", "-", "DNN", ")", "for", "learning", "representations", "across", "multiple", "natural", "language", "understanding", "(", "NLU", ")", "tasks", ".", "MT", "-", "DNN", "not", "only", "leverages", "large", "amounts", "of", "cross", "-", "task", "data", ",", "but", "also", "benefits", "from", "a", "regularization", "effect", "that", "leads", "to", "more", "general", "representations", "to", "help", "adapt", "to", "new", "tasks", "and", "domains", ".", "MT", "-", "DNN", "extends", "the", "model", "proposed", "in", "liu2015mtl", "by", "incorporating", "a", "pre", "-", "trained", "bidirectional", "transformer", "language", "model", ",", "known", "as", "BERT", "bert2018", ".", "MT", "-", "DNN", "obtains", "new", "state", "-", "of", "-", "the", "-", "art", "results", "on", "ten", "NLU", "tasks", ",", "including", "SNLI", ",", "SciTail", ",", "and", "eight", "out", "of", "nine", "GLUE", "tasks", ",", "pushing", "the", "GLUE", "benchmark", "to", "82.2", "%", "(", "1.8", "%", "absolute", "improvement", ")", ".", "We", "also", "demonstrate", "using", "the", "SNLI", "and", "SciTail", "datasets", "that", "the", "representations", "learned", "by", "MT", "-", "DNN", "allow", "domain", "adaptation", "with", "substantially", "fewer", "in", "-", "domain", "labels", "than", "the", "pre", "-", "trained", "BERT", "representations", ".", "Our", "code", "and", "pre", "-", "trained", "models", "will", "be", "made", "publicly", "available", ".", "section", ":", "Introduction", "Learning", "vector", "-", "space", "representations", "of", "text", ",", "e.g.", ",", "words", "and", "sentences", ",", "is", "fundamental", "to", "many", "natural", "language", "understanding", "(", "NLU", ")", "tasks", ".", "Two", "popular", "approaches", "are", "multi", "-", "task", "learning", "and", "language", "model", "pre", "-", "training", ".", "In", "this", "paper", "we", "strive", "to", "combine", "the", "strengths", "of", "both", "approaches", "by", "proposing", "a", "new", "Multi", "-", "Task", "Deep", "Neural", "Network", "(", "MT", "-", "DNN", ")", ".", "Multi", "-", "Task", "Learning", "(", "MTL", ")", "is", "inspired", "by", "human", "learning", "activities", "where", "people", "often", "apply", "the", "knowledge", "learned", "from", "previous", "tasks", "to", "help", "learn", "a", "new", "task", "caruana1997multitask", ",", "zhang2017survey", ".", "For", "example", ",", "it", "is", "easier", "for", "a", "person", "who", "knows", "how", "to", "ski", "to", "learn", "skating", "than", "the", "one", "who", "does", "not", ".", "Similarly", ",", "it", "is", "useful", "for", "multiple", "(", "related", ")", "tasks", "to", "be", "learned", "jointly", "so", "that", "the", "knowledge", "learned", "in", "one", "task", "can", "benefit", "other", "tasks", ".", "Recently", ",", "there", "is", "a", "growing", "interest", "in", "applying", "MTL", "to", "representation", "learning", "using", "deep", "neural", "networks", "(", "DNNs", ")", "collobert2011natural", ",", "liu2015mtl", ",", "luong2015multi", ",", "mt", "-", "mrc2018", "for", "two", "reasons", ".", "First", ",", "supervised", "learning", "of", "DNNs", "requires", "large", "amounts", "of", "task", "-", "specific", "labeled", "data", ",", "which", "is", "not", "always", "available", ".", "MTL", "provides", "an", "effective", "way", "of", "leveraging", "supervised", "data", "from", "many", "related", "tasks", ".", "Second", ",", "the", "use", "of", "multi", "-", "task", "learning", "profits", "from", "a", "regularization", "effect", "via", "alleviating", "overfitting", "to", "a", "specific", "task", ",", "thus", "making", "the", "learned", "representations", "universal", "across", "tasks", ".", "In", "contrast", "to", "MTL", ",", "language", "model", "pre", "-", "training", "has", "shown", "to", "be", "effective", "for", "learning", "universal", "language", "representations", "by", "leveraging", "large", "amounts", "of", "unlabeled", "data", ".", "A", "recent", "survey", "is", "included", "in", "gao2018neural", ".", "Some", "of", "the", "most", "prominent", "examples", "are", "ELMo", "elmo2018", ",", "GPT", "gpt2018", "and", "BERT", "bert2018", ".", "These", "are", "neural", "network", "language", "models", "trained", "on", "text", "data", "using", "unsupervised", "objectives", ".", "For", "example", ",", "BERT", "is", "based", "on", "a", "multi", "-", "layer", "bidirectional", "Transformer", ",", "and", "is", "trained", "on", "plain", "text", "for", "masked", "word", "prediction", "and", "next", "sentence", "prediction", "tasks", ".", "To", "apply", "a", "pre", "-", "trained", "model", "to", "specific", "NLU", "tasks", ",", "we", "often", "need", "to", "fine", "-", "tune", ",", "for", "each", "task", ",", "the", "model", "with", "additional", "task", "-", "specific", "layers", "using", "task", "-", "specific", "training", "data", ".", "For", "example", ",", "bert2018", "shows", "that", "BERT", "can", "be", "fine", "-", "tuned", "this", "way", "to", "create", "state", "-", "of", "-", "the", "-", "art", "models", "for", "a", "range", "of", "NLU", "tasks", ",", "such", "as", "question", "answering", "and", "natural", "language", "inference", ".", "We", "argue", "that", "MTL", "and", "language", "model", "pre", "-", "training", "are", "complementary", "technologies", ",", "and", "can", "be", "combined", "to", "improve", "the", "learning", "of", "text", "representations", "to", "boost", "the", "performance", "of", "various", "NLU", "tasks", ".", "To", "this", "end", ",", "we", "extend", "the", "MT", "-", "DNN", "model", "originally", "proposed", "in", "liu2015mtl", "by", "incorporating", "BERT", "as", "its", "shared", "text", "encoding", "layers", ".", "As", "shown", "in", "Figure", "1", ",", "the", "lower", "layers", "(", "i.e.", ",", "text", "encoding", "layers", ")", "are", "shared", "across", "all", "tasks", ",", "while", "the", "top", "layers", "are", "task", "-", "specific", ",", "combining", "different", "types", "of", "NLU", "tasks", "such", "as", "single", "-", "sentence", "classification", ",", "pairwise", "text", "classification", ",", "text", "similarity", ",", "and", "relevance", "ranking", ".", "Similar", "to", "the", "BERT", "model", ",", "MT", "-", "DNN", "is", "trained", "in", "two", "stages", ":", "pre", "-", "training", "and", "fine", "-", "tuning", ".", "Unlike", "BERT", ",", "MT", "-", "DNN", "uses", "MTL", "in", "the", "fine", "-", "tuning", "stage", "with", "multiple", "task", "-", "specific", "layers", "in", "its", "model", "architecture", ".", "MT", "-", "DNN", "obtains", "new", "state", "-", "of", "-", "the", "-", "art", "results", "on", "eight", "out", "of", "nine", "NLU", "tasks", "used", "in", "the", "General", "Language", "Understanding", "Evaluation", "(", "GLUE", ")", "benchmark", "wang2018glue", ",", "pushing", "the", "GLUE", "benchmark", "score", "to", "82.2", "%", ",", "amounting", "to", "1.8", "%", "absolute", "improvement", "over", "BERT", ".", "We", "further", "extend", "the", "superiority", "of", "MT", "-", "DNN", "to", "the", "SNLI", "and", "SciTail", "tasks", ".", "The", "representations", "learned", "by", "MT", "-", "DNN", "allow", "domain", "adaptation", "with", "substantially", "fewer", "in", "-", "domain", "labels", "than", "the", "pre", "-", "trained", "BERT", "representations", ".", "For", "example", ",", "our", "adapted", "models", "achieve", "the", "accuracy", "of", "91.1", "%", "on", "SNLI", "and", "94.1", "%", "on", "SciTail", ",", "outperforming", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "performance", "by", "1.0", "%", "and", "5.8", "%", ",", "respectively", ".", "Even", "with", "only", "0.1", "%", "or", "1.0", "%", "of", "the", "original", "training", "data", ",", "the", "performance", "of", "MT", "-", "DNN", "on", "both", "SNLI", "and", "SciTail", "datasets", "is", "fairly", "good", "and", "much", "better", "than", "many", "existing", "models", ".", "All", "of", "these", "clearly", "demonstrate", "MT", "-", "DNN", "\u2019s", "exceptional", "generalization", "capability", "via", "multi", "-", "task", "learning", ".", "section", ":", "Tasks", "The", "MT", "-", "DNN", "model", "combines", "four", "types", "of", "NLU", "tasks", ":", "single", "-", "sentence", "classification", ",", "pairwise", "text", "classification", ",", "text", "similarity", "scoring", ",", "and", "relevance", "ranking", ".", "For", "concreteness", ",", "we", "describe", "them", "using", "the", "NLU", "tasks", "defined", "in", "the", "GLUE", "benchmark", "as", "examples", ".", "paragraph", ":", "Single", "-", "Sentence", "Classification", ":", "Given", "a", "sentence", ",", "the", "model", "labels", "it", "using", "one", "of", "the", "pre", "-", "defined", "class", "labels", ".", "For", "example", ",", "the", "CoLA", "task", "is", "to", "predict", "whether", "an", "English", "sentence", "is", "grammatically", "plausible", ".", "The", "SST", "-", "2", "task", "is", "to", "determine", "whether", "the", "sentiment", "of", "a", "sentence", "extracted", "from", "movie", "reviews", "is", "positive", "or", "negative", ".", "paragraph", ":", "Text", "Similarity", ":", "This", "is", "a", "regression", "task", ".", "Given", "a", "pair", "of", "sentences", ",", "the", "model", "predicts", "a", "real", "-", "value", "score", "indicating", "the", "semantic", "similarity", "of", "the", "two", "sentences", ".", "STS", "-", "B", "is", "the", "only", "example", "of", "the", "task", "in", "GLUE", ".", "paragraph", ":", "Pairwise", "Text", "Classification", ":", "Given", "a", "pair", "of", "sentences", ",", "the", "model", "determines", "the", "relationship", "of", "the", "two", "sentences", "based", "on", "a", "set", "of", "pre", "-", "defined", "labels", ".", "For", "example", ",", "both", "RTE", "and", "MNLI", "are", "language", "inference", "tasks", ",", "where", "the", "goal", "is", "to", "predict", "whether", "a", "sentence", "is", "an", "entailment", ",", "contradiction", ",", "or", "neutral", "with", "respect", "to", "the", "other", ".", "QQP", "and", "MRPC", "are", "paragraph", "datasets", "that", "consist", "of", "sentence", "pairs", ".", "The", "task", "is", "to", "predict", "whether", "the", "sentences", "in", "the", "pair", "are", "semantically", "equivalent", ".", "paragraph", ":", "Relevance", "Ranking", ":", "Given", "a", "query", "and", "a", "list", "of", "candidate", "answers", ",", "the", "model", "ranks", "all", "the", "candidates", "in", "the", "order", "of", "relevance", "to", "the", "query", ".", "QNLI", "is", "a", "version", "of", "Stanford", "Question", "Answering", "Dataset", "rajpurkar2016squad", ".", "The", "task", "involves", "assessing", "whether", "a", "sentence", "contains", "the", "correct", "answer", "to", "a", "given", "query", ".", "Although", "QNLI", "is", "defined", "as", "a", "binary", "classification", "task", "in", "GLUE", ",", "in", "this", "study", "we", "formulate", "it", "as", "a", "pairwise", "ranking", "task", ",", "where", "the", "model", "is", "expected", "to", "rank", "the", "candidate", "that", "contains", "the", "correct", "answer", "higher", "than", "the", "candidate", "that", "does", "not", ".", "We", "will", "show", "that", "this", "formulation", "leads", "to", "a", "significant", "improvement", "in", "accuracy", "over", "binary", "classification", ".", "section", ":", "The", "Proposed", "MT", "-", "DNN", "Model", "The", "architecture", "of", "the", "MT", "-", "DNN", "model", "is", "shown", "in", "Figure", "[", "reference", "]", ".", "The", "lower", "layers", "are", "shared", "across", "all", "tasks", ",", "while", "the", "top", "layers", "represent", "task", "-", "specific", "outputs", ".", "The", "input", ",", "which", "is", "a", "word", "sequence", "(", "either", "a", "sentence", "or", "a", "pair", "of", "sentences", "packed", "together", ")", "is", "first", "represented", "as", "a", "sequence", "of", "embedding", "vectors", ",", "one", "for", "each", "word", ",", "in", ".", "Then", "the", "transformer", "encoder", "captures", "the", "contextual", "information", "for", "each", "word", "via", "self", "-", "attention", ",", "and", "generates", "a", "sequence", "of", "contextual", "embeddings", "in", ".", "This", "is", "the", "shared", "semantic", "representation", "that", "is", "trained", "by", "our", "multi", "-", "task", "objectives", ".", "In", "what", "follows", ",", "we", "elaborate", "on", "the", "model", "in", "detail", ".", "paragraph", ":", "Lexicon", "Encoder", "(", ")", ":", "The", "input", "is", "a", "sequence", "of", "tokens", "of", "length", ".", "Following", "bert2018", ",", "the", "first", "token", "is", "always", "the", "[", "CLS", "]", "token", ".", "If", "is", "packed", "by", "a", "sentence", "pair", ",", "we", "separate", "the", "two", "sentences", "with", "a", "special", "token", "[", "SEP", "]", ".", "The", "lexicon", "encoder", "maps", "into", "a", "sequence", "of", "input", "embedding", "vectors", ",", "one", "for", "each", "token", ",", "constructed", "by", "summing", "the", "corresponding", "word", ",", "segment", ",", "and", "positional", "embeddings", ".", "paragraph", ":", "Transformer", "Encoder", "(", ")", ":", "We", "use", "a", "multi", "-", "layer", "bidirectional", "Transformer", "encoder", "vaswani2017attention", "to", "map", "the", "input", "representation", "vectors", "(", ")", "into", "a", "sequence", "of", "contextual", "embedding", "vectors", ".", "This", "is", "the", "shared", "representation", "across", "different", "tasks", ".", "Unlike", "the", "BERT", "model", "bert2018", "that", "learns", "the", "representation", "via", "pre", "-", "training", "and", "adapts", "it", "to", "each", "individual", "task", "via", "fine", "-", "tuning", ",", "MT", "-", "DNN", "learns", "the", "representation", "using", "multi", "-", "task", "objectives", ".", "paragraph", ":", "Single", "-", "Sentence", "Classification", "Output", ":", "Suppose", "that", "is", "the", "contextual", "embedding", "(", ")", "of", "the", "token", "[", "CLS", "]", ",", "which", "can", "be", "viewed", "as", "the", "semantic", "representation", "of", "input", "sentence", ".", "Take", "the", "SST", "-", "2", "task", "as", "an", "example", ".", "The", "probability", "that", "is", "labeled", "as", "class", "(", "i.e.", ",", "the", "sentiment", ")", "is", "predicted", "by", "a", "logistic", "regression", "with", "softmax", ":", "where", "is", "the", "task", "-", "specific", "parameter", "matrix", ".", "paragraph", ":", "Text", "Similarity", "Output", ":", "Take", "the", "STS", "-", "B", "task", "as", "an", "example", ".", "Suppose", "that", "is", "the", "contextual", "embedding", "(", ")", "of", "[", "CLS", "]", "which", "can", "be", "viewed", "as", "the", "semantic", "representation", "of", "the", "input", "sentence", "pair", ".", "We", "introduce", "a", "task", "-", "specific", "parameter", "vector", "to", "compute", "the", "similarity", "score", "as", ":", "where", "is", "a", "sigmoid", "function", "that", "maps", "the", "score", "to", "a", "real", "value", "of", "the", "range", ".", "paragraph", ":", "Pairwise", "Text", "Classification", "Output", ":", "Take", "natural", "language", "inference", "(", "NLI", ")", "as", "an", "example", ".", "The", "NLI", "task", "defined", "here", "involves", "a", "premise", "of", "words", "and", "a", "hypothesis", "of", "words", ",", "and", "aims", "to", "find", "a", "logical", "relationship", "between", "and", ".", "The", "design", "of", "the", "output", "module", "follows", "the", "answer", "module", "of", "the", "stochastic", "answer", "network", "(", "SAN", ")", "liu2018san4nli", ",", "a", "state", "-", "of", "-", "the", "-", "art", "neural", "NLI", "model", ".", "SAN", "\u2019s", "answer", "module", "uses", "multi", "-", "step", "reasoning", ".", "Rather", "than", "directly", "predicting", "the", "entailment", "given", "the", "input", ",", "it", "maintains", "a", "state", "and", "iteratively", "refines", "its", "predictions", ".", "The", "SAN", "answer", "module", "works", "as", "follows", ".", "We", "first", "construct", "the", "working", "memory", "of", "premise", "by", "concatenating", "the", "contextual", "embeddings", "of", "the", "words", "in", ",", "which", "are", "the", "output", "of", "the", "transformer", "encoder", ",", "denoted", "as", ",", "and", "similarly", "the", "working", "memory", "of", "hypothesis", ",", "denoted", "as", ".", "Then", ",", "we", "perform", "-", "step", "reasoning", "on", "the", "memory", "to", "output", "the", "relation", "label", ",", "where", "is", "a", "hyperparameter", ".", "At", "the", "beginning", ",", "the", "initial", "state", "is", "the", "summary", "of", ":", ",", "where", ".", "At", "time", "step", "in", "the", "range", "of", ",", "the", "state", "is", "defined", "by", ".", "Here", ",", "is", "computed", "from", "the", "previous", "state", "and", "memory", ":", "and", ".", "A", "one", "-", "layer", "classifier", "is", "used", "to", "determine", "the", "relation", "at", "each", "step", ":", "At", "last", ",", "we", "utilize", "all", "of", "the", "outputs", "by", "averaging", "the", "scores", ":", "Each", "is", "a", "probability", "distribution", "over", "all", "the", "relations", ".", "During", "training", ",", "we", "apply", "stochastic", "prediction", "dropout", "liu2018san", "before", "the", "above", "averaging", "operation", ".", "During", "decoding", ",", "we", "average", "all", "outputs", "to", "improve", "robustness", ".", "paragraph", ":", "Relevance", "Ranking", "Output", ":", "Take", "QNLI", "as", "an", "example", ".", "Suppose", "that", "is", "the", "contextual", "embedding", "vector", "of", "[", "CLS", "]", "which", "is", "the", "semantic", "representation", "of", "a", "pair", "of", "question", "and", "its", "candidate", "answer", ".", "We", "compute", "the", "relevance", "score", "as", ":", "For", "a", "given", ",", "we", "rank", "all", "of", "its", "candidate", "answers", "based", "on", "their", "relevance", "scores", "computed", "using", "Equation", "[", "reference", "]", ".", "subsection", ":", "The", "Training", "Procedure", "The", "training", "procedure", "of", "MT", "-", "DNN", "consists", "of", "two", "stages", ":", "pretraining", "and", "multi", "-", "task", "fine", "-", "tuning", ".", "The", "pretraining", "stage", "follows", "that", "of", "the", "BERT", "model", "bert2018", ".", "The", "parameters", "of", "the", "lexicon", "encoder", "and", "Transformer", "encoder", "are", "learned", "using", "two", "unsupervised", "prediction", "tasks", ":", "masked", "language", "modeling", "and", "next", "sentence", "prediction", ".", "In", "the", "multi", "-", "task", "fine", "-", "tuning", "stage", ",", "we", "use", "mini", "-", "batch", "based", "stochastic", "gradient", "descent", "(", "SGD", ")", "to", "learn", "the", "parameters", "of", "our", "model", "(", "i.e.", ",", "the", "parameters", "of", "all", "shared", "layers", "and", "task", "-", "specific", "layers", ")", "as", "shown", "in", "Algorithm", "[", "reference", "]", ".", "In", "each", "epoch", ",", "a", "mini", "-", "batch", "is", "selected", "(", "e.g", ".", ",", "among", "all", "9", "GLUE", "tasks", ")", ",", "and", "the", "model", "is", "updated", "according", "to", "the", "task", "-", "specific", "objective", "for", "the", "task", ".", "This", "approximately", "optimizes", "the", "sum", "of", "all", "multi", "-", "task", "objectives", ".", "[", "ht", "!", "]", "Initialize", "model", "parameters", "randomly", ".", "Pre", "-", "train", "the", "shared", "layers", "(", "i.e.", ",", "the", "lexicon", "encoder", "and", "the", "transformer", "encoder", ")", ".", "Set", "the", "max", "number", "of", "epoch", ":", ".", "//", "Prepare", "the", "data", "for", "T", "tasks", ".", "in", "Pack", "the", "dataset", "into", "mini", "-", "batch", ":", ".", "in", "1", ".", "Merge", "all", "the", "datasets", ":", "2", ".", "Shuffle", "in", "D", "//", "bt", "is", "a", "mini", "-", "batch", "of", "task", "t.", "3", ".", "Compute", "loss", ":", "Eq", ".", "[", "reference", "]", "for", "classification", "Eq", ".", "[", "reference", "]", "for", "regression", "Eq", ".", "[", "reference", "]", "for", "ranking", "4", ".", "Compute", "gradient", ":", "5", ".", "Update", "model", ":", "Training", "a", "MT", "-", "DNN", "model", ".", "For", "the", "classification", "tasks", "(", "i.e.", ",", "single", "-", "sentence", "or", "pairwise", "text", "classification", ")", ",", "we", "use", "the", "cross", "-", "entropy", "loss", "as", "the", "objective", ":", "where", "is", "the", "binary", "indicator", "(", "0", "or", "1", ")", "if", "class", "label", "is", "the", "correct", "classification", "for", ",", "and", "is", "defined", "by", "e.g.", ",", "Equation", "[", "reference", "]", "or", "[", "reference", "]", ".", "For", "the", "text", "similarity", "tasks", ",", "such", "as", "STS", "-", "B", ",", "where", "each", "sentence", "pair", "is", "annotated", "with", "a", "real", "-", "valued", "score", ",", "we", "use", "the", "mean", "squared", "error", "as", "the", "objective", ":", "where", "is", "defined", "by", "Equation", "[", "reference", "]", ".", "The", "objective", "for", "the", "relevance", "ranking", "tasks", "follows", "the", "pairwise", "learning", "-", "to", "-", "rank", "paradigm", "learning", "-", "to", "-", "rank2005burges", ",", "huang2013dssm", ".", "Take", "QNLI", "as", "an", "example", ".", "Given", "a", "query", ",", "we", "obtain", "a", "list", "of", "candidate", "answers", "which", "contains", "a", "positive", "example", "that", "includes", "the", "correct", "answer", ",", "and", "negative", "examples", ".", "We", "then", "minimize", "the", "negative", "log", "likelihood", "of", "the", "positive", "example", "given", "queries", "across", "the", "training", "data", "where", "is", "defined", "by", "Equation", "[", "reference", "]", "and", "is", "a", "tuning", "factor", "determined", "on", "held", "-", "out", "data", ".", "In", "our", "experiment", ",", "we", "simply", "set", "to", "1", ".", "section", ":", "Experiments", "We", "evaluate", "the", "proposed", "MT", "-", "DNN", "on", "three", "popular", "NLU", "benchmarks", ":", "GLUE", ",", "Stanford", "Natural", "Language", "Inference", "(", "SNLI", ")", ",", "and", "SciTail", ".", "We", "compare", "MT", "-", "DNN", "with", "existing", "state", "-", "of", "-", "the", "-", "art", "models", "including", "BERT", "and", "demonstrate", "the", "effectiveness", "of", "MTL", "for", "model", "fine", "-", "tuning", "using", "GLUE", "and", "domain", "adaptation", "using", "SNLI", "and", "SciTail", ".", "subsection", ":", "Datasets", "This", "section", "briefly", "describes", "the", "GLUE", ",", "SNLI", ",", "and", "SciTail", "datasets", ",", "as", "summarized", "in", "Table", "[", "reference", "]", ".", "The", "GLUE", "benchmark", "is", "a", "collection", "of", "nine", "NLU", "tasks", ",", "including", "question", "answering", ",", "sentiment", "analysis", ",", "and", "textual", "entailment", ";", "it", "is", "considered", "well", "-", "designed", "for", "evaluating", "the", "generalization", "and", "robustness", "of", "NLU", "models", ".", "Both", "SNLI", "and", "SciTail", "are", "NLI", "tasks", ".", "paragraph", ":", "CoLA", "The", "Corpus", "of", "Linguistic", "Acceptability", "is", "to", "predict", "whether", "an", "English", "sentence", "is", "linguistically", "\u00e2\u0080\u009cacceptable\u00e2\u0080\u009d", "or", "not", ".", "It", "uses", "Matthews", "correlation", "coefficient", "as", "the", "evaluation", "metric", ".", "paragraph", ":", "SST", "-", "2", "The", "Stanford", "Sentiment", "Treebank", "is", "to", "determine", "the", "sentiment", "of", "sentences", ".", "The", "sentences", "are", "extracted", "from", "movie", "reviews", "with", "human", "annotations", "of", "their", "sentiment", ".", "Accuracy", "is", "used", "as", "the", "evaluation", "metric", ".", "paragraph", ":", "STS", "-", "B", "The", "Semantic", "Textual", "Similarity", "Benchmark", "is", "a", "collection", "of", "sentence", "pairs", "collected", "from", "multiple", "data", "resources", "including", "news", "headlines", ",", "video", ",", "and", "image", "captions", ",", "and", "NLI", "data", ".", "Each", "pair", "is", "human", "-", "annotated", "with", "a", "similarity", "score", "from", "one", "to", "five", ",", "indicating", "how", "similar", "the", "two", "sentences", "are", ".", "The", "task", "is", "evaluated", "using", "two", "metrics", ":", "the", "Pearson", "and", "Spearman", "correlation", "coefficients", ".", "paragraph", ":", "QNLI", "This", "is", "derived", "from", "the", "Stanford", "Question", "Answering", "Dataset", "rajpurkar2016squad", "which", "has", "been", "converted", "to", "a", "binary", "classification", "task", "in", "GLUE", ".", "A", "query", "-", "candidate", "-", "answer", "tuple", "is", "labeled", "as", "positive", "if", "the", "candidate", "contains", "the", "correct", "answer", "to", "the", "query", "and", "negative", "otherwise", ".", "In", "this", "study", ",", "however", ",", "we", "formulate", "QNLI", "as", "a", "relevance", "ranking", "task", ",", "where", "for", "a", "given", "query", ",", "its", "positive", "candidate", "answers", "are", "considered", "more", "relevant", ",", "and", "thus", "should", "be", "ranked", "higher", "than", "its", "negative", "candidates", ".", "paragraph", ":", "QQP", "The", "Quora", "Question", "Pairs", "dataset", "is", "a", "collection", "of", "question", "pairs", "extracted", "from", "the", "community", "question", "-", "answering", "website", "Quora", ".", "The", "task", "is", "to", "predict", "whether", "two", "questions", "are", "semantically", "equivalent", ".", "As", "the", "distribution", "of", "positive", "and", "negative", "labels", "is", "unbalanced", ",", "both", "accuracy", "and", "F1", "score", "are", "used", "as", "evaluation", "metrics", ".", "paragraph", ":", "MRPC", "The", "Microsoft", "Research", "Paraphrase", "Corpus", "consists", "of", "sentence", "pairs", "automatically", "extracted", "from", "online", "news", "sources", "with", "human", "annotations", "denoting", "whether", "a", "sentence", "pair", "is", "semantically", "equivalent", "to", "the", "other", "in", "the", "pair", ".", "Similar", "to", "QQP", ",", "both", "accuracy", "and", "F1", "score", "are", "used", "as", "evaluation", "metrics", ".", "paragraph", ":", "MNLI", "Multi", "-", "Genre", "Natural", "Language", "Inference", "is", "a", "large", "-", "scale", ",", "crowd", "-", "sourced", "entailment", "classification", "task", ".", "Given", "a", "pair", "of", "sentences", "(", "i.e.", ",", "a", "premise", "-", "hypothesis", "pair", ")", ",", "the", "goal", "is", "to", "predict", "whether", "the", "hypothesis", "is", "an", "entailment", ",", "contradiction", ",", "or", "neutral", "with", "respect", "to", "the", "premise", ".", "The", "test", "and", "development", "sets", "are", "split", "into", "in", "-", "domain", "(", "matched", ")", "and", "cross", "-", "domain", "(", "mismatched", ")", "sets", ".", "The", "evaluation", "metric", "is", "accuracy", ".", "paragraph", ":", "RTE", "The", "Recognizing", "Textual", "Entailment", "dataset", "is", "collected", "from", "a", "series", "of", "annual", "challenges", "on", "textual", "entailment", ".", "The", "task", "is", "similar", "to", "MNLI", ",", "but", "uses", "only", "two", "labels", ":", "entailment", "and", "not_entailment", ".", "paragraph", ":", "WNLI", "The", "Winograd", "NLI", "(", "WNLI", ")", "is", "a", "natural", "language", "inference", "dataset", "derived", "from", "the", "Winograd", "Schema", "dataset", ".", "This", "is", "a", "reading", "comprehension", "task", ".", "The", "goal", "is", "to", "select", "the", "referent", "of", "a", "pronoun", "from", "a", "list", "of", "choices", "in", "a", "given", "sentence", "which", "contains", "the", "pronoun", ".", "paragraph", ":", "SNLI", "The", "Stanford", "Natural", "Language", "Inference", "(", "SNLI", ")", "dataset", "contains", "570k", "human", "annotated", "sentence", "pairs", ",", "in", "which", "the", "premises", "are", "drawn", "from", "the", "captions", "of", "the", "Flickr30", "corpus", "and", "hypotheses", "are", "manually", "annotated", ".", "This", "is", "the", "most", "widely", "used", "entailment", "dataset", "for", "NLI", ".", "The", "dataset", "is", "used", "only", "for", "domain", "adaptation", "in", "this", "study", ".", "paragraph", ":", "SciTail", "This", "is", "a", "textual", "entailment", "dataset", "derived", "from", "a", "science", "question", "answering", "(", "SciQ", ")", "dataset", ".", "The", "task", "involves", "assessing", "whether", "a", "given", "premise", "entails", "a", "given", "hypothesis", ".", "In", "contrast", "to", "other", "entailment", "datasets", "mentioned", "previously", ",", "the", "hypotheses", "in", "SciTail", "are", "created", "from", "science", "questions", "while", "the", "corresponding", "answer", "candidates", "and", "premises", "come", "from", "relevant", "web", "sentences", "retrieved", "from", "a", "large", "corpus", ".", "As", "a", "result", ",", "these", "sentences", "are", "linguistically", "challenging", "and", "the", "lexical", "similarity", "of", "premise", "and", "hypothesis", "is", "often", "high", ",", "thus", "making", "SciTail", "particularly", "difficult", ".", "The", "dataset", "is", "used", "only", "for", "domain", "adaptation", "in", "this", "study", ".", "subsection", ":", "Implementation", "details", "Our", "implementation", "of", "MT", "-", "DNN", "is", "based", "on", "the", "PyTorch", "implementation", "of", "BERT", ".", "We", "used", "Adamax", "as", "our", "optimizer", "with", "a", "learning", "rate", "of", "5e", "-", "5", "and", "a", "batch", "size", "of", "32", ".", "The", "maximum", "number", "of", "epochs", "was", "set", "to", "5", ".", "A", "linear", "learning", "rate", "decay", "schedule", "with", "warm", "-", "up", "over", "0.1", "was", "used", ",", "unless", "stated", "otherwise", ".", "Following", ",", "we", "set", "the", "number", "of", "steps", "to", "5", "with", "a", "dropout", "rate", "of", "0.1", ".", "To", "avoid", "the", "exploding", "gradient", "problem", ",", "we", "clipped", "the", "gradient", "norm", "within", "1", ".", "All", "the", "texts", "were", "tokenized", "using", "wordpieces", ",", "and", "were", "chopped", "to", "spans", "no", "longer", "than", "512", "tokens", ".", "subsection", ":", "GLUE", "Results", "The", "test", "results", "on", "GLUE", "are", "presented", "in", "Table", "[", "reference", "]", ".", "MT", "-", "DNN", "outperforms", "all", "existing", "systems", "on", "all", "tasks", ",", "except", "WNLI", ",", "creating", "new", "state", "-", "of", "-", "the", "-", "art", "results", "on", "eight", "GLUE", "tasks", "and", "pushing", "the", "benchmark", "to", "82.2", "%", ",", "which", "amounts", "to", "1.8", "%", "absolution", "improvement", "over", "BERT", "LARGE", ".", "Since", "MT", "-", "DNN", "uses", "BERT", "LARGE", "for", "its", "shared", "layers", ",", "the", "gain", "is", "solely", "attributed", "to", "the", "use", "of", "MTL", "in", "fine", "-", "tuning", ".", "MTL", "is", "particularly", "useful", "for", "the", "tasks", "with", "little", "in", "-", "domain", "training", "data", ".", "As", "we", "observe", "in", "the", "table", ",", "on", "the", "same", "type", "of", "tasks", ",", "the", "improvements", "over", "BERT", "are", "much", "more", "substantial", "for", "the", "tasks", "with", "less", "in", "-", "domain", "training", "data", "e.g.", ",", "the", "two", "NLI", "tasks", ":", "RTE", "vs.", "MNLI", ",", "and", "the", "two", "paraphrase", "tasks", ":", "MRPC", "vs.", "QQP", ".", "The", "gain", "of", "MT", "-", "DNN", "is", "also", "attributed", "to", "its", "flexible", "modeling", "framework", "which", "allows", "us", "to", "incorporate", "the", "task", "-", "specific", "model", "structures", "and", "training", "methods", "which", "have", "been", "developed", "in", "the", "single", "-", "task", "setting", ",", "effectively", "leveraging", "the", "existing", "body", "of", "research", ".", "Two", "such", "examples", "use", "the", "SAN", "answer", "module", "for", "the", "pairwise", "text", "classification", "output", "module", ",", "and", "the", "pairwise", "ranking", "loss", "for", "the", "QNLI", "task", "which", "by", "design", "is", "a", "binary", "classification", "problem", "in", "GLUE", ".", "To", "investigate", "the", "relative", "contributions", "of", "the", "above", "two", "modeling", "design", "choices", ",", "we", "implement", "different", "versions", "of", "MT", "-", "DNNs", "and", "compare", "their", "performance", "on", "the", "development", "sets", ".", "The", "results", "are", "shown", "in", "Table", "[", "reference", "]", ".", "BERT", "is", "the", "base", "BERT", "model", "released", "by", "the", "authors", ",", "which", "we", "used", "as", "a", "baseline", ".", "We", "fine", "-", "tuned", "the", "model", "for", "each", "single", "task", ".", "MT", "-", "DNN", "is", "the", "proposed", "model", "described", "in", "Section", "[", "reference", "]", "using", "the", "pre", "-", "trained", "BERT", "BASE", "as", "its", "shared", "layers", ".", "We", "then", "fine", "-", "tuned", "the", "model", "using", "MTL", "on", "all", "GLUE", "tasks", ".", "Comparing", "MT", "-", "DNN", "vs.", "BERT", "BASE", ",", "we", "see", "that", "the", "results", "on", "dev", "sets", "are", "consistent", "with", "the", "GLUE", "test", "results", "in", "Table", "[", "reference", "]", ".", "ST", "-", "DNN", ",", "standing", "for", "Single", "-", "Task", "DNN", ",", "uses", "the", "same", "model", "architecture", "as", "MT", "-", "DNN", ".", "But", ",", "instead", "of", "fine", "-", "tuning", "one", "model", "for", "all", "tasks", "using", "MTL", ",", "we", "create", "multiple", "ST", "-", "DNNs", ",", "one", "for", "each", "task", "using", "only", "its", "in", "-", "domain", "data", "for", "fine", "-", "tuning", ".", "Thus", ",", "for", "pairwise", "text", "classification", "tasks", ",", "the", "only", "difference", "between", "their", "ST", "-", "DNNs", "and", "BERT", "models", "is", "the", "design", "of", "the", "task", "-", "specific", "output", "module", ".", "The", "results", "show", "that", "on", "three", "out", "of", "four", "tasks", "(", "MNLI", ",", "QQP", "and", "MRPC", ")", "ST", "-", "DNNs", "outperform", "their", "BERT", "counterparts", ",", "justifying", "the", "effectiveness", "of", "the", "SAN", "answer", "module", ".", "We", "also", "compare", "the", "results", "of", "ST", "-", "DNN", "and", "BERT", "on", "QNLI", ".", "While", "ST", "-", "DNN", "is", "fine", "-", "tuned", "using", "the", "pairwise", "ranking", "loss", ",", "BERT", "views", "QNLI", "as", "binary", "classification", "and", "is", "fine", "-", "tuned", "using", "the", "cross", "entropy", "loss", ".", "That", "ST", "-", "DNN", "significantly", "outperforms", "BERT", "demonstrates", "clearly", "the", "importance", "of", "problem", "formulation", ".", "subsection", ":", "SNLI", "and", "SciTail", "Results", "In", "Table", "4", ",", "we", "compare", "our", "adapted", "models", ",", "using", "all", "in", "-", "domain", "training", "samples", ",", "against", "several", "strong", "baselines", "including", "the", "best", "results", "reported", "in", "the", "leaderboards", ".", "We", "see", "that", "MT", "-", "DNN", "generates", "new", "state", "-", "of", "-", "the", "-", "art", "results", "on", "both", "datasets", ",", "pushing", "the", "benchmarks", "to", "91.1", "%", "on", "SNLI", "(", "1.0", "%", "absolute", "improvement", ")", "and", "94.1", "%", "on", "SciTail", "(", "5.8", "%", "absolute", "improvement", ")", ",", "respectively", ".", "subsection", ":", "Domain", "Adaptation", "Results", "One", "of", "the", "most", "important", "criteria", "for", "building", "practical", "systems", "is", "fast", "adaptation", "to", "new", "tasks", "and", "domains", ".", "This", "is", "because", "it", "is", "prohibitively", "expensive", "to", "collect", "labeled", "training", "data", "for", "new", "domains", "or", "tasks", ".", "Very", "often", ",", "we", "only", "have", "very", "small", "training", "data", "or", "even", "no", "training", "data", ".", "To", "evaluate", "the", "models", "using", "the", "above", "criterion", ",", "we", "perform", "domain", "adaptation", "experiments", "on", "two", "NLI", "tasks", ",", "SNLI", "and", "SciTail", ",", "using", "the", "following", "procedure", ":", "fine", "-", "tune", "the", "MT", "-", "DNN", "model", "on", "eight", "GLUE", "tasks", ",", "excluding", "WNLI", ";", "create", "for", "each", "new", "task", "(", "SNLI", "or", "SciTail", ")", "a", "task", "-", "specific", "model", ",", "by", "adapting", "the", "trained", "MT", "-", "DNN", "using", "task", "-", "specific", "training", "data", ";", "evaluate", "the", "models", "using", "task", "-", "specific", "test", "data", ".", "We", "denote", "the", "two", "task", "-", "specific", "models", "as", "MT", "-", "DNN", ".", "For", "comparison", ",", "we", "also", "perform", "the", "same", "adaptation", "procedure", "to", "the", "pre", "-", "trained", "BERT", "model", ",", "creating", "two", "task", "-", "specific", "BERT", "models", "for", "SNLI", "and", "SciTail", ",", "respectively", ",", "denoted", "as", "BERT", ".", "We", "split", "the", "training", "data", "of", "SNLI", "and", "SciTail", ",", "and", "randomly", "sample", "0.1", "%", ",", "1", "%", ",", "10", "%", "and", "100", "%", "of", "its", "training", "data", ".", "As", "a", "result", ",", "we", "obtain", "four", "sets", "of", "training", "data", "for", "SciTail", ",", "which", "includes", "23", ",", "235", ",", "2.3k", "and", "23.5k", "training", "samples", ".", "Similarly", ",", "we", "obtain", "four", "sets", "of", "training", "data", "for", "SNLI", ",", "which", "includes", "549", ",", "5.5k", ",", "54.9k", "and", "549.3k", "training", "samples", ".", "Results", "on", "different", "amounts", "of", "training", "data", "of", "SNLI", "and", "SciTail", "are", "reported", "in", "Figure", "[", "reference", "]", "and", "Table", "[", "reference", "]", ".", "We", "observe", "that", "our", "model", "pre", "-", "trained", "on", "GLUE", "via", "multi", "-", "task", "learning", "outplays", "the", "BERT", "baseline", "consistently", ".", "The", "fewer", "the", "training", "data", "used", ",", "the", "larger", "improvement", "MT", "-", "DNN", "demonstrates", "over", "BERT", ".", "For", "example", ",", "with", "only", "0.1", "%", "(", "23", "samples", ")", "of", "the", "SNLI", "training", "data", ",", "MT", "-", "DNN", "achieves", "82.1", "%", "in", "accuracy", "while", "BERT", "\u2019s", "accuracy", "is", "52.5", "%", ";", "with", "1", "%", "of", "the", "training", "data", ",", "the", "accuracy", "of", "our", "model", "is", "85.2", "%", "and", "BERT", "is", "78.1", "%", ".", "We", "observe", "similar", "results", "on", "SciTail", ".", "The", "results", "indicate", "that", "the", "representations", "learned", "by", "MT", "-", "DNN", "are", "more", "effective", "for", "domain", "adaptation", "than", "that", "of", "BERT", ".", "section", ":", "Conclusion", "In", "this", "work", "we", "proposed", "a", "model", "called", "MT", "-", "DNN", "to", "combine", "multi", "-", "task", "learning", "and", "language", "model", "pre", "-", "training", "for", "language", "representation", "learning", ".", "MT", "-", "DNN", "obtains", "new", "state", "-", "of", "-", "the", "-", "art", "results", "on", "ten", "NLU", "tasks", "across", "three", "popular", "benchmarks", ":", "SNLI", ",", "SciTail", ",", "and", "GLUE", ".", "MT", "-", "DNN", "also", "demonstrates", "an", "exceptional", "generalization", "capability", "in", "domain", "adaptation", "experiments", ".", "There", "are", "many", "future", "areas", "to", "explore", "to", "improve", "MT", "-", "DNN", ",", "including", "a", "deeper", "understanding", "of", "model", "structure", "sharing", "in", "MTL", ",", "a", "more", "effective", "training", "method", "that", "leverages", "relatedness", "among", "multiple", "tasks", ",", "and", "ways", "of", "incorporating", "the", "linguistic", "structure", "of", "text", "in", "a", "more", "explicit", "and", "controllable", "manner", ".", "section", ":", "Acknowledgements", "We", "would", "like", "to", "thanks", "Jade", "Huang", "from", "Microsoft", "for", "her", "generous", "help", "on", "this", "work", ".", "bibliography", ":", "References"]}