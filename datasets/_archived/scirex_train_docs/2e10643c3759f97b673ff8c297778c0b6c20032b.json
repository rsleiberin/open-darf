{"coref": {"AG_News": [[2518, 2522], [2526, 2532], [2542, 2544], [2589, 2592], [3181, 3184]], "Char-level_CNN": [[2, 7], [20, 25], [26, 27], [42, 47], [191, 192], [281, 282], [292, 293], [348, 349], [421, 422], [476, 477], [511, 512], [585, 590], [1131, 1132], [2414, 2415], [3295, 3300], [3310, 3311], [3323, 3327], [3477, 3478], [3483, 3484], [3568, 3569], [3583, 3584], [3605, 3606], [3904, 3909], [3939, 3943], [89, 90], [263, 264], [493, 494], [602, 603], [810, 811], [2078, 2079], [2091, 2092], [2173, 2174], [3344, 3345], [3397, 3398], [3410, 3411], [3991, 3992]], "DBpedia": [[2763, 2766], [2767, 2768], [2780, 2781], [3399, 3401], [2783, 2784], [2796, 2797]], "Error": [[3265, 3267], [3305, 3308]], "Sentiment_Analysis": [[3710, 3712]], "Text_Classification": [[8, 10], [29, 31], [98, 100], [126, 128], [151, 153], [274, 276], [330, 332], [604, 606], [2036, 2038], [2093, 2095], [2454, 2456], [2870, 2872], [3348, 3350], [3719, 3721], [3815, 3817], [3910, 3912]], "Yelp_Binary_classification": [[3713, 3714], [2842, 2843], [2849, 2850], [3078, 3079]], "Yelp_Fine-grained_classification": [[3713, 3714], [2842, 2843], [2849, 2850], [3078, 3079]]}, "coref_non_salient": {"0": [[72, 75], [404, 410], [3440, 3444]], "1": [[2838, 2840], [3036, 3039], [3046, 3047], [3715, 3717]], "10": [[452, 454], [3805, 3808]], "11": [[427, 429], [2053, 2057]], "12": [[1943, 1944], [2123, 2126], [2242, 2245], [3754, 3755]], "13": [[354, 360], [1779, 1785], [2237, 2240]], "14": [[772, 776], [804, 806]], "15": [[68, 71], [1756, 1762], [1860, 1865], [1870, 1876], [1915, 1921], [2007, 2013]], "16": [[105, 108], [333, 336], [398, 400], [3828, 3831], [3997, 4000]], "17": [[204, 207], [682, 683], [778, 780]], "18": [[77, 79], [1749, 1750], [1799, 1811], [1867, 1868]], "19": [[1366, 1368], [1453, 1455], [1489, 1491], [1505, 1507], [2195, 2197]], "2": [[963, 968], [971, 976]], "20": [[2299, 2303]], "21": [[2193, 2194], [3190, 3192], [3319, 3321]], "22": [[3402, 3404]], "23": [[1376, 1379], [1647, 1650]], "24": [[188, 190], [231, 233], [327, 329], [679, 681], [849, 851], [1147, 1149], [1235, 1237], [1326, 1328]], "25": [[3089, 3092]], "26": [[3857, 3860]], "27": [[1151, 1155], [1213, 1217], [1247, 1254], [1358, 1362]], "28": [[1273, 1275], [1277, 1279], [3316, 3318]], "29": [[3589, 3593]], "3": [[140, 143], [411, 413], [1726, 1728], [1730, 1731]], "30": [[208, 210], [1427, 1431]], "31": [[2727, 2731]], "32": [[2989, 2992]], "33": [[3544, 3549]], "34": [[91, 94], [2214, 2218]], "35": [[1532, 1533]], "36": [[1922, 1927], [2146, 2152], [3745, 3750], [3780, 3786]], "37": [[2600, 2601], [2602, 2605]], "38": [[3694, 3697]], "39": [[868, 869]], "4": [[81, 84], [322, 325], [1386, 1389], [1673, 1678], [2023, 2026], [2027, 2030], [3921, 3925]], "40": [[1422, 1424]], "41": [[226, 229]], "42": [[623, 624]], "43": [[2307, 2309]], "44": [[1207, 1209]], "45": [[2723, 2725]], "46": [[1176, 1179]], "47": [[2733, 2734]], "48": [[2970, 2972]], "49": [[3094, 3096]], "5": [[2062, 2067], [2220, 2225], [3291, 3293]], "50": [[3041, 3045]], "51": [[1702, 1704]], "52": [[857, 858]], "53": [[1383, 1385]], "54": [[530, 531]], "55": [[2128, 2130]], "56": [[2068, 2069], [2226, 2227], [2293, 2294], [2230, 2231], [2265, 2266]], "57": [[1939, 1942]], "58": [[1734, 1737], [2276, 2279]], "59": [[618, 621]], "6": [[196, 198], [464, 466]], "60": [[2132, 2134]], "61": [[755, 757]], "62": [[2679, 2680]], "63": [[924, 926]], "64": [[2201, 2206]], "65": [[642, 646]], "66": [[853, 856]], "67": [[2107, 2115]], "68": [[1518, 1520]], "69": [[864, 867]], "7": [[758, 762], [788, 792]], "70": [[457, 463]], "71": [[841, 845]], "72": [[929, 931]], "73": [[1222, 1224]], "74": [[216, 220]], "75": [[1719, 1724]], "76": [[1928, 1930]], "77": [[634, 637]], "8": [[656, 659], [693, 695]], "9": [[1261, 1263], [1590, 1592], [1612, 1614]]}, "doc_id": "2e10643c3759f97b673ff8c297778c0b6c20032b", "method_subrelations": {"Char-level_CNN": [[[0, 14], "Char-level_CNN"]]}, "n_ary_relations": [{"Material": "AG_News", "Method": "Char-level_CNN", "Metric": "Error", "Task": "Text_Classification", "score": "9.51"}, {"Material": "DBpedia", "Method": "Char-level_CNN", "Metric": "Error", "Task": "Text_Classification", "score": "1.55"}, {"Material": "Yelp_Binary_classification", "Method": "Char-level_CNN", "Metric": "Error", "Task": "Sentiment_Analysis", "score": "4.88"}, {"Material": "Yelp_Fine-grained_classification", "Method": "Char-level_CNN", "Metric": "Error", "Task": "Sentiment_Analysis", "score": "37.95"}], "ner": [[2, 7, "Method"], [8, 10, "Task"], [20, 25, "Method"], [26, 27, "Method"], [29, 31, "Task"], [42, 47, "Method"], [68, 71, "Method"], [72, 75, "Method"], [77, 79, "Method"], [81, 84, "Method"], [91, 94, "Method"], [98, 100, "Task"], [105, 108, "Task"], [126, 128, "Task"], [140, 143, "Method"], [151, 153, "Task"], [188, 190, "Method"], [191, 192, "Method"], [196, 198, "Task"], [204, 207, "Task"], [208, 210, "Task"], [216, 220, "Method"], [226, 229, "Task"], [231, 233, "Method"], [274, 276, "Task"], [281, 282, "Method"], [292, 293, "Method"], [322, 325, "Method"], [327, 329, "Method"], [330, 332, "Task"], [333, 336, "Task"], [348, 349, "Method"], [354, 360, "Task"], [398, 400, "Task"], [404, 410, "Method"], [411, 413, "Method"], [421, 422, "Method"], [427, 429, "Method"], [452, 454, "Method"], [457, 463, "Task"], [464, 466, "Task"], [476, 477, "Method"], [511, 512, "Method"], [530, 531, "Task"], [585, 590, "Method"], [604, 606, "Task"], [618, 621, "Method"], [623, 624, "Task"], [634, 637, "Method"], [642, 646, "Method"], [656, 659, "Method"], [679, 681, "Method"], [682, 683, "Task"], [693, 695, "Method"], [755, 757, "Method"], [758, 762, "Method"], [772, 776, "Method"], [778, 780, "Task"], [788, 792, "Method"], [804, 806, "Method"], [841, 845, "Method"], [849, 851, "Method"], [853, 856, "Method"], [857, 858, "Method"], [864, 867, "Method"], [868, 869, "Method"], [924, 926, "Method"], [929, 931, "Task"], [963, 968, "Method"], [971, 976, "Method"], [1131, 1132, "Method"], [1147, 1149, "Method"], [1151, 1155, "Method"], [1176, 1179, "Method"], [1207, 1209, "Method"], [1213, 1217, "Method"], [1222, 1224, "Metric"], [1235, 1237, "Method"], [1247, 1254, "Method"], [1261, 1263, "Method"], [1273, 1275, "Method"], [1277, 1279, "Method"], [1326, 1328, "Method"], [1358, 1362, "Method"], [1366, 1368, "Task"], [1376, 1379, "Method"], [1383, 1385, "Task"], [1386, 1389, "Method"], [1422, 1424, "Method"], [1427, 1431, "Task"], [1453, 1455, "Task"], [1489, 1491, "Task"], [1505, 1507, "Task"], [1518, 1520, "Method"], [1532, 1533, "Material"], [1590, 1592, "Method"], [1612, 1614, "Method"], [1647, 1650, "Method"], [1673, 1678, "Method"], [1702, 1704, "Method"], [1719, 1724, "Method"], [1726, 1728, "Method"], [1730, 1731, "Method"], [1734, 1737, "Method"], [1749, 1750, "Method"], [1756, 1762, "Method"], [1779, 1785, "Task"], [1799, 1811, "Method"], [1860, 1865, "Method"], [1867, 1868, "Method"], [1870, 1876, "Method"], [1915, 1921, "Method"], [1922, 1927, "Method"], [1928, 1930, "Method"], [1939, 1942, "Method"], [1943, 1944, "Method"], [2007, 2013, "Method"], [2023, 2026, "Method"], [2027, 2030, "Method"], [2036, 2038, "Task"], [2053, 2057, "Method"], [2062, 2067, "Method"], [2068, 2069, "Method"], [2093, 2095, "Task"], [2107, 2115, "Method"], [2123, 2126, "Method"], [2128, 2130, "Method"], [2132, 2134, "Metric"], [2146, 2152, "Method"], [2193, 2194, "Method"], [2195, 2197, "Task"], [2201, 2206, "Task"], [2214, 2218, "Method"], [2220, 2225, "Method"], [2226, 2227, "Method"], [2237, 2240, "Task"], [2242, 2245, "Method"], [2276, 2279, "Method"], [2293, 2294, "Method"], [2299, 2303, "Method"], [2307, 2309, "Method"], [2414, 2415, "Method"], [2454, 2456, "Task"], [2518, 2522, "Material"], [2526, 2532, "Material"], [2542, 2544, "Material"], [2589, 2592, "Material"], [2600, 2601, "Material"], [2602, 2605, "Material"], [2679, 2680, "Task"], [2723, 2725, "Method"], [2727, 2731, "Method"], [2733, 2734, "Method"], [2763, 2766, "Material"], [2767, 2768, "Material"], [2780, 2781, "Material"], [2838, 2840, "Material"], [2870, 2872, "Task"], [2970, 2972, "Method"], [2989, 2992, "Method"], [3036, 3039, "Material"], [3041, 3045, "Material"], [3046, 3047, "Material"], [3089, 3092, "Method"], [3094, 3096, "Task"], [3181, 3184, "Material"], [3190, 3192, "Method"], [3265, 3267, "Metric"], [3291, 3293, "Method"], [3295, 3300, "Method"], [3305, 3308, "Metric"], [3310, 3311, "Method"], [3316, 3318, "Method"], [3319, 3321, "Method"], [3323, 3327, "Method"], [3348, 3350, "Task"], [3399, 3401, "Material"], [3402, 3404, "Metric"], [3440, 3444, "Method"], [3477, 3478, "Method"], [3483, 3484, "Method"], [3544, 3549, "Method"], [3568, 3569, "Method"], [3583, 3584, "Method"], [3589, 3593, "Task"], [3605, 3606, "Method"], [3694, 3697, "Task"], [3710, 3712, "Task"], [3713, 3714, "Material"], [3715, 3717, "Material"], [3719, 3721, "Task"], [3745, 3750, "Method"], [3754, 3755, "Method"], [3780, 3786, "Method"], [3805, 3808, "Method"], [3815, 3817, "Task"], [3828, 3831, "Task"], [3857, 3860, "Method"], [3904, 3909, "Method"], [3910, 3912, "Task"], [3921, 3925, "Method"], [3939, 3943, "Method"], [3997, 4000, "Task"], [89, 90, "Method"], [263, 264, "Method"], [493, 494, "Method"], [602, 603, "Method"], [810, 811, "Method"], [2078, 2079, "Method"], [2091, 2092, "Method"], [2173, 2174, "Method"], [2230, 2231, "Method"], [2265, 2266, "Method"], [2783, 2784, "Material"], [2796, 2797, "Material"], [2842, 2843, "Material"], [2849, 2850, "Material"], [3078, 3079, "Material"], [3344, 3345, "Method"], [3397, 3398, "Method"], [3410, 3411, "Method"], [3991, 3992, "Method"]], "sections": [[0, 95], [95, 583], [583, 625], [625, 927], [927, 1124], [1124, 1364], [1364, 1653], [1653, 1705], [1705, 2021], [2021, 2327], [2327, 2403], [2403, 3206], [3206, 3892], [3892, 4007], [4007, 4051], [4051, 4054]], "sentences": [[0, 10], [10, 32], [32, 60], [60, 95], [95, 98], [98, 123], [123, 144], [144, 179], [179, 213], [213, 238], [238, 267], [267, 288], [288, 310], [310, 326], [326, 343], [343, 374], [374, 386], [386, 401], [401, 423], [423, 455], [455, 469], [469, 481], [481, 527], [527, 561], [561, 583], [583, 590], [590, 607], [607, 625], [625, 629], [629, 647], [647, 660], [660, 675], [675, 708], [708, 733], [733, 747], [747, 763], [763, 781], [781, 802], [802, 821], [821, 831], [831, 860], [860, 893], [893, 908], [908, 919], [919, 927], [927, 931], [931, 942], [942, 978], [978, 997], [997, 1024], [1024, 1062], [1062, 1092], [1092, 1124], [1124, 1128], [1128, 1139], [1139, 1156], [1156, 1164], [1164, 1188], [1188, 1203], [1203, 1220], [1220, 1227], [1227, 1255], [1255, 1264], [1264, 1280], [1280, 1304], [1304, 1315], [1315, 1330], [1330, 1341], [1341, 1364], [1364, 1370], [1370, 1390], [1390, 1408], [1408, 1446], [1446, 1480], [1480, 1503], [1503, 1525], [1525, 1555], [1555, 1582], [1582, 1597], [1597, 1617], [1617, 1639], [1639, 1653], [1653, 1657], [1657, 1679], [1679, 1705], [1705, 1709], [1709, 1729], [1729, 1742], [1742, 1751], [1751, 1777], [1777, 1797], [1797, 1822], [1822, 1849], [1849, 1860], [1860, 1869], [1869, 1904], [1904, 1922], [1922, 1931], [1931, 1966], [1966, 1984], [1984, 1992], [1992, 2014], [2014, 2021], [2021, 2026], [2026, 2039], [2039, 2075], [2075, 2080], [2080, 2116], [2116, 2131], [2131, 2153], [2153, 2190], [2190, 2201], [2201, 2207], [2207, 2229], [2229, 2253], [2253, 2284], [2284, 2290], [2290, 2304], [2304, 2319], [2319, 2327], [2327, 2332], [2332, 2355], [2355, 2380], [2380, 2403], [2403, 2411], [2411, 2448], [2448, 2476], [2476, 2510], [2510, 2518], [2518, 2523], [2523, 2536], [2536, 2551], [2551, 2573], [2573, 2589], [2589, 2593], [2593, 2621], [2621, 2640], [2640, 2654], [2654, 2669], [2669, 2673], [2673, 2678], [2678, 2682], [2682, 2694], [2694, 2713], [2713, 2741], [2741, 2755], [2755, 2763], [2763, 2767], [2767, 2782], [2782, 2799], [2799, 2822], [2822, 2838], [2838, 2841], [2841, 2855], [2855, 2869], [2869, 2910], [2910, 2949], [2949, 2951], [2951, 2954], [2954, 2958], [2958, 2970], [2970, 2973], [2973, 2986], [2986, 3001], [3001, 3016], [3016, 3030], [3030, 3033], [3033, 3075], [3075, 3097], [3097, 3139], [3139, 3149], [3149, 3169], [3169, 3193], [3193, 3206], [3206, 3209], [3209, 3257], [3257, 3278], [3278, 3309], [3309, 3323], [3323, 3332], [3332, 3356], [3356, 3378], [3378, 3402], [3402, 3413], [3413, 3437], [3437, 3483], [3483, 3493], [3493, 3509], [3509, 3540], [3540, 3542], [3542, 3579], [3579, 3594], [3594, 3631], [3631, 3638], [3638, 3659], [3659, 3676], [3676, 3694], [3694, 3701], [3701, 3726], [3726, 3745], [3745, 3758], [3758, 3792], [3792, 3818], [3818, 3840], [3840, 3846], [3846, 3869], [3869, 3892], [3892, 3897], [3897, 3913], [3913, 3932], [3932, 3948], [3948, 3980], [3980, 4007], [4007, 4010], [4010, 4031], [4031, 4051], [4051, 4054]], "words": ["document", ":", "Character", "-", "level", "Convolutional", "Networks", "for", "Text", "Classification", "This", "article", "offers", "an", "empirical", "exploration", "on", "the", "use", "of", "character", "-", "level", "convolutional", "networks", "(", "ConvNets", ")", "for", "text", "classification", ".", "We", "constructed", "several", "large", "-", "scale", "datasets", "to", "show", "that", "character", "-", "level", "convolutional", "networks", "could", "achieve", "state", "-", "of", "-", "the", "-", "art", "or", "competitive", "results", ".", "Comparisons", "are", "offered", "against", "traditional", "models", "such", "as", "bag", "of", "words", ",", "n", "-", "grams", "and", "their", "TFIDF", "variants", ",", "and", "deep", "learning", "models", "such", "as", "word", "-", "based", "ConvNets", "and", "recurrent", "neural", "networks", ".", "section", ":", "Introduction", "Text", "classification", "is", "a", "classic", "topic", "for", "natural", "language", "processing", ",", "in", "which", "one", "needs", "to", "assign", "predefined", "categories", "to", "free", "-", "text", "documents", ".", "The", "range", "of", "text", "classification", "research", "goes", "from", "designing", "the", "best", "features", "to", "choosing", "the", "best", "possible", "machine", "learning", "classifiers", ".", "To", "date", ",", "almost", "all", "techniques", "of", "text", "classification", "are", "based", "on", "words", ",", "in", "which", "simple", "statistics", "of", "some", "ordered", "word", "combinations", "(", "such", "as", "n", "-", "grams", ")", "usually", "perform", "the", "best", ".", "On", "the", "other", "hand", ",", "many", "researchers", "have", "found", "convolutional", "networks", "(", "ConvNets", ")", "are", "useful", "in", "extracting", "information", "from", "raw", "signals", ",", "ranging", "from", "computer", "vision", "applications", "to", "speech", "recognition", "and", "others", ".", "In", "particular", ",", "time", "-", "delay", "networks", "used", "in", "the", "early", "days", "of", "deep", "learning", "research", "are", "essentially", "convolutional", "networks", "that", "model", "sequential", "data", ".", "In", "this", "article", "we", "explore", "treating", "text", "as", "a", "kind", "of", "raw", "signal", "at", "character", "level", ",", "and", "applying", "temporal", "(", "one", "-", "dimensional", ")", "ConvNets", "to", "it", ".", "For", "this", "article", "we", "only", "used", "a", "classification", "task", "as", "a", "way", "to", "exemplify", "ConvNets", "\u2019", "ability", "to", "understand", "texts", ".", "Historically", "we", "know", "that", "ConvNets", "usually", "require", "large", "-", "scale", "datasets", "to", "work", ",", "therefore", "we", "also", "build", "several", "of", "them", ".", "An", "extensive", "set", "of", "comparisons", "is", "offered", "with", "traditional", "models", "and", "other", "deep", "learning", "models", ".", "Applying", "convolutional", "networks", "to", "text", "classification", "or", "natural", "language", "processing", "at", "large", "was", "explored", "in", "literature", ".", "It", "has", "been", "shown", "that", "ConvNets", "can", "be", "directly", "applied", "to", "distributed", "or", "discrete", "embedding", "of", "words", ",", "without", "any", "knowledge", "on", "the", "syntactic", "or", "semantic", "structures", "of", "a", "language", ".", "These", "approaches", "have", "been", "proven", "to", "be", "competitive", "to", "traditional", "models", ".", "There", "are", "also", "related", "works", "that", "use", "character", "-", "level", "features", "for", "language", "processing", ".", "These", "include", "using", "character", "-", "level", "n", "-", "grams", "with", "linear", "classifiers", ",", "and", "incorporating", "character", "-", "level", "features", "to", "ConvNets", ".", "In", "particular", ",", "these", "ConvNet", "approaches", "use", "words", "as", "a", "basis", ",", "in", "which", "character", "-", "level", "features", "extracted", "at", "word", "or", "word", "n", "-", "gram", "level", "form", "a", "distributed", "representation", ".", "Improvements", "for", "part", "-", "of", "-", "speech", "tagging", "and", "information", "retrieval", "were", "observed", ".", "This", "article", "is", "the", "first", "to", "apply", "ConvNets", "only", "on", "characters", ".", "We", "show", "that", "when", "trained", "on", "large", "-", "scale", "datasets", ",", "deep", "ConvNets", "do", "not", "require", "the", "knowledge", "of", "words", ",", "in", "addition", "to", "the", "conclusion", "from", "previous", "research", "that", "ConvNets", "do", "not", "require", "the", "knowledge", "about", "the", "syntactic", "or", "semantic", "structure", "of", "a", "language", ".", "This", "simplification", "of", "engineering", "could", "be", "crucial", "for", "a", "single", "system", "that", "can", "work", "for", "different", "languages", ",", "since", "characters", "always", "constitute", "a", "necessary", "construct", "regardless", "of", "whether", "segmentation", "into", "words", "is", "possible", ".", "Working", "on", "only", "characters", "also", "has", "the", "advantage", "that", "abnormal", "character", "combinations", "such", "as", "misspellings", "and", "emoticons", "may", "be", "naturally", "learnt", ".", "section", ":", "Character", "-", "level", "Convolutional", "Networks", "In", "this", "section", ",", "we", "introduce", "the", "design", "of", "character", "-", "level", "ConvNets", "for", "text", "classification", ".", "The", "design", "is", "modular", ",", "where", "the", "gradients", "are", "obtained", "by", "back", "-", "propagation", "to", "perform", "optimization", ".", "subsection", ":", "Key", "Modules", "The", "main", "component", "is", "the", "temporal", "convolutional", "module", ",", "which", "simply", "computes", "a", "1", "-", "D", "convolution", ".", "Suppose", "we", "have", "a", "discrete", "input", "function", "and", "a", "discrete", "kernel", "function", ".", "The", "convolution", "between", "and", "with", "stride", "is", "defined", "as", "where", "is", "an", "offset", "constant", ".", "Just", "as", "in", "traditional", "convolutional", "networks", "in", "vision", ",", "the", "module", "is", "parameterized", "by", "a", "set", "of", "such", "kernel", "functions", "which", "we", "call", "weights", ",", "on", "a", "set", "of", "inputs", "and", "outputs", ".", "We", "call", "each", "(", "or", ")", "input", "(", "or", "output", ")", "features", ",", "and", "(", "or", ")", "input", "(", "or", "output", ")", "feature", "size", ".", "The", "outputs", "is", "obtained", "by", "a", "sum", "over", "of", "the", "convolutions", "between", "and", ".", "One", "key", "module", "that", "helped", "us", "to", "train", "deeper", "models", "is", "temporal", "max", "-", "pooling", ".", "It", "is", "the", "1", "-", "D", "version", "of", "the", "max", "-", "pooling", "module", "used", "in", "computer", "vision", ".", "Given", "a", "discrete", "input", "function", ",", "the", "max", "-", "pooling", "function", "of", "is", "defined", "as", "where", "is", "an", "offset", "constant", ".", "This", "very", "pooling", "module", "enabled", "us", "to", "train", "ConvNets", "deeper", "than", "6", "layers", ",", "where", "all", "others", "fail", ".", "The", "analysis", "by", "might", "shed", "some", "light", "on", "this", ".", "The", "non", "-", "linearity", "used", "in", "our", "model", "is", "the", "rectifier", "or", "thresholding", "function", ",", "which", "makes", "our", "convolutional", "layers", "similar", "to", "rectified", "linear", "units", "(", "ReLUs", ")", ".", "The", "algorithm", "used", "is", "stochastic", "gradient", "descent", "(", "SGD", ")", "with", "a", "minibatch", "of", "size", "128", ",", "using", "momentum", "and", "initial", "step", "size", "which", "is", "halved", "every", "3", "epoches", "for", "10", "times", ".", "Each", "epoch", "takes", "a", "fixed", "number", "of", "random", "training", "samples", "uniformly", "sampled", "across", "classes", ".", "This", "number", "will", "later", "be", "detailed", "for", "each", "dataset", "sparately", ".", "The", "implementation", "is", "done", "using", "Torch", "7", ".", "subsection", ":", "Character", "quantization", "Our", "models", "accept", "a", "sequence", "of", "encoded", "characters", "as", "input", ".", "The", "encoding", "is", "done", "by", "prescribing", "an", "alphabet", "of", "size", "for", "the", "input", "language", ",", "and", "then", "quantize", "each", "character", "using", "1", "-", "of", "-", "encoding", "(", "or", "\u201c", "one", "-", "hot", "\u201d", "encoding", ")", ".", "Then", ",", "the", "sequence", "of", "characters", "is", "transformed", "to", "a", "sequence", "of", "such", "sized", "vectors", "with", "fixed", "length", ".", "Any", "character", "exceeding", "length", "is", "ignored", ",", "and", "any", "characters", "that", "are", "not", "in", "the", "alphabet", "including", "blank", "characters", "are", "quantized", "as", "all", "-", "zero", "vectors", ".", "The", "character", "quantization", "order", "is", "backward", "so", "that", "the", "latest", "reading", "on", "characters", "is", "always", "placed", "near", "the", "begin", "of", "the", "output", ",", "making", "it", "easy", "for", "fully", "connected", "layers", "to", "associate", "weights", "with", "the", "latest", "reading", ".", "The", "alphabet", "used", "in", "all", "of", "our", "models", "consists", "of", "70", "characters", ",", "including", "26", "english", "letters", ",", "10", "digits", ",", "33", "other", "characters", "and", "the", "new", "line", "character", ".", "The", "non", "-", "space", "characters", "are", ":", "Later", "we", "also", "compare", "with", "models", "that", "use", "a", "different", "alphabet", "in", "which", "we", "distinguish", "between", "upper", "-", "case", "and", "lower", "-", "case", "letters", ".", "subsection", ":", "Model", "Design", "We", "designed", "2", "ConvNets", "\u2013", "one", "large", "and", "one", "small", ".", "They", "are", "both", "9", "layers", "deep", "with", "6", "convolutional", "layers", "and", "3", "fully", "-", "connected", "layers", ".", "Figure", "[", "reference", "]", "gives", "an", "illustration", ".", "The", "input", "have", "number", "of", "features", "equal", "to", "70", "due", "to", "our", "character", "quantization", "method", ",", "and", "the", "input", "feature", "length", "is", "1014", ".", "It", "seems", "that", "1014", "characters", "could", "already", "capture", "most", "of", "the", "texts", "of", "interest", ".", "We", "also", "insert", "2", "dropout", "modules", "in", "between", "the", "3", "fully", "-", "connected", "layers", "to", "regularize", ".", "They", "have", "dropout", "probability", "of", "0.5", ".", "Table", "[", "reference", "]", "lists", "the", "configurations", "for", "convolutional", "layers", ",", "and", "table", "[", "reference", "]", "lists", "the", "configurations", "for", "fully", "-", "connected", "(", "linear", ")", "layers", ".", "We", "initialize", "the", "weights", "using", "a", "Gaussian", "distribution", ".", "The", "mean", "and", "standard", "deviation", "used", "for", "initializing", "the", "large", "model", "is", "and", "small", "model", ".", "For", "different", "problems", "the", "input", "lengths", "may", "be", "different", "(", "for", "example", "in", "our", "case", ")", ",", "and", "so", "are", "the", "frame", "lengths", ".", "From", "our", "model", "design", ",", "it", "is", "easy", "to", "know", "that", "given", "input", "length", ",", "the", "output", "frame", "length", "after", "the", "last", "convolutional", "layer", "(", "but", "before", "any", "of", "the", "fully", "-", "connected", "layers", ")", "is", ".", "This", "number", "multiplied", "with", "the", "frame", "size", "at", "layer", "6", "will", "give", "the", "input", "dimension", "the", "first", "fully", "-", "connected", "layer", "accepts", ".", "subsection", ":", "Data", "Augmentation", "using", "Thesaurus", "Many", "researchers", "have", "found", "that", "appropriate", "data", "augmentation", "techniques", "are", "useful", "for", "controlling", "generalization", "error", "for", "deep", "learning", "models", ".", "These", "techniques", "usually", "work", "well", "when", "we", "could", "find", "appropriate", "invariance", "properties", "that", "the", "model", "should", "possess", ".", "In", "terms", "of", "texts", ",", "it", "is", "not", "reasonable", "to", "augment", "the", "data", "using", "signal", "transformations", "as", "done", "in", "image", "or", "speech", "recognition", ",", "because", "the", "exact", "order", "of", "characters", "may", "form", "rigorous", "syntactic", "and", "semantic", "meaning", ".", "Therefore", ",", "the", "best", "way", "to", "do", "data", "augmentation", "would", "have", "been", "using", "human", "rephrases", "of", "sentences", ",", "but", "this", "is", "unrealistic", "and", "expensive", "due", "the", "large", "volume", "of", "samples", "in", "our", "datasets", ".", "As", "a", "result", ",", "the", "most", "natural", "choice", "in", "data", "augmentation", "for", "us", "is", "to", "replace", "words", "or", "phrases", "with", "their", "synonyms", ".", "We", "experimented", "data", "augmentation", "by", "using", "an", "English", "thesaurus", ",", "which", "is", "obtained", "from", "the", "mytheas", "component", "used", "in", "LibreOffice", "project", ".", "That", "thesaurus", "in", "turn", "was", "obtained", "from", "WordNet", ",", "where", "every", "synonym", "to", "a", "word", "or", "phrase", "is", "ranked", "by", "the", "semantic", "closeness", "to", "the", "most", "frequently", "seen", "meaning", ".", "To", "decide", "on", "how", "many", "words", "to", "replace", ",", "we", "extract", "all", "replaceable", "words", "from", "the", "given", "text", "and", "randomly", "choose", "of", "them", "to", "be", "replaced", ".", "The", "probability", "of", "number", "is", "determined", "by", "a", "geometric", "distribution", "with", "parameter", "in", "which", ".", "The", "index", "of", "the", "synonym", "chosen", "given", "a", "word", "is", "also", "determined", "by", "a", "another", "geometric", "distribution", "in", "which", ".", "This", "way", ",", "the", "probability", "of", "a", "synonym", "chosen", "becomes", "smaller", "when", "it", "moves", "distant", "from", "the", "most", "frequently", "seen", "meaning", ".", "We", "will", "report", "the", "results", "using", "this", "new", "data", "augmentation", "technique", "with", "and", ".", "section", ":", "Comparison", "Models", "To", "offer", "fair", "comparisons", "to", "competitive", "models", ",", "we", "conducted", "a", "series", "of", "experiments", "with", "both", "traditional", "and", "deep", "learning", "methods", ".", "We", "tried", "our", "best", "to", "choose", "models", "that", "can", "provide", "comparable", "and", "competitive", "results", ",", "and", "the", "results", "are", "reported", "faithfully", "without", "any", "model", "selection", ".", "subsection", ":", "Traditional", "Methods", "We", "refer", "to", "traditional", "methods", "as", "those", "that", "using", "a", "hand", "-", "crafted", "feature", "extractor", "and", "a", "linear", "classifier", ".", "The", "classifier", "used", "is", "a", "multinomial", "logistic", "regression", "in", "all", "these", "models", ".", "Bag", "-", "of", "-", "words", "and", "its", "TFIDF", ".", "For", "each", "dataset", ",", "the", "bag", "-", "of", "-", "words", "model", "is", "constructed", "by", "selecting", "50", ",", "000", "most", "frequent", "words", "from", "the", "training", "subset", ".", "For", "the", "normal", "bag", "-", "of", "-", "words", ",", "we", "use", "the", "counts", "of", "each", "word", "as", "the", "features", ".", "For", "the", "TFIDF", "(", "term", "-", "frequency", "inverse", "-", "document", "-", "frequency", ")", "version", ",", "we", "use", "the", "counts", "as", "the", "term", "-", "frequency", ".", "The", "inverse", "document", "frequency", "is", "the", "logarithm", "of", "the", "division", "between", "total", "number", "of", "samples", "and", "number", "of", "samples", "with", "the", "word", "in", "the", "training", "subset", ".", "The", "features", "are", "normalized", "by", "dividing", "the", "largest", "feature", "value", ".", "Bag", "-", "of", "-", "ngrams", "and", "its", "TFIDF", ".", "The", "bag", "-", "of", "-", "ngrams", "models", "are", "constructed", "by", "selecting", "the", "500", ",", "000", "most", "frequent", "n", "-", "grams", "(", "up", "to", "5", "-", "grams", ")", "from", "the", "training", "subset", "for", "each", "dataset", ".", "The", "feature", "values", "are", "computed", "the", "same", "way", "as", "in", "the", "bag", "-", "of", "-", "words", "model", ".", "Bag", "-", "of", "-", "means", "on", "word", "embedding", ".", "We", "also", "have", "an", "experimental", "model", "that", "uses", "k", "-", "means", "on", "word2vec", "learnt", "from", "the", "training", "subset", "of", "each", "dataset", ",", "and", "then", "use", "these", "learnt", "means", "as", "representatives", "of", "the", "clustered", "words", ".", "We", "take", "into", "consideration", "all", "the", "words", "that", "appeared", "more", "than", "5", "times", "in", "the", "training", "subset", ".", "The", "dimension", "of", "the", "embedding", "is", "300", ".", "The", "bag", "-", "of", "-", "means", "features", "are", "computed", "the", "same", "way", "as", "in", "the", "bag", "-", "of", "-", "words", "model", ".", "The", "number", "of", "means", "is", "5000", ".", "subsection", ":", "Deep", "Learning", "Methods", "Recently", "deep", "learning", "methods", "have", "started", "to", "be", "applied", "to", "text", "classification", ".", "We", "choose", "two", "simple", "and", "representative", "models", "for", "comparison", ",", "in", "which", "one", "is", "word", "-", "based", "ConvNet", "and", "the", "other", "a", "simple", "long", "-", "short", "term", "memory", "(", "LSTM", ")", "recurrent", "neural", "network", "model", ".", "Word", "-", "based", "ConvNets", ".", "Among", "the", "large", "number", "of", "recent", "works", "on", "word", "-", "based", "ConvNets", "for", "text", "classification", ",", "one", "of", "the", "differences", "is", "the", "choice", "of", "using", "pretrained", "or", "end", "-", "to", "-", "end", "learned", "word", "representations", ".", "We", "offer", "comparisons", "with", "both", "using", "the", "pretrained", "word2vec", "embedding", "and", "using", "lookup", "tables", ".", "The", "embedding", "size", "is", "300", "in", "both", "cases", ",", "in", "the", "same", "way", "as", "our", "bag", "-", "of", "-", "means", "model", ".", "To", "ensure", "fair", "comparison", ",", "the", "models", "for", "each", "case", "are", "of", "the", "same", "size", "as", "our", "character", "-", "level", "ConvNets", ",", "in", "terms", "of", "both", "the", "number", "of", "layers", "and", "each", "layer", "\u2019s", "output", "size", ".", "Experiments", "using", "a", "thesaurus", "for", "data", "augmentation", "are", "also", "conducted", ".", "Long", "-", "short", "term", "memory", ".", "We", "also", "offer", "a", "comparison", "with", "a", "recurrent", "neural", "network", "model", ",", "namely", "long", "-", "short", "term", "memory", "(", "LSTM", ")", ".", "The", "LSTM", "model", "used", "in", "our", "case", "is", "word", "-", "based", ",", "using", "pretrained", "word2vec", "embedding", "of", "size", "300", "as", "in", "previous", "models", ".", "The", "model", "is", "formed", "by", "taking", "mean", "of", "the", "outputs", "of", "all", "LSTM", "cells", "to", "form", "a", "feature", "vector", ",", "and", "then", "using", "multinomial", "logistic", "regression", "on", "this", "feature", "vector", ".", "The", "output", "dimension", "is", "512", ".", "The", "variant", "of", "LSTM", "we", "used", "is", "the", "common", "\u201c", "vanilla", "\u201d", "architecture", ".", "We", "also", "used", "gradient", "clipping", "in", "which", "the", "gradient", "norm", "is", "limited", "to", "5", ".", "Figure", "[", "reference", "]", "gives", "an", "illustration", ".", "subsection", ":", "Choice", "of", "Alphabet", "For", "the", "alphabet", "of", "English", ",", "one", "apparent", "choice", "is", "whether", "to", "distinguish", "between", "upper", "-", "case", "and", "lower", "-", "case", "letters", ".", "We", "report", "experiments", "on", "this", "choice", "and", "observed", "that", "it", "usually", "(", "but", "not", "always", ")", "gives", "worse", "results", "when", "such", "distinction", "is", "made", ".", "One", "possible", "explanation", "might", "be", "that", "semantics", "do", "not", "change", "with", "different", "letter", "cases", ",", "therefore", "there", "is", "a", "benefit", "of", "regularization", ".", "section", ":", "Large", "-", "scale", "Datasets", "and", "Results", "Previous", "research", "on", "ConvNets", "in", "different", "areas", "has", "shown", "that", "they", "usually", "work", "well", "with", "large", "-", "scale", "datasets", ",", "especially", "when", "the", "model", "takes", "in", "low", "-", "level", "raw", "features", "like", "characters", "in", "our", "case", ".", "However", ",", "most", "open", "datasets", "for", "text", "classification", "are", "quite", "small", ",", "and", "large", "-", "scale", "datasets", "are", "splitted", "with", "a", "significantly", "smaller", "training", "set", "than", "testing", ".", "Therefore", ",", "instead", "of", "confusing", "our", "community", "more", "by", "using", "them", ",", "we", "built", "several", "large", "-", "scale", "datasets", "for", "our", "experiments", ",", "ranging", "from", "hundreds", "of", "thousands", "to", "several", "millions", "of", "samples", ".", "Table", "[", "reference", "]", "is", "a", "summary", ".", "AG", "\u2019s", "news", "corpus", ".", "We", "obtained", "the", "AG", "\u2019s", "corpus", "of", "news", "article", "on", "the", "web", ".", "It", "contains", "496", ",", "835", "categorized", "news", "articles", "from", "more", "than", "2000", "news", "sources", ".", "We", "choose", "the", "4", "largest", "classes", "from", "this", "corpus", "to", "construct", "our", "dataset", ",", "using", "only", "the", "title", "and", "description", "fields", ".", "The", "number", "of", "training", "samples", "for", "each", "class", "is", "30", ",", "000", "and", "testing", "1900", ".", "Sogou", "news", "corpus", ".", "This", "dataset", "is", "a", "combination", "of", "the", "SogouCA", "and", "SogouCS", "news", "corpora", ",", "containing", "in", "total", "2", ",", "909", ",", "551", "news", "articles", "in", "various", "topic", "channels", ".", "We", "then", "labeled", "each", "piece", "of", "news", "using", "its", "URL", ",", "by", "manually", "classifying", "the", "their", "domain", "names", ".", "This", "gives", "us", "a", "large", "corpus", "of", "news", "articles", "labeled", "with", "their", "categories", ".", "There", "are", "a", "large", "number", "categories", "but", "most", "of", "them", "contain", "only", "few", "articles", ".", "We", "choose", "5", "categories", "\u2013", "\u201c", "sports", "\u201d", ",", "\u201c", "finance", "\u201d", ",", "\u201c", "entertainment", "\u201d", ",", "\u201c", "automobile", "\u201d", "and", "\u201c", "technology", "\u201d", ".", "The", "number", "of", "training", "samples", "selected", "for", "each", "class", "is", "90", ",", "000", "and", "testing", "12", ",", "000", ".", "Although", "this", "is", "a", "dataset", "in", "Chinese", ",", "we", "used", "pypinyin", "package", "combined", "with", "jieba", "Chinese", "segmentation", "system", "to", "produce", "Pinyin", "\u2013", "a", "phonetic", "romanization", "of", "Chinese", ".", "The", "models", "for", "English", "can", "then", "be", "applied", "to", "this", "dataset", "without", "change", ".", "The", "fields", "used", "are", "title", "and", "content", ".", "DBPedia", "ontology", "dataset", ".", "DBpedia", "is", "a", "crowd", "-", "sourced", "community", "effort", "to", "extract", "structured", "information", "from", "Wikipedia", ".", "The", "DBpedia", "ontology", "dataset", "is", "constructed", "by", "picking", "14", "non", "-", "overlapping", "classes", "from", "DBpedia", "2014", ".", "From", "each", "of", "these", "14", "ontology", "classes", ",", "we", "randomly", "choose", "40", ",", "000", "training", "samples", "and", "5", ",", "000", "testing", "samples", ".", "The", "fields", "we", "used", "for", "this", "dataset", "contain", "title", "and", "abstract", "of", "each", "Wikipedia", "article", ".", "Yelp", "reviews", ".", "The", "Yelp", "reviews", "dataset", "is", "obtained", "from", "the", "Yelp", "Dataset", "Challenge", "in", "2015", ".", "This", "dataset", "contains", "1", ",", "569", ",", "264", "samples", "that", "have", "review", "texts", ".", "Two", "classification", "tasks", "are", "constructed", "from", "this", "dataset", "\u2013", "one", "predicting", "full", "number", "of", "stars", "the", "user", "has", "given", ",", "and", "the", "other", "predicting", "a", "polarity", "label", "by", "considering", "stars", "1", "and", "2", "negative", ",", "and", "3", "and", "4", "positive", ".", "The", "full", "dataset", "has", "130", ",", "000", "training", "samples", "and", "10", ",", "000", "testing", "samples", "in", "each", "star", ",", "and", "the", "polarity", "dataset", "has", "280", ",", "000", "training", "samples", "and", "19", ",", "000", "test", "samples", "in", "each", "polarity", ".", "Yahoo", "!", "Answers", "dataset", ".", "We", "obtained", "Yahoo", "!", "Answers", "Comprehensive", "Questions", "and", "Answers", "version", "1.0", "dataset", "through", "the", "Yahoo", "!", "Webscope", "program", ".", "The", "corpus", "contains", "4", ",", "483", ",", "032", "questions", "and", "their", "answers", ".", "We", "constructed", "a", "topic", "classification", "dataset", "from", "this", "corpus", "using", "10", "largest", "main", "categories", ".", "Each", "class", "contains", "140", ",", "000", "training", "samples", "and", "5", ",", "000", "testing", "samples", ".", "The", "fields", "we", "used", "include", "question", "title", ",", "question", "content", "and", "best", "answer", ".", "Amazon", "reviews", ".", "We", "obtained", "an", "Amazon", "review", "dataset", "from", "the", "Stanford", "Network", "Analysis", "Project", "(", "SNAP", ")", ",", "which", "spans", "18", "years", "with", "34", ",", "686", ",", "770", "reviews", "from", "6", ",", "643", ",", "669", "users", "on", "2", ",", "441", ",", "053", "products", ".", "Similarly", "to", "the", "Yelp", "review", "dataset", ",", "we", "also", "constructed", "2", "datasets", "\u2013", "one", "full", "score", "prediction", "and", "another", "polarity", "prediction", ".", "The", "full", "dataset", "contains", "600", ",", "000", "training", "samples", "and", "130", ",", "000", "testing", "samples", "in", "each", "class", ",", "whereas", "the", "polarity", "dataset", "contains", "1", ",", "800", ",", "000", "training", "samples", "and", "200", ",", "000", "testing", "samples", "in", "each", "polarity", "sentiment", ".", "The", "fields", "used", "are", "review", "title", "and", "review", "content", ".", "Table", "[", "reference", "]", "lists", "all", "the", "testing", "errors", "we", "obtained", "from", "these", "datasets", "for", "all", "the", "applicable", "models", ".", "Note", "that", "since", "we", "do", "not", "have", "a", "Chinese", "thesaurus", ",", "the", "Sogou", "News", "dataset", "does", "not", "have", "any", "results", "using", "thesaurus", "augmentation", ".", "We", "labeled", "the", "best", "result", "in", "blue", "and", "worse", "result", "in", "red", ".", "section", ":", "Discussion", "[", "b", "]", "0.3", "[", "b", "]", "0.3", "[", "b", "]", "0.3", "[", "b", "]", "0.3", "[", "b", "]", "0.3", "[", "b", "]", "0.3", "[", "b", "]", "0.7", "To", "understand", "the", "results", "in", "table", "[", "reference", "]", "further", ",", "we", "offer", "some", "empirical", "analysis", "in", "this", "section", ".", "To", "facilitate", "our", "analysis", ",", "we", "present", "the", "relative", "errors", "in", "figure", "[", "reference", "]", "with", "respect", "to", "comparison", "models", ".", "Each", "of", "these", "plots", "is", "computed", "by", "taking", "the", "difference", "between", "errors", "on", "comparison", "model", "and", "our", "character", "-", "level", "ConvNet", "model", ",", "then", "divided", "by", "the", "comparison", "model", "error", ".", "All", "ConvNets", "in", "the", "figure", "are", "the", "large", "models", "with", "thesaurus", "augmentation", "respectively", ".", "Character", "-", "level", "ConvNet", "is", "an", "effective", "method", ".", "The", "most", "important", "conclusion", "from", "our", "experiments", "is", "that", "character", "-", "level", "ConvNets", "could", "work", "for", "text", "classification", "without", "the", "need", "for", "words", ".", "This", "is", "a", "strong", "indication", "that", "language", "could", "also", "be", "thought", "of", "as", "a", "signal", "no", "different", "from", "any", "other", "kind", ".", "Figure", "[", "reference", "]", "shows", "12", "random", "first", "-", "layer", "patches", "learnt", "by", "one", "of", "our", "character", "-", "level", "ConvNets", "for", "DBPedia", "dataset", ".", "Dataset", "size", "forms", "a", "dichotomy", "between", "traditional", "and", "ConvNets", "models", ".", "The", "most", "obvious", "trend", "coming", "from", "all", "the", "plots", "in", "figure", "[", "reference", "]", "is", "that", "the", "larger", "datasets", "tend", "to", "perform", "better", ".", "Traditional", "methods", "like", "n", "-", "grams", "TFIDF", "remain", "strong", "candidates", "for", "dataset", "of", "size", "up", "to", "several", "hundreds", "of", "thousands", ",", "and", "only", "until", "the", "dataset", "goes", "to", "the", "scale", "of", "several", "millions", "do", "we", "observe", "that", "character", "-", "level", "ConvNets", "start", "to", "do", "better", ".", "ConvNets", "may", "work", "well", "for", "user", "-", "generated", "data", ".", "User", "-", "generated", "data", "vary", "in", "the", "degree", "of", "how", "well", "the", "texts", "are", "curated", ".", "For", "example", ",", "in", "our", "million", "scale", "datasets", ",", "Amazon", "reviews", "tend", "to", "be", "raw", "user", "-", "inputs", ",", "whereas", "users", "might", "be", "extra", "careful", "in", "their", "writings", "on", "Yahoo", "!", "Answers", ".", "Plots", "comparing", "word", "-", "based", "deep", "models", "(", "figures", "[", "reference", "]", ",", "[", "reference", "]", "and", "[", "reference", "]", ")", "show", "that", "character", "-", "level", "ConvNets", "work", "better", "for", "less", "curated", "user", "-", "generated", "texts", ".", "This", "property", "suggests", "that", "ConvNets", "may", "have", "better", "applicability", "to", "real", "-", "world", "scenarios", ".", "However", ",", "further", "analysis", "is", "needed", "to", "validate", "the", "hypothesis", "that", "ConvNets", "are", "truly", "good", "at", "identifying", "exotic", "character", "combinations", "such", "as", "misspellings", "and", "emoticons", ",", "as", "our", "experiments", "alone", "do", "not", "show", "any", "explicit", "evidence", ".", "Choice", "of", "alphabet", "makes", "a", "difference", ".", "Figure", "[", "reference", "]", "shows", "that", "changing", "the", "alphabet", "by", "distinguishing", "between", "uppercase", "and", "lowercase", "letters", "could", "make", "a", "difference", ".", "For", "million", "-", "scale", "datasets", ",", "it", "seems", "that", "not", "making", "such", "distinction", "usually", "works", "better", ".", "One", "possible", "explanation", "is", "that", "there", "is", "a", "regularization", "effect", ",", "but", "this", "is", "to", "be", "validated", ".", "Semantics", "of", "tasks", "may", "not", "matter", ".", "Our", "datasets", "consist", "of", "two", "kinds", "of", "tasks", ":", "sentiment", "analysis", "(", "Yelp", "and", "Amazon", "reviews", ")", "and", "topic", "classification", "(", "all", "others", ")", ".", "This", "dichotomy", "in", "task", "semantics", "does", "not", "seem", "to", "play", "a", "role", "in", "deciding", "which", "method", "is", "better", ".", "Bag", "-", "of", "-", "means", "is", "a", "misuse", "of", "word2vec", "[", "]", ".", "One", "of", "the", "most", "obvious", "facts", "one", "could", "observe", "from", "table", "[", "reference", "]", "and", "figure", "[", "reference", "]", "is", "that", "the", "bag", "-", "of", "-", "means", "model", "performs", "worse", "in", "every", "case", ".", "Comparing", "with", "traditional", "models", ",", "this", "suggests", "such", "a", "simple", "use", "of", "a", "distributed", "word", "representation", "may", "not", "give", "us", "an", "advantage", "to", "text", "classification", ".", "However", ",", "our", "experiments", "does", "not", "speak", "for", "any", "other", "language", "processing", "tasks", "or", "use", "of", "word2vec", "in", "any", "other", "way", ".", "There", "is", "no", "free", "lunch", ".", "Our", "experiments", "once", "again", "verifies", "that", "there", "is", "not", "a", "single", "machine", "learning", "model", "that", "can", "work", "for", "all", "kinds", "of", "datasets", ".", "The", "factors", "discussed", "in", "this", "section", "could", "all", "play", "a", "role", "in", "deciding", "which", "method", "is", "the", "best", "for", "some", "specific", "application", ".", "section", ":", "Conclusion", "and", "Outlook", "This", "article", "offers", "an", "empirical", "study", "on", "character", "-", "level", "convolutional", "networks", "for", "text", "classification", ".", "We", "compared", "with", "a", "large", "number", "of", "traditional", "and", "deep", "learning", "models", "using", "several", "large", "-", "scale", "datasets", ".", "On", "one", "hand", ",", "analysis", "shows", "that", "character", "-", "level", "ConvNet", "is", "an", "effective", "method", ".", "On", "the", "other", "hand", ",", "how", "well", "our", "model", "performs", "in", "comparisons", "depends", "on", "many", "factors", ",", "such", "as", "dataset", "size", ",", "whether", "the", "texts", "are", "curated", "and", "choice", "of", "alphabet", ".", "In", "the", "future", ",", "we", "hope", "to", "apply", "character", "-", "level", "ConvNets", "for", "a", "broader", "range", "of", "language", "processing", "tasks", "especially", "when", "structured", "outputs", "are", "needed", ".", "section", ":", "Acknowledgement", "We", "gratefully", "acknowledge", "the", "support", "of", "NVIDIA", "Corporation", "with", "the", "donation", "of", "2", "Tesla", "K40", "GPUs", "used", "for", "this", "research", ".", "We", "gratefully", "acknowledge", "the", "support", "of", "Amazon.com", "Inc", "for", "an", "AWS", "in", "Education", "Research", "grant", "used", "for", "this", "research", ".", "bibliography", ":", "References"]}