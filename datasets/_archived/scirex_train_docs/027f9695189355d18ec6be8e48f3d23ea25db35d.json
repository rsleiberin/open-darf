{"coref": {"300D_Gumbel_TreeLSTM_encoders": [[74, 78], [496, 500], [2067, 2071], [4693, 4697], [527, 531], [2073, 2077], [2829, 2833], [3048, 3052], [3514, 3518], [4925, 4929]], "600D_Gumbel_TreeLSTM_encoders": [[74, 78], [496, 500], [954, 955], [2067, 2071], [4693, 4697], [527, 531], [2073, 2077], [2829, 2833], [3048, 3052], [3514, 3518], [4925, 4929]], "Natural_Language_Inference": [[140, 143], [180, 183], [695, 698], [2838, 2841], [2868, 2871], [2871, 2874], [2875, 2876]], "Parameters": [], "SNLI": [[2895, 2899], [2900, 2903], [2966, 2968], [3015, 3016], [3268, 3269], [3676, 3678], [4259, 4260], [4277, 4279], [4576, 4578], [5216, 5217]], "__Test_Accuracy": [[3343, 3344], [3409, 3411], [5294, 5295], [5555, 5556], [4122, 4123]], "__Train_Accuracy": [[3343, 3344], [3565, 3568], [5294, 5295], [5555, 5556], [4122, 4123]]}, "coref_non_salient": {"0": [[5005, 5014], [5153, 5157]], "1": [[1592, 1593], [1846, 1848], [1915, 1918]], "10": [[81, 90], [1317, 1325], [1381, 1390]], "100": [[3324, 3328]], "101": [[956, 959]], "102": [[2912, 2914]], "103": [[1900, 1902]], "104": [[3470, 3471]], "105": [[931, 936]], "106": [[1215, 1217]], "107": [[1541, 1543]], "108": [[2178, 2180]], "109": [[64, 66]], "11": [[113, 117], [619, 627], [640, 645], [1657, 1661], [1882, 1892], [2027, 2032], [2338, 2343], [2560, 2565], [4750, 4755]], "110": [[411, 414]], "111": [[3182, 3184]], "112": [[3121, 3125]], "113": [[1911, 1913]], "114": [[1420, 1425]], "115": [[1796, 1800]], "116": [[2724, 2728]], "117": [[308, 311]], "118": [[4873, 4875]], "119": [[993, 996]], "12": [[1524, 1527], [1527, 1530], [1623, 1627], [1791, 1794], [1868, 1872], [1981, 1985], [3237, 3240], [5076, 5079]], "120": [[3251, 3253]], "121": [[3863, 3864]], "122": [[3959, 3961]], "123": [[61, 63]], "124": [[961, 963]], "125": [[3747, 3748]], "126": [[1067, 1070]], "127": [[1933, 1937]], "128": [[5133, 5135]], "13": [[1192, 1194], [1234, 1237]], "14": [[3537, 3541], [3548, 3552], [4153, 4157], [4791, 4793]], "15": [[3010, 3012], [3670, 3671], [5604, 5605]], "16": [[251, 253], [3133, 3135], [4101, 4103], [5349, 5351], [5424, 5426]], "17": [[717, 720], [741, 744]], "18": [[224, 225], [5336, 5337], [5412, 5413]], "19": [[1179, 1181], [4243, 4245]], "2": [[1154, 1156], [1185, 1187], [1224, 1226]], "20": [[184, 186], [1256, 1260]], "21": [[14, 17], [357, 360]], "22": [[3095, 3096], [3179, 3180], [5444, 5445], [5460, 5461]], "23": [[2118, 2120], [2658, 2660], [5238, 5240], [5498, 5500]], "24": [[5380, 5384], [5403, 5405], [5620, 5622]], "25": [[1630, 1632], [1801, 1805], [5039, 5043]], "26": [[264, 266], [303, 305]], "27": [[144, 146], [699, 702], [2842, 2844], [3581, 3583]], "28": [[3667, 3668], [3777, 3778]], "29": [[667, 668], [1029, 1030], [1590, 1591], [2611, 2612], [4765, 4766]], "3": [[3260, 3262], [3858, 3860], [3904, 3906], [5279, 5281], [5539, 5541]], "30": [[2288, 2290], [2502, 2504], [2631, 2633]], "31": [[5286, 5288], [5547, 5549]], "32": [[1488, 1489], [2689, 2690], [2713, 2715]], "33": [[5342, 5345], [5418, 5421]], "34": [[937, 939], [976, 977], [1013, 1014]], "35": [[1440, 1442], [1463, 1465]], "36": [[5431, 5433]], "37": [[1426, 1427], [1511, 1512], [2770, 2771], [2805, 2806], [546, 547], [1010, 1011], [1050, 1051], [1335, 1336], [1380, 1381], [1393, 1394], [1434, 1435], [1502, 1503], [2646, 2647], [2701, 2702], [2710, 2711], [2736, 2737], [2762, 2763], [3282, 3283], [3479, 3480], [3489, 3490], [3499, 3500], [3505, 3506], [4702, 4703]], "38": [[3339, 3341], [4282, 4284]], "39": [[339, 340], [421, 422]], "4": [[1230, 1231], [1617, 1619], [3310, 3311]], "40": [[5226, 5229], [5486, 5489]], "41": [[2136, 2138], [2654, 2656]], "42": [[3428, 3431], [3782, 3786], [3894, 3898], [3928, 3932], [4127, 4130], [4556, 4559], [5562, 5565]], "43": [[2638, 2640]], "44": [[5685, 5692]], "45": [[907, 909], [2962, 2964], [4032, 4034]], "46": [[4016, 4021]], "47": [[1903, 1904], [1922, 1923]], "48": [[3100, 3101], [3822, 3823], [3912, 3913], [5446, 5447], [5606, 5607]], "49": [[3947, 3949]], "5": [[505, 507], [564, 565], [1254, 1255], [1405, 1406], [4831, 4832]], "50": [[3454, 3455], [4003, 4004]], "51": [[3775, 3776]], "52": [[1674, 1677]], "53": [[2817, 2819]], "54": [[5591, 5594]], "55": [[3434, 3437], [3867, 3871], [4043, 4047], [4135, 4138], [5566, 5569], [5648, 5651]], "56": [[2249, 2253]], "57": [[3446, 3447]], "58": [[5129, 5132]], "59": [[4884, 4893]], "6": [[984, 987], [3591, 3595]], "60": [[4113, 4120]], "61": [[1939, 1941]], "62": [[3844, 3846]], "63": [[2263, 2265], [2413, 2415], [2679, 2681]], "64": [[689, 691]], "65": [[5062, 5066]], "66": [[1770, 1772]], "67": [[3750, 3755], [5328, 5334], [5596, 5601]], "68": [[3440, 3441]], "69": [[1639, 1640]], "7": [[18, 19], [51, 52], [409, 410], [802, 804]], "70": [[4191, 4193]], "71": [[964, 966]], "72": [[4777, 4779]], "73": [[4680, 4683]], "74": [[330, 334]], "75": [[5623, 5625]], "76": [[335, 338]], "77": [[3257, 3259]], "78": [[914, 916]], "79": [[2588, 2590]], "8": [[3117, 3120], [3833, 3836]], "80": [[4178, 4187]], "81": [[3957, 3958]], "82": [[4306, 4309]], "83": [[257, 259]], "84": [[312, 313]], "85": [[2772, 2774]], "86": [[1600, 1605]], "87": [[40, 44]], "88": [[1607, 1608]], "89": [[213, 217]], "9": [[3528, 3530], [4590, 4592]], "90": [[4553, 4555]], "91": [[1200, 1201]], "92": [[2750, 2754]], "93": [[997, 998]], "94": [[1585, 1587]], "95": [[5628, 5631]], "96": [[3293, 3299]], "97": [[3757, 3760]], "98": [[4310, 4312]], "99": [[1615, 1616]]}, "doc_id": "027f9695189355d18ec6be8e48f3d23ea25db35d", "method_subrelations": {"300D_Gumbel_TreeLSTM_encoders": [[[0, 29], "300D_Gumbel_TreeLSTM_encoders"]], "600D_Gumbel_TreeLSTM_encoders": [[[0, 29], "600D_Gumbel_TreeLSTM_encoders"]]}, "n_ary_relations": [{"Material": "SNLI", "Method": "300D_Gumbel_TreeLSTM_encoders", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "85.6"}, {"Material": "SNLI", "Method": "600D_Gumbel_TreeLSTM_encoders", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "86.0"}, {"Material": "SNLI", "Method": "300D_Gumbel_TreeLSTM_encoders", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "91.2"}, {"Material": "SNLI", "Method": "600D_Gumbel_TreeLSTM_encoders", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "93.1"}, {"Material": "SNLI", "Method": "300D_Gumbel_TreeLSTM_encoders", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "2.9m"}, {"Material": "SNLI", "Method": "600D_Gumbel_TreeLSTM_encoders", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "10m"}], "ner": [[14, 17, "Method"], [18, 19, "Method"], [40, 44, "Task"], [51, 52, "Method"], [61, 63, "Task"], [64, 66, "Task"], [74, 78, "Method"], [81, 90, "Method"], [113, 117, "Method"], [140, 143, "Task"], [144, 146, "Task"], [180, 183, "Task"], [184, 186, "Task"], [213, 217, "Task"], [224, 225, "Method"], [251, 253, "Method"], [257, 259, "Task"], [264, 266, "Task"], [303, 305, "Task"], [308, 311, "Method"], [312, 313, "Method"], [330, 334, "Method"], [335, 338, "Method"], [339, 340, "Method"], [357, 360, "Method"], [409, 410, "Method"], [411, 414, "Method"], [421, 422, "Method"], [496, 500, "Method"], [505, 507, "Method"], [564, 565, "Method"], [619, 627, "Method"], [640, 645, "Method"], [667, 668, "Method"], [689, 691, "Task"], [695, 698, "Task"], [699, 702, "Task"], [717, 720, "Method"], [741, 744, "Method"], [802, 804, "Method"], [907, 909, "Method"], [914, 916, "Task"], [931, 936, "Method"], [937, 939, "Method"], [954, 955, "Method"], [956, 959, "Task"], [961, 963, "Method"], [964, 966, "Method"], [976, 977, "Method"], [984, 987, "Task"], [993, 996, "Method"], [997, 998, "Method"], [1013, 1014, "Method"], [1029, 1030, "Method"], [1067, 1070, "Method"], [1154, 1156, "Metric"], [1179, 1181, "Method"], [1185, 1187, "Metric"], [1192, 1194, "Method"], [1200, 1201, "Task"], [1215, 1217, "Task"], [1224, 1226, "Metric"], [1230, 1231, "Metric"], [1234, 1237, "Method"], [1254, 1255, "Method"], [1256, 1260, "Task"], [1317, 1325, "Method"], [1381, 1390, "Method"], [1405, 1406, "Method"], [1420, 1425, "Method"], [1426, 1427, "Method"], [1440, 1442, "Method"], [1463, 1465, "Method"], [1488, 1489, "Method"], [1511, 1512, "Method"], [1524, 1527, "Method"], [1527, 1530, "Method"], [1541, 1543, "Method"], [1585, 1587, "Method"], [1590, 1591, "Method"], [1592, 1593, "Method"], [1600, 1605, "Method"], [1607, 1608, "Method"], [1615, 1616, "Metric"], [1617, 1619, "Metric"], [1623, 1627, "Method"], [1630, 1632, "Method"], [1639, 1640, "Task"], [1657, 1661, "Method"], [1674, 1677, "Method"], [1770, 1772, "Task"], [1791, 1794, "Method"], [1796, 1800, "Method"], [1801, 1805, "Method"], [1846, 1848, "Method"], [1868, 1872, "Method"], [1882, 1892, "Method"], [1900, 1902, "Method"], [1903, 1904, "Method"], [1911, 1913, "Method"], [1915, 1918, "Method"], [1922, 1923, "Method"], [1933, 1937, "Method"], [1939, 1941, "Method"], [1981, 1985, "Method"], [2027, 2032, "Method"], [2067, 2071, "Method"], [2118, 2120, "Method"], [2136, 2138, "Method"], [2178, 2180, "Method"], [2249, 2253, "Method"], [2263, 2265, "Method"], [2288, 2290, "Metric"], [2338, 2343, "Method"], [2413, 2415, "Method"], [2502, 2504, "Metric"], [2560, 2565, "Method"], [2588, 2590, "Method"], [2611, 2612, "Method"], [2631, 2633, "Metric"], [2638, 2640, "Task"], [2654, 2656, "Method"], [2658, 2660, "Method"], [2679, 2681, "Method"], [2689, 2690, "Method"], [2713, 2715, "Method"], [2724, 2728, "Method"], [2750, 2754, "Method"], [2770, 2771, "Method"], [2772, 2774, "Method"], [2805, 2806, "Method"], [2817, 2819, "Method"], [2838, 2841, "Task"], [2842, 2844, "Task"], [2868, 2871, "Task"], [2871, 2874, "Task"], [2875, 2876, "Task"], [2895, 2899, "Material"], [2900, 2903, "Material"], [2912, 2914, "Task"], [2962, 2964, "Method"], [2966, 2968, "Material"], [3010, 3012, "Method"], [3015, 3016, "Material"], [3095, 3096, "Method"], [3100, 3101, "Method"], [3117, 3120, "Method"], [3121, 3125, "Method"], [3133, 3135, "Method"], [3179, 3180, "Method"], [3182, 3184, "Method"], [3237, 3240, "Method"], [3251, 3253, "Method"], [3257, 3259, "Method"], [3260, 3262, "Method"], [3268, 3269, "Material"], [3293, 3299, "Method"], [3310, 3311, "Metric"], [3324, 3328, "Method"], [3339, 3341, "Method"], [3343, 3344, "Metric"], [3409, 3411, "Metric"], [3428, 3431, "Method"], [3434, 3437, "Method"], [3440, 3441, "Method"], [3446, 3447, "Method"], [3454, 3455, "Method"], [3470, 3471, "Method"], [3528, 3530, "Method"], [3537, 3541, "Method"], [3548, 3552, "Method"], [3565, 3568, "Metric"], [3581, 3583, "Task"], [3591, 3595, "Task"], [3667, 3668, "Task"], [3670, 3671, "Method"], [3676, 3678, "Material"], [3747, 3748, "Method"], [3750, 3755, "Method"], [3757, 3760, "Method"], [3775, 3776, "Task"], [3777, 3778, "Task"], [3782, 3786, "Method"], [3822, 3823, "Method"], [3833, 3836, "Method"], [3844, 3846, "Method"], [3858, 3860, "Method"], [3863, 3864, "Task"], [3867, 3871, "Method"], [3894, 3898, "Method"], [3904, 3906, "Method"], [3912, 3913, "Method"], [3928, 3932, "Method"], [3947, 3949, "Method"], [3957, 3958, "Method"], [3959, 3961, "Method"], [4016, 4021, "Method"], [4032, 4034, "Method"], [4043, 4047, "Method"], [4101, 4103, "Method"], [4113, 4120, "Method"], [4127, 4130, "Method"], [4135, 4138, "Method"], [4153, 4157, "Method"], [4178, 4187, "Method"], [4191, 4193, "Task"], [4243, 4245, "Method"], [4259, 4260, "Material"], [4277, 4279, "Material"], [4282, 4284, "Method"], [4306, 4309, "Task"], [4310, 4312, "Metric"], [4553, 4555, "Method"], [4556, 4559, "Method"], [4576, 4578, "Material"], [4590, 4592, "Method"], [4680, 4683, "Method"], [4693, 4697, "Method"], [4750, 4755, "Method"], [4765, 4766, "Method"], [4777, 4779, "Method"], [4791, 4793, "Method"], [4831, 4832, "Method"], [4873, 4875, "Task"], [4884, 4893, "Method"], [5005, 5014, "Method"], [5039, 5043, "Method"], [5062, 5066, "Method"], [5076, 5079, "Method"], [5129, 5132, "Method"], [5133, 5135, "Task"], [5153, 5157, "Method"], [5216, 5217, "Material"], [5226, 5229, "Method"], [5238, 5240, "Method"], [5279, 5281, "Method"], [5286, 5288, "Metric"], [5294, 5295, "Metric"], [5328, 5334, "Method"], [5336, 5337, "Method"], [5342, 5345, "Method"], [5349, 5351, "Method"], [5380, 5384, "Method"], [5403, 5405, "Method"], [5412, 5413, "Method"], [5418, 5421, "Method"], [5424, 5426, "Method"], [5431, 5433, "Method"], [5444, 5445, "Method"], [5446, 5447, "Method"], [5460, 5461, "Method"], [5486, 5489, "Method"], [5498, 5500, "Method"], [5539, 5541, "Method"], [5547, 5549, "Metric"], [5555, 5556, "Metric"], [5562, 5565, "Method"], [5566, 5569, "Method"], [5591, 5594, "Method"], [5596, 5601, "Method"], [5604, 5605, "Method"], [5606, 5607, "Method"], [5620, 5622, "Method"], [5623, 5625, "Method"], [5628, 5631, "Task"], [5648, 5651, "Method"], [5685, 5692, "Task"], [527, 531, "Method"], [546, 547, "Method"], [1010, 1011, "Method"], [1050, 1051, "Method"], [1335, 1336, "Method"], [1380, 1381, "Method"], [1393, 1394, "Method"], [1434, 1435, "Method"], [1502, 1503, "Method"], [2073, 2077, "Method"], [2646, 2647, "Method"], [2701, 2702, "Method"], [2710, 2711, "Method"], [2736, 2737, "Method"], [2762, 2763, "Method"], [2829, 2833, "Method"], [3048, 3052, "Method"], [3282, 3283, "Method"], [3479, 3480, "Method"], [3489, 3490, "Method"], [3499, 3500, "Method"], [3505, 3506, "Method"], [3514, 3518, "Method"], [4003, 4004, "Method"], [4122, 4123, "Metric"], [4702, 4703, "Method"], [4925, 4929, "Method"]], "sections": [[0, 8], [8, 175], [175, 884], [884, 1305], [1305, 1376], [1376, 1522], [1522, 2065], [2065, 2820], [2820, 2852], [2852, 2866], [2866, 3576], [3576, 3579], [3579, 4189], [4189, 4684], [4684, 4908], [4908, 5197], [5197, 5211], [5211, 5214], [5214, 5474], [5474, 5675], [5675, 5677], [5677, 5725], [5725, 5727], [5727, 5754], [5754, 5756]], "sentences": [[0, 8], [8, 11], [11, 45], [45, 68], [68, 107], [107, 134], [134, 162], [162, 175], [175, 178], [178, 208], [208, 260], [260, 289], [289, 308], [308, 335], [335, 357], [357, 404], [404, 434], [434, 468], [468, 490], [490, 526], [526, 566], [566, 595], [595, 617], [617, 640], [640, 669], [669, 692], [692, 727], [727, 736], [736, 737], [737, 757], [757, 758], [758, 780], [780, 791], [791, 792], [792, 839], [839, 859], [859, 873], [873, 884], [884, 888], [888, 910], [910, 924], [924, 960], [960, 976], [976, 999], [999, 1015], [1015, 1047], [1047, 1079], [1079, 1113], [1113, 1141], [1141, 1166], [1166, 1188], [1188, 1202], [1202, 1210], [1210, 1227], [1227, 1248], [1248, 1282], [1282, 1305], [1305, 1309], [1309, 1326], [1326, 1362], [1362, 1376], [1376, 1381], [1381, 1395], [1395, 1428], [1428, 1453], [1453, 1469], [1469, 1470], [1470, 1479], [1479, 1504], [1504, 1522], [1522, 1527], [1527, 1559], [1559, 1592], [1592, 1619], [1619, 1623], [1623, 1645], [1645, 1646], [1646, 1662], [1662, 1678], [1678, 1696], [1696, 1732], [1732, 1738], [1738, 1750], [1750, 1790], [1790, 1813], [1813, 1832], [1832, 1851], [1851, 1882], [1882, 1919], [1919, 1938], [1938, 1966], [1966, 1991], [1991, 1992], [1992, 2014], [2014, 2027], [2027, 2065], [2065, 2071], [2071, 2113], [2113, 2133], [2133, 2139], [2139, 2142], [2142, 2143], [2143, 2159], [2159, 2178], [2178, 2196], [2196, 2211], [2211, 2219], [2219, 2231], [2231, 2256], [2256, 2259], [2259, 2266], [2266, 2286], [2286, 2313], [2313, 2359], [2359, 2408], [2408, 2413], [2413, 2416], [2416, 2456], [2456, 2472], [2472, 2488], [2488, 2499], [2499, 2509], [2509, 2520], [2520, 2539], [2539, 2541], [2541, 2568], [2568, 2613], [2613, 2634], [2634, 2646], [2646, 2652], [2652, 2689], [2689, 2708], [2708, 2732], [2732, 2759], [2759, 2785], [2785, 2789], [2789, 2796], [2796, 2820], [2820, 2823], [2823, 2845], [2845, 2852], [2852, 2855], [2855, 2866], [2866, 2871], [2871, 2893], [2893, 2926], [2926, 2965], [2965, 2985], [2985, 3009], [3009, 3018], [3018, 3024], [3024, 3097], [3097, 3109], [3109, 3138], [3138, 3160], [3160, 3172], [3172, 3174], [3174, 3193], [3193, 3209], [3209, 3232], [3232, 3256], [3256, 3265], [3265, 3276], [3276, 3300], [3300, 3312], [3312, 3338], [3338, 3373], [3373, 3388], [3388, 3406], [3406, 3428], [3428, 3458], [3458, 3469], [3469, 3474], [3474, 3495], [3495, 3512], [3512, 3521], [3521, 3531], [3531, 3542], [3542, 3553], [3553, 3576], [3576, 3579], [3579, 3583], [3583, 3606], [3606, 3611], [3611, 3641], [3641, 3669], [3669, 3679], [3679, 3761], [3761, 3779], [3779, 3803], [3803, 3820], [3820, 3847], [3847, 3865], [3865, 3891], [3891, 3918], [3918, 3927], [3927, 3940], [3940, 3956], [3956, 3977], [3977, 4001], [4001, 4035], [4035, 4104], [4104, 4140], [4140, 4189], [4189, 4193], [4193, 4208], [4208, 4236], [4236, 4267], [4267, 4269], [4269, 4293], [4293, 4305], [4305, 4317], [4317, 4356], [4356, 4387], [4387, 4402], [4402, 4411], [4411, 4419], [4419, 4425], [4425, 4426], [4426, 4433], [4433, 4441], [4441, 4465], [4465, 4471], [4471, 4480], [4480, 4487], [4487, 4493], [4493, 4494], [4494, 4498], [4498, 4513], [4513, 4518], [4518, 4529], [4529, 4537], [4537, 4544], [4544, 4546], [4546, 4569], [4569, 4600], [4600, 4616], [4616, 4620], [4620, 4654], [4654, 4684], [4684, 4687], [4687, 4714], [4714, 4738], [4738, 4780], [4780, 4817], [4817, 4833], [4833, 4861], [4861, 4876], [4876, 4894], [4894, 4908], [4908, 4912], [4912, 4931], [4931, 4947], [4947, 4950], [4950, 4957], [4957, 4967], [4967, 4992], [4992, 5044], [5044, 5054], [5054, 5086], [5086, 5127], [5127, 5158], [5158, 5165], [5165, 5180], [5180, 5197], [5197, 5202], [5202, 5211], [5211, 5214], [5214, 5217], [5217, 5233], [5233, 5236], [5236, 5263], [5263, 5277], [5277, 5299], [5299, 5311], [5311, 5352], [5352, 5362], [5362, 5395], [5395, 5398], [5398, 5418], [5418, 5431], [5431, 5446], [5446, 5474], [5474, 5477], [5477, 5493], [5493, 5496], [5496, 5523], [5523, 5537], [5537, 5560], [5560, 5576], [5576, 5606], [5606, 5626], [5626, 5646], [5646, 5675], [5675, 5677], [5677, 5680], [5680, 5700], [5700, 5725], [5725, 5727], [5727, 5730], [5730, 5754], [5754, 5756]], "words": ["Learning", "to", "Compose", "Task", "-", "Specific", "Tree", "Structures", "section", ":", "Abstract", "For", "years", ",", "recursive", "neural", "networks", "(", "RvNNs", ")", "have", "been", "shown", "to", "be", "suitable", "for", "representing", "text", "into", "fixed", "-", "length", "vectors", "and", "achieved", "good", "performance", "on", "several", "natural", "language", "processing", "tasks", ".", "However", ",", "the", "main", "drawback", "of", "RvNNs", "is", "that", "they", "require", "structured", "input", ",", "which", "makes", "data", "preparation", "and", "model", "implementation", "hard", ".", "In", "this", "paper", ",", "we", "propose", "Gumbel", "Tree", "-", "LSTM", ",", "a", "novel", "tree", "-", "structured", "long", "short", "-", "term", "memory", "architecture", "that", "learns", "how", "to", "compose", "task", "-", "specific", "tree", "structures", "only", "from", "plain", "text", "data", "efficiently", ".", "Our", "model", "uses", "Straight", "-", "Through", "Gumbel", "-", "Softmax", "estimator", "to", "decide", "the", "parent", "node", "among", "candidates", "dynamically", "and", "to", "calculate", "gradients", "of", "the", "discrete", "decision", ".", "We", "evaluate", "the", "proposed", "model", "on", "natural", "language", "inference", "and", "sentiment", "analysis", ",", "and", "show", "that", "our", "model", "outperforms", "or", "is", "at", "least", "comparable", "to", "previous", "models", ".", "We", "also", "find", "that", "our", "model", "converges", "significantly", "faster", "than", "other", "models", ".", "section", ":", "Introduction", "Techniques", "for", "mapping", "natural", "language", "into", "vector", "space", "have", "received", "a", "lot", "of", "attention", ",", "due", "to", "their", "capability", "of", "representing", "ambiguous", "semantics", "of", "natural", "language", "using", "dense", "vectors", ".", "Among", "them", ",", "methods", "of", "learning", "representations", "of", "words", ",", "e.g.", "word2vec", "[", "reference", "]", "or", "GloVe", ",", "are", "relatively", "well", "-", "studied", "empirically", "and", "theoretically", "[", "reference", "][", "reference", "]", ",", "and", "some", "of", "them", "became", "typical", "choices", "to", "consider", "when", "initializing", "word", "representations", "for", "better", "performance", "at", "downstream", "tasks", ".", "Meanwhile", ",", "research", "on", "sentence", "representation", "is", "still", "in", "active", "progress", ",", "and", "accordingly", "various", "architecturesdesigned", "with", "different", "intuition", "and", "tailored", "for", "different", "tasks", "-", "are", "being", "proposed", ".", "In", "the", "midst", "of", "them", ",", "three", "architectures", "are", "most", "frequently", "used", "in", "obtaining", "sentence", "representation", "from", "words", ".", "Convolutional", "neural", "networks", "(", "CNNs", ")", "[", "reference", "][", "reference", "]", "utilize", "local", "distribution", "of", "words", "to", "encode", "sentences", ",", "similar", "to", "n", "-", "gram", "models", ".", "Recurrent", "neural", "networks", "(", "RNNs", ")", "[", "reference", "][", "reference", "][", "reference", "]", "encode", "sentences", "by", "reading", "words", "in", "sequential", "order", ".", "Recursive", "neural", "networks", "[", "reference", "][", "reference", "][", "reference", "]", ")", ",", "on", "which", "this", "paper", "focuses", ",", "rely", "on", "structured", "input", "(", "e.g.", "parse", "tree", ")", "to", "encode", "sentences", ",", "based", "on", "the", "intuition", "that", "there", "is", "significant", "semantics", "in", "the", "hierarchical", "structure", "of", "words", ".", "It", "is", "also", "notable", "that", "RvNNs", "are", "generalization", "of", "RNNs", ",", "as", "linear", "chain", "structures", "on", "which", "RNNs", "operate", "are", "equivalent", "to", "left", "-", "or", "right", "-", "skewed", "trees", ".", "Although", "there", "is", "significant", "benefit", "in", "processing", "a", "sentence", "in", "a", "tree", "-", "structured", "recursive", "manner", ",", "data", "annotated", "with", "parse", "trees", "could", "be", "expensive", "to", "prepare", "and", "hard", "to", "be", "computed", "in", "batches", "[", "reference", "]", ".", "Furthermore", ",", "the", "optimal", "hierarchical", "composition", "of", "words", "might", "differ", "depending", "on", "the", "properties", "of", "a", "task", ".", "In", "this", "paper", ",", "we", "propose", "Gumbel", "Tree", "-", "LSTM", ",", "which", "is", "a", "novel", "RvNN", "architecture", "that", "does", "not", "require", "structured", "data", "and", "learns", "to", "compose", "task", "-", "specific", "tree", "structures", "without", "explicit", "guidance", ".", "Our", "Gumbel", "Tree", "-", "LSTM", "model", "is", "based", "on", "tree", "-", "structured", "long", "short", "-", "term", "memory", "(", "Tree", "-", "LSTM", ")", "architecture", "[", "reference", "][", "reference", "]", ",", "which", "is", "one", "of", "the", "most", "renowned", "variants", "of", "RvNN", ".", "To", "learn", "how", "to", "compose", "task", "-", "specific", "tree", "structures", "without", "depending", "on", "structured", "input", ",", "our", "model", "introduces", "composition", "query", "vector", "that", "measures", "validity", "of", "a", "composition", ".", "Using", "validity", "scores", "computed", "by", "the", "composition", "query", "vector", ",", "our", "model", "recursively", "selects", "compositions", "until", "only", "a", "single", "representation", "remains", ".", "We", "use", "StraightThrough", "(", "ST", ")", "Gumbel", "-", "Softmax", "estimator", "[", "reference", "][", "reference", "]", "to", "sample", "compositions", "in", "the", "training", "phase", ".", "ST", "Gumbel", "-", "Softmax", "estimator", "relaxes", "the", "discrete", "sampling", "operation", "to", "be", "continuous", "in", "the", "backward", "pass", ",", "thus", "our", "model", "can", "be", "trained", "via", "the", "standard", "backpropagation", ".", "Also", ",", "since", "the", "computation", "is", "performed", "layer", "-", "wise", ",", "our", "model", "is", "easy", "to", "implement", "and", "naturally", "supports", "batched", "computation", ".", "From", "experiments", "on", "natural", "language", "inference", "and", "sentiment", "analysis", "tasks", ",", "we", "find", "that", "our", "proposed", "model", "outperforms", "or", "is", "at", "least", "comparable", "to", "previous", "sentence", "encoder", "models", "and", "converges", "significantly", "faster", "than", "them", ".", "The", "contributions", "of", "our", "work", "are", "as", "follows", ":", "\u2022", "We", "designed", "a", "novel", "sentence", "encoder", "architecture", "that", "learns", "to", "compose", "task", "-", "specific", "trees", "from", "plain", "text", "data", ".", "\u2022", "We", "showed", "from", "experiments", "that", "the", "proposed", "architecture", "outperforms", "or", "is", "competitive", "to", "state", "-", "of", "-", "the", "-", "art", "models", ".", "We", "also", "observed", "that", "our", "model", "converges", "faster", "than", "others", ".", "\u2022", "Specifically", ",", "we", "saw", "that", "our", "model", "significantly", "outperforms", "previous", "RvNN", "works", "trained", "on", "parse", "trees", "in", "all", "conducted", "experiments", ",", "from", "which", "we", "hypothesize", "that", "syntactic", "parse", "tree", "may", "not", "be", "the", "best", "structure", "for", "every", "task", "and", "the", "optimal", "structure", "could", "differ", "per", "task", ".", "In", "the", "next", "section", ",", "we", "briefly", "introduce", "previous", "works", "which", "have", "similar", "objectives", "to", "that", "of", "our", "work", ".", "Then", "we", "describe", "the", "proposed", "model", "in", "detail", "and", "present", "findings", "from", "experiments", ".", "Lastly", "we", "summarize", "the", "overall", "content", "and", "discuss", "future", "work", ".", "section", ":", "Related", "Work", "There", "have", "been", "several", "works", "that", "aim", "to", "learn", "hierarchical", "latent", "structure", "of", "text", "by", "recursively", "composing", "words", "into", "sentence", "representation", ".", "Some", "of", "them", "carry", "unsupervised", "learning", "on", "structures", "by", "making", "composition", "operations", "soft", ".", "To", "the", "best", "of", "our", "knowledge", ",", "gated", "recursive", "convolutional", "neural", "network", "(", "grConv", ")", "[", "reference", "]", ")", "is", "the", "first", "model", "of", "its", "kind", "and", "used", "as", "an", "encoder", "for", "neural", "machine", "translation", ".", "The", "grConv", "architecture", "uses", "gating", "mechanism", "to", "control", "the", "information", "flow", "from", "children", "to", "parent", ".", "grConv", "and", "its", "variants", "are", "also", "applied", "to", "sentence", "classification", "tasks", "[", "reference", "][", "reference", "]", ".", "Neural", "tree", "indexer", "(", "NTI", ")", "[", "reference", "]", "utilizes", "soft", "hierarchical", "structures", "by", "using", "Tree", "-", "LSTM", "instead", "of", "grConv", ".", "Although", "models", "that", "operate", "with", "soft", "structures", "are", "naturally", "capable", "of", "being", "trained", "via", "backpropagation", ",", "the", "structures", "predicted", "by", "them", "are", "ambiguous", "and", "thus", "it", "is", "hard", "to", "interpret", "them", ".", "CYK", "Tree", "-", "LSTM", "[", "reference", "]", "resolves", "this", "ambiguity", "while", "maintaining", "the", "soft", "property", "by", "introducing", "the", "concept", "of", "CYK", "parsing", "algorithm", "[", "reference", "][", "reference", "][", "reference", "]", ")", ".", "Though", "their", "model", "reduces", "the", "ambiguity", "by", "explicitly", "representing", "a", "node", "as", "a", "weighted", "sum", "of", "all", "candidate", "compositions", ",", "it", "is", "memory", "intensive", "since", "the", "number", "of", "candidates", "linearly", "increases", "by", "depth", ".", "On", "the", "other", "hand", ",", "there", "exist", "some", "previous", "works", "that", "maintain", "the", "discreteness", "of", "tree", "composition", "processes", ",", "instead", "of", "relying", "on", "the", "soft", "hierarchical", "structure", ".", "The", "architecture", "proposed", "by", "[", "reference", "]", "greedily", "selects", "two", "adjacent", "nodes", "whose", "reconstruction", "error", "is", "the", "smallest", "and", "merges", "them", "into", "the", "parent", ".", "In", "their", "work", ",", "rather", "than", "directly", "optimized", "on", "classification", "loss", ",", "a", "composition", "function", "is", "optimized", "to", "minimize", "reconstruction", "error", ".", "[", "reference", "]", "introduce", "reinforcement", "learning", "to", "achieve", "the", "desired", "effect", "of", "discretization", ".", "They", "show", "that", "REINFORCE", "[", "reference", "]", ")", "algorithm", "can", "be", "used", "in", "estimating", "gradients", "to", "learn", "a", "tree", "composition", "function", "minimizing", "classification", "error", ".", "However", ",", "slow", "convergence", "due", "to", "the", "reinforcement", "learning", "setting", "is", "one", "of", "its", "drawbacks", ",", "according", "to", "the", "authors", ".", "In", "the", "research", "area", "outside", "the", "RvNN", ",", "compositionality", "in", "vector", "space", "also", "has", "been", "a", "longstanding", "subject", "[", "reference", "][", "reference", "][", "reference", "][", "reference", "]", ",", "to", "name", "a", "few", ")", ".", "And", "more", "recently", ",", "there", "exist", "works", "aiming", "to", "learn", "hierarchical", "latent", "structure", "from", "unstructured", "data", "[", "reference", "][", "reference", "]", ")", ".", "section", ":", "Model", "Description", "Our", "proposed", "architecture", "is", "built", "based", "on", "the", "treestructured", "long", "short", "-", "term", "memory", "network", "architecture", ".", "We", "introduce", "several", "additional", "components", "into", "the", "Tree", "-", "LSTM", "architecture", "to", "allow", "the", "model", "to", "dynamically", "compose", "tree", "structure", "in", "a", "bottom", "-", "up", "manner", "and", "to", "effectively", "encode", "a", "sentence", "into", "a", "vector", ".", "In", "this", "section", ",", "we", "describe", "the", "components", "of", "our", "model", "in", "detail", ".", "section", ":", "Tree", "-", "LSTM", "Tree", "-", "structured", "long", "short", "-", "term", "memory", "network", "(", "Tree", "-", "LSTM", ")", "[", "reference", "][", "reference", "]", "is", "an", "elegant", "variant", "of", "RvNN", ",", "where", "it", "controls", "information", "flow", "from", "children", "to", "parent", "using", "similar", "mechanism", "to", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "[", "reference", "]", ".", "Tree", "-", "LSTM", "introduces", "cell", "state", "in", "computing", "parent", "representation", ",", "which", "assists", "each", "cell", "to", "capture", "distant", "vertical", "dependencies", ".", "The", "following", "are", "formulae", "that", "our", "model", "uses", "to", "compute", "parent", "representation", "from", "its", "children", ":", "where", ",", "and", "is", "the", "element", "-", "wise", "product", ".", "Note", "that", "our", "formulation", "is", "akin", "to", "that", "of", "SPINN", "[", "reference", "]", ")", ",", "but", "our", "version", "does", "not", "include", "the", "tracking", "LSTM", ".", "Instead", ",", "our", "model", "can", "apply", "an", "LSTM", "to", "leaf", "nodes", ",", "which", "we", "will", "soon", "describe", ".", "section", ":", "Gumbel", "-", "Softmax", "Gumbel", "-", "Softmax", "(", "Jang", ",", "Gu", ",", "and", "Poole", "2017", ")", "(", "or", "Concrete", "distribution", "[", "reference", "]", ")", "is", "a", "method", "of", "utilizing", "discrete", "random", "variables", "in", "a", "network", ".", "Since", "it", "approximates", "one", "-", "hot", "vectors", "sampled", "from", "a", "categorical", "distribution", "by", "making", "them", "continuous", ",", "gradients", "of", "model", "parameters", "can", "be", "calculated", "using", "the", "reparameterization", "trick", "and", "the", "standard", "backpropagation", ".", "GumbelSoftmax", "is", "known", "to", "have", "an", "advantage", "over", "score", "-", "functionbased", "gradient", "estimators", "such", "as", "REINFORCE", "[", "reference", "]", "which", "suffer", "from", "high", "variance", "and", "slow", "convergence", "[", "reference", "]", ".", "Gumbel", "-", "Softmax", "distribution", "is", "motivated", "by", "GumbelMax", "trick", "[", "reference", "]", ",", "an", "algorithm", "for", "sampling", "from", "a", "categorical", "distribution", ".", "Consider", "Figure", "1", ":", "Visualization", "of", "forward", "and", "backward", "computation", "path", "of", "ST", "Gumbel", "-", "Softmax", ".", "In", "the", "forward", "pass", ",", "a", "model", "can", "maintain", "sparseness", "due", "to", "arg", "max", "operation", ".", "In", "the", "backward", "pass", ",", "since", "there", "is", "no", "discrete", "operation", ",", "the", "error", "signal", "can", "backpropagate", ".", "a", "k", "-", "dimensional", "categorical", "distribution", "whose", "class", "probabilities", "p", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "p", "k", "are", "defined", "in", "terms", "of", "unnormalized", "log", "probabilities", "\u03c0", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "\u03c0", "k", ":", "Then", "a", "one", "-", "hot", "sample", "from", "the", "distribution", "can", "be", "easily", "drawn", "by", "the", "following", "equations", ":", "Here", ",", "g", "i", ",", "namely", "Gumbel", "noise", ",", "perturbs", "each", "log", "(", "\u03c0", "i", ")", "term", "so", "that", "taking", "arg", "max", "becomes", "equivalent", "to", "drawing", "a", "sample", "weighted", "on", "p", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "p", "k", ".", "In", "Gumbel", "-", "Softmax", ",", "the", "discontinuous", "arg", "max", "function", "of", "Gumbel", "-", "Max", "trick", "is", "replaced", "by", "the", "differentiable", "softmax", "function", ".", "That", "is", ",", "given", "unnormalized", "probabilities", "\u03c0", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "\u03c0", "k", ",", "a", "sample", "y", "=", "(", "y", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "y", "k", ")", "from", "the", "GumbelSoftmax", "distribution", "is", "drawn", "by", "where", "\u03c4", "is", "a", "temperature", "parameter", ";", "as", "\u03c4", "diminishes", "to", "zero", ",", "a", "sample", "from", "the", "Gumbel", "-", "Softmax", "distribution", "becomes", "cold", "and", "resembles", "the", "one", "-", "hot", "sample", ".", "Straight", "-", "Through", "(", "ST", ")", "Gumbel", "-", "Softmax", "estimator", "[", "reference", "]", ",", "whose", "name", "reminds", "of", "StraightThrough", "estimator", "(", "STE", ")", "[", "reference", "]", ",", "is", "a", "discrete", "version", "of", "the", "continuous", "GumbelSoftmax", "estimator", ".", "Similar", "to", "the", "STE", ",", "it", "maintains", "sparsity", "by", "taking", "different", "paths", "in", "the", "forward", "and", "backward", "propagation", ".", "Obviously", "ST", "estimators", "are", "biased", ",", "however", "they", "perform", "well", "in", "practice", ",", "according", "to", "several", "previous", "works", "[", "reference", "][", "reference", "]", "and", "our", "own", "result", ".", "In", "the", "forward", "pass", ",", "it", "discretizes", "a", "continuous", "probability", "vector", "y", "sampled", "from", "the", "Gumbel", "-", "Softmax", "distribution", "into", "the", "one", "-", "hot", "vector", "where", "And", "in", "the", "backward", "pass", "it", "simply", "uses", "the", "continuous", "y", ",", "thus", "the", "error", "signal", "is", "still", "able", "to", "backpropagate", ".", "See", "Figure", "1", "for", "the", "visualization", "of", "the", "forward", "and", "backward", "pass", ".", "ST", "Gumbel", "-", "Softmax", "estimator", "is", "useful", "when", "a", "model", "needs", "to", "utilize", "discrete", "values", "directly", ",", "for", "example", "in", "the", "case", "that", "a", "model", "alters", "its", "computation", "path", "based", "on", "samples", "drawn", "from", "a", "categorical", "distribution", ".", "section", ":", "Gumbel", "Tree", "-", "LSTM", "In", "our", "Gumbel", "Tree", "-", "LSTM", "model", ",", "an", "input", "sentence", "composed", "of", "N", "words", "is", "represented", "as", "a", "sequence", "of", "word", "vectors", "(", "x", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "x", "N", ")", ",", "where", "x", "i", "\u2208", "R", "Dx", ".", "Our", "basic", "model", "applies", "an", "affine", "transformation", "to", "each", "x", "i", "to", "obtain", "the", "initial", "hidden", "and", "cell", "state", ":", "which", "we", "call", "leaf", "transformation", ".", "In", "Eq", ".", "10", ",", "W", "leaf", "\u2208", "R", "2D", "h", "\u00d7Dx", "and", "b", "leaf", "\u2208", "R", "2D", "h", ".", "Note", "that", "we", "denote", "the", "representation", "of", "i", "-", "th", "node", "at", "t", "-", "th", "layer", "as", "r", ".", "Node", "representations", "which", "are", "not", "selected", "are", "copied", "to", "the", "corresponding", "positions", "at", "layer", "t", "+", "1", ".", "In", "other", "words", ",", "the", "(", "t", "+", "1", ")-", "th", "layer", "is", "composed", "of", "This", "procedure", "is", "repeated", "until", "the", "model", "reaches", "N", "-", "th", "layer", "and", "only", "a", "single", "node", "is", "left", ".", "It", "is", "notable", "that", "the", "property", "of", "selecting", "the", "best", "node", "pair", "at", "each", "stage", "resembles", "that", "of", "easy", "-", "first", "parsing", "[", "reference", "]", "Figure", "2", ":", "An", "example", "of", "the", "parent", "selection", ".", "At", "layer", "t", "(", "the", "bottom", "layer", ")", ",", "the", "model", "computes", "parent", "candidates", "(", "the", "middle", "layer", ")", ".", "Then", "the", "validity", "score", "of", "each", "candidate", "is", "computed", "using", "the", "query", "vector", "q", "(", "denoted", "as", "v", "1", ",", "v", "2", ",", "v", "3", ")", ".", "In", "the", "training", "time", ",", "the", "model", "samples", "a", "parent", "node", "among", "candidates", "weighted", "on", "v", "1", ",", "v", "2", ",", "v", "3", ",", "using", "ST", "Gumbel", "-", "Softmax", "estimator", ",", "and", "in", "the", "testing", "time", "the", "model", "selects", "the", "candidate", "with", "the", "highest", "validity", ".", "At", "layer", "t", "+", "1", "(", "the", "top", "layer", ")", ",", "the", "representation", "of", "the", "selected", "candidate", "(", "'", "the", "cat", "'", ")", "is", "used", "as", "a", "parent", ",", "and", "the", "rest", "are", "copied", "from", "those", "of", "layer", "t", "(", "'", "sat", "'", ",", "'", "on", "'", ")", ".", "Best", "viewed", "in", "color", ".", "Parent", "selection", ".", "Since", "information", "about", "the", "tree", "structure", "of", "an", "input", "is", "not", "given", "to", "the", "model", ",", "a", "special", "mechanism", "is", "needed", "for", "the", "model", "to", "learn", "to", "compose", "taskspecific", "tree", "structures", "in", "an", "end", "-", "to", "-", "end", "manner", ".", "We", "now", "describe", "the", "mechanism", "for", "building", "up", "the", "tree", "structure", "from", "an", "unstructured", "sentence", ".", "First", ",", "our", "model", "introduces", "the", "trainable", "composition", "query", "vector", "q", "\u2208", "R", "D", "h", ".", "The", "composition", "query", "vector", "measures", "how", "valid", "a", "representation", "is", ".", "Specifically", ",", "the", "validity", "score", "of", "a", "representation", "r", "=", "[", "h", ";", "c", "]", "is", "defined", "by", "q", "\u00b7", "h.", "At", "layer", "t", ",", "the", "model", "computes", "candidates", "for", "the", "parent", "representations", "using", "Eqs", ".", "1", "-", "3", ":", "(", "r", "In", "the", "training", "phase", ",", "the", "model", "samples", "a", "parent", "from", "candidates", "weighted", "on", "v", "i", ",", "using", "the", "ST", "Gumbel", "-", "Softmax", "estimator", "described", "above", ".", "Since", "the", "continuous", "GumbelSoftmax", "function", "is", "used", "in", "the", "backward", "pass", ",", "the", "error", "backpropagation", "signal", "safely", "passes", "through", "the", "sampling", "operation", ",", "hence", "the", "model", "is", "able", "to", "learn", "to", "construct", "the", "task", "-", "specific", "tree", "structures", "that", "minimize", "the", "loss", "by", "backpropagation", ".", "In", "the", "validation", "(", "or", "testing", ")", "phase", ",", "the", "model", "simply", "selects", "the", "parent", "which", "maximizes", "the", "validity", "score", ".", "An", "example", "of", "the", "parent", "selection", "is", "depicted", "in", "Figure", "2", ".", "LSTM", "-", "based", "leaf", "transformation", ".", "The", "basic", "leaf", "transformation", "using", "an", "affine", "transformation", "(", "Eq", ".", "10", ")", "does", "not", "consider", "information", "about", "the", "entire", "sentence", "of", "an", "input", "and", "thus", "the", "parent", "selection", "is", "performed", "based", "only", "on", "local", "information", ".", "SPINN", "[", "reference", "]", ")", "addresses", "this", "issue", "by", "using", "the", "tracking", "LSTM", "which", "sequentially", "reads", "input", "words", ".", "The", "tracking", "LSTM", "makes", "the", "SPINN", "model", "hybrid", ",", "where", "the", "model", "takes", "advantage", "of", "both", "tree", "-", "structured", "composition", "and", "sequential", "reading", ".", "However", ",", "the", "tracking", "LSTM", "is", "not", "applicable", "to", "our", "model", ",", "since", "our", "model", "does", "not", "use", "shift", "-", "reduce", "parsing", "or", "maintain", "a", "stack", ".", "In", "the", "tracking", "LSTM", "'s", "stead", ",", "our", "model", "applies", "an", "LSTM", "on", "input", "representations", "to", "give", "information", "about", "previous", "words", "to", "each", "leaf", "node", ":", "where", "h", "1", "0", "=", "c", "1", "0", "=", "0", ".", "From", "the", "experimental", "results", ",", "we", "validate", "that", "the", "LSTM", "applied", "to", "leaf", "nodes", "has", "a", "substantial", "gain", "over", "the", "basic", "leaf", "transformer", ".", "section", ":", "Experiments", "We", "evaluate", "performance", "of", "the", "proposed", "Gumbel", "Tree", "-", "LSTM", "model", "on", "two", "tasks", ":", "natural", "language", "inference", "and", "sentiment", "analysis", ".", "The", "implementation", "is", "made", "publicly", "available", ".", "section", ":", "2", "The", "detailed", "experimental", "settings", "are", "described", "in", "the", "supplementary", "material", ".", "section", ":", "Natural", "Language", "Inference", "Natural", "language", "inference", "(", "NLI", ")", "is", "a", "task", "of", "predicting", "the", "relationship", "between", "two", "sentences", "(", "hypothesis", "and", "premise", ")", ".", "In", "the", "Stanford", "Natural", "Language", "Inference", "(", "SNLI", ")", "dataset", "[", "reference", "]", ")", ",", "which", "we", "use", "for", "NLI", "experiments", ",", "a", "relationship", "is", "either", "contradiction", ",", "entailment", ",", "or", "neutral", ".", "For", "a", "model", "to", "correctly", "predict", "the", "relationship", "between", "two", "sentences", ",", "it", "should", "encode", "semantics", "of", "sentences", "accurately", ",", "thus", "the", "task", "has", "been", "used", "as", "one", "of", "standard", "tasks", "for", "evaluating", "the", "quality", "of", "sentence", "representations", ".", "The", "SNLI", "dataset", "is", "composed", "of", "about", "550", ",", "000", "sentences", ",", "each", "of", "which", "is", "binary", "-", "parsed", ".", "However", ",", "since", "our", "model", "operate", "on", "plain", "text", ",", "we", "do", "not", "use", "the", "parse", "tree", "information", "in", "both", "training", "and", "testing", ".", "The", "classifier", "architecture", "used", "in", "our", "SNLI", "experiments", "follows", "[", "reference", "][", "reference", "]", ".", "Given", "the", "premise", "sentence", "vector", "(", "h", "pre", ")", "and", "the", "hypothesis", "sentence", "vector", "(", "h", "hyp", ")", "which", "are", "encoded", "by", "the", "proposed", "Gumbel", "Tree", "-", "LSTM", "model", ",", "the", "probability", "of", "relationship", "r", "\u2208", "{", "entailment", ",", "contradiction", ",", "neutral", "}", "is", "computed", "by", "the", "following", "equations", ":", "[", "reference", "]", "followed", "by", "dropout", "[", "reference", "]", ")", "with", "probability", "0.1", "to", "the", "input", "and", "the", "output", "of", "the", "MLP", ".", "We", "also", "apply", "dropout", "on", "the", "word", "vectors", "with", "probability", "0.1", ".", "Similar", "to", "100D", "experiments", ",", "we", "initialize", "the", "word", "embedding", "matrix", "with", "GloVe", "300D", "pretrained", "vectors", "4", ",", "however", "we", "do", "not", "update", "the", "word", "representations", "during", "training", ".", "Since", "our", "model", "converges", "relatively", "fast", ",", "it", "is", "possible", "to", "train", "a", "model", "of", "larger", "size", "in", "a", "reasonable", "time", ".", "In", "the", "600D", "experiment", ",", "we", "set", "D", "x", "=", "300", ",", "D", "h", "=", "600", ",", "and", "an", "MLP", "with", "three", "hidden", "layers", "(", "D", "c", "=", "1024", ")", "is", "used", ".", "The", "dropout", "probability", "is", "set", "to", "0.2", "and", "word", "embeddings", "are", "not", "updated", "during", "training", ".", "The", "size", "of", "mini", "-", "batches", "is", "set", "to", "128", "in", "all", "experiments", ",", "and", "hyperparameters", "are", "tuned", "using", "the", "validation", "split", ".", "The", "temperature", "parameter", "\u03c4", "of", "Gumbel", "-", "Softmax", "is", "set", "to", "1.0", ",", "and", "we", "did", "not", "find", "that", "temperature", "annealing", "improves", "performance", ".", "For", "training", "models", ",", "Adam", "optimizer", "is", "used", ".", "The", "results", "of", "SNLI", "experiments", "are", "summarized", "in", "Table", "1", ".", "First", ",", "we", "can", "see", "that", "LSTM", "-", "based", "leaf", "transformation", "has", "a", "clear", "advantage", "over", "the", "affine", "-", "transformation", "-", "based", "one", ".", "It", "improves", "the", "performance", "substantially", "and", "also", "leads", "to", "faster", "convergence", ".", "Secondly", ",", "comparing", "ours", "with", "other", "models", ",", "we", "find", "that", "our", "100D", "and", "300D", "model", "outperform", "all", "other", "models", "of", "similar", "numbers", "of", "parameters", ".", "Our", "600D", "model", "achieves", "the", "accuracy", "of", "86.0", "%", ",", "which", "is", "comparable", "to", "that", "of", "the", "state", "-", "of", "-", "the", "-", "art", "model", "[", "reference", "]", ",", "while", "using", "far", "less", "parameters", ".", "It", "is", "also", "worth", "noting", "that", "our", "models", "converge", "much", "faster", "than", "other", "models", ".", "All", "of", "our", "models", "converged", "within", "a", "few", "hours", "on", "a", "machine", "with", "NVIDIA", "Titan", "Xp", "GPU", ".", "We", "also", "plot", "validation", "accuracies", "of", "various", "models", "during", "first", "5", "training", "epochs", "in", "Figure", "3", ",", "and", "validate", "that", "our", "Model", "SST", "-", "2", "(", "%", ")", "SST", "-", "5", "(", "%", ")", "DMN", "[", "reference", "]", "88.6", "52.1", "NSE", "[", "reference", "]", "89.7", "52.8", "byte", "-", "mLSTM", "[", "reference", "]", "91.8", "52.9", "BCN", "+", "Char", "+", "CoVe", "[", "reference", "]", "90.3", "53.7", "RNTN", "[", "reference", "]", "85.4", "45.7", "Constituency", "Tree", "-", "LSTM", "[", "reference", "]", "88.0", "51.0", "NTI", "-", "SLSTM", "-", "LSTM", "[", "reference", "]", "89.3", "53.1", "Latent", "Syntax", "Tree", "-", "LSTM", "86.5", "-", "Constituency", "Tree", "-", "LSTM", "+", "Recurrent", "Dropout", "[", "reference", "]", "89.4", "52.3", "Gumbel", "Tree", "-", "LSTM", "(", "Ours", ")", "90.7", "53.7", "Table", "2", ":", "Results", "of", "SST", "experiments", ".", "The", "bottom", "section", "contains", "results", "of", "RvNN", "-", "based", "models", ".", "Underlined", "score", "indicates", "the", "best", "among", "RvNN", "-", "based", "models", ".", "models", "converge", "significantly", "faster", "than", "others", ",", "not", "only", "in", "terms", "of", "total", "training", "time", "but", "also", "in", "the", "number", "of", "iterations", ".", "section", ":", "5", "section", ":", "Sentiment", "Analysis", "To", "evaluate", "the", "performance", "of", "our", "model", "in", "single", "-", "sentence", "classification", ",", "we", "conducted", "experiments", "on", "Stanford", "Sentiment", "Treebank", "(", "SST", ")", "[", "reference", "]", "dataset", ".", "In", "the", "SST", "dataset", ",", "each", "sentence", "is", "represented", "as", "a", "binary", "parse", "tree", ",", "and", "each", "subtree", "of", "a", "parse", "tree", "is", "annotated", "with", "the", "corresponding", "sentiment", "score", ".", "Following", "the", "experimental", "setting", "of", "previous", "works", ",", "we", "use", "all", "subtrees", "and", "their", "labels", "for", "training", ",", "and", "only", "the", "root", "labels", "are", "used", "for", "evaluation", ".", "The", "classifier", "has", "a", "similar", "architecture", "to", "SNLI", "experiments", ".", "Specifically", ",", "for", "a", "sentence", "embedding", "h", ",", "the", "probability", "for", "the", "sentence", "to", "be", "predicted", "as", "label", "s", "\u2208", "{", "0", ",", "1", "}", "(", "in", "the", "binary", "setting", ",", "SST", "-", "2", ")", "or", "s", "\u2208", "{", "1", ",", "2", ",", "3", ",", "4", ",", "5", "}", "(", "in", "the", "fine", "-", "grained", "setting", ",", "SST", "-", "5", ")", "is", "computed", "as", "follows", ":", ",", "and", "\u03a6", "is", "a", "single", "-", "hidden", "layer", "MLP", "with", "the", "ReLU", "activation", "function", ".", "Note", "that", "subtrees", "labeled", "as", "neutral", "are", "ignored", "in", "the", "binary", "setting", "in", "both", "training", "and", "evaluation", ".", "We", "trained", "our", "SST", "-", "2", "model", "with", "hyperparameters", "D", "x", "=", "300", ",", "D", "h", "=", "300", ",", "D", "c", "=", "300", ".", "The", "word", "vectors", "are", "initialized", "with", "GloVe", "300D", "pretrained", "vectors", "and", "fine", "-", "tuned", "during", "training", ".", "We", "apply", "dropout", "(", "p", "=", "0.5", ")", "on", "the", "output", "of", "the", "word", "embedding", "layer", "and", "the", "input", "and", "the", "output", "of", "the", "MLP", "layer", ".", "The", "size", "of", "mini", "-", "batches", "is", "set", "to", "32", "and", "Adadelta", "optimizer", "is", "used", "for", "optimization", ".", "For", "our", "SST", "-", "5", "model", ",", "hyperparameters", "are", "set", "to", "D", "x", "=", "300", ",", "D", "h", "=", "300", ",", "D", "c", "=", "1024", ".", "Similar", "to", "the", "SST", "-", "2", "model", ",", "we", "optimize", "the", "model", "using", "Adadelta", "optimizer", "with", "batch", "size", "64", "and", "apply", "dropout", "with", "p", "=", "0.5", ".", "Table", "2", "summarizes", "the", "results", "of", "SST", "experiments", ".", "Our", "SST", "-", "2", "model", "outperforms", "all", "other", "models", "substantially", "[", "reference", "]", "In", "the", "figure", ",", "our", "models", "and", "300D", "NSE", "are", "trained", "with", "batch", "size", "128", ".", "100D", "CYK", "and", "300D", "SPINN", "are", "trained", "with", "batch", "size", "16", "and", "32", "respectively", ",", "as", "in", "the", "original", "papers", ".", "We", "observed", "that", "our", "models", "still", "converge", "faster", "than", "others", "when", "a", "smaller", "batch", "size", "(", "16", "or", "32", ")", "is", "used", ".", "except", "byte", "-", "mLSTM", "(", "Radford", ",", "Jozefowicz", ",", "and", "Sutskever", "2017", ")", ",", "where", "a", "byte", "-", "level", "language", "model", "trained", "on", "the", "large", "product", "review", "dataset", "is", "used", "to", "obtain", "sentence", "representations", ".", "We", "also", "see", "that", "the", "performance", "of", "our", "SST", "-", "5", "model", "is", "on", "par", "with", "that", "of", "the", "current", "state", "-", "of", "-", "the", "-", "art", "model", "[", "reference", "]", ",", "which", "is", "pretrained", "on", "large", "parallel", "datasets", "and", "uses", "character", "n", "-", "gram", "embeddings", "alongside", "word", "embeddings", ",", "even", "though", "our", "model", "does", "not", "utilize", "external", "resources", "other", "than", "GloVe", "vectors", "and", "only", "uses", "wordlevel", "representations", ".", "The", "authors", "of", "[", "reference", "]", "stated", "that", "utilizing", "pretraining", "and", "character", "n", "-", "gram", "embeddings", "improves", "validation", "accuracy", "by", "2.8", "%", "(", "SST", "-", "2", ")", "or", "1.7", "%", "(", "SST", "-", "5", ")", ".", "In", "addition", ",", "from", "the", "fact", "that", "our", "models", "substantially", "outperform", "all", "other", "RvNN", "-", "based", "models", ",", "we", "conjecture", "that", "task", "-", "specific", "tree", "structures", "built", "by", "our", "model", "help", "encode", "sentences", "into", "vectors", "more", "efficiently", "than", "constituency", "-", "based", "or", "dependency", "-", "based", "parse", "trees", "do", ".", "section", ":", "Qualitative", "Analysis", "We", "conduct", "a", "set", "of", "experiments", "to", "observe", "various", "properties", "of", "our", "trained", "models", ".", "First", ",", "to", "see", "how", "well", "the", "model", "encodes", "sentences", "with", "similar", "meaning", "or", "syntax", "into", "close", "vectors", ",", "we", "find", "nearest", "neighbors", "of", "a", "query", "sentence", ".", "Second", ",", "to", "validate", "that", "the", "trained", "composition", "functions", "are", "non", "-", "trivial", "and", "task", "-", "specific", ",", "we", "visualize", "trees", "composed", "by", "SNLI", "and", "SST", "model", "given", "identical", "sentence", ".", "Nearest", "neighbors", "We", "encode", "sentences", "in", "the", "test", "split", "of", "SNLI", "dataset", "using", "the", "trained", "300D", "model", "and", "find", "nearest", "neighbors", "given", "a", "query", "sentence", ".", "Table", "3", "presents", "five", "nearest", "neighbors", "for", "each", "selected", "query", "sentence", ".", "In", "finding", "nearest", "neighbors", ",", "cosine", "distance", "is", "used", "as", "metric", ".", "The", "result", "shows", "that", "our", "model", "effectively", "maps", "similar", "sentences", "into", "vectors", "close", "to", "each", "other", ";", "the", "neighboring", "sentences", "are", "similar", "to", "a", "query", "sentence", "not", "only", "in", "terms", "of", "word", "overlap", ",", "but", "also", "in", "semantics", ".", "For", "example", "in", "the", "second", "column", ",", "the", "nearest", "sentence", "is", "'", "the", "woman", "is", "looking", "at", "a", "dog", "'", ",", "whose", "meaning", "is", "almost", "same", "as", "the", "query", "sentence", ".", "We", "can", "also", "see", "that", "other", "neighbors", "partially", "share", "semantics", "with", "the", "query", "sentence", ".", "#", "sunshine", "is", "on", "a", "man", "'s", "face", ".", "a", "girl", "is", "staring", "at", "a", "dog", ".", "the", "woman", "is", "wearing", "boots", ".", "1", "a", "man", "is", "walking", "on", "sunshine", ".", "the", "woman", "is", "looking", "at", "a", "dog", ".", "the", "girl", "is", "wearing", "shoes", "2", "a", "guy", "is", "in", "a", "hot", ",", "sunny", "place", "a", "girl", "takes", "a", "photo", "of", "a", "dog", ".", "a", "person", "is", "wearing", "boots", ".", "3", "a", "man", "is", "working", "in", "the", "sun", ".", "a", "girl", "is", "petting", "her", "dog", ".", "the", "woman", "is", "wearing", "jeans", ".", "4", "it", "is", "sunny", ".", "a", "man", "is", "taking", "a", "picture", "of", "a", "dog", ",", "while", "a", "woman", "watches", ".", "a", "woman", "wearing", "sunglasses", ".", "5", "a", "man", "enjoys", "the", "sun", "coming", "through", "the", "window", ".", "a", "woman", "is", "playing", "with", "her", "dog", ".", "the", "woman", "is", "wearing", "a", "vest", ".", "Tree", "examples", "Figure", "4", "show", "that", "two", "models", "(", "300D", "SNLI", "and", "SST", "-", "2", ")", "generate", "different", "tree", "structures", "given", "an", "identical", "sentence", ".", "In", "Figure", "4a", "and", "4b", ",", "the", "SNLI", "model", "groups", "the", "phrase", "'", "i", "love", "this", "'", "first", ",", "while", "the", "SST", "model", "groups", "'", "this", "very", "much", "'", "first", ".", "Figure", "4c", "and", "4d", "present", "how", "differently", "the", "two", "models", "process", "a", "sentence", "containing", "relative", "pronoun", "'", "which", "'", ".", "It", "is", "intriguing", "that", "the", "models", "compose", "visually", "plausible", "tree", "structures", ",", "where", "the", "sentence", "is", "divided", "into", "two", "phrases", "by", "relative", "pronoun", ",", "even", "though", "they", "are", "trained", "without", "explicit", "parse", "trees", ".", "We", "hypothesize", "that", "these", "examples", "demonstrate", "that", "each", "model", "generates", "a", "distinct", "tree", "structure", "based", "on", "semantic", "properties", "of", "the", "task", "and", "learns", "non", "-", "trivial", "tree", "composition", "scheme", ".", "section", ":", "Conclusion", "In", "this", "paper", ",", "we", "propose", "Gumbel", "Tree", "-", "LSTM", ",", "a", "novel", "Tree", "-", "LSTM", "-", "based", "architecture", "that", "learns", "to", "compose", "taskspecific", "tree", "structures", ".", "Our", "model", "introduces", "the", "composition", "query", "vector", "to", "compute", "validity", "of", "the", "candidate", "parents", "and", "selects", "the", "appropriate", "parent", "according", "to", "validity", "scores", ".", "In", "training", "time", ",", "the", "model", "samples", "the", "parent", "from", "candidates", "using", "ST", "Gumbel", "-", "Softmax", "estimator", ",", "hence", "it", "is", "able", "to", "be", "trained", "by", "standard", "backpropagation", "while", "maintaining", "its", "property", "of", "discretely", "determining", "the", "computation", "path", "in", "forward", "propagation", ".", "From", "experiments", ",", "we", "validate", "that", "our", "model", "outperforms", "all", "other", "RvNN", "models", "and", "is", "competitive", "to", "state", "-", "ofthe", "-", "art", "models", ",", "and", "also", "observed", "that", "our", "model", "converges", "faster", "than", "other", "complex", "models", ".", "The", "result", "poses", "an", "important", "question", ":", "what", "is", "the", "optimal", "input", "structure", "for", "RvNN", "?", "We", "empirically", "showed", "that", "the", "optimal", "structure", "might", "differ", "per", "task", ",", "and", "investigating", "task", "-", "specific", "latent", "tree", "structures", "could", "be", "an", "interesting", "future", "research", "direction", ".", "For", "future", "work", ",", "we", "plan", "to", "apply", "the", "core", "idea", "beyond", "sentence", "encoding", ".", "The", "performance", "could", "be", "further", "improved", "by", "applying", "intra", "-", "sentence", "or", "inter", "-", "sentence", "attention", "mechanisms", ".", "We", "also", "plan", "to", "design", "an", "architecture", "that", "generates", "sentences", "using", "recursive", "structures", ".", "section", ":", "Implementation", "Details", "Implementation", "-", "wise", ",", "we", "used", "multiple", "mask", "matrices", "in", "implementing", "the", "proposed", "Gumbel", "Tree", "-", "LSTM", "model", ".", "Using", "the", "mask", "matrices", ",", "Eq", ".", "11", "can", "be", "rewritten", "as", "a", "single", "equation", ":", ",", "and", "r", "is", "a", "matrix", "whose", "columns", "are", "r", "The", "mask", "matrices", "are", "defined", "by", "the", "following", "equations", ".", "is", "a", "vector", "which", "will", "be", "defined", "below", ",", "and", "1", "\u2208", "R", "Mt", "+", "1", "is", "a", "vector", "whose", "values", "are", "all", "ones", ".", "In", "the", "forward", "pass", ",", "\u0233", "1:Mt", "+", "1", "is", "defined", "by", "a", "one", "-", "hot", "vector", "y", "ST", "1:Mt", "+", "1", ",", "which", "is", "sampled", "from", "the", "categorical", "distribution", "of", "validity", "scores", "(", "v", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "v", "Mt", "+", "1", ")", "using", "Gumbel", "-", "Max", "trick", ".", "is", "added", "when", "calculating", "g", "i", "for", "numerical", "stability", ".", "In", "the", "backward", "pass", ",", "instead", "of", "the", "one", "-", "hot", "version", ",", "the", "continuous", "vector", "y", "1:Mt", "+", "1", "obtained", "from", "Gumbel", "-", "Softmax", "is", "used", "as\u0233", "1:Mt", "+", "1", ".", "Note", "that", "the", "Gumbel", "noise", "samples", "g", "1", ",", "\u00b7", "\u00b7", "\u00b7", ",", "g", "Mt", "+", "1", "drawn", "in", "the", "forward", "pass", "are", "reused", "in", "the", "backward", "pass", "(", "i.e.", "noise", "values", "are", "not", "resampled", "in", "the", "backward", "pass", ")", ".", "In", "typical", "deep", "learning", "libraries", "supporting", "automatic", "differentiation", "(", "e.g.", "PyTorch", ",", "TensorFlow", ")", ",", "this", "discrepancy", "between", "forward", "and", "backward", "pass", "can", "be", "implemented", "as", "y", "1:Mt", "+", "1", "=", "detach", "(", "y", "ST", "1:Mt", "+", "1", "\u2212", "y", "1:Mt", "+", "1", ")", "+", "y", "1:Mt", "+", "1", ",", "(", "S11", ")", "where", "detach", "(", "\u00b7", ")", "is", "a", "function", "that", "prevents", "error", "from", "backpropagating", "through", "its", "input", ".", "section", ":", "Detailed", "Experimental", "Settings", "All", "experiments", "are", "conducted", "using", "the", "publicized", "codebase", ".", "section", ":", "1", "section", ":", "SNLI", "The", "composition", "query", "vector", "is", "initialized", "by", "sampling", "from", "Gaussian", "distribution", "N", "(", "0", ",", "0.01", "2", ")", ".", "The", "last", "linear", "transformation", "that", "outputs", "the", "unnormalized", "log", "probability", "for", "each", "class", "is", "initialized", "by", "sampling", "from", "uniform", "distribution", "U", "(", "\u22120.005", ",", "0.005", ")", ".", "All", "other", "parameters", "are", "initialized", "following", "the", "scheme", "proposed", "by", "[", "reference", "]", ".", "We", "used", "Adam", "optimizer", "with", "default", "hyperparameters", "and", "halved", "learning", "rate", "if", "there", "is", "no", "improvement", "in", "accuracy", "for", "one", "epoch", ".", "The", "size", "of", "minibatch", "is", "set", "to", "128", "in", "all", "experiments", ".", "In", "100D", "experiments", "(", "D", "x", "=", "D", "h", "=", "100", ",", "D", "c", "=", "200", ",", "single", "-", "hidden", "layer", "MLP", "classifier", ")", ",", "GloVe", "(", "6B", ",", "100D", ")", "pretrained", "word", "embeddings", "are", "used", "in", "initializing", "word", "representations", ".", "We", "fine", "-", "tuned", "word", "embedding", "parameters", "during", "training", ".", "In", "300D", "(", "D", "x", "=", "D", "h", "=", "300", ",", "D", "c", "=", "1024", ",", "single", "-", "hidden", "layer", "MLP", "classifier", ")", "and", "600D", "(", "D", "x", "=", "300", ",", "D", "h", "=", "600", ",", "D", "c", "=", "1024", ",", "MLP", "classifier", "with", "three", "hidden", "layers", ")", "experiments", ",", "GloVe", "(", "840B", ",", "300D", ")", "pretrained", "word", "embeddings", "are", "used", "as", "word", "representations", "and", "fixed", "during", "training", ".", "Batch", "normalization", "is", "applied", "before", "the", "input", "and", "after", "the", "output", "of", "the", "MLP", ".", "Dropout", "is", "applied", "to", "word", "embeddings", "and", "the", "input", "and", "the", "output", "of", "the", "MLP", "with", "dropout", "probability", "0.1", "(", "300D", ")", "or", "0.2", "(", "600D", ")", ".", "section", ":", "SST", "The", "composition", "query", "vector", "is", "initialized", "by", "sampling", "from", "Gaussian", "distribution", "N", "(", "0", ",", "0.01", "2", ")", ".", "The", "last", "linear", "transformation", "that", "outputs", "the", "unnormalized", "log", "probability", "for", "each", "class", "is", "initialized", "by", "sampling", "from", "uniform", "distribution", "U", "(", "\u22120.002", ",", "0.002", ")", ".", "All", "other", "parameters", "are", "initialized", "following", "the", "scheme", "proposed", "by", "[", "reference", "]", ".", "We", "used", "Adadelta", "optimizer", ")", "with", "default", "hyperparameters", "and", "halved", "learning", "rate", "if", "there", "is", "no", "improvement", "in", "accuracy", "for", "two", "epochs", ".", "In", "both", "SST", "-", "2", "and", "SST", "-", "5", "experiments", ",", "we", "set", "D", "x", "=", "D", "h", "=", "300", ",", "used", "GloVe", "(", "840B", ",", "300D", ")", "pretrained", "vectors", "with", "fine", "-", "tuning", ",", "and", "single", "-", "hidden", "layer", "MLP", "is", "used", "as", "classifier", ".", "Dropout", "is", "applied", "to", "word", "embeddings", "and", "the", "input", "and", "the", "output", "of", "the", "MLP", "classifier", "with", "probability", "0.5", ".", "In", "the", "SST", "-", "2", "experiment", ",", "we", "set", "D", "c", "to", "300", "and", "set", "batch", "size", "to", "32", ".", "In", "the", "SST", "-", "5", "experiment", ",", "D", "c", "is", "increased", "to", "1024", ",", "and", "mini", "-", "batches", "of", "64", "sentences", "are", "fed", "to", "the", "model", "during", "training", ".", "section", ":", "section", ":", "Acknowledgments", "This", "work", "is", "part", "of", "SNU", "-", "Samsung", "smart", "campus", "research", "program", ",", "which", "is", "supported", "by", "Samsung", "Electronics", ".", "The", "authors", "would", "like", "to", "thank", "anonymous", "reviewers", "for", "valuable", "comments", "and", "Volkan", "Cirik", "for", "helpful", "feedback", "on", "the", "early", "version", "of", "the", "manuscript", ".", "section", ":", "section", ":", "Appendix", "The", "supplementary", "material", "is", "available", "at", "https:", "//", "github.com", "/", "jihunchoi", "/", "unsupervised", "-", "treelstm", "/", "blob", "/", "master", "/", "aaai18", "/", "supp.pdf", ".", "section", ":"]}