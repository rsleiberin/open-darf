{"coref": {"CoNLL_2005": [[207, 211], [909, 912], [941, 944], [3822, 3825], [4034, 4037], [4574, 4577], [4638, 4641], [4755, 4758], [4868, 4871], [4899, 4902], [5882, 5885], [5909, 5912], [6156, 6159], [1072, 1075], [4226, 4229], [5557, 5560], [5766, 5769], [5831, 5834], [6160, 6163]], "CoNLL_2012": [[272, 277], [913, 917], [1020, 1023], [3830, 3835], [4560, 4565], [4759, 4762], [4872, 4875], [5886, 5889], [5993, 5996]], "ELMo": [[302, 310], [1617, 1619], [3507, 3508], [3534, 3537], [3876, 3878], [4123, 4125], [4352, 4354], [4642, 4644], [4666, 4667], [4689, 4690], [4724, 4726], [4773, 4777], [4809, 4811], [4882, 4883], [5959, 5961], [5969, 5971]], "F1": [[236, 237], [255, 256], [286, 287], [314, 315], [321, 322], [1009, 1010], [1060, 1061], [1080, 1081], [4223, 4224], [4345, 4346], [4400, 4401], [4423, 4424], [4541, 4542], [4557, 4558], [4592, 4593], [4635, 4636], [4718, 4719], [4865, 4866], [4922, 4923], [5089, 5090], [5127, 5128], [975, 976], [1030, 1031], [4302, 4303], [4362, 4363], [4405, 4406], [4650, 4651], [4911, 4912], [4940, 4941], [4974, 4975], [4989, 4990], [5122, 5123], [5647, 5648], [5846, 5847]], "LISA": [[71, 77], [78, 79], [128, 129], [212, 213], [288, 289], [666, 672], [673, 674], [1138, 1139], [2102, 2106], [3526, 3527], [4172, 4173], [4179, 4180], [4269, 4270], [4316, 4317], [4376, 4377], [4390, 4391], [4431, 4432], [4514, 4515], [4524, 4525], [4599, 4600], [4826, 4827], [4878, 4879], [4913, 4914], [4999, 5000], [5341, 5347], [5366, 5367], [5576, 5577], [5611, 5612], [5670, 5671], [3973, 3974], [4287, 4288], [4337, 4338], [4366, 4367], [4619, 4620], [4656, 4657], [4788, 4789], [4890, 4891], [5037, 5038], [5396, 5397]], "LISA___ELMo": [], "Predicate_Detection": [[109, 111], [149, 151], [1368, 1372], [2724, 2726], [2757, 2760], [2791, 2793], [3439, 3441], [4262, 4264], [4731, 4735]], "Semantic_Role_Labeling": [[9, 12], [20, 23], [24, 25], [50, 52], [112, 113], [152, 154], [201, 203], [333, 336], [337, 338], [403, 405], [437, 438], [532, 533], [825, 826], [1410, 1411], [1554, 1555], [1637, 1639], [2727, 2728], [2750, 2752], [3262, 3263], [3320, 3321], [3327, 3328], [3344, 3345], [3421, 3422], [3449, 3450], [3556, 3557], [4063, 4065], [4170, 4171], [4201, 4203], [4211, 4214], [4267, 4268], [5330, 5331], [5362, 5365], [5381, 5382], [5817, 5818], [5902, 5903], [6238, 6239], [564, 565], [1136, 1137], [3472, 3473], [3487, 3488], [4361, 4362], [4837, 4838], [4963, 4964], [4973, 4974], [4988, 4989], [5121, 5122], [5646, 5647]], "Semantic_Role_Labeling__predicted_predicates_": [[2876, 2879], [3294, 3295]]}, "coref_non_salient": {"0": [[5824, 5828], [5866, 5869]], "1": [[1459, 1463], [2945, 2949]], "10": [[58, 59], [505, 506], [556, 559], [3529, 3530], [4379, 4381], [4707, 4709], [4823, 4825], [4979, 4981]], "100": [[1505, 1507]], "101": [[377, 379]], "102": [[589, 593]], "103": [[3772, 3774]], "104": [[3443, 3444]], "105": [[6370, 6375]], "106": [[2939, 2943]], "107": [[1155, 1158]], "108": [[1340, 1343]], "109": [[5931, 5932]], "11": [[2846, 2850], [4739, 4744], [4916, 4919], [5853, 5856]], "110": [[3308, 3311]], "111": [[3704, 3705]], "112": [[4745, 4746]], "113": [[3806, 3809]], "114": [[3400, 3402]], "115": [[901, 904]], "116": [[451, 457]], "117": [[389, 390]], "118": [[2097, 2099]], "119": [[2546, 2547]], "12": [[1378, 1382], [3734, 3736], [6461, 6470]], "120": [[2305, 2307]], "121": [[5762, 5764]], "122": [[94, 98], [680, 684], [1162, 1166], [2732, 2736], [4956, 4960]], "123": [[3226, 3227]], "124": [[2153, 2156]], "125": [[1352, 1353], [3244, 3245], [6254, 6255]], "126": [[1587, 1591]], "127": [[2611, 2614]], "128": [[3231, 3233]], "129": [[4307, 4315]], "13": [[99, 101], [147, 148], [718, 719], [2721, 2723], [3317, 3319], [3329, 3331], [3775, 3779], [4729, 4730], [5863, 5864], [5933, 5935]], "14": [[1811, 1815], [6429, 6434]], "15": [[87, 93], [686, 695], [1208, 1214], [1800, 1806]], "16": [[3204, 3205], [6263, 6264]], "17": [[1050, 1055], [1201, 1206]], "18": [[1508, 1511], [6267, 6271]], "19": [[2638, 2642], [2737, 2738], [3219, 3220], [3643, 3644], [3798, 3799], [3654, 3655]], "2": [[3117, 3120], [3121, 3126]], "20": [[2211, 2216], [4417, 4418]], "21": [[4859, 4862], [5839, 5843]], "22": [[6289, 6291], [6297, 6299], [6329, 6331], [6355, 6358], [6394, 6396]], "23": [[269, 270], [1017, 1018]], "24": [[3726, 3730]], "25": [[1905, 1909], [2258, 2260]], "26": [[28, 31], [410, 414]], "27": [[4796, 4803], [5016, 5017], [5048, 5049], [5093, 5094], [5692, 5694]], "28": [[1520, 1525], [1532, 1540]], "29": [[4219, 4220], [4221, 4222], [4553, 4554], [4555, 4556], [4863, 4864], [5844, 5845]], "3": [[2, 8], [1817, 1822], [3706, 3709]], "30": [[1873, 1877], [2017, 2022], [2040, 2044]], "31": [[3312, 3314], [3737, 3740]], "32": [[526, 531], [753, 758], [929, 932]], "33": [[5476, 5479]], "34": [[5631, 5634]], "35": [[231, 233], [949, 951], [1198, 1200], [1605, 1607], [3349, 3351], [3520, 3522], [4070, 4071], [5794, 5796]], "36": [[549, 551], [3289, 3291]], "37": [[3957, 3961]], "38": [[1650, 1653], [4495, 4498]], "39": [[1750, 1752], [2122, 2124]], "4": [[416, 420], [921, 925], [2836, 2840]], "40": [[3347, 3348], [3573, 3575]], "41": [[102, 108], [1412, 1416], [2718, 2720], [2788, 2790]], "42": [[1676, 1680], [1698, 1701], [1847, 1850], [2130, 2134], [3409, 3412], [4008, 4012], [6362, 6366], [6412, 6416]], "43": [[2525, 2527], [3028, 3030]], "44": [[1885, 1889], [2239, 2243]], "45": [[586, 587], [2552, 2554], [2598, 2599], [4273, 4274], [5732, 5733]], "46": [[2842, 2844], [3282, 3284]], "47": [[6277, 6278]], "48": [[3912, 3914]], "49": [[3108, 3112]], "5": [[762, 765], [3992, 3994], [3995, 3996]], "50": [[2310, 2312]], "51": [[4519, 4523], [4699, 4703], [4844, 4848], [5075, 5081]], "52": [[3038, 3040], [3403, 3405]], "53": [[2771, 2772]], "54": [[60, 64], [441, 445]], "55": [[5140, 5142]], "56": [[3865, 3871], [5798, 5803]], "57": [[2851, 2852]], "58": [[801, 802], [2960, 2961]], "59": [[1597, 1599], [1712, 1714], [1790, 1792], [2065, 2067], [2935, 2937]], "6": [[3042, 3044], [5573, 5575], [5604, 5605]], "60": [[2621, 2625], [2775, 2779]], "61": [[4814, 4819]], "62": [[4240, 4242], [4491, 4493], [4579, 4581], [4769, 4770]], "63": [[3756, 3757]], "64": [[1720, 1728], [4277, 4278], [4876, 4877], [5045, 5046], [5064, 5065], [4019, 4020], [4247, 4248], [4582, 4583]], "65": [[3896, 3897]], "66": [[359, 361], [3504, 3506]], "67": [[1418, 1420]], "68": [[4475, 4477]], "69": [[3794, 3796]], "7": [[873, 876], [897, 899], [4751, 4753]], "70": [[4805, 4806]], "71": [[2714, 2716]], "72": [[4113, 4116]], "73": [[6281, 6282]], "74": [[3315, 3316]], "75": [[748, 750]], "76": [[482, 486]], "77": [[5403, 5405]], "78": [[3648, 3649], [3770, 3771], [5436, 5437]], "79": [[6401, 6403]], "8": [[1465, 1467], [2906, 2908], [2957, 2959]], "80": [[6454, 6456]], "81": [[2503, 2507]], "82": [[6131, 6133]], "83": [[3763, 3765]], "84": [[4778, 4781], [4828, 4831], [5103, 5107], [5578, 5581], [5982, 5986]], "85": [[383, 385]], "86": [[82, 85], [498, 501], [640, 644], [1114, 1117], [3340, 3343], [3365, 3367], [3384, 3386], [3553, 3555], [5352, 5355]], "87": [[4842, 4843]], "88": [[569, 573]], "89": [[3491, 3495]], "9": [[1215, 1219], [1729, 1733], [2088, 2092], [2113, 2116]], "90": [[3357, 3361]], "91": [[617, 621], [3623, 3627], [4322, 4326], [4605, 4609]], "92": [[1944, 1949]], "93": [[6025, 6032]], "94": [[2352, 2358]], "95": [[6272, 6274]], "96": [[2667, 2673]], "97": [[1151, 1154]], "98": [[1190, 1192]], "99": [[373, 375]]}, "doc_id": "060ff1aad5619a7d6d6cdfaf8be5da29bff3808c", "method_subrelations": {"LISA": [[[0, 4], "LISA"]], "LISA___ELMo": [[[0, 4], "LISA"], [[7, 11], "ELMo"]]}, "n_ary_relations": [{"Material": "CoNLL_2005", "Method": "LISA", "Metric": "F1", "Task": "Predicate_Detection", "score": "98.4"}, {"Material": "CoNLL_2005", "Method": "LISA", "Metric": "F1", "Task": "Semantic_Role_Labeling", "score": "86.04"}, {"Material": "CoNLL_2005", "Method": "LISA", "Metric": "F1", "Task": "Semantic_Role_Labeling__predicted_predicates_", "score": "84.99"}, {"Material": "CoNLL_2005", "Method": "LISA___ELMo", "Metric": "F1", "Task": "Semantic_Role_Labeling__predicted_predicates_", "score": "86.90"}, {"Material": "CoNLL_2012", "Method": "LISA", "Metric": "F1", "Task": "Predicate_Detection", "score": "97.2"}, {"Material": "CoNLL_2012", "Method": "LISA", "Metric": "F1", "Task": "Semantic_Role_Labeling__predicted_predicates_", "score": "82.33"}, {"Material": "CoNLL_2012", "Method": "LISA___ELMo", "Metric": "F1", "Task": "Semantic_Role_Labeling__predicted_predicates_", "score": "83.38"}], "ner": [[2, 8, "Task"], [9, 12, "Task"], [20, 23, "Task"], [24, 25, "Task"], [28, 31, "Method"], [50, 52, "Task"], [58, 59, "Metric"], [60, 64, "Method"], [71, 77, "Method"], [78, 79, "Method"], [82, 85, "Method"], [87, 93, "Method"], [94, 98, "Method"], [99, 101, "Task"], [102, 108, "Task"], [109, 111, "Task"], [112, 113, "Task"], [128, 129, "Method"], [147, 148, "Task"], [149, 151, "Task"], [152, 154, "Task"], [201, 203, "Task"], [207, 211, "Material"], [212, 213, "Method"], [231, 233, "Method"], [236, 237, "Metric"], [255, 256, "Metric"], [269, 270, "Metric"], [272, 277, "Material"], [286, 287, "Metric"], [288, 289, "Method"], [302, 310, "Method"], [314, 315, "Metric"], [321, 322, "Metric"], [333, 336, "Task"], [337, 338, "Task"], [359, 361, "Method"], [373, 375, "Task"], [377, 379, "Task"], [383, 385, "Task"], [389, 390, "Task"], [403, 405, "Task"], [410, 414, "Method"], [416, 420, "Method"], [437, 438, "Task"], [441, 445, "Method"], [451, 457, "Method"], [482, 486, "Method"], [498, 501, "Method"], [505, 506, "Metric"], [526, 531, "Method"], [532, 533, "Task"], [549, 551, "Method"], [556, 559, "Metric"], [569, 573, "Metric"], [586, 587, "Method"], [589, 593, "Method"], [617, 621, "Method"], [640, 644, "Method"], [666, 672, "Method"], [673, 674, "Method"], [680, 684, "Method"], [686, 695, "Method"], [718, 719, "Task"], [748, 750, "Task"], [753, 758, "Method"], [762, 765, "Method"], [801, 802, "Task"], [825, 826, "Task"], [873, 876, "Method"], [897, 899, "Method"], [901, 904, "Task"], [909, 912, "Material"], [913, 917, "Material"], [921, 925, "Method"], [929, 932, "Method"], [941, 944, "Material"], [949, 951, "Method"], [1009, 1010, "Metric"], [1017, 1018, "Metric"], [1020, 1023, "Material"], [1050, 1055, "Method"], [1060, 1061, "Metric"], [1080, 1081, "Metric"], [1114, 1117, "Method"], [1138, 1139, "Method"], [1151, 1154, "Method"], [1155, 1158, "Task"], [1162, 1166, "Method"], [1190, 1192, "Method"], [1198, 1200, "Method"], [1201, 1206, "Method"], [1208, 1214, "Method"], [1215, 1219, "Method"], [1340, 1343, "Method"], [1352, 1353, "Task"], [1368, 1372, "Task"], [1378, 1382, "Task"], [1410, 1411, "Task"], [1412, 1416, "Task"], [1418, 1420, "Method"], [1459, 1463, "Method"], [1465, 1467, "Method"], [1505, 1507, "Method"], [1508, 1511, "Method"], [1520, 1525, "Method"], [1532, 1540, "Method"], [1554, 1555, "Task"], [1587, 1591, "Method"], [1597, 1599, "Method"], [1605, 1607, "Method"], [1617, 1619, "Method"], [1637, 1639, "Task"], [1650, 1653, "Method"], [1676, 1680, "Method"], [1698, 1701, "Method"], [1712, 1714, "Method"], [1720, 1728, "Method"], [1729, 1733, "Method"], [1750, 1752, "Method"], [1790, 1792, "Method"], [1800, 1806, "Method"], [1811, 1815, "Method"], [1817, 1822, "Task"], [1847, 1850, "Method"], [1873, 1877, "Method"], [1885, 1889, "Method"], [1905, 1909, "Method"], [1944, 1949, "Method"], [2017, 2022, "Method"], [2040, 2044, "Method"], [2065, 2067, "Method"], [2088, 2092, "Method"], [2097, 2099, "Method"], [2102, 2106, "Method"], [2113, 2116, "Method"], [2122, 2124, "Method"], [2130, 2134, "Method"], [2153, 2156, "Method"], [2211, 2216, "Method"], [2239, 2243, "Method"], [2258, 2260, "Method"], [2305, 2307, "Method"], [2310, 2312, "Method"], [2352, 2358, "Method"], [2503, 2507, "Method"], [2525, 2527, "Method"], [2546, 2547, "Task"], [2552, 2554, "Method"], [2598, 2599, "Method"], [2611, 2614, "Method"], [2621, 2625, "Method"], [2638, 2642, "Task"], [2667, 2673, "Task"], [2714, 2716, "Method"], [2718, 2720, "Task"], [2721, 2723, "Task"], [2724, 2726, "Task"], [2727, 2728, "Task"], [2732, 2736, "Method"], [2737, 2738, "Task"], [2750, 2752, "Task"], [2757, 2760, "Task"], [2771, 2772, "Metric"], [2775, 2779, "Method"], [2788, 2790, "Task"], [2791, 2793, "Task"], [2836, 2840, "Method"], [2842, 2844, "Method"], [2846, 2850, "Metric"], [2851, 2852, "Task"], [2876, 2879, "Task"], [2906, 2908, "Method"], [2935, 2937, "Method"], [2939, 2943, "Method"], [2945, 2949, "Method"], [2957, 2959, "Method"], [2960, 2961, "Task"], [3028, 3030, "Method"], [3038, 3040, "Method"], [3042, 3044, "Method"], [3108, 3112, "Method"], [3117, 3120, "Method"], [3121, 3126, "Method"], [3204, 3205, "Method"], [3219, 3220, "Task"], [3226, 3227, "Method"], [3231, 3233, "Method"], [3244, 3245, "Task"], [3262, 3263, "Task"], [3282, 3284, "Method"], [3289, 3291, "Method"], [3294, 3295, "Task"], [3308, 3311, "Method"], [3312, 3314, "Task"], [3315, 3316, "Method"], [3317, 3319, "Task"], [3320, 3321, "Task"], [3327, 3328, "Task"], [3329, 3331, "Task"], [3340, 3343, "Method"], [3344, 3345, "Task"], [3347, 3348, "Method"], [3349, 3351, "Method"], [3357, 3361, "Method"], [3365, 3367, "Method"], [3384, 3386, "Method"], [3400, 3402, "Method"], [3403, 3405, "Method"], [3409, 3412, "Method"], [3421, 3422, "Task"], [3439, 3441, "Task"], [3443, 3444, "Method"], [3449, 3450, "Task"], [3491, 3495, "Method"], [3504, 3506, "Method"], [3507, 3508, "Method"], [3520, 3522, "Method"], [3526, 3527, "Method"], [3529, 3530, "Metric"], [3534, 3537, "Method"], [3553, 3555, "Method"], [3556, 3557, "Task"], [3573, 3575, "Method"], [3623, 3627, "Method"], [3643, 3644, "Task"], [3648, 3649, "Task"], [3704, 3705, "Task"], [3706, 3709, "Task"], [3726, 3730, "Method"], [3734, 3736, "Task"], [3737, 3740, "Task"], [3756, 3757, "Task"], [3763, 3765, "Task"], [3770, 3771, "Task"], [3772, 3774, "Task"], [3775, 3779, "Task"], [3794, 3796, "Method"], [3798, 3799, "Task"], [3806, 3809, "Method"], [3822, 3825, "Material"], [3830, 3835, "Material"], [3865, 3871, "Method"], [3876, 3878, "Method"], [3896, 3897, "Method"], [3912, 3914, "Method"], [3957, 3961, "Task"], [3992, 3994, "Method"], [3995, 3996, "Method"], [4008, 4012, "Method"], [4034, 4037, "Material"], [4063, 4065, "Task"], [4070, 4071, "Method"], [4113, 4116, "Task"], [4123, 4125, "Method"], [4170, 4171, "Task"], [4172, 4173, "Method"], [4179, 4180, "Method"], [4201, 4203, "Task"], [4211, 4214, "Task"], [4219, 4220, "Metric"], [4221, 4222, "Metric"], [4223, 4224, "Metric"], [4240, 4242, "Method"], [4262, 4264, "Task"], [4267, 4268, "Task"], [4269, 4270, "Method"], [4273, 4274, "Method"], [4277, 4278, "Method"], [4307, 4315, "Material"], [4316, 4317, "Method"], [4322, 4326, "Method"], [4345, 4346, "Metric"], [4352, 4354, "Method"], [4376, 4377, "Method"], [4379, 4381, "Metric"], [4390, 4391, "Method"], [4400, 4401, "Metric"], [4417, 4418, "Method"], [4423, 4424, "Metric"], [4431, 4432, "Method"], [4475, 4477, "Task"], [4491, 4493, "Method"], [4495, 4498, "Method"], [4514, 4515, "Method"], [4519, 4523, "Method"], [4524, 4525, "Method"], [4541, 4542, "Metric"], [4553, 4554, "Metric"], [4555, 4556, "Metric"], [4557, 4558, "Metric"], [4560, 4565, "Material"], [4574, 4577, "Material"], [4579, 4581, "Method"], [4592, 4593, "Metric"], [4599, 4600, "Method"], [4605, 4609, "Method"], [4635, 4636, "Metric"], [4638, 4641, "Material"], [4642, 4644, "Method"], [4666, 4667, "Method"], [4689, 4690, "Method"], [4699, 4703, "Method"], [4707, 4709, "Metric"], [4718, 4719, "Metric"], [4724, 4726, "Method"], [4729, 4730, "Task"], [4731, 4735, "Task"], [4739, 4744, "Metric"], [4745, 4746, "Metric"], [4751, 4753, "Method"], [4755, 4758, "Material"], [4759, 4762, "Material"], [4769, 4770, "Method"], [4773, 4777, "Method"], [4778, 4781, "Method"], [4796, 4803, "Method"], [4805, 4806, "Method"], [4809, 4811, "Method"], [4814, 4819, "Method"], [4823, 4825, "Metric"], [4826, 4827, "Method"], [4828, 4831, "Method"], [4842, 4843, "Task"], [4844, 4848, "Method"], [4859, 4862, "Metric"], [4863, 4864, "Metric"], [4865, 4866, "Metric"], [4868, 4871, "Material"], [4872, 4875, "Material"], [4876, 4877, "Method"], [4878, 4879, "Method"], [4882, 4883, "Method"], [4899, 4902, "Material"], [4913, 4914, "Method"], [4916, 4919, "Metric"], [4922, 4923, "Metric"], [4956, 4960, "Method"], [4979, 4981, "Metric"], [4999, 5000, "Method"], [5016, 5017, "Method"], [5045, 5046, "Method"], [5048, 5049, "Method"], [5064, 5065, "Method"], [5075, 5081, "Method"], [5089, 5090, "Metric"], [5093, 5094, "Method"], [5103, 5107, "Method"], [5127, 5128, "Metric"], [5140, 5142, "Task"], [5330, 5331, "Task"], [5341, 5347, "Method"], [5362, 5365, "Task"], [5366, 5367, "Method"], [5381, 5382, "Task"], [5403, 5405, "Method"], [5476, 5479, "Task"], [5573, 5575, "Method"], [5576, 5577, "Method"], [5578, 5581, "Method"], [5604, 5605, "Method"], [5611, 5612, "Method"], [5631, 5634, "Metric"], [5670, 5671, "Method"], [5692, 5694, "Method"], [5732, 5733, "Method"], [5762, 5764, "Metric"], [5794, 5796, "Method"], [5798, 5803, "Method"], [5817, 5818, "Task"], [5824, 5828, "Method"], [5839, 5843, "Metric"], [5844, 5845, "Metric"], [5853, 5856, "Metric"], [5863, 5864, "Task"], [5866, 5869, "Method"], [5882, 5885, "Material"], [5886, 5889, "Material"], [5902, 5903, "Task"], [5909, 5912, "Material"], [5931, 5932, "Task"], [5933, 5935, "Task"], [5959, 5961, "Method"], [5969, 5971, "Method"], [5982, 5986, "Method"], [5993, 5996, "Material"], [6025, 6032, "Method"], [6131, 6133, "Method"], [6156, 6159, "Material"], [6238, 6239, "Task"], [6254, 6255, "Task"], [6263, 6264, "Method"], [6267, 6271, "Method"], [6272, 6274, "Method"], [6277, 6278, "Method"], [6281, 6282, "Method"], [6289, 6291, "Metric"], [6297, 6299, "Metric"], [6329, 6331, "Metric"], [6355, 6358, "Metric"], [6362, 6366, "Method"], [6370, 6375, "Method"], [6394, 6396, "Metric"], [6401, 6403, "Metric"], [6412, 6416, "Method"], [6429, 6434, "Method"], [6454, 6456, "Method"], [6461, 6470, "Task"], [564, 565, "Task"], [975, 976, "Metric"], [1030, 1031, "Metric"], [1072, 1075, "Material"], [1136, 1137, "Task"], [3472, 3473, "Task"], [3487, 3488, "Task"], [3654, 3655, "Task"], [3973, 3974, "Method"], [4019, 4020, "Method"], [4226, 4229, "Material"], [4247, 4248, "Method"], [4287, 4288, "Method"], [4302, 4303, "Metric"], [4337, 4338, "Method"], [4361, 4362, "Task"], [4362, 4363, "Metric"], [4366, 4367, "Method"], [4405, 4406, "Metric"], [4582, 4583, "Method"], [4619, 4620, "Method"], [4650, 4651, "Metric"], [4656, 4657, "Method"], [4788, 4789, "Method"], [4837, 4838, "Task"], [4890, 4891, "Method"], [4911, 4912, "Metric"], [4940, 4941, "Metric"], [4963, 4964, "Task"], [4973, 4974, "Task"], [4974, 4975, "Metric"], [4988, 4989, "Task"], [4989, 4990, "Metric"], [5037, 5038, "Method"], [5121, 5122, "Task"], [5122, 5123, "Metric"], [5352, 5355, "Method"], [5396, 5397, "Method"], [5436, 5437, "Task"], [5557, 5560, "Material"], [5646, 5647, "Task"], [5647, 5648, "Metric"], [5766, 5769, "Material"], [5831, 5834, "Material"], [5846, 5847, "Metric"], [6160, 6163, "Material"]], "sections": [[0, 330], [330, 1090], [1090, 1518], [1518, 2143], [2143, 2636], [2636, 2874], [2874, 3065], [3065, 3255], [3255, 3813], [3813, 4209], [4209, 4727], [4727, 4967], [4967, 5336], [5336, 5411], [5411, 5527], [5527, 5530], [5530, 5534], [5534, 5738], [5738, 5784], [5784, 5991], [5991, 6154], [6154, 6252], [6252, 6493]], "sentences": [[0, 12], [12, 37], [37, 65], [65, 114], [114, 158], [158, 175], [175, 204], [204, 271], [271, 288], [288, 330], [330, 333], [333, 359], [359, 394], [394, 446], [446, 487], [487, 516], [516, 578], [578, 629], [629, 661], [661, 739], [739, 803], [803, 865], [865, 905], [905, 940], [940, 979], [979, 1019], [1019, 1043], [1043, 1090], [1090, 1093], [1093, 1138], [1138, 1171], [1171, 1183], [1183, 1226], [1226, 1261], [1261, 1285], [1285, 1306], [1306, 1350], [1350, 1394], [1394, 1433], [1433, 1470], [1470, 1495], [1495, 1518], [1518, 1525], [1525, 1575], [1575, 1580], [1580, 1592], [1592, 1641], [1641, 1658], [1658, 1681], [1681, 1709], [1709, 1734], [1734, 1765], [1765, 1793], [1793, 1816], [1816, 1846], [1846, 1881], [1881, 1892], [1892, 1919], [1919, 1940], [1940, 2004], [2004, 2027], [2027, 2036], [2036, 2069], [2069, 2107], [2107, 2139], [2139, 2143], [2143, 2151], [2151, 2169], [2169, 2202], [2202, 2225], [2225, 2235], [2235, 2244], [2244, 2265], [2265, 2302], [2302, 2338], [2338, 2341], [2341, 2344], [2344, 2365], [2365, 2400], [2400, 2433], [2433, 2452], [2452, 2464], [2464, 2489], [2489, 2528], [2528, 2538], [2538, 2558], [2558, 2600], [2600, 2619], [2619, 2636], [2636, 2642], [2642, 2660], [2660, 2685], [2685, 2708], [2708, 2761], [2761, 2825], [2825, 2853], [2853, 2874], [2874, 2879], [2879, 2894], [2894, 2930], [2930, 2950], [2950, 2962], [2962, 2992], [2992, 2996], [2996, 3013], [3013, 3032], [3032, 3065], [3065, 3068], [3068, 3080], [3080, 3134], [3134, 3142], [3142, 3146], [3146, 3149], [3149, 3159], [3159, 3165], [3165, 3167], [3167, 3176], [3176, 3179], [3179, 3182], [3182, 3199], [3199, 3216], [3216, 3229], [3229, 3241], [3241, 3255], [3255, 3259], [3259, 3297], [3297, 3315], [3315, 3332], [3332, 3362], [3362, 3382], [3382, 3391], [3391, 3392], [3392, 3426], [3426, 3474], [3474, 3523], [3523, 3547], [3547, 3558], [3558, 3592], [3592, 3611], [3611, 3643], [3643, 3699], [3699, 3741], [3741, 3785], [3785, 3800], [3800, 3813], [3813, 3817], [3817, 3860], [3860, 3873], [3873, 3896], [3896, 3937], [3937, 3950], [3950, 3970], [3970, 4022], [4022, 4046], [4046, 4072], [4072, 4126], [4126, 4139], [4139, 4174], [4174, 4191], [4191, 4209], [4209, 4214], [4214, 4237], [4237, 4269], [4269, 4305], [4305, 4351], [4351, 4358], [4358, 4428], [4428, 4462], [4462, 4486], [4486, 4513], [4513, 4548], [4548, 4566], [4566, 4578], [4578, 4594], [4594, 4637], [4637, 4663], [4663, 4681], [4681, 4704], [4704, 4727], [4727, 4735], [4735, 4778], [4778, 4786], [4786, 4820], [4820, 4852], [4852, 4876], [4876, 4886], [4886, 4894], [4894, 4913], [4913, 4924], [4924, 4967], [4967, 4970], [4970, 4982], [4982, 5015], [5015, 5025], [5025, 5047], [5047, 5070], [5070, 5129], [5129, 5154], [5154, 5177], [5177, 5237], [5237, 5256], [5256, 5281], [5281, 5305], [5305, 5332], [5332, 5336], [5336, 5339], [5339, 5366], [5366, 5391], [5391, 5411], [5411, 5414], [5414, 5455], [5455, 5492], [5492, 5500], [5500, 5527], [5527, 5530], [5530, 5534], [5534, 5538], [5538, 5549], [5549, 5566], [5566, 5601], [5601, 5635], [5635, 5643], [5643, 5661], [5661, 5684], [5684, 5701], [5701, 5738], [5738, 5742], [5742, 5757], [5757, 5784], [5784, 5792], [5792, 5814], [5814, 5849], [5849, 5861], [5861, 5874], [5874, 5890], [5890, 5936], [5936, 5941], [5941, 5942], [5942, 5948], [5948, 5950], [5950, 5953], [5953, 5991], [5991, 5996], [5996, 6036], [6036, 6064], [6064, 6101], [6101, 6125], [6125, 6154], [6154, 6159], [6159, 6189], [6189, 6213], [6213, 6235], [6235, 6252], [6252, 6257], [6257, 6285], [6285, 6314], [6314, 6352], [6352, 6367], [6367, 6386], [6386, 6407], [6407, 6435], [6435, 6451], [6451, 6473], [6473, 6485], [6485, 6493]], "words": ["document", ":", "Linguistically", "-", "Informed", "Self", "-", "Attention", "for", "Semantic", "Role", "Labeling", "Current", "state", "-", "of", "-", "the", "-", "art", "semantic", "role", "labeling", "(", "SRL", ")", "uses", "a", "deep", "neural", "network", "with", "no", "explicit", "linguistic", "features", ".", "However", ",", "prior", "work", "has", "shown", "that", "gold", "syntax", "trees", "can", "dramatically", "improve", "SRL", "decoding", ",", "suggesting", "the", "possibility", "of", "increased", "accuracy", "from", "explicit", "modeling", "of", "syntax", ".", "In", "this", "work", ",", "we", "present", "linguistically", "-", "informed", "self", "-", "attention", "(", "LISA", ")", ":", "a", "neural", "network", "model", "that", "combines", "multi", "-", "head", "self", "-", "attention", "with", "multi", "-", "task", "learning", "across", "dependency", "parsing", ",", "part", "-", "of", "-", "speech", "tagging", ",", "predicate", "detection", "and", "SRL", ".", "Unlike", "previous", "models", "which", "require", "significant", "pre", "-", "processing", "to", "prepare", "linguistic", "features", ",", "LISA", "can", "incorporate", "syntax", "using", "merely", "raw", "tokens", "as", "input", ",", "encoding", "the", "sequence", "only", "once", "to", "simultaneously", "perform", "parsing", ",", "predicate", "detection", "and", "role", "labeling", "for", "all", "predicates", ".", "Syntax", "is", "incorporated", "by", "training", "one", "attention", "head", "to", "attend", "to", "syntactic", "parents", "for", "each", "token", ".", "Moreover", ",", "if", "a", "high", "-", "quality", "syntactic", "parse", "is", "already", "available", ",", "it", "can", "be", "beneficially", "injected", "at", "test", "time", "without", "re", "-", "training", "our", "SRL", "model", ".", "In", "experiments", "on", "CoNLL", "-", "2005", "SRL", ",", "LISA", "achieves", "new", "state", "-", "of", "-", "the", "-", "art", "performance", "for", "a", "model", "using", "predicted", "predicates", "and", "standard", "word", "embeddings", ",", "attaining", "2.5", "F1", "absolute", "higher", "than", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "on", "newswire", "and", "more", "than", "3.5", "F1", "on", "out", "-", "of", "-", "domain", "data", ",", "nearly", "10", "%", "reduction", "in", "error", ".", "On", "ConLL", "-", "2012", "English", "SRL", "we", "also", "show", "an", "improvement", "of", "more", "than", "2.5", "F1", ".", "LISA", "also", "out", "-", "performs", "the", "state", "-", "of", "-", "the", "-", "art", "with", "contextually", "-", "encoded", "(", "ELMo", ")", "word", "representations", ",", "by", "nearly", "1.0", "F1", "on", "news", "and", "more", "than", "2.0", "F1", "on", "out", "-", "of", "-", "domain", "text", ".", "section", ":", "Introduction", "Semantic", "role", "labeling", "(", "SRL", ")", "extracts", "a", "high", "-", "level", "representation", "of", "meaning", "from", "a", "sentence", ",", "labeling", "e.g.", "who", "did", "what", "to", "whom", ".", "Explicit", "representations", "of", "such", "semantic", "information", "have", "been", "shown", "to", "improve", "results", "in", "challenging", "downstream", "tasks", "such", "as", "dialog", "systems", "tur2005semi", ",", "chen2013unsupervised", ",", "machine", "reading", "berant2014modeling", ",", "wang2015machine", "and", "translation", "liu2010semantic", ",", "bazrafshan2013semantic", ".", "Though", "syntax", "was", "long", "considered", "an", "obvious", "prerequisite", "for", "SRL", "systems", "levin1993english", ",", "punyakanok2008importance", ",", "recently", "deep", "neural", "network", "architectures", "have", "surpassed", "syntactically", "-", "informed", "models", "zhou2015end", ",", "marcheggiani2017simple", ",", "he2017deep", ",", "tan2018deep", ",", "he2018jointly", ",", "achieving", "state", "-", "of", "-", "the", "art", "SRL", "performance", "with", "no", "explicit", "modeling", "of", "syntax", ".", "An", "additional", "benefit", "of", "these", "end", "-", "to", "-", "end", "models", "is", "that", "they", "require", "just", "raw", "tokens", "and", "(", "usually", ")", "detected", "predicates", "as", "input", ",", "whereas", "richer", "linguistic", "features", "typically", "require", "extraction", "by", "an", "auxiliary", "pipeline", "of", "models", ".", "Still", ",", "recent", "work", "roth2016neural", ",", "he2017deep", ",", "marcheggiani2017encoding", "indicates", "that", "neural", "network", "models", "could", "see", "even", "higher", "accuracy", "gains", "by", "leveraging", "syntactic", "information", "rather", "than", "ignoring", "it", ".", "he2017deep", "indicate", "that", "many", "of", "the", "errors", "made", "by", "a", "syntax", "-", "free", "neural", "network", "on", "SRL", "are", "tied", "to", "certain", "syntactic", "confusions", "such", "as", "prepositional", "phrase", "attachment", ",", "and", "show", "that", "while", "constrained", "inference", "using", "a", "relatively", "low", "-", "accuracy", "predicted", "parse", "can", "provide", "small", "improvements", "in", "SRL", "accuracy", ",", "providing", "a", "gold", "-", "quality", "parse", "leads", "to", "substantial", "gains", ".", "marcheggiani2017encoding", "incorporate", "syntax", "from", "a", "high", "-", "quality", "parser", "kiperwasser2016simple", "using", "graph", "convolutional", "neural", "networks", "kipf2017semi", ",", "but", "like", "he2017deep", "they", "attain", "only", "small", "increases", "over", "a", "model", "with", "no", "syntactic", "parse", ",", "and", "even", "perform", "worse", "than", "a", "syntax", "-", "free", "model", "on", "out", "-", "of", "-", "domain", "data", ".", "These", "works", "suggest", "that", "though", "syntax", "has", "the", "potential", "to", "improve", "neural", "network", "SRL", "models", ",", "we", "have", "not", "yet", "designed", "an", "architecture", "which", "maximizes", "the", "benefits", "of", "auxiliary", "syntactic", "information", ".", "In", "response", ",", "we", "propose", "linguistically", "-", "informed", "self", "-", "attention", "(", "LISA", ")", ":", "a", "model", "that", "combines", "multi", "-", "task", "learning", "caruana1993multitask", "with", "stacked", "layers", "of", "multi", "-", "head", "self", "-", "attention", "vaswani2017attention", ";", "the", "model", "is", "trained", "to", ":", "(", "1", ")", "jointly", "predict", "parts", "of", "speech", "and", "predicates", ";", "(", "2", ")", "perform", "parsing", ";", "and", "(", "3", ")", "attend", "to", "syntactic", "parse", "parents", ",", "while", "(", "4", ")", "assigning", "semantic", "role", "labels", ".", "Whereas", "prior", "work", "typically", "requires", "separate", "models", "to", "provide", "linguistic", "analysis", ",", "including", "most", "syntax", "-", "free", "neural", "models", "which", "still", "rely", "on", "external", "predicate", "detection", ",", "our", "model", "is", "truly", "end", "-", "to", "-", "end", ":", "earlier", "layers", "are", "trained", "to", "predict", "prerequisite", "parts", "-", "of", "-", "speech", "and", "predicates", ",", "the", "latter", "of", "which", "are", "supplied", "to", "later", "layers", "for", "scoring", ".", "Though", "prior", "work", "re", "-", "encodes", "each", "sentence", "to", "predict", "each", "desired", "task", "and", "again", "with", "respect", "to", "each", "predicate", "to", "perform", "SRL", ",", "we", "more", "efficiently", "encode", "each", "sentence", "only", "once", ",", "predict", "its", "predicates", ",", "part", "-", "of", "-", "speech", "tags", "and", "labeled", "syntactic", "parse", ",", "then", "predict", "the", "semantic", "roles", "for", "all", "predicates", "in", "the", "sentence", "in", "parallel", ".", "The", "model", "is", "trained", "such", "that", ",", "as", "syntactic", "parsing", "models", "improve", ",", "providing", "high", "-", "quality", "parses", "at", "test", "time", "will", "improve", "its", "performance", ",", "allowing", "the", "model", "to", "leverage", "updated", "parsing", "models", "without", "requiring", "re", "-", "training", ".", "In", "experiments", "on", "the", "CoNLL", "-", "2005", "and", "CoNLL", "-", "2012", "datasets", "we", "show", "that", "our", "linguistically", "-", "informed", "models", "out", "-", "perform", "the", "syntax", "-", "free", "state", "-", "of", "-", "the", "-", "art", ".", "On", "CoNLL", "-", "2005", "with", "predicted", "predicates", "and", "standard", "word", "embeddings", ",", "our", "single", "model", "out", "-", "performs", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "model", "on", "the", "WSJ", "test", "set", "by", "2.5", "F1", "points", "absolute", ".", "On", "the", "challenging", "out", "-", "of", "-", "domain", "Brown", "test", "set", ",", "our", "model", "improves", "substantially", "over", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "by", "more", "than", "3.5", "F1", ",", "a", "nearly", "10", "%", "reduction", "in", "error", ".", "On", "CoNLL", "-", "2012", ",", "our", "model", "gains", "more", "than", "2.5", "F1", "absolute", "over", "the", "previous", "state", "-", "of", "-", "the", "-", "art", ".", "Our", "models", "also", "show", "improvements", "when", "using", "contextually", "-", "encoded", "word", "representations", "peters2018deep", ",", "obtaining", "nearly", "1.0", "F1", "higher", "than", "the", "state", "-", "of", "-", "the", "-", "art", "on", "CoNLL", "-", "2005", "news", "and", "more", "than", "2.0", "F1", "improvement", "on", "out", "-", "of", "-", "domain", "text", ".", "section", ":", "Model", "[", "scale=.8", "]", "no_words_simpler_compact", "-", "srl", "-", "model.pdf", "[", "scale=.24", "]", "attention", "-", "keynote", "Our", "goal", "is", "to", "design", "an", "efficient", "neural", "network", "model", "which", "makes", "use", "of", "linguistic", "information", "as", "effectively", "as", "possible", "in", "order", "to", "perform", "end", "-", "to", "-", "end", "SRL", ".", "LISA", "achieves", "this", "by", "combining", ":", "(", "1", ")", "A", "new", "technique", "of", "supervising", "neural", "attention", "to", "predict", "syntactic", "dependencies", "with", "(", "2", ")", "multi", "-", "task", "learning", "across", "four", "related", "tasks", ".", "Figure", "[", "reference", "]", "depicts", "the", "overall", "architecture", "of", "our", "model", ".", "The", "basis", "for", "our", "model", "is", "the", "Transformer", "encoder", "introduced", "by", "vaswani2017attention", ":", "we", "transform", "word", "embeddings", "into", "contextually", "-", "encoded", "token", "representations", "using", "stacked", "multi", "-", "head", "self", "-", "attention", "and", "feed", "-", "forward", "layers", "(", "\u00a7", "[", "reference", "]", ")", ".", "To", "incorporate", "syntax", ",", "one", "self", "-", "attention", "head", "is", "trained", "to", "attend", "to", "each", "token", "\u2019s", "syntactic", "parent", ",", "allowing", "the", "model", "to", "use", "this", "attention", "head", "as", "an", "oracle", "for", "syntactic", "dependencies", ".", "We", "introduce", "this", "syntactically", "-", "informed", "self", "-", "attention", "(", "Figure", "[", "reference", "]", ")", "in", "more", "detail", "in", "\u00a7", "[", "reference", "]", ".", "Our", "model", "is", "designed", "for", "the", "more", "realistic", "setting", "in", "which", "gold", "predicates", "are", "not", "provided", "at", "test", "-", "time", ".", "Our", "model", "predicts", "predicates", "and", "integrates", "part", "-", "of", "-", "speech", "(", "POS", ")", "information", "into", "earlier", "layers", "by", "re", "-", "purposing", "representations", "closer", "to", "the", "input", "to", "predict", "predicate", "and", "POS", "tags", "using", "hard", "parameter", "sharing", "(", "\u00a7", "[", "reference", "]", ")", ".", "We", "simplify", "optimization", "and", "benefit", "from", "shared", "statistical", "strength", "derived", "from", "highly", "correlated", "POS", "and", "predicates", "by", "treating", "tagging", "and", "predicate", "detection", "as", "a", "single", "task", ",", "performing", "multi", "-", "class", "classification", "into", "the", "joint", "Cartesian", "product", "space", "of", "POS", "and", "predicate", "labels", ".", "Though", "typical", "models", ",", "which", "re", "-", "encode", "the", "sentence", "for", "each", "predicate", ",", "can", "simplify", "SRL", "to", "token", "-", "wise", "tagging", ",", "our", "joint", "model", "requires", "a", "different", "approach", "to", "classify", "roles", "with", "respect", "to", "each", "predicate", ".", "Contextually", "encoded", "tokens", "are", "projected", "to", "distinct", "predicate", "and", "role", "embeddings", "(", "\u00a7", "[", "reference", "]", ")", ",", "and", "each", "predicted", "predicate", "is", "scored", "with", "the", "sequence", "\u2019s", "role", "representations", "using", "a", "bilinear", "model", "(", "Eqn", ".", "[", "reference", "]", ")", ",", "producing", "per", "-", "label", "scores", "for", "BIO", "-", "encoded", "semantic", "role", "labels", "for", "each", "token", "and", "each", "semantic", "frame", ".", "The", "model", "is", "trained", "end", "-", "to", "-", "end", "by", "maximum", "likelihood", "using", "stochastic", "gradient", "descent", "(", "\u00a7", "[", "reference", "]", ")", ".", "subsection", ":", "Self", "-", "attention", "token", "encoder", "The", "basis", "for", "our", "model", "is", "a", "multi", "-", "head", "self", "-", "attention", "token", "encoder", ",", "recently", "shown", "to", "achieve", "state", "-", "of", "-", "the", "-", "art", "performance", "on", "SRL", "tan2018deep", ",", "and", "which", "provides", "a", "natural", "mechanism", "for", "incorporating", "syntax", ",", "as", "described", "in", "\u00a7", "[", "reference", "]", ".", "Our", "implementation", "replicates", "vaswani2017attention", ".", "The", "input", "to", "the", "network", "is", "a", "sequence", "of", "token", "representations", ".", "In", "the", "standard", "setting", "these", "token", "representations", "are", "initialized", "to", "pre", "-", "trained", "word", "embeddings", ",", "but", "we", "also", "experiment", "with", "supplying", "pre", "-", "trained", "ELMo", "representations", "combined", "with", "task", "-", "specific", "learned", "parameters", ",", "which", "have", "been", "shown", "to", "substantially", "improve", "performance", "of", "other", "SRL", "models", "peters2018deep", ".", "For", "experiments", "with", "gold", "predicates", ",", "we", "concatenate", "a", "predicate", "indicator", "embedding", "following", "previous", "work", "he2017deep", ".", "We", "project", "these", "input", "embeddings", "to", "a", "representation", "that", "is", "the", "same", "size", "as", "the", "output", "of", "the", "self", "-", "attention", "layers", ".", "We", "then", "add", "a", "positional", "encoding", "vector", "computed", "as", "a", "deterministic", "sinusoidal", "function", "of", ",", "since", "the", "self", "-", "attention", "has", "no", "innate", "notion", "of", "token", "position", ".", "We", "feed", "this", "token", "representation", "as", "input", "to", "a", "series", "of", "residual", "multi", "-", "head", "self", "-", "attention", "layers", "with", "feed", "-", "forward", "connections", ".", "Denoting", "the", "th", "self", "-", "attention", "layer", "as", ",", "the", "output", "of", "that", "layer", ",", "and", "layer", "normalization", ",", "the", "following", "recurrence", "applied", "to", "initial", "input", ":", "s_t^", "(", "j", ")", "=", "LN", "(", "s_t^", "(", "j", "-", "1", ")", "+", "T^", "(", "j", ")(", "s_t^", "(", "j", "-", "1", ")", ")", ")", "gives", "our", "final", "token", "representations", ".", "Each", "consists", "of", ":", "(", "a", ")", "multi", "-", "head", "self", "-", "attention", "and", "(", "b", ")", "a", "feed", "-", "forward", "projection", ".", "The", "multi", "-", "head", "self", "attention", "consists", "of", "attention", "heads", ",", "each", "of", "which", "learns", "a", "distinct", "attention", "function", "to", "attend", "to", "all", "of", "the", "tokens", "in", "the", "sequence", ".", "This", "self", "-", "attention", "is", "performed", "for", "each", "token", "for", "each", "head", ",", "and", "the", "results", "of", "the", "self", "-", "attentions", "are", "concatenated", "to", "form", "the", "final", "self", "-", "attended", "representation", "for", "each", "token", ".", "Specifically", ",", "consider", "the", "matrix", "of", "token", "representations", "at", "layer", ".", "For", "each", "attention", "head", ",", "we", "project", "this", "matrix", "into", "distinct", "key", ",", "value", "and", "query", "representations", ",", "and", "of", "dimensions", ",", ",", "and", ",", "respectively", ".", "We", "can", "then", "multiply", "by", "to", "obtain", "a", "matrix", "of", "attention", "weights", "between", "each", "pair", "of", "tokens", "in", "the", "sentence", ".", "Following", "vaswani2017attention", "we", "perform", "scaled", "dot", "-", "product", "attention", ":", "We", "scale", "the", "weights", "by", "the", "inverse", "square", "root", "of", "their", "embedding", "dimension", "and", "normalize", "with", "the", "softmax", "function", "to", "produce", "a", "distinct", "distribution", "for", "each", "token", "over", "all", "the", "tokens", "in", "the", "sentence", ":", "A_h^", "(", "j", ")", "=", "softmax", "(", "d_k^", "-", "0.5Q_h^", "(", "j", ")", "K_h^", "(", "j", ")", "^T", ")", "These", "attention", "weights", "are", "then", "multiplied", "by", "for", "each", "token", "to", "obtain", "the", "self", "-", "attended", "token", "representations", ":", "M_h^", "(", "j", ")", "=", "A_h^", "(", "j", ")", "V_h^", "(", "j", ")", "Row", "of", ",", "the", "self", "-", "attended", "representation", "for", "token", "at", "layer", ",", "is", "thus", "the", "weighted", "sum", "with", "respect", "to", "(", "with", "weights", "given", "by", ")", "over", "the", "token", "representations", "in", ".", "The", "outputs", "of", "all", "attention", "heads", "for", "each", "token", "are", "concatenated", ",", "and", "this", "representation", "is", "passed", "to", "the", "feed", "-", "forward", "layer", ",", "which", "consists", "of", "two", "linear", "projections", "each", "followed", "by", "leaky", "ReLU", "activations", "maas2012rectifier", ".", "We", "add", "the", "output", "of", "the", "feed", "-", "forward", "to", "the", "initial", "representation", "and", "apply", "layer", "normalization", "to", "give", "the", "final", "output", "of", "self", "-", "attention", "layer", ",", "as", "in", "Eqn", ".", "[", "reference", "]", ".", "subsection", ":", "Syntactically", "-", "informed", "self", "-", "attention", "Typically", ",", "neural", "attention", "mechanisms", "are", "left", "on", "their", "own", "to", "learn", "to", "attend", "to", "relevant", "inputs", ".", "Instead", ",", "we", "propose", "training", "the", "self", "-", "attention", "to", "attend", "to", "specific", "tokens", "corresponding", "to", "the", "syntactic", "structure", "of", "the", "sentence", "as", "a", "mechanism", "for", "passing", "linguistic", "knowledge", "to", "later", "layers", ".", "Specifically", ",", "we", "replace", "one", "attention", "head", "with", "the", "deep", "bi", "-", "affine", "model", "of", "dozat2017deep", ",", "trained", "to", "predict", "syntactic", "dependencies", ".", "Let", "be", "the", "parse", "attention", "weights", ",", "at", "layer", ".", "Its", "input", "is", "the", "matrix", "of", "token", "representations", ".", "As", "with", "the", "other", "attention", "heads", ",", "we", "project", "into", "key", ",", "value", "and", "query", "representations", ",", "denoted", ",", ",", ".", "Here", "the", "key", "and", "query", "projections", "correspond", "to", "and", "representations", "of", "the", "tokens", ",", "and", "we", "allow", "their", "dimensions", "to", "differ", "from", "the", "rest", "of", "the", "attention", "heads", "to", "more", "closely", "follow", "the", "implementation", "of", "dozat2017deep", ".", "Unlike", "the", "other", "attention", "heads", "which", "use", "a", "dot", "product", "to", "score", "key", "-", "query", "pairs", ",", "we", "score", "the", "compatibility", "between", "and", "using", "a", "bi", "-", "affine", "operator", "to", "obtain", "attention", "weights", ":", "A_parse", "=", "softmax", "(", "Q_parse", "U_heads", "K_parse^T", ")", "These", "attention", "weights", "are", "used", "to", "compose", "a", "weighted", "average", "of", "the", "value", "representations", "as", "in", "the", "other", "attention", "heads", ".", "We", "apply", "auxiliary", "supervision", "at", "this", "attention", "head", "to", "encourage", "it", "to", "attend", "to", "each", "token", "\u2019s", "parent", "in", "a", "syntactic", "dependency", "tree", ",", "and", "to", "encode", "information", "about", "the", "token", "\u2019s", "dependency", "label", ".", "Denoting", "the", "attention", "weight", "from", "token", "to", "a", "candidate", "head", "as", ",", "we", "model", "the", "probability", "of", "token", "having", "parent", "as", ":", "P", "(", "q", "=", "head", "(", "t", ")", "X", ")", "=", "A_parse", "[", "t", ",", "q", "]", "using", "the", "attention", "weights", "as", "the", "distribution", "over", "possible", "heads", "for", "token", ".", "We", "define", "the", "root", "token", "as", "having", "a", "self", "-", "loop", ".", "This", "attention", "head", "thus", "emits", "a", "directed", "graph", "where", "each", "token", "\u2019s", "parent", "is", "the", "token", "to", "which", "the", "attention", "assigns", "the", "highest", "weight", ".", "We", "also", "predict", "dependency", "labels", "using", "per", "-", "class", "bi", "-", "affine", "operations", "between", "parent", "and", "dependent", "representations", "and", "to", "produce", "per", "-", "label", "scores", ",", "with", "locally", "normalized", "probabilities", "over", "dependency", "labels", "given", "by", "the", "softmax", "function", ".", "We", "refer", "the", "reader", "to", "dozat2017deep", "for", "more", "details", ".", "This", "attention", "head", "now", "becomes", "an", "oracle", "for", "syntax", ",", "denoted", ",", "providing", "a", "dependency", "parse", "to", "downstream", "layers", ".", "This", "model", "not", "only", "predicts", "its", "own", "dependency", "arcs", ",", "but", "allows", "for", "the", "injection", "of", "auxiliary", "parse", "information", "at", "test", "time", "by", "simply", "setting", "to", "the", "parse", "parents", "produced", "by", "e.g.", "a", "state", "-", "of", "-", "the", "-", "art", "parser", ".", "In", "this", "way", ",", "our", "model", "can", "benefit", "from", "improved", ",", "external", "parsing", "models", "without", "re", "-", "training", ".", "Unlike", "typical", "multi", "-", "task", "models", ",", "ours", "maintains", "the", "ability", "to", "leverage", "external", "syntactic", "information", ".", "subsection", ":", "Multi", "-", "task", "learning", "We", "also", "share", "the", "parameters", "of", "lower", "layers", "in", "our", "model", "to", "predict", "POS", "tags", "and", "predicates", ".", "Following", "he2017deep", ",", "we", "focus", "on", "the", "end", "-", "to", "-", "end", "setting", ",", "where", "predicates", "must", "be", "predicted", "on", "-", "the", "-", "fly", ".", "Since", "we", "also", "train", "our", "model", "to", "predict", "syntactic", "dependencies", ",", "it", "is", "beneficial", "to", "give", "the", "model", "knowledge", "of", "POS", "information", ".", "While", "much", "previous", "work", "employs", "a", "pipelined", "approach", "to", "both", "POS", "tagging", "for", "dependency", "parsing", "and", "predicate", "detection", "for", "SRL", ",", "we", "take", "a", "multi", "-", "task", "learning", "(", "MTL", ")", "approach", "caruana1993multitask", ",", "sharing", "the", "parameters", "of", "earlier", "layers", "in", "our", "SRL", "model", "with", "a", "joint", "POS", "and", "predicate", "detection", "objective", ".", "Since", "POS", "is", "a", "strong", "predictor", "of", "predicates", "and", "the", "complexity", "of", "training", "a", "multi", "-", "task", "model", "increases", "with", "the", "number", "of", "tasks", ",", "we", "combine", "POS", "tagging", "and", "predicate", "detection", "into", "a", "joint", "label", "space", ":", "For", "each", "POS", "tag", "tag", "which", "is", "observed", "co", "-", "occurring", "with", "a", "predicate", ",", "we", "add", "a", "label", "of", "the", "form", "tag", ":", "predicate", ".", "Specifically", ",", "we", "feed", "the", "representation", "from", "a", "layer", "preceding", "the", "syntactically", "-", "informed", "layer", "to", "a", "linear", "classifier", "to", "produce", "per", "-", "class", "scores", "for", "token", ".", "We", "compute", "locally", "-", "normalized", "probabilities", "using", "the", "softmax", "function", ":", ",", "where", "is", "a", "label", "in", "the", "joint", "space", ".", "subsection", ":", "Predicting", "semantic", "roles", "Our", "final", "goal", "is", "to", "predict", "semantic", "roles", "for", "each", "predicate", "in", "the", "sequence", ".", "We", "score", "each", "predicate", "against", "each", "token", "in", "the", "sequence", "using", "a", "bilinear", "operation", ",", "producing", "per", "-", "label", "scores", "for", "each", "token", "for", "each", "predicate", ",", "with", "predicates", "and", "syntax", "determined", "by", "oracles", "and", ".", "First", ",", "we", "project", "each", "token", "representation", "to", "a", "predicate", "-", "specific", "representation", "and", "a", "role", "-", "specific", "representation", ".", "We", "then", "provide", "these", "representations", "to", "a", "bilinear", "transformation", "for", "scoring", ".", "So", ",", "the", "role", "label", "scores", "for", "the", "token", "at", "index", "with", "respect", "to", "the", "predicate", "at", "index", "(", "i.e.", "token", "and", "frame", ")", "are", "given", "by", ":", "s_ft", "=", "(", "s_f^pred", ")", "^T", "U", "s_t^role", "which", "can", "be", "computed", "in", "parallel", "across", "all", "semantic", "frames", "in", "an", "entire", "minibatch", ".", "We", "calculate", "a", "locally", "normalized", "distribution", "over", "role", "labels", "for", "token", "in", "frame", "using", "the", "softmax", "function", ":", ".", "At", "test", "time", ",", "we", "perform", "constrained", "decoding", "using", "the", "Viterbi", "algorithm", "to", "emit", "valid", "sequences", "of", "BIO", "tags", ",", "using", "unary", "scores", "and", "the", "transition", "probabilities", "given", "by", "the", "training", "data", ".", "subsection", ":", "Training", "We", "maximize", "the", "sum", "of", "the", "likelihoods", "of", "the", "individual", "tasks", ".", "In", "order", "to", "maximize", "our", "model", "\u2019s", "ability", "to", "leverage", "syntax", ",", "during", "training", "we", "clamp", "to", "the", "gold", "parse", "(", ")", "and", "to", "gold", "predicates", "when", "passing", "parse", "and", "predicate", "representations", "to", "later", "layers", ",", "whereas", "syntactic", "head", "prediction", "and", "joint", "predicate", "/", "POS", "prediction", "are", "conditioned", "only", "on", "the", "input", "sequence", ".", "The", "overall", "objective", "is", "thus", ":", "_", "t=1^T", "[", "Align\u2211", "_", "f=1^F", "P", "(", "y_ft^role", "P", "_", "G", ",", "V", "_", "G", ",", "X", ")", "+", "P", "(", "y_t^prp", "X", ")", "+", "_", "1", "P", "(", "head", "(", "t", ")", "X", ")", "+", "_", "2", "P", "(", "y_t^dep", "P", "_", "G", ",", "X", ")", "]", "where", "and", "are", "penalties", "on", "the", "syntactic", "attention", "loss", ".", "We", "train", "the", "model", "using", "Nadam", "dozat2016incorporating", "SGD", "combined", "with", "the", "learning", "rate", "schedule", "in", "vaswani2017attention", ".", "In", "addition", "to", "MTL", ",", "we", "regularize", "our", "model", "using", "dropout", "srivastava2014dropout", ".", "We", "use", "gradient", "clipping", "to", "avoid", "exploding", "gradients", "bengio1994learning", ",", "pascanu2013on", ".", "Additional", "details", "on", "optimization", "and", "hyperparameters", "are", "included", "in", "Appendix", "[", "reference", "]", ".", "section", ":", "Related", "work", "Early", "approaches", "to", "SRL", "pradhan2005semantic", ",", "surdeanu2007combination", ",", "johansson2008dependency", ",", "toutanova2008global", "focused", "on", "developing", "rich", "sets", "of", "linguistic", "features", "as", "input", "to", "a", "linear", "model", ",", "often", "combined", "with", "complex", "constrained", "inference", "e.g.", "with", "an", "ILP", "punyakanok2008importance", ".", "tackstrom2015efficient", "showed", "that", "constraints", "could", "be", "enforced", "more", "efficiently", "using", "a", "clever", "dynamic", "program", "for", "exact", "inference", ".", "sutton2005joint", "modeled", "syntactic", "parsing", "and", "SRL", "jointly", ",", "and", "lewis2015joint", "jointly", "modeled", "SRL", "and", "CCG", "parsing", ".", "collobert2011natural", "were", "among", "the", "first", "to", "use", "a", "neural", "network", "model", "for", "SRL", ",", "a", "CNN", "over", "word", "embeddings", "which", "failed", "to", "out", "-", "perform", "non", "-", "neural", "models", ".", "fitzgerald2015semantic", "successfully", "employed", "neural", "networks", "by", "embedding", "lexicalized", "features", "and", "providing", "them", "as", "factors", "in", "the", "model", "of", "tackstrom2015efficient", ".", "More", "recent", "neural", "models", "are", "syntax", "-", "free", ".", "zhou2015end", ",", "marcheggiani2017simple", "and", "he2017deep", "all", "use", "variants", "of", "deep", "LSTMs", "with", "constrained", "decoding", ",", "while", "tan2018deep", "apply", "self", "-", "attention", "to", "obtain", "state", "-", "of", "-", "the", "-", "art", "SRL", "with", "gold", "predicates", ".", "Like", "this", "work", ",", "he2017deep", "present", "end", "-", "to", "-", "end", "experiments", ",", "predicting", "predicates", "using", "an", "LSTM", ",", "and", "he2018jointly", "jointly", "predict", "SRL", "spans", "and", "predicates", "in", "a", "model", "based", "on", "that", "of", "lee2017end", ",", "obtaining", "state", "-", "of", "-", "the", "-", "art", "predicted", "predicate", "SRL", ".", "Concurrent", "to", "this", "work", ",", "peters2018deep", "and", "he2018jointly", "report", "significant", "gains", "on", "PropBank", "SRL", "by", "training", "a", "wide", "LSTM", "language", "model", "and", "using", "a", "task", "-", "specific", "transformation", "of", "its", "hidden", "representations", "(", "ELMo", ")", "as", "a", "deep", ",", "and", "computationally", "expensive", ",", "alternative", "to", "typical", "word", "embeddings", ".", "We", "find", "that", "LISA", "obtains", "further", "accuracy", "increases", "when", "provided", "with", "ELMo", "word", "representations", ",", "especially", "on", "out", "-", "of", "-", "domain", "data", ".", "Some", "work", "has", "incorporated", "syntax", "into", "neural", "models", "for", "SRL", ".", "roth2016neural", "incorporate", "syntax", "by", "embedding", "dependency", "paths", ",", "and", "similarly", "marcheggiani2017encoding", "encode", "syntax", "using", "a", "graph", "CNN", "over", "a", "predicted", "syntax", "tree", ",", "out", "-", "performing", "models", "without", "syntax", "on", "CoNLL", "-", "2009", ".", "These", "works", "are", "limited", "to", "incorporating", "partial", "dependency", "paths", "between", "tokens", "whereas", "our", "technique", "incorporates", "the", "entire", "parse", ".", "Additionally", ",", "marcheggiani2017encoding", "report", "that", "their", "model", "does", "not", "out", "-", "perform", "syntax", "-", "free", "models", "on", "out", "-", "of", "-", "domain", "data", ",", "a", "setting", "in", "which", "our", "technique", "excels", ".", "MTL", "caruana1993multitask", "is", "popular", "in", "NLP", ",", "and", "others", "have", "proposed", "MTL", "models", "which", "incorporate", "subsets", "of", "the", "tasks", "we", "do", "collobert2011natural", ",", "zhang2016stack", ",", "hashimoto2017joint", ",", "peng2017deep", ",", "swayamdipta2017", ",", "and", "we", "build", "off", "work", "that", "investigates", "where", "and", "when", "to", "combine", "different", "tasks", "to", "achieve", "the", "best", "results", "sogaard2016deep", ",", "bingel2017identifying", ",", "alonso2017when", ".", "Our", "specific", "method", "of", "incorporating", "supervision", "into", "self", "-", "attention", "is", "most", "similar", "to", "the", "concurrent", "work", "of", "liu2018learning", ",", "who", "use", "edge", "marginals", "produced", "by", "the", "matrix", "-", "tree", "algorithm", "as", "attention", "weights", "for", "document", "classification", "and", "natural", "language", "inference", ".", "The", "question", "of", "training", "on", "gold", "versus", "predicted", "labels", "is", "closely", "related", "to", "learning", "to", "search", "daume2009search", ",", "ross2011reduction", ",", "chang2015learning", "and", "scheduled", "sampling", "bengio2015scheduled", ",", "with", "applications", "in", "NLP", "to", "sequence", "labeling", "and", "transition", "-", "based", "parsing", "choi2011getting", ",", "goldberg2012dynamic", ",", "ballesteros2016training", ".", "Our", "approach", "may", "be", "interpreted", "as", "an", "extension", "of", "teacher", "forcing", "williams1989learning", "to", "MTL", ".", "We", "leave", "exploration", "of", "more", "advanced", "scheduled", "sampling", "techniques", "to", "future", "work", ".", "section", ":", "Experimental", "results", "We", "present", "results", "on", "the", "CoNLL", "-", "2005", "shared", "task", "carreras2005introduction", "and", "the", "CoNLL", "-", "2012", "English", "subset", "of", "OntoNotes", "5.0", "pradhan2013towards", ",", "achieving", "state", "-", "of", "-", "the", "-", "art", "results", "for", "a", "single", "model", "with", "predicted", "predicates", "on", "both", "corpora", ".", "We", "experiment", "with", "both", "standard", "pre", "-", "trained", "GloVe", "word", "embeddings", "pennington2014glove", "and", "pre", "-", "trained", "ELMo", "representations", "with", "fine", "-", "tuned", "task", "-", "specific", "parameters", "peters2018deep", "in", "order", "to", "best", "compare", "to", "prior", "work", ".", "Hyperparameters", "that", "resulted", "in", "the", "best", "performance", "on", "the", "validation", "set", "were", "selected", "via", "a", "small", "grid", "search", ",", "and", "models", "were", "trained", "for", "a", "maximum", "of", "4", "days", "on", "one", "TitanX", "GPU", "using", "early", "stopping", "on", "the", "validation", "set", ".", "We", "convert", "constituencies", "to", "dependencies", "using", "the", "Stanford", "head", "rules", "v3.5", "deMarneffe2008", ".", "A", "detailed", "description", "of", "hyperparameter", "settings", "and", "data", "pre", "-", "processing", "can", "be", "found", "in", "Appendix", "[", "reference", "]", ".", "We", "compare", "our", "LISA", "models", "to", "four", "strong", "baselines", ":", "For", "experiments", "using", "predicted", "predicates", ",", "we", "compare", "to", "he2018jointly", "and", "the", "ensemble", "model", "(", "PoE", ")", "from", "he2017deep", ",", "as", "well", "as", "a", "version", "of", "our", "own", "self", "-", "attention", "model", "which", "does", "not", "incorporate", "syntactic", "information", "(", "SA", ")", ".", "To", "compare", "to", "more", "prior", "work", ",", "we", "present", "additional", "results", "on", "CoNLL", "-", "2005", "with", "models", "given", "gold", "predicates", "at", "test", "time", ".", "In", "these", "experiments", "we", "also", "compare", "to", "tan2018deep", ",", "the", "previous", "state", "-", "of", "-", "the", "art", "SRL", "model", "using", "gold", "predicates", "and", "standard", "embeddings", ".", "We", "demonstrate", "that", "our", "models", "benefit", "from", "injecting", "state", "-", "of", "-", "the", "-", "art", "predicted", "parses", "at", "test", "time", "(", "+", "D", "&", "M", ")", "by", "fixing", "the", "attention", "to", "parses", "predicted", "by", "dozat2017deep", ",", "the", "winner", "of", "the", "2017", "CoNLL", "shared", "task", "zeman2017conll", "which", "we", "re", "-", "train", "using", "ELMo", "embeddings", ".", "In", "all", "cases", ",", "using", "these", "parses", "at", "test", "time", "improves", "performance", ".", "We", "also", "evaluate", "our", "model", "using", "the", "gold", "syntactic", "parse", "at", "test", "time", "(", "+", "Gold", ")", ",", "to", "provide", "an", "upper", "bound", "for", "the", "benefit", "that", "syntax", "could", "have", "for", "SRL", "using", "LISA", ".", "These", "experiments", "show", "that", "despite", "LISA", "\u2019s", "strong", "performance", ",", "there", "remains", "substantial", "room", "for", "improvement", ".", "In", "\u00a7", "[", "reference", "]", "we", "perform", "further", "analysis", "comparing", "SRL", "models", "using", "gold", "and", "predicted", "parses", ".", "subsection", ":", "Semantic", "role", "labeling", "Table", "[", "reference", "]", "lists", "precision", ",", "recall", "and", "F1", "on", "the", "CoNLL", "-", "2005", "development", "and", "test", "sets", "using", "predicted", "predicates", ".", "For", "models", "using", "GloVe", "embeddings", ",", "our", "syntax", "-", "free", "SA", "model", "already", "achieves", "a", "new", "state", "-", "of", "-", "the", "-", "art", "by", "jointly", "predicting", "predicates", ",", "POS", "and", "SRL", ".", "LISA", "with", "its", "own", "parses", "performs", "comparably", "to", "SA", ",", "but", "when", "supplied", "with", "D", "&", "M", "parses", "LISA", "out", "-", "performs", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "by", "2.5", "F1", "points", ".", "On", "the", "out", "-", "of", "-", "domain", "Brown", "test", "set", ",", "LISA", "also", "performs", "comparably", "to", "its", "syntax", "-", "free", "counterpart", "with", "its", "own", "parses", ",", "but", "with", "D", "&", "M", "parses", "LISA", "performs", "exceptionally", "well", ",", "more", "than", "3.5", "F1", "points", "higher", "than", "he2018jointly", ".", "Incorporating", "ELMo", "embeddings", "improves", "all", "scores", ".", "The", "gap", "in", "SRL", "F1", "between", "models", "using", "LISA", "and", "D", "&", "M", "parses", "is", "smaller", "due", "to", "LISA", "\u2019s", "improved", "parsing", "accuracy", "(", "see", "\u00a7", "[", "reference", "]", ")", ",", "but", "LISA", "with", "D", "&", "M", "parses", "still", "achieves", "the", "highest", "F1", ":", "nearly", "1.0", "absolute", "F1", "higher", "than", "the", "previous", "state", "-", "of", "-", "the", "art", "on", "WSJ", ",", "and", "more", "than", "2.0", "F1", "higher", "on", "Brown", ".", "In", "both", "settings", "LISA", "leverages", "domain", "-", "agnostic", "syntactic", "information", "rather", "than", "over", "-", "fitting", "to", "the", "newswire", "training", "data", "which", "leads", "to", "high", "performance", "even", "on", "out", "-", "of", "-", "domain", "text", ".", "To", "compare", "to", "more", "prior", "work", "we", "also", "evaluate", "our", "models", "in", "the", "artificial", "setting", "where", "gold", "predicates", "are", "provided", "at", "test", "time", ".", "For", "fair", "comparison", "we", "use", "GloVe", "embeddings", ",", "provide", "predicate", "indicator", "embeddings", "on", "the", "input", "and", "re", "-", "encode", "the", "sequence", "relative", "to", "each", "gold", "predicate", ".", "Here", "LISA", "still", "excels", ":", "with", "D", "&", "M", "parses", ",", "LISA", "out", "-", "performs", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "by", "more", "than", "2", "F1", "on", "both", "WSJ", "and", "Brown", ".", "Table", "[", "reference", "]", "reports", "precision", ",", "recall", "and", "F1", "on", "the", "CoNLL", "-", "2012", "test", "set", ".", "We", "observe", "performance", "similar", "to", "that", "observed", "on", "ConLL", "-", "2005", ":", "Using", "GloVe", "embeddings", "our", "SA", "baseline", "already", "out", "-", "performs", "he2018jointly", "by", "nearly", "1.5", "F1", ".", "With", "its", "own", "parses", ",", "LISA", "slightly", "under", "-", "performs", "our", "syntax", "-", "free", "model", ",", "but", "when", "provided", "with", "stronger", "D", "&", "M", "parses", "LISA", "out", "-", "performs", "the", "state", "-", "of", "-", "the", "-", "art", "by", "more", "than", "2.5", "F1", ".", "Like", "CoNLL", "-", "2005", ",", "ELMo", "representations", "improve", "all", "models", "and", "close", "the", "F1", "gap", "between", "models", "supplied", "with", "LISA", "and", "D", "&", "M", "parses", ".", "On", "this", "dataset", "ELMo", "also", "substantially", "narrows", "the", "difference", "between", "models", "with", "-", "and", "without", "syntactic", "information", ".", "This", "suggests", "that", "for", "this", "challenging", "dataset", ",", "ELMo", "already", "encodes", "much", "of", "the", "information", "available", "in", "the", "D", "&", "M", "parses", ".", "Yet", ",", "higher", "accuracy", "parses", "could", "still", "yield", "improvements", "since", "providing", "gold", "parses", "increases", "F1", "by", "4", "points", "even", "with", "ELMo", "embeddings", ".", "subsection", ":", "Parsing", ",", "POS", "and", "predicate", "detection", "We", "first", "report", "the", "labeled", "and", "unlabeled", "attachment", "scores", "(", "LAS", ",", "UAS", ")", "of", "our", "parsing", "models", "on", "the", "CoNLL", "-", "2005", "and", "2012", "test", "sets", "(", "Table", "[", "reference", "]", ")", "with", "GloVe", "(", ")", "and", "ELMo", "(", ")", "embeddings", ".", "D", "&", "M", "achieves", "the", "best", "scores", ".", "Still", ",", "LISA", "\u2019s", "GloVe", "UAS", "is", "comparable", "to", "popular", "off", "-", "the", "-", "shelf", "dependency", "parsers", "such", "as", "spaCy", ",", "and", "with", "ELMo", "embeddings", "comparable", "to", "the", "standalone", "D", "&", "M", "parser", ".", "The", "difference", "in", "parse", "accuracy", "between", "LISA", "and", "D", "&", "M", "likely", "explains", "the", "large", "increase", "in", "SRL", "performance", "we", "see", "from", "decoding", "with", "D", "&", "M", "parses", "in", "that", "setting", ".", "In", "Table", "[", "reference", "]", "we", "present", "predicate", "detection", "precision", ",", "recall", "and", "F1", "on", "the", "CoNLL", "-", "2005", "and", "2012", "test", "sets", ".", "SA", "and", "LISA", "with", "and", "without", "ELMo", "attain", "comparable", "scores", "so", "we", "report", "only", "LISA", "+", "GloVe", ".", "We", "compare", "to", "he2017deep", "on", "CoNLL", "-", "2005", ",", "the", "only", "cited", "work", "reporting", "comparable", "predicate", "detection", "F1", ".", "LISA", "attains", "high", "predicate", "detection", "scores", ",", "above", "97", "F1", ",", "on", "both", "in", "-", "domain", "datasets", ",", "and", "out", "-", "performs", "he2017deep", "by", "1.5", "-", "2", "F1", "points", "even", "on", "the", "out", "-", "of", "-", "domain", "Brown", "test", "set", ",", "suggesting", "that", "multi", "-", "task", "learning", "works", "well", "for", "SRL", "predicate", "detection", ".", "subsection", ":", "Analysis", "First", "we", "assess", "SRL", "F1", "on", "sentences", "divided", "by", "parse", "accuracy", ".", "Table", "[", "reference", "]", "lists", "average", "SRL", "F1", "(", "across", "sentences", ")", "for", "the", "four", "conditions", "of", "LISA", "and", "D", "&", "M", "parses", "being", "correct", "or", "not", "(", "L\u00b1", ",", "D\u00b1", ")", ".", "Both", "parsers", "are", "correct", "on", "26", "%", "of", "sentences", ".", "Here", "there", "is", "little", "difference", "between", "any", "of", "the", "models", ",", "with", "LISA", "models", "tending", "to", "perform", "slightly", "better", "than", "SA", ".", "Both", "parsers", "make", "mistakes", "on", "the", "majority", "of", "sentences", "(", "57", "%", ")", ",", "difficult", "sentences", "where", "SA", "also", "performs", "the", "worst", ".", "These", "examples", "are", "likely", "where", "gold", "and", "D", "&", "M", "parses", "improve", "the", "most", "over", "other", "models", "in", "overall", "F1", ":", "Though", "both", "parsers", "fail", "to", "correctly", "parse", "the", "entire", "sentence", ",", "the", "D", "&", "M", "parser", "is", "less", "wrong", "(", "87.5", "vs.", "85.7", "average", "LAS", ")", ",", "leading", "to", "higher", "SRL", "F1", "by", "about", "1.5", "average", "F1", ".", "Following", "he2017deep", ",", "we", "next", "apply", "a", "series", "of", "corrections", "to", "model", "predictions", "in", "order", "to", "understand", "which", "error", "types", "the", "gold", "parse", "resolves", ":", "e.g.", "Fix", "Labels", "fixes", "labels", "on", "spans", "matching", "gold", "boundaries", ",", "and", "Merge", "Spans", "merges", "adjacent", "predicted", "spans", "into", "a", "gold", "span", ".", "[", "scale=0.52", "]", "errors.pdf", "In", "Figure", "[", "reference", "]", "we", "see", "that", "much", "of", "the", "performance", "gap", "between", "the", "gold", "and", "predicted", "parses", "is", "due", "to", "span", "boundary", "errors", "(", "Merge", "Spans", ",", "Split", "Spans", "and", "Fix", "Span", "Boundary", ")", ",", "which", "supports", "the", "hypothesis", "proposed", "by", "he2017deep", "that", "incorporating", "syntax", "could", "be", "particularly", "helpful", "for", "resolving", "these", "errors", ".", "he2017deep", "also", "point", "out", "that", "these", "errors", "are", "due", "mainly", "to", "prepositional", "phrase", "(", "PP", ")", "attachment", "mistakes", ".", "We", "also", "find", "this", "to", "be", "the", "case", ":", "Figure", "[", "reference", "]", "shows", "a", "breakdown", "of", "split", "/", "merge", "corrections", "by", "phrase", "type", ".", "Though", "the", "number", "of", "corrections", "decreases", "substantially", "across", "phrase", "types", ",", "the", "proportion", "of", "corrections", "attributed", "to", "PPs", "remains", "the", "same", "(", "approx", ".", "50", "%", ")", "even", "after", "providing", "the", "correct", "PP", "attachment", "to", "the", "model", ",", "indicating", "that", "PP", "span", "boundary", "mistakes", "are", "a", "fundamental", "difficulty", "for", "SRL", ".", "[", "scale=0.55", "]", "phrase_bar_percent.pdf", "section", ":", "Conclusion", "We", "present", "linguistically", "-", "informed", "self", "-", "attention", ":", "a", "multi", "-", "task", "neural", "network", "model", "that", "effectively", "incorporates", "rich", "linguistic", "information", "for", "semantic", "role", "labeling", ".", "LISA", "out", "-", "performs", "the", "state", "-", "of", "-", "the", "-", "art", "on", "two", "benchmark", "SRL", "datasets", ",", "including", "out", "-", "of", "-", "domain", ".", "Future", "work", "will", "explore", "improving", "LISA", "\u2019s", "parsing", "accuracy", ",", "developing", "better", "training", "techniques", "and", "adapting", "to", "more", "tasks", ".", "section", ":", "Acknowledgments", "We", "are", "grateful", "to", "Luheng", "He", "for", "helpful", "discussions", "and", "code", ",", "Timothy", "Dozat", "for", "sharing", "his", "code", ",", "and", "to", "the", "NLP", "reading", "groups", "at", "Google", "and", "UMass", "and", "the", "anonymous", "reviewers", "for", "feedback", "on", "drafts", "of", "this", "work", ".", "This", "work", "was", "supported", "in", "part", "by", "an", "IBM", "PhD", "Fellowship", "Award", "to", "E.S.", ",", "in", "part", "by", "the", "Center", "for", "Intelligent", "Information", "Retrieval", ",", "and", "in", "part", "by", "the", "National", "Science", "Foundation", "under", "Grant", "Nos", ".", "DMR", "-", "1534431", "and", "IIS", "-", "1514053", ".", "Any", "opinions", ",", "findings", ",", "conclusions", "or", "recommendations", "expressed", "in", "this", "material", "are", "those", "of", "the", "authors", "and", "do", "not", "necessarily", "reflect", "those", "of", "the", "sponsor", ".", "bibliography", ":", "References", "appendix", ":", "Supplemental", "Material", "subsection", ":", "Supplemental", "analysis", "Here", "we", "continue", "the", "analysis", "from", "\u00a7", "[", "reference", "]", ".", "All", "experiments", "in", "this", "section", "are", "performed", "on", "CoNLL", "-", "2005", "development", "data", "unless", "stated", "otherwise", ".", "First", ",", "we", "compare", "the", "impact", "of", "Viterbi", "decoding", "with", "LISA", ",", "D", "&", "M", ",", "and", "gold", "syntax", "trees", "(", "Table", "[", "reference", "]", ")", ",", "finding", "the", "same", "trends", "across", "both", "datasets", ".", "We", "find", "that", "Viterbi", "has", "nearly", "the", "same", "impact", "for", "LISA", ",", "D", "&", "M", "and", "gold", "parses", ":", "Gold", "parses", "provide", "little", "improvement", "over", "predicted", "parses", "in", "terms", "of", "BIO", "label", "consistency", ".", "[", "scale=0.52", "]", "f1_by_sent_len.pdf", "[", "scale=0.52", "]", "f1_by_pred_dist.pdf", "We", "also", "assess", "SRL", "F1", "as", "a", "function", "of", "sentence", "length", "and", "distance", "from", "span", "to", "predicate", ".", "In", "Figure", "[", "reference", "]", "we", "see", "that", "providing", "LISA", "with", "gold", "parses", "is", "particularly", "helpful", "for", "sentences", "longer", "than", "10", "tokens", ".", "This", "likely", "directly", "follows", "from", "the", "tendency", "of", "syntactic", "parsers", "to", "perform", "worse", "on", "longer", "sentences", ".", "With", "respect", "to", "distance", "between", "arguments", "and", "predicates", ",", "(", "Figure", "[", "reference", "]", ")", ",", "we", "do", "not", "observe", "this", "same", "trend", ",", "with", "all", "distances", "performing", "better", "with", "better", "parses", ",", "and", "especially", "gold", ".", "subsection", ":", "Supplemental", "results", "Due", "to", "space", "constraints", "in", "the", "main", "paper", "we", "list", "additional", "experimental", "results", "here", ".", "Table", "[", "reference", "]", "lists", "development", "scores", "on", "the", "CoNLL", "-", "2005", "dataset", "with", "predicted", "predicates", ",", "which", "follow", "the", "same", "trends", "as", "the", "test", "data", ".", "subsection", ":", "Data", "and", "pre", "-", "processing", "details", "We", "initialize", "word", "embeddings", "with", "100d", "pre", "-", "trained", "GloVe", "embeddings", "trained", "on", "6", "billion", "tokens", "of", "Wikipedia", "and", "Gigaword", "pennington2014glove", ".", "We", "evaluate", "the", "SRL", "performance", "of", "our", "models", "using", "the", "srl", "-", "eval.pl", "script", "provided", "by", "the", "CoNLL", "-", "2005", "shared", "task", ",", "which", "computes", "segment", "-", "level", "precision", ",", "recall", "and", "F1", "score", ".", "We", "also", "report", "the", "predicate", "detection", "scores", "output", "by", "this", "script", ".", "We", "evaluate", "parsing", "using", "the", "eval.pl", "CoNLL", "script", ",", "which", "excludes", "punctuation", ".", "We", "train", "distinct", "D", "&", "M", "parsers", "for", "CoNLL", "-", "2005", "and", "CoNLL", "-", "2012", ".", "Our", "D", "&", "M", "parsers", "are", "trained", "and", "validated", "using", "the", "same", "SRL", "data", "splits", ",", "except", "that", "for", "CoNLL", "-", "2005", "section", "22", "is", "used", "for", "development", "(", "rather", "than", "24", ")", ",", "as", "this", "section", "is", "typically", "used", "for", "validation", "in", "PTB", "parsing", ".", "We", "use", "Stanford", "dependencies", "v3.5", "deMarneffe2008", "and", "POS", "tags", "from", "the", "Stanford", "CoreNLP", "left3words", "model", "toutanova2003feature", ".", "We", "use", "the", "pre", "-", "trained", "ELMo", "models", "and", "learn", "task", "-", "specific", "combinations", "of", "the", "ELMo", "representations", "which", "are", "provided", "as", "input", "instead", "of", "GloVe", "embeddings", "to", "the", "D", "&", "M", "parser", "with", "otherwise", "default", "settings", ".", "subsubsection", ":", "CoNLL", "-", "2012", "We", "follow", "the", "CoNLL", "-", "2012", "split", "used", "by", "he2018jointly", "to", "evaluate", "our", "models", ",", "which", "uses", "the", "annotations", "from", "here", "but", "the", "subset", "of", "those", "documents", "from", "the", "CoNLL", "-", "2012", "co", "-", "reference", "split", "described", "here", "pradhan2013towards", ".", "This", "dataset", "is", "drawn", "from", "seven", "domains", ":", "newswire", ",", "web", ",", "broadcast", "news", "and", "conversation", ",", "magazines", ",", "telephone", "conversations", ",", "and", "text", "from", "the", "bible", ".", "The", "text", "is", "annotated", "with", "gold", "part", "-", "of", "-", "speech", ",", "syntactic", "constituencies", ",", "named", "entities", ",", "word", "sense", ",", "speaker", ",", "co", "-", "reference", "and", "semantic", "role", "labels", "based", "on", "the", "PropBank", "guidelines", "palmer2005proposition", ".", "Propositions", "may", "be", "verbal", "or", "nominal", ",", "and", "there", "are", "41", "distinct", "semantic", "role", "labels", ",", "excluding", "continuation", "roles", "and", "including", "the", "predicate", ".", "We", "convert", "the", "semantic", "proposition", "and", "role", "segmentations", "to", "BIO", "boundary", "-", "encoded", "tags", ",", "resulting", "in", "129", "distinct", "BIO", "-", "encoded", "tags", "(", "including", "continuation", "roles", ")", ".", "subsubsection", ":", "CoNLL", "-", "2005", "The", "CoNLL", "-", "2005", "data", "carreras2005introduction", "is", "based", "on", "the", "original", "PropBank", "corpus", "palmer2005proposition", ",", "which", "labels", "the", "Wall", "Street", "Journal", "portion", "of", "the", "Penn", "TreeBank", "corpus", "(", "PTB", ")", "marcus1993building", "with", "predicate", "-", "argument", "structures", ",", "plus", "a", "challenging", "out", "-", "of", "-", "domain", "test", "set", "derived", "from", "the", "Brown", "corpus", "francis1964manual", ".", "This", "dataset", "contains", "only", "verbal", "predicates", ",", "though", "some", "are", "multi", "-", "word", "verbs", ",", "and", "28", "distinct", "role", "label", "types", ".", "We", "obtain", "105", "SRL", "labels", "including", "continuations", "after", "encoding", "predicate", "argument", "segment", "boundaries", "with", "BIO", "tags", ".", "subsection", ":", "Optimization", "and", "hyperparameters", "We", "train", "the", "model", "using", "the", "Nadam", "dozat2016incorporating", "algorithm", "for", "adaptive", "stochastic", "gradient", "descent", "(", "SGD", ")", ",", "which", "combines", "Adam", "kingma2014adam", "SGD", "with", "Nesterov", "momentum", "nesterov1983method", ".", "We", "additionally", "vary", "the", "learning", "rate", "as", "a", "function", "of", "an", "initial", "learning", "rate", "and", "the", "current", "training", "step", "as", "described", "in", "vaswani2017attention", "using", "the", "following", "function", ":", "lr", "=", "lr_0", "(", "step^", "-", "0.5", ",", "step", "warm^", "-", "1.5", ")", "which", "increases", "the", "learning", "rate", "linearly", "for", "the", "first", "training", "steps", ",", "then", "decays", "it", "proportionally", "to", "the", "inverse", "square", "root", "of", "the", "step", "number", ".", "We", "found", "this", "learning", "rate", "schedule", "essential", "for", "training", "the", "self", "-", "attention", "model", ".", "We", "only", "update", "optimization", "moving", "-", "average", "accumulators", "for", "parameters", "which", "receive", "gradient", "updates", "at", "a", "given", "step", ".", "In", "all", "of", "our", "experiments", "we", "used", "initial", "learning", "rate", "0.04", ",", ",", ",", "and", "dropout", "rates", "of", "0.1", "everywhere", ".", "We", "use", "10", "or", "12", "self", "-", "attention", "layers", "made", "up", "of", "8", "attention", "heads", "each", "with", "embedding", "dimension", "25", ",", "with", "800d", "feed", "-", "forward", "projections", ".", "In", "the", "syntactically", "-", "informed", "attention", "head", ",", "has", "dimension", "500", "and", "has", "dimension", "100", ".", "The", "size", "of", "and", "representations", "and", "the", "representation", "used", "for", "joint", "part", "-", "of", "-", "speech", "/", "predicate", "classification", "is", "200", ".", "We", "train", "with", "warmup", "steps", "and", "clip", "gradient", "norms", "to", "1", ".", "We", "use", "batches", "of", "approximately", "5000", "tokens", "."]}