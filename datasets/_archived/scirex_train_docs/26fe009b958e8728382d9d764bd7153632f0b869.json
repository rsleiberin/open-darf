{"coref": {"300D_Residual_stacked_encoders": [[2255, 2260]], "600D_Residual_stacked_encoders": [], "Natural_Language_Inference": [[20, 26], [105, 114], [158, 161], [162, 163], [180, 183], [246, 247], [607, 613], [2261, 2264]], "Parameters": [], "SNLI": [[210, 215], [216, 217], [419, 420], [568, 569], [845, 846], [1244, 1245], [1661, 1662], [2030, 2031], [2125, 2126], [2219, 2220], [2388, 2389], [150, 151], [620, 621], [664, 665], [1381, 1382], [1431, 1432], [1445, 1446], [1461, 1462], [2147, 2148], [2156, 2157], [2298, 2299], [2370, 2371], [2441, 2442], [2471, 2472]], "__Test_Accuracy": [[1726, 1727], [1734, 1735], [2343, 2344], [1817, 1818]], "__Train_Accuracy": [[1478, 1480]]}, "coref_non_salient": {"0": [[1722, 1724], [1892, 1896]], "1": [[989, 991], [2426, 2430]], "10": [[707, 709], [1056, 1058], [1106, 1108], [1161, 1163], [1259, 1261], [1303, 1305]], "11": [[734, 738], [2105, 2108]], "12": [[165, 171], [1926, 1930]], "13": [[1931, 1935]], "14": [[326, 337], [402, 403]], "15": [[481, 491], [866, 869], [32, 35]], "16": [[145, 146], [2214, 2215]], "17": [[879, 882], [1200, 1204]], "18": [[69, 70], [539, 540], [565, 567], [711, 712], [837, 839], [1511, 1512], [1982, 1983]], "19": [[41, 47], [509, 514]], "2": [[125, 127], [220, 227], [228, 231], [1246, 1249], [1369, 1372], [1663, 1668], [1854, 1857], [2007, 2010], [2034, 2037], [449, 452], [641, 644], [1355, 1358], [1420, 1423], [1643, 1646], [2277, 2280]], "20": [[870, 871], [884, 885], [1102, 1103], [1568, 1569], [416, 417], [888, 889], [903, 904], [915, 916], [950, 951], [1082, 1083], [1124, 1125], [1197, 1198], [1683, 1684], [1696, 1697], [1753, 1754], [1809, 1810], [1843, 1844], [1953, 1954], [2045, 2046], [2077, 2078], [2166, 2167], [2247, 2248], [2405, 2406], [2458, 2459]], "21": [[2, 7], [93, 98], [521, 525], [577, 581], [2060, 2065], [2130, 2134], [2191, 2195], [2312, 2317]], "22": [[923, 927], [935, 939], [1710, 1716]], "23": [[303, 307], [433, 437], [832, 834]], "24": [[2028, 2029]], "25": [[38, 40], [492, 494], [972, 976], [1001, 1003], [2227, 2232], [2353, 2355], [2449, 2453]], "26": [[1554, 1558]], "27": [[1072, 1074]], "28": [[1609, 1611]], "29": [[1987, 1988]], "3": [[8, 12], [1004, 1007]], "30": [[1151, 1156]], "31": [[50, 52], [517, 519]], "32": [[174, 176], [2320, 2322]], "33": [[997, 999]], "34": [[338, 339]], "35": [[1551, 1553]], "36": [[2446, 2448]], "37": [[344, 348]], "38": [[1481, 1485]], "39": [[1964, 1966], [2412, 2414]], "4": [[1597, 1599], [2011, 2014]], "40": [[1336, 1338]], "41": [[72, 74], [542, 544]], "42": [[987, 988]], "43": [[2338, 2340]], "44": [[1331, 1333], [1509, 1510], [1527, 1528], [1579, 1580]], "45": [[601, 606]], "46": [[717, 718], [1341, 1342]], "47": [[1113, 1118]], "48": [[1492, 1494]], "49": [[1472, 1476]], "5": [[324, 325], [364, 368], [818, 820], [2463, 2468]], "50": [[321, 322]], "51": [[658, 659]], "52": [[241, 245]], "53": [[381, 382]], "54": [[257, 263]], "55": [[369, 371]], "56": [[992, 994]], "57": [[1426, 1428]], "58": [[1515, 1517]], "59": [[1227, 1233]], "6": [[375, 380], [1271, 1273], [1292, 1294]], "60": [[1827, 1835]], "61": [[995, 996]], "62": [[1092, 1094]], "7": [[55, 56], [586, 587], [856, 858], [1659, 1660], [2295, 2296], [18, 19], [28, 29], [103, 104], [310, 311], [319, 320], [390, 391], [399, 400], [412, 413], [446, 447], [455, 456], [692, 693], [700, 701], [743, 744], [788, 789], [792, 793], [860, 861], [1180, 1181], [2003, 2004], [2100, 2101], [2144, 2145], [2207, 2208], [2234, 2235]], "8": [[638, 640], [2116, 2121], [2274, 2276]], "9": [[695, 697], [1174, 1176], [1253, 1255]]}, "doc_id": "26fe009b958e8728382d9d764bd7153632f0b869", "method_subrelations": {"300D_Residual_stacked_encoders": [[[0, 30], "300D_Residual_stacked_encoders"]], "600D_Residual_stacked_encoders": [[[0, 30], "600D_Residual_stacked_encoders"]]}, "n_ary_relations": [{"Material": "SNLI", "Method": "300D_Residual_stacked_encoders", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "85.7"}, {"Material": "SNLI", "Method": "600D_Residual_stacked_encoders", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "86.0"}, {"Material": "SNLI", "Method": "300D_Residual_stacked_encoders", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "89.8"}, {"Material": "SNLI", "Method": "600D_Residual_stacked_encoders", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "91.0"}, {"Material": "SNLI", "Method": "300D_Residual_stacked_encoders", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "9.7m"}, {"Material": "SNLI", "Method": "600D_Residual_stacked_encoders", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "29m"}], "ner": [[2, 7, "Method"], [8, 12, "Task"], [20, 26, "Task"], [38, 40, "Method"], [41, 47, "Method"], [50, 52, "Method"], [55, 56, "Method"], [69, 70, "Method"], [72, 74, "Method"], [93, 98, "Method"], [105, 114, "Task"], [125, 127, "Material"], [145, 146, "Metric"], [158, 161, "Task"], [162, 163, "Task"], [165, 171, "Task"], [174, 176, "Task"], [180, 183, "Task"], [210, 215, "Material"], [216, 217, "Material"], [220, 227, "Material"], [228, 231, "Material"], [241, 245, "Method"], [246, 247, "Task"], [257, 263, "Task"], [303, 307, "Method"], [321, 322, "Method"], [324, 325, "Method"], [326, 337, "Method"], [338, 339, "Method"], [344, 348, "Method"], [364, 368, "Method"], [369, 371, "Method"], [375, 380, "Method"], [381, 382, "Method"], [402, 403, "Method"], [419, 420, "Material"], [433, 437, "Method"], [481, 491, "Method"], [492, 494, "Method"], [509, 514, "Method"], [517, 519, "Method"], [521, 525, "Method"], [539, 540, "Method"], [542, 544, "Method"], [565, 567, "Method"], [568, 569, "Material"], [577, 581, "Method"], [586, 587, "Method"], [601, 606, "Metric"], [607, 613, "Task"], [638, 640, "Task"], [658, 659, "Task"], [695, 697, "Method"], [707, 709, "Method"], [711, 712, "Method"], [717, 718, "Task"], [734, 738, "Method"], [818, 820, "Method"], [832, 834, "Method"], [837, 839, "Method"], [845, 846, "Material"], [856, 858, "Method"], [866, 869, "Method"], [870, 871, "Method"], [879, 882, "Method"], [884, 885, "Method"], [923, 927, "Method"], [935, 939, "Method"], [972, 976, "Method"], [987, 988, "Method"], [989, 991, "Task"], [992, 994, "Method"], [995, 996, "Task"], [997, 999, "Task"], [1001, 1003, "Method"], [1004, 1007, "Task"], [1056, 1058, "Method"], [1072, 1074, "Method"], [1092, 1094, "Method"], [1102, 1103, "Method"], [1106, 1108, "Method"], [1113, 1118, "Method"], [1151, 1156, "Method"], [1161, 1163, "Method"], [1174, 1176, "Method"], [1200, 1204, "Method"], [1227, 1233, "Method"], [1244, 1245, "Material"], [1246, 1249, "Material"], [1253, 1255, "Method"], [1259, 1261, "Method"], [1271, 1273, "Method"], [1292, 1294, "Method"], [1303, 1305, "Method"], [1331, 1333, "Method"], [1336, 1338, "Method"], [1341, 1342, "Task"], [1369, 1372, "Material"], [1426, 1428, "Task"], [1472, 1476, "Metric"], [1478, 1480, "Metric"], [1481, 1485, "Method"], [1492, 1494, "Metric"], [1509, 1510, "Method"], [1511, 1512, "Method"], [1515, 1517, "Method"], [1527, 1528, "Method"], [1551, 1553, "Task"], [1554, 1558, "Method"], [1568, 1569, "Method"], [1579, 1580, "Method"], [1597, 1599, "Task"], [1609, 1611, "Method"], [1659, 1660, "Method"], [1661, 1662, "Material"], [1663, 1668, "Material"], [1710, 1716, "Method"], [1722, 1724, "Method"], [1726, 1727, "Metric"], [1734, 1735, "Metric"], [1827, 1835, "Method"], [1854, 1857, "Material"], [1892, 1896, "Method"], [1926, 1930, "Task"], [1931, 1935, "Task"], [1964, 1966, "Method"], [1982, 1983, "Method"], [1987, 1988, "Method"], [2007, 2010, "Material"], [2011, 2014, "Task"], [2028, 2029, "Material"], [2030, 2031, "Material"], [2034, 2037, "Material"], [2060, 2065, "Method"], [2105, 2108, "Method"], [2116, 2121, "Task"], [2125, 2126, "Material"], [2130, 2134, "Method"], [2191, 2195, "Method"], [2214, 2215, "Metric"], [2219, 2220, "Material"], [2227, 2232, "Method"], [2255, 2260, "Method"], [2261, 2264, "Task"], [2274, 2276, "Task"], [2295, 2296, "Method"], [2312, 2317, "Method"], [2320, 2322, "Task"], [2338, 2340, "Method"], [2343, 2344, "Metric"], [2353, 2355, "Method"], [2388, 2389, "Material"], [2412, 2414, "Method"], [2426, 2430, "Task"], [2446, 2448, "Method"], [2449, 2453, "Method"], [2463, 2468, "Method"], [18, 19, "Method"], [28, 29, "Method"], [32, 35, "Method"], [103, 104, "Method"], [150, 151, "Material"], [310, 311, "Method"], [319, 320, "Method"], [390, 391, "Method"], [399, 400, "Method"], [412, 413, "Method"], [416, 417, "Method"], [446, 447, "Method"], [449, 452, "Material"], [455, 456, "Method"], [620, 621, "Material"], [641, 644, "Material"], [664, 665, "Material"], [692, 693, "Method"], [700, 701, "Method"], [743, 744, "Method"], [788, 789, "Method"], [792, 793, "Method"], [860, 861, "Method"], [888, 889, "Method"], [903, 904, "Method"], [915, 916, "Method"], [950, 951, "Method"], [1082, 1083, "Method"], [1124, 1125, "Method"], [1180, 1181, "Method"], [1197, 1198, "Method"], [1355, 1358, "Material"], [1381, 1382, "Material"], [1420, 1423, "Material"], [1431, 1432, "Material"], [1445, 1446, "Material"], [1461, 1462, "Material"], [1643, 1646, "Material"], [1683, 1684, "Method"], [1696, 1697, "Method"], [1753, 1754, "Method"], [1809, 1810, "Method"], [1817, 1818, "Metric"], [1843, 1844, "Method"], [1953, 1954, "Method"], [2003, 2004, "Method"], [2045, 2046, "Method"], [2077, 2078, "Method"], [2100, 2101, "Method"], [2144, 2145, "Method"], [2147, 2148, "Material"], [2156, 2157, "Material"], [2166, 2167, "Method"], [2207, 2208, "Method"], [2234, 2235, "Method"], [2247, 2248, "Method"], [2277, 2280, "Material"], [2298, 2299, "Material"], [2370, 2371, "Material"], [2405, 2406, "Method"], [2441, 2442, "Material"], [2458, 2459, "Method"], [2471, 2472, "Material"]], "sections": [[0, 153], [153, 678], [678, 854], [854, 1251], [1251, 1343], [1343, 1347], [1347, 1466], [1466, 1590], [1590, 1595], [1595, 2005], [2005, 2236], [2236, 2323], [2323, 2477], [2477, 2520], [2520, 2523]], "sentences": [[0, 12], [12, 27], [27, 48], [48, 92], [92, 128], [128, 132], [132, 153], [153, 158], [158, 184], [184, 204], [204, 264], [264, 386], [386, 423], [423, 454], [454, 466], [466, 477], [477, 515], [515, 575], [575, 623], [623, 641], [641, 667], [667, 671], [671, 678], [678, 681], [681, 698], [698, 729], [729, 778], [778, 812], [812, 823], [823, 854], [854, 858], [858, 883], [883, 911], [911, 940], [940, 969], [969, 1033], [1033, 1041], [1041, 1076], [1076, 1095], [1095, 1131], [1131, 1178], [1178, 1213], [1213, 1251], [1251, 1255], [1255, 1299], [1299, 1312], [1312, 1343], [1343, 1347], [1347, 1350], [1350, 1415], [1415, 1420], [1420, 1429], [1429, 1466], [1466, 1470], [1470, 1490], [1490, 1503], [1503, 1515], [1515, 1536], [1536, 1551], [1551, 1590], [1590, 1595], [1595, 1600], [1600, 1616], [1616, 1649], [1649, 1669], [1669, 1691], [1691, 1717], [1717, 1757], [1757, 1793], [1793, 1837], [1837, 1901], [1901, 1956], [1956, 1971], [1971, 1994], [1994, 2005], [2005, 2014], [2014, 2032], [2032, 2054], [2054, 2097], [2097, 2122], [2122, 2160], [2160, 2185], [2185, 2236], [2236, 2239], [2239, 2265], [2265, 2277], [2277, 2301], [2301, 2323], [2323, 2330], [2330, 2356], [2356, 2432], [2432, 2477], [2477, 2480], [2480, 2491], [2491, 2520], [2520, 2523]], "words": ["document", ":", "Shortcut", "-", "Stacked", "Sentence", "Encoders", "for", "Multi", "-", "Domain", "Inference", "We", "present", "a", "simple", "sequential", "sentence", "encoder", "for", "multi", "-", "domain", "natural", "language", "inference", ".", "Our", "encoder", "is", "based", "on", "stacked", "bidirectional", "LSTM", "-", "RNNs", "with", "shortcut", "connections", "and", "fine", "-", "tuning", "of", "word", "embeddings", ".", "The", "overall", "supervised", "model", "uses", "the", "above", "encoder", "to", "encode", "two", "input", "sentences", "into", "two", "vectors", ",", "and", "then", "uses", "a", "classifier", "over", "the", "vector", "combination", "to", "label", "the", "relationship", "between", "these", "two", "sentences", "as", "that", "of", "entailment", ",", "contradiction", ",", "or", "neural", ".", "Our", "Shortcut", "-", "Stacked", "sentence", "encoders", "achieve", "strong", "improvements", "over", "existing", "encoders", "on", "matched", "and", "mismatched", "multi", "-", "domain", "natural", "language", "inference", "(", "top", "non", "-", "ensemble", "single", "-", "model", "result", "in", "the", "EMNLP", "RepEval", "2017", "Shared", "Task", ")", ".", "Moreover", ",", "they", "achieve", "the", "new", "state", "-", "of", "-", "the", "-", "art", "encoding", "result", "on", "the", "original", "SNLI", "dataset", ".", "section", ":", "Introduction", "and", "Background", "Natural", "language", "inference", "(", "NLI", ")", "or", "recognizing", "textual", "entailment", "(", "RTE", ")", "is", "a", "fundamental", "semantic", "task", "in", "the", "field", "of", "natural", "language", "processing", ".", "The", "problem", "is", "to", "determine", "whether", "a", "given", "hypothesis", "sentence", "can", "be", "logically", "inferred", "from", "a", "given", "premise", "sentence", ".", "Recently", "released", "datasets", "such", "as", "the", "Stanford", "Natural", "Language", "Inference", "Corpus", "(", "SNLI", ")", "and", "the", "Multi", "-", "Genre", "Natural", "Language", "Inference", "Corpus", "(", "Multi", "-", "NLI", ")", "have", "not", "only", "encouraged", "several", "end", "-", "to", "-", "end", "neural", "network", "approaches", "to", "NLI", ",", "but", "have", "also", "served", "as", "an", "evaluation", "resource", "for", "general", "representation", "learning", "of", "natural", "language", ".", "Depending", "on", "whether", "a", "model", "will", "first", "encode", "a", "sentence", "into", "a", "fixed", "-", "length", "vector", "without", "any", "incorporating", "information", "from", "the", "other", "sentence", ",", "the", "several", "proposed", "models", "can", "be", "categorized", "into", "two", "groups", ":", "(", "1", ")", "encoding", "-", "based", "models", "(", "or", "sentence", "encoders", ")", ",", "such", "as", "Tree", "-", "based", "CNN", "encoders", "(", "TBCNN", ")", "in", "mou2015natural", "or", "Stack", "-", "augmented", "Parser", "-", "Interpreter", "Neural", "Network", "(", "SPINN", ")", "in", "bowman2016fast", ",", "and", "(", "2", ")", "joint", ",", "pairwise", "models", "that", "use", "cross", "-", "features", "between", "the", "two", "sentences", "to", "encode", "them", ",", "such", "as", "the", "Enhanced", "Sequential", "Inference", "Model", "(", "ESIM", ")", "in", "chen2017enhanced", "or", "the", "bilateral", "multi", "-", "perspective", "matching", "(", "BiMPM", ")", "model", "wang2017bilateral", ".", "Moreover", ",", "common", "sentence", "encoders", "can", "again", "be", "classified", "into", "tree", "-", "based", "encoders", "such", "as", "SPINN", "in", "bowman2016fast", "which", "we", "mentioned", "before", ",", "or", "sequential", "encoders", "such", "as", "the", "biLSTM", "model", "by", "snli", ":", "emnlp2015", ".", "In", "this", "paper", ",", "we", "follow", "the", "former", "approach", "of", "encoding", "-", "based", "models", ",", "and", "propose", "a", "novel", "yet", "simple", "sequential", "sentence", "encoder", "for", "the", "Multi", "-", "NLI", "problem", ".", "Our", "encoder", "does", "not", "require", "any", "syntactic", "information", "of", "the", "sentence", ".", "It", "also", "does", "not", "contain", "any", "attention", "or", "memory", "structure", ".", "It", "is", "basically", "a", "stacked", "(", "multi", "-", "layered", ")", "bidirectional", "LSTM", "-", "RNN", "with", "shortcut", "connections", "(", "feeding", "all", "previous", "layers", "\u2019", "outputs", "and", "word", "embeddings", "to", "each", "layer", ")", "and", "word", "embedding", "fine", "-", "tuning", ".", "The", "overall", "supervised", "model", "uses", "these", "shortcut", "-", "stacked", "encoders", "to", "encode", "two", "input", "sentences", "into", "two", "vectors", ",", "and", "then", "we", "use", "a", "classifier", "over", "the", "vector", "combination", "to", "label", "the", "relationship", "between", "these", "two", "sentences", "as", "that", "of", "entailment", ",", "contradiction", ",", "or", "neural", "(", "similar", "to", "the", "classifier", "setup", "of", "snli", ":", "emnlp2015", "and", "conneau2017supervised", ")", ".", "Our", "simple", "shortcut", "-", "stacked", "encoders", "achieve", "strong", "improvements", "over", "existing", "encoders", "due", "to", "its", "multi", "-", "layered", "and", "shortcut", "-", "connected", "properties", ",", "on", "both", "matched", "and", "mismatched", "evaluation", "settings", "for", "multi", "-", "domain", "natural", "language", "inference", ",", "as", "well", "as", "on", "the", "original", "SNLI", "dataset", ".", "It", "is", "the", "top", "single", "-", "model", "(", "non", "-", "ensemble", ")", "result", "in", "the", "EMNLP", "RepEval", "2017", "Multi", "-", "NLI", "Shared", "Task", ",", "and", "the", "new", "state", "-", "of", "-", "the", "-", "art", "for", "encoding", "-", "based", "results", "on", "the", "SNLI", "dataset", ".", "Github", "Code", "Link", ":", "https:", "//", "github.com", "/", "easonnie", "/", "multiNLI_encoder", "section", ":", "Model", "Our", "model", "mainly", "consists", "of", "two", "separate", "components", ",", "a", "sentence", "encoder", "and", "an", "entailment", "classifier", ".", "The", "sentence", "encoder", "compresses", "each", "source", "sentence", "into", "a", "vector", "representation", "and", "the", "classifier", "makes", "a", "three", "-", "way", "classification", "based", "on", "the", "two", "vectors", "of", "the", "two", "source", "sentences", ".", "The", "model", "follows", "the", "\u2018", "encoding", "-", "based", "rule", "\u2019", ",", "i.e.", ",", "the", "encoder", "will", "encode", "each", "source", "sentence", "into", "a", "fixed", "length", "vector", "without", "any", "information", "or", "function", "based", "on", "the", "other", "sentence", "(", "e.g.", ",", "cross", "-", "attention", "or", "memory", "comparing", "the", "two", "sentences", ")", ".", "In", "order", "to", "fully", "explore", "the", "generalization", "of", "the", "sentence", "encoder", ",", "the", "same", "encoder", "is", "applied", "to", "both", "the", "premise", "and", "the", "hypothesis", "with", "shared", "parameters", "projecting", "them", "into", "the", "same", "space", ".", "This", "setting", "follows", "the", "idea", "of", "Siamese", "Networks", "in", "bromley1994signature", ".", "Figure", "[", "reference", "]", "shows", "the", "overview", "of", "our", "encoding", "model", "(", "the", "standard", "classifier", "setup", "is", "not", "shown", "here", ";", "see", "snli", ":", "emnlp2015", "and", "conneau2017supervised", "for", "that", ")", ".", "subsection", ":", "Sentence", "Encoder", "Our", "sentence", "encoder", "is", "simply", "composed", "of", "multiple", "stacked", "bidirectional", "LSTM", "(", "biLSTM", ")", "layers", "with", "shortcut", "connections", "followed", "by", "a", "max", "pooling", "layer", ".", "Let", "bilstm", "represent", "the", "th", "biLSTM", "layer", ",", "which", "is", "defined", "as", ":", "where", "is", "the", "output", "of", "the", "th", "biLSTM", "at", "time", "t", "over", "input", "sequence", ".", "In", "a", "typical", "stacked", "biLSTM", "structure", ",", "the", "input", "of", "the", "next", "LSTM", "-", "RNN", "layer", "is", "simply", "the", "output", "sequence", "of", "the", "previous", "LSTM", "-", "RNN", "layer", ".", "In", "our", "settings", ",", "the", "input", "sequences", "for", "the", "th", "biLSTM", "layer", "are", "the", "concatenated", "outputs", "of", "all", "the", "previous", "layers", ",", "plus", "the", "original", "word", "embedding", "sequence", ".", "This", "gives", "a", "shortcut", "connection", "style", "setup", ",", "related", "to", "the", "widely", "used", "idea", "of", "residual", "connections", "in", "CNNs", "for", "computer", "vision", ",", "highway", "networks", "for", "RNNs", "in", "speech", "processing", ",", "and", "shortcut", "connections", "in", "hierarchical", "multitasking", "learning", ";", "but", "in", "our", "case", "we", "feed", "in", "all", "the", "previous", "layers", "\u2019", "output", "sequences", "as", "well", "as", "the", "word", "embedding", "sequence", "to", "every", "layer", ".", "Let", "represent", "words", "in", "the", "source", "sentence", ".", "We", "assume", "is", "a", "word", "embedding", "vector", "which", "are", "initialized", "using", "some", "pre", "-", "trained", "vector", "embeddings", "(", "and", "is", "then", "fine", "-", "tuned", "end", "-", "to", "-", "end", "via", "the", "NLI", "supervision", ")", ".", "Then", ",", "the", "input", "of", "th", "biLSTM", "layer", "at", "time", "is", "defined", "as", ":", "where", "represents", "vector", "concatenation", ".", "Then", ",", "assuming", "we", "have", "layers", "of", "biLSTM", ",", "the", "final", "vector", "representation", "will", "be", "obtained", "by", "applying", "row", "-", "max", "-", "pool", "over", "the", "output", "of", "the", "last", "biLSTM", "layer", ",", "similar", "to", "conneau2017supervised", ".", "The", "final", "layer", "is", "defined", "as", ":", "where", ",", ",", "is", "the", "dimension", "of", "the", "hidden", "state", "of", "the", "last", "forward", "and", "backward", "LSTM", "layers", ",", "and", "is", "the", "final", "vector", "representation", "for", "the", "source", "sentence", "(", "which", "is", "later", "fed", "to", "the", "NLI", "classifier", ")", ".", "The", "closest", "encoder", "architecture", "to", "ours", "is", "that", "of", "conneau2017supervised", ",", "whose", "model", "consists", "of", "a", "single", "-", "layer", "biLSTM", "with", "a", "max", "-", "pooling", "layer", ",", "which", "we", "treat", "as", "our", "starting", "point", ".", "Our", "experiments", "(", "Section", "[", "reference", "]", ")", "demonstrate", "that", "our", "enhancements", "of", "the", "stacked", "-", "biRNN", "with", "shortcut", "connections", "provide", "significant", "gains", "on", "top", "of", "this", "baseline", "(", "for", "both", "SNLI", "and", "Multi", "-", "NLI", ")", ".", "subsection", ":", "Entailment", "Classifier", "After", "we", "obtain", "the", "vector", "representation", "for", "the", "premise", "and", "hypothesis", "sentence", ",", "we", "apply", "three", "matching", "methods", "to", "the", "two", "vectors", "for", "these", "two", "vectors", "and", "then", "concatenate", "these", "three", "match", "vectors", "(", "based", "on", "the", "heuristic", "matching", "presented", "in", "mou2015natural", ")", ".", "Let", "and", "be", "the", "vector", "representations", "for", "premise", "and", "hypothesis", ",", "respectively", ".", "The", "matching", "vector", "is", "then", "defined", "as", ":", "At", "last", ",", "we", "feed", "this", "final", "concatenated", "result", "into", "a", "MLP", "layer", "and", "use", "a", "softmax", "layer", "to", "make", "final", "classification", ".", "section", ":", "Experimental", "Setup", "subsection", ":", "Datasets", "As", "instructed", "in", "the", "RepEval", "Multi", "-", "NLI", "shared", "task", ",", "we", "use", "all", "of", "the", "training", "data", "in", "Multi", "-", "NLI", "combined", "with", "15", "%", "randomly", "selected", "samples", "from", "the", "SNLI", "training", "set", "resampled", "at", "each", "epoch", ")", "as", "our", "final", "training", "set", "for", "all", "models", ";", "and", "we", "use", "both", "the", "cross", "-", "domain", "(", "\u2018", "mismatched", "\u2019", ")", "and", "in", "-", "domain", "(", "\u2018", "matched", "\u2019", ")", "Multi", "-", "NLI", "development", "sets", "for", "model", "selection", ".", "For", "the", "SNLI", "test", "results", "in", "Table", "[", "reference", "]", ",", "we", "train", "on", "only", "the", "SNLI", "training", "set", "(", "and", "we", "also", "verify", "that", "the", "tuning", "decisions", "hold", "true", "on", "the", "SNLI", "dev", "set", ")", ".", "subsection", ":", "Parameter", "Settings", "We", "use", "cross", "-", "entropy", "loss", "as", "the", "training", "objective", "with", "Adam", "-", "based", "optimization", "with", "32", "batch", "size", ".", "The", "starting", "learning", "rate", "is", "0.0002", "with", "half", "decay", "every", "two", "epochs", ".", "The", "number", "of", "hidden", "units", "for", "MLP", "in", "classifier", "is", "1600", ".", "Dropout", "layer", "is", "also", "applied", "on", "the", "output", "of", "each", "layer", "of", "MLP", ",", "with", "dropout", "rate", "set", "to", "0.1", ".", "We", "used", "pre", "-", "trained", "300D", "Glove", "840B", "vectors", "to", "initialize", "the", "word", "embeddings", ".", "Tuning", "decisions", "for", "word", "embedding", "training", "strategy", ",", "the", "hyperparameters", "of", "dimension", "and", "number", "of", "layers", "for", "biLSTM", ",", "and", "the", "activation", "type", "and", "number", "of", "layers", "for", "MLP", ",", "are", "all", "explained", "in", "Section", "[", "reference", "]", ".", "section", ":", "Results", "and", "Analysis", "subsection", ":", "Ablation", "Analysis", "Results", "We", "now", "investigate", "the", "effectiveness", "of", "each", "of", "the", "enhancement", "components", "in", "our", "overall", "model", ".", "These", "ablation", "results", "are", "shown", "in", "Tables", "[", "reference", "]", ",", "[", "reference", "]", ",", "[", "reference", "]", "and", "[", "reference", "]", ",", "all", "based", "on", "the", "Multi", "-", "NLI", "development", "sets", ".", "Finally", ",", "Table", "[", "reference", "]", "shows", "results", "for", "different", "encoders", "on", "SNLI", "and", "Multi", "-", "NLI", "test", "sets", ".", "First", ",", "Table", "[", "reference", "]", "shows", "the", "performance", "changes", "for", "different", "number", "of", "biLSTM", "layers", "and", "their", "varying", "dimension", "size", ".", "The", "dimension", "size", "of", "a", "biLSTM", "layer", "is", "referring", "to", "the", "dimension", "of", "the", "hidden", "state", "for", "both", "the", "forward", "and", "backward", "LSTM", "-", "RNNs", ".", "As", "shown", ",", "each", "added", "layer", "model", "improves", "the", "accuracy", "and", "we", "achieve", "a", "substantial", "improvement", "in", "accuracy", "(", "around", "2", "%", ")", "on", "both", "matched", "and", "mismatched", "settings", ",", "compared", "to", "the", "single", "-", "layer", "biLSTM", "in", "conneau2017supervised", ".", "We", "only", "experimented", "with", "up", "to", "3", "layers", "with", "512", ",", "1024", ",", "2048", "dimensions", "each", ",", "so", "the", "model", "still", "has", "potential", "to", "improve", "the", "result", "further", "with", "a", "larger", "dimension", "and", "more", "layers", ".", "Next", ",", "in", "Table", "[", "reference", "]", ",", "we", "show", "that", "the", "shortcut", "connections", "among", "the", "biLSTM", "layers", "is", "also", "an", "important", "contributor", "to", "accuracy", "improvement", "(", "around", "1.5", "%", "on", "top", "of", "the", "full", "3", "-", "layered", "stacked", "-", "RNN", "model", ")", ".", "This", "demonstrates", "that", "simply", "stacking", "the", "biLSTM", "layers", "is", "not", "sufficient", "to", "handle", "a", "complex", "task", "like", "Multi", "-", "NLI", "and", "it", "is", "significantly", "better", "to", "have", "the", "higher", "layer", "connected", "to", "both", "the", "output", "and", "the", "original", "input", "of", "all", "the", "previous", "layers", "(", "note", "that", "Table", "[", "reference", "]", "results", "are", "based", "on", "multi", "-", "layered", "models", "with", "shortcut", "connections", ")", ".", "Next", ",", "in", "Table", "[", "reference", "]", ",", "we", "show", "that", "fine", "-", "tuning", "the", "word", "embeddings", "also", "improves", "results", ",", "again", "for", "both", "the", "in", "-", "domain", "task", "and", "cross", "-", "domain", "tasks", "(", "the", "ablation", "results", "are", "based", "on", "a", "smaller", "model", "with", "a", "128", "+", "256", "2", "-", "layer", "biLSTM", ")", ".", "Hence", ",", "all", "our", "models", "were", "trained", "with", "word", "embeddings", "being", "fine", "-", "tuned", ".", "The", "last", "ablation", "in", "Table", "[", "reference", "]", "shows", "that", "a", "classifier", "with", "two", "layers", "of", "relu", "is", "preferable", "than", "other", "options", ".", "Thus", ",", "we", "use", "that", "setting", "for", "our", "strongest", "encoder", ".", "subsection", ":", "Multi", "-", "NLI", "and", "SNLI", "Test", "Results", "Finally", ",", "in", "Table", "[", "reference", "]", ",", "we", "report", "the", "test", "results", "for", "MNLI", "and", "SNLI", ".", "First", "for", "Multi", "-", "NLI", ",", "we", "improve", "substantially", "over", "the", "CBOW", "and", "biLSTM", "Encoder", "baselines", "reported", "in", "the", "dataset", "paper", ".", "We", "also", "show", "that", "our", "final", "shortcut", "-", "based", "stacked", "encoder", "achieves", "around", "3", "%", "improvement", "as", "compared", "to", "the", "1", "-", "layer", "biLSTM", "-", "Max", "Encoder", "in", "the", "second", "last", "row", "(", "using", "the", "exact", "same", "classifier", "and", "optimizer", "settings", ")", ".", "Our", "shortcut", "-", "encoder", "was", "also", "the", "top", "singe", "-", "model", "(", "non", "-", "ensemble", ")", "result", "on", "the", "EMNLP", "RepEval", "Shared", "Task", "leaderboard", ".", "Next", ",", "for", "SNLI", ",", "we", "compare", "our", "shortcut", "-", "stacked", "encoder", "with", "the", "current", "state", "-", "of", "-", "the", "-", "art", "encoders", "from", "the", "SNLI", "leaderboard", "(", "https:", "//", "nlp.stanford.edu", "/", "projects", "/", "snli", "/", ")", ".", "We", "also", "compare", "to", "the", "recent", "biLSTM", "-", "Max", "Encoder", "of", "conneau2017supervised", ",", "which", "served", "as", "our", "model", "\u2019s", "1", "-", "layer", "starting", "point", ".", "The", "results", "indicate", "that", "\u2018", "Our", "Shortcut", "-", "Stacked", "Encoder", "\u2019", "surpasses", "all", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "encoders", ",", "and", "achieves", "the", "new", "best", "encoding", "-", "based", "result", "on", "SNLI", ",", "suggesting", "the", "general", "effectiveness", "of", "simple", "shortcut", "-", "connected", "stacked", "layers", "in", "sentence", "encoders", ".", "section", ":", "Conclusion", "We", "explored", "various", "simple", "combinations", "and", "connections", "of", "biLSTM", "-", "RNN", "layered", "architectures", "and", "developed", "a", "Shortcut", "-", "Stacked", "Sentence", "Encoder", "for", "natural", "language", "inference", ".", "Our", "model", "is", "the", "top", "single", "result", "in", "the", "EMNLP", "RepEval", "2017", "Multi", "-", "NLI", "Shared", "Task", ",", "and", "it", "also", "surpasses", "the", "state", "-", "of", "-", "the", "-", "art", "encoders", "for", "the", "SNLI", "dataset", ".", "In", "future", "work", ",", "we", "are", "also", "evaluating", "the", "effectiveness", "of", "shortcut", "-", "stacked", "sentence", "encoders", "on", "several", "other", "semantic", "tasks", ".", "section", ":", "Addendum", ":", "Shortcut", "vs.", "Residual", "In", "later", "experiments", ",", "we", "found", "that", "a", "residual", "connection", "can", "achieve", "similar", "accuracies", "with", "fewer", "number", "of", "parameters", ",", "compared", "to", "a", "shortcut", "connection", ".", "Therefore", ",", "in", "order", "to", "reduce", "the", "model", "size", "and", "to", "also", "follow", "the", "SNLI", "leader", "-", "board", "settings", "(", "e.g.", ",", "300D", "and", "600D", "embeddings", ")", ",", "we", "performed", "some", "additional", "SNLI", "experiments", "with", "the", "shortcut", "connections", "replaced", "with", "residual", "connections", ",", "where", "the", "input", "to", "each", "next", "biLSTM", "layer", "is", "the", "concatenation", "of", "the", "word", "embedding", "and", "the", "summation", "of", "outputs", "of", "all", "previous", "layers", "(", "related", "to", "ResNet", "in", "computer", "vision", ")", ".", "Table", "[", "reference", "]", "shows", "these", "residual", "-", "connection", "SNLI", "test", "results", "and", "the", "parameter", "comparison", "to", "shortcut", "-", "connection", "models", "(", "using", "3", "stacked", "-", "biLSTM", "layers", ",", "and", "one", "800", "-", "unit", "MLP", "layer", ",", "based", "on", "SNLI", "dev", "set", "tuning", ")", ".", "section", ":", "Acknowledgments", "We", "thank", "the", "shared", "task", "organizers", "and", "the", "anonymous", "reviewers", ".", "This", "work", "was", "partially", "supported", "by", "a", "Google", "Faculty", "Research", "Award", ",", "an", "IBM", "Faculty", "Award", ",", "a", "Bloomberg", "Data", "Science", "Research", "Grant", ",", "and", "NVidia", "GPU", "awards", ".", "bibliography", ":", "References"]}