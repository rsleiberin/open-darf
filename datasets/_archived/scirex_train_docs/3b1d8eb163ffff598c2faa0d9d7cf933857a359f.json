{"coref": {"KIM": [[40, 47], [2122, 2129], [4430, 4435], [4470, 4471], [5333, 5334], [5377, 5378], [5390, 5391], [5494, 5495], [5514, 5515], [5569, 5570], [5615, 5616], [5688, 5689], [5751, 5752], [5775, 5776], [4436, 4437], [4492, 4493], [5658, 5659]], "KIM_Ensemble": [], "Natural_Language_Inference": [[11, 15], [79, 82], [83, 84], [178, 179], [188, 191], [192, 193], [238, 242], [252, 255], [315, 316], [412, 413], [459, 460], [593, 594], [752, 755], [1162, 1163], [1649, 1650], [1671, 1672], [1682, 1683], [1730, 1733], [3899, 3902], [3939, 3945], [5480, 5481], [5767, 5770], [5846, 5847], [99, 100], [109, 110], [146, 147], [628, 629], [661, 662], [742, 743], [1212, 1213], [1222, 1223], [1240, 1241], [1245, 1246], [1283, 1284], [1361, 1362], [1471, 1472], [1486, 1487], [1545, 1546], [1566, 1567], [1581, 1582], [1634, 1635], [1696, 1697], [1706, 1707], [1810, 1811], [1932, 1933], [3145, 3146], [3915, 3916], [4059, 4060], [4279, 4280]], "Parameters": [], "SNLI": [[161, 162], [669, 670], [1291, 1292], [3923, 3927], [3928, 3929], [4696, 4697], [272, 273], [817, 818], [4041, 4042], [4143, 4144], [4389, 4390], [4462, 4463], [5297, 5298], [5610, 5611]], "__Test_Accuracy": [[4023, 4025], [4420, 4424], [4448, 4449], [4887, 4888], [4910, 4911], [4940, 4941], [4972, 4973], [5228, 5229], [5327, 5328], [5373, 5374], [5499, 5500], [5786, 5787]], "__Train_Accuracy": [[4448, 4449], [4887, 4888], [4910, 4911], [4940, 4941], [4972, 4973], [5327, 5328], [5373, 5374]]}, "coref_non_salient": {"0": [[1173, 1177], [2916, 2918]], "1": [[2247, 2251], [3062, 3063]], "10": [[2775, 2780], [2786, 2791]], "11": [[3646, 3648], [3688, 3690], [3736, 3738], [3767, 3769], [3790, 3792], [3843, 3845], [4554, 4557], [4573, 4576]], "12": [[2459, 2463], [5359, 5362]], "13": [[3653, 3655], [3713, 3715], [3728, 3730]], "14": [[4930, 4933], [5808, 5811]], "15": [[855, 861], [870, 876], [1059, 1065]], "16": [[2957, 2959], [2974, 2976], [3037, 3039]], "17": [[2810, 2812], [2835, 2837], [2845, 2848]], "18": [[4298, 4301]], "19": [[4843, 4845], [5813, 5815]], "2": [[1178, 1180], [4275, 4276], [4441, 4442], [4468, 4469], [4648, 4649], [4672, 4673], [4883, 4884], [5375, 5376], [5392, 5393], [5474, 5476], [5496, 5497], [5518, 5519], [5571, 5572], [5622, 5623], [5701, 5702]], "20": [[634, 637], [5096, 5099], [5803, 5807]], "21": [[5527, 5531], [5553, 5557]], "22": [[906, 909], [2940, 2941], [3054, 3060]], "23": [[198, 201], [756, 759]], "24": [[4728, 4729], [4736, 4738]], "25": [[401, 403], [920, 922], [1025, 1027], [1259, 1261], [1375, 1377], [1727, 1729]], "26": [[3298, 3299], [3804, 3805], [3847, 3848], [4353, 4354]], "27": [[3707, 3710]], "28": [[299, 305], [5760, 5766]], "29": [[2548, 2552]], "3": [[4351, 4352], [4596, 4598]], "30": [[3203, 3204]], "31": [[4234, 4235]], "32": [[5830, 5833]], "33": [[2961, 2963], [4265, 4267], [4395, 4397]], "34": [[1082, 1083], [1143, 1144], [1164, 1165], [1225, 1226], [2554, 2555], [4007, 4008], [273, 274], [283, 284], [332, 333], [340, 341], [348, 349], [356, 357], [364, 365], [372, 373], [380, 381], [388, 389], [818, 819], [827, 828], [879, 880], [932, 933], [941, 942], [950, 951], [962, 963], [970, 971], [978, 979], [986, 987], [998, 999], [1010, 1011], [1027, 1028], [1035, 1036], [1043, 1044], [1051, 1052], [1100, 1101], [1108, 1109], [1116, 1117], [1124, 1125], [1132, 1133], [1385, 1386], [1393, 1394], [1401, 1402], [1409, 1410], [1417, 1418], [1427, 1428], [1435, 1436], [1445, 1446], [1456, 1457], [1961, 1962], [1969, 1970], [2058, 2059], [2562, 2563], [3155, 3156], [3197, 3198], [3696, 3697], [3743, 3744], [3864, 3865], [3931, 3932], [3948, 3949], [4015, 4016], [4071, 4072], [4096, 4097], [4194, 4195], [4224, 4225], [4307, 4308], [5432, 5433]], "35": [[4315, 4319]], "36": [[4238, 4240]], "37": [[3092, 3096]], "38": [[3860, 3862]], "39": [[2038, 2040], [3799, 3801]], "4": [[163, 165], [281, 282], [671, 673], [1293, 1295], [4153, 4154], [4698, 4699], [826, 827], [3946, 3947], [4643, 4644], [5430, 5431]], "40": [[2734, 2742], [3011, 3019]], "41": [[2, 7], [129, 134]], "42": [[4027, 4029]], "43": [[2454, 2456]], "44": [[638, 641], [2414, 2417]], "45": [[958, 959], [1988, 1989], [2829, 2830], [2860, 2861]], "46": [[2720, 2725]], "47": [[3201, 3202]], "48": [[2744, 2747], [3020, 3023]], "49": [[4093, 4095]], "5": [[3313, 3314], [3400, 3401]], "50": [[176, 177]], "51": [[2253, 2256]], "52": [[3874, 3876]], "53": [[2281, 2284]], "54": [[2707, 2708]], "55": [[1068, 1073]], "56": [[212, 215]], "57": [[883, 884]], "58": [[2433, 2435]], "59": [[833, 834]], "6": [[208, 210], [589, 591], [1379, 1381]], "60": [[183, 187]], "61": [[926, 928]], "62": [[3105, 3109]], "63": [[885, 886]], "64": [[2171, 2173]], "65": [[2542, 2545]], "66": [[4055, 4058]], "67": [[423, 429], [3530, 3537]], "68": [[2932, 2934]], "69": [[820, 825]], "7": [[4963, 4965], [5100, 5101]], "70": [[2652, 2656]], "71": [[2954, 2956]], "72": [[3850, 3853]], "73": [[3237, 3238]], "74": [[3862, 3863]], "75": [[2897, 2898]], "76": [[829, 832]], "77": [[574, 576]], "78": [[1985, 1987]], "79": [[3165, 3166]], "8": [[202, 203], [789, 790]], "80": [[1717, 1723]], "81": [[36, 38]], "82": [[1156, 1161]], "83": [[643, 646]], "84": [[5365, 5366]], "85": [[2097, 2103]], "86": [[1653, 1654]], "87": [[888, 893]], "88": [[994, 998]], "89": [[1790, 1791]], "9": [[2855, 2858], [4114, 4116]]}, "doc_id": "3b1d8eb163ffff598c2faa0d9d7cf933857a359f", "method_subrelations": {"KIM": [[[0, 3], "KIM"]], "KIM_Ensemble": [[[0, 12], "KIM_Ensemble"]]}, "n_ary_relations": [{"Material": "SNLI", "Method": "KIM", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "88.6"}, {"Material": "SNLI", "Method": "KIM_Ensemble", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "89.1"}, {"Material": "SNLI", "Method": "KIM", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "94.1"}, {"Material": "SNLI", "Method": "KIM_Ensemble", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "93.6"}, {"Material": "SNLI", "Method": "KIM", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "4.3m"}, {"Material": "SNLI", "Method": "KIM_Ensemble", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "43m"}], "ner": [[2, 7, "Method"], [11, 15, "Task"], [36, 38, "Method"], [40, 47, "Method"], [79, 82, "Task"], [83, 84, "Task"], [129, 134, "Method"], [161, 162, "Material"], [163, 165, "Material"], [176, 177, "Task"], [178, 179, "Task"], [183, 187, "Task"], [188, 191, "Task"], [192, 193, "Task"], [198, 201, "Task"], [202, 203, "Task"], [208, 210, "Task"], [212, 215, "Task"], [238, 242, "Task"], [252, 255, "Task"], [281, 282, "Material"], [299, 305, "Method"], [315, 316, "Task"], [401, 403, "Method"], [412, 413, "Task"], [423, 429, "Task"], [459, 460, "Task"], [574, 576, "Method"], [589, 591, "Task"], [593, 594, "Task"], [634, 637, "Task"], [638, 641, "Task"], [643, 646, "Method"], [669, 670, "Material"], [671, 673, "Material"], [752, 755, "Task"], [756, 759, "Task"], [789, 790, "Task"], [820, 825, "Material"], [829, 832, "Material"], [833, 834, "Material"], [855, 861, "Method"], [870, 876, "Method"], [883, 884, "Material"], [885, 886, "Material"], [888, 893, "Method"], [906, 909, "Method"], [920, 922, "Method"], [926, 928, "Task"], [958, 959, "Method"], [994, 998, "Method"], [1025, 1027, "Method"], [1059, 1065, "Method"], [1068, 1073, "Method"], [1082, 1083, "Method"], [1143, 1144, "Method"], [1156, 1161, "Method"], [1162, 1163, "Task"], [1164, 1165, "Method"], [1173, 1177, "Method"], [1178, 1180, "Method"], [1225, 1226, "Method"], [1259, 1261, "Method"], [1291, 1292, "Material"], [1293, 1295, "Material"], [1375, 1377, "Method"], [1379, 1381, "Task"], [1649, 1650, "Task"], [1653, 1654, "Task"], [1671, 1672, "Task"], [1682, 1683, "Task"], [1717, 1723, "Task"], [1727, 1729, "Method"], [1730, 1733, "Task"], [1790, 1791, "Task"], [1985, 1987, "Method"], [1988, 1989, "Method"], [2038, 2040, "Method"], [2097, 2103, "Task"], [2122, 2129, "Method"], [2171, 2173, "Method"], [2247, 2251, "Method"], [2253, 2256, "Method"], [2281, 2284, "Task"], [2414, 2417, "Task"], [2433, 2435, "Method"], [2454, 2456, "Method"], [2459, 2463, "Method"], [2542, 2545, "Method"], [2548, 2552, "Method"], [2554, 2555, "Method"], [2652, 2656, "Method"], [2707, 2708, "Task"], [2720, 2725, "Method"], [2734, 2742, "Method"], [2744, 2747, "Method"], [2775, 2780, "Task"], [2786, 2791, "Task"], [2810, 2812, "Method"], [2829, 2830, "Method"], [2835, 2837, "Method"], [2845, 2848, "Method"], [2855, 2858, "Method"], [2860, 2861, "Method"], [2897, 2898, "Task"], [2916, 2918, "Method"], [2932, 2934, "Method"], [2940, 2941, "Method"], [2954, 2956, "Method"], [2957, 2959, "Method"], [2961, 2963, "Method"], [2974, 2976, "Method"], [3011, 3019, "Method"], [3020, 3023, "Method"], [3037, 3039, "Method"], [3054, 3060, "Method"], [3062, 3063, "Method"], [3092, 3096, "Metric"], [3105, 3109, "Task"], [3165, 3166, "Method"], [3201, 3202, "Method"], [3203, 3204, "Method"], [3237, 3238, "Method"], [3298, 3299, "Material"], [3313, 3314, "Method"], [3400, 3401, "Method"], [3530, 3537, "Task"], [3646, 3648, "Method"], [3653, 3655, "Task"], [3688, 3690, "Method"], [3707, 3710, "Method"], [3713, 3715, "Task"], [3728, 3730, "Task"], [3736, 3738, "Method"], [3767, 3769, "Method"], [3790, 3792, "Method"], [3799, 3801, "Method"], [3804, 3805, "Material"], [3843, 3845, "Method"], [3847, 3848, "Material"], [3850, 3853, "Method"], [3860, 3862, "Material"], [3862, 3863, "Material"], [3874, 3876, "Method"], [3899, 3902, "Task"], [3923, 3927, "Material"], [3928, 3929, "Material"], [3939, 3945, "Task"], [4007, 4008, "Method"], [4023, 4025, "Metric"], [4027, 4029, "Metric"], [4055, 4058, "Task"], [4093, 4095, "Method"], [4114, 4116, "Method"], [4153, 4154, "Material"], [4234, 4235, "Task"], [4238, 4240, "Metric"], [4265, 4267, "Method"], [4275, 4276, "Method"], [4298, 4301, "Method"], [4315, 4319, "Task"], [4351, 4352, "Method"], [4353, 4354, "Material"], [4395, 4397, "Method"], [4420, 4424, "Metric"], [4430, 4435, "Method"], [4441, 4442, "Method"], [4448, 4449, "Metric"], [4468, 4469, "Method"], [4470, 4471, "Method"], [4554, 4557, "Method"], [4573, 4576, "Method"], [4596, 4598, "Method"], [4648, 4649, "Method"], [4672, 4673, "Method"], [4696, 4697, "Material"], [4698, 4699, "Material"], [4728, 4729, "Task"], [4736, 4738, "Task"], [4843, 4845, "Task"], [4883, 4884, "Method"], [4887, 4888, "Metric"], [4910, 4911, "Metric"], [4930, 4933, "Task"], [4940, 4941, "Metric"], [4963, 4965, "Task"], [4972, 4973, "Metric"], [5096, 5099, "Task"], [5100, 5101, "Task"], [5228, 5229, "Metric"], [5327, 5328, "Metric"], [5333, 5334, "Method"], [5359, 5362, "Method"], [5365, 5366, "Task"], [5373, 5374, "Metric"], [5375, 5376, "Method"], [5377, 5378, "Method"], [5390, 5391, "Method"], [5392, 5393, "Method"], [5474, 5476, "Method"], [5480, 5481, "Task"], [5494, 5495, "Method"], [5496, 5497, "Method"], [5499, 5500, "Metric"], [5514, 5515, "Method"], [5518, 5519, "Method"], [5527, 5531, "Task"], [5553, 5557, "Task"], [5569, 5570, "Method"], [5571, 5572, "Method"], [5615, 5616, "Method"], [5622, 5623, "Method"], [5688, 5689, "Method"], [5701, 5702, "Method"], [5751, 5752, "Method"], [5760, 5766, "Method"], [5767, 5770, "Task"], [5775, 5776, "Method"], [5786, 5787, "Metric"], [5803, 5807, "Task"], [5808, 5811, "Task"], [5813, 5815, "Task"], [5830, 5833, "Method"], [5846, 5847, "Task"], [99, 100, "Task"], [109, 110, "Task"], [146, 147, "Task"], [272, 273, "Material"], [273, 274, "Method"], [283, 284, "Method"], [332, 333, "Method"], [340, 341, "Method"], [348, 349, "Method"], [356, 357, "Method"], [364, 365, "Method"], [372, 373, "Method"], [380, 381, "Method"], [388, 389, "Method"], [628, 629, "Task"], [661, 662, "Task"], [742, 743, "Task"], [817, 818, "Material"], [818, 819, "Method"], [826, 827, "Material"], [827, 828, "Method"], [879, 880, "Method"], [932, 933, "Method"], [941, 942, "Method"], [950, 951, "Method"], [962, 963, "Method"], [970, 971, "Method"], [978, 979, "Method"], [986, 987, "Method"], [998, 999, "Method"], [1010, 1011, "Method"], [1027, 1028, "Method"], [1035, 1036, "Method"], [1043, 1044, "Method"], [1051, 1052, "Method"], [1100, 1101, "Method"], [1108, 1109, "Method"], [1116, 1117, "Method"], [1124, 1125, "Method"], [1132, 1133, "Method"], [1212, 1213, "Task"], [1222, 1223, "Task"], [1240, 1241, "Task"], [1245, 1246, "Task"], [1283, 1284, "Task"], [1361, 1362, "Task"], [1385, 1386, "Method"], [1393, 1394, "Method"], [1401, 1402, "Method"], [1409, 1410, "Method"], [1417, 1418, "Method"], [1427, 1428, "Method"], [1435, 1436, "Method"], [1445, 1446, "Method"], [1456, 1457, "Method"], [1471, 1472, "Task"], [1486, 1487, "Task"], [1545, 1546, "Task"], [1566, 1567, "Task"], [1581, 1582, "Task"], [1634, 1635, "Task"], [1696, 1697, "Task"], [1706, 1707, "Task"], [1810, 1811, "Task"], [1932, 1933, "Task"], [1961, 1962, "Method"], [1969, 1970, "Method"], [2058, 2059, "Method"], [2562, 2563, "Method"], [3145, 3146, "Task"], [3155, 3156, "Method"], [3197, 3198, "Method"], [3696, 3697, "Method"], [3743, 3744, "Method"], [3864, 3865, "Method"], [3915, 3916, "Task"], [3931, 3932, "Method"], [3946, 3947, "Material"], [3948, 3949, "Method"], [4015, 4016, "Method"], [4041, 4042, "Material"], [4059, 4060, "Task"], [4071, 4072, "Method"], [4096, 4097, "Method"], [4143, 4144, "Material"], [4194, 4195, "Method"], [4224, 4225, "Method"], [4279, 4280, "Task"], [4307, 4308, "Method"], [4389, 4390, "Material"], [4436, 4437, "Method"], [4462, 4463, "Material"], [4492, 4493, "Method"], [4643, 4644, "Material"], [5297, 5298, "Material"], [5430, 5431, "Material"], [5432, 5433, "Method"], [5610, 5611, "Material"], [5658, 5659, "Method"]], "sections": [[0, 173], [173, 745], [745, 1464], [1464, 1655], [1655, 1950], [1950, 2095], [2095, 2412], [2412, 2773], [2773, 3097], [3097, 3103], [3103, 3109], [3109, 3644], [3644, 3913], [3913, 4119], [4119, 4363], [4363, 4367], [4367, 4726], [4726, 5265], [5265, 5407], [5407, 5597], [5597, 5756], [5756, 5848], [5848, 5863], [5863, 5866]], "sentences": [[0, 11], [11, 21], [21, 63], [63, 89], [89, 115], [115, 138], [138, 166], [166, 173], [173, 176], [176, 188], [188, 234], [234, 256], [256, 269], [269, 282], [282, 317], [317, 337], [337, 342], [342, 350], [350, 366], [366, 370], [370, 374], [374, 382], [382, 400], [400, 443], [443, 461], [461, 464], [464, 475], [475, 477], [477, 485], [485, 566], [566, 618], [618, 647], [647, 674], [674, 707], [707, 745], [745, 749], [749, 805], [805, 845], [845, 870], [870, 887], [887, 904], [904, 919], [919, 962], [962, 968], [968, 983], [983, 988], [988, 1048], [1048, 1059], [1059, 1082], [1082, 1084], [1084, 1090], [1090, 1102], [1102, 1110], [1110, 1113], [1113, 1126], [1126, 1140], [1140, 1164], [1164, 1166], [1166, 1202], [1202, 1218], [1218, 1267], [1267, 1296], [1296, 1329], [1329, 1364], [1364, 1390], [1390, 1399], [1399, 1407], [1407, 1410], [1410, 1411], [1411, 1429], [1429, 1437], [1437, 1447], [1447, 1458], [1458, 1464], [1464, 1476], [1476, 1518], [1518, 1548], [1548, 1564], [1564, 1624], [1624, 1655], [1655, 1659], [1659, 1687], [1687, 1712], [1712, 1734], [1734, 1792], [1792, 1813], [1813, 1847], [1847, 1879], [1879, 1924], [1924, 1950], [1950, 1956], [1956, 1966], [1966, 1971], [1971, 1991], [1991, 2010], [2010, 2041], [2041, 2060], [2060, 2095], [2095, 2103], [2103, 2130], [2130, 2174], [2174, 2200], [2200, 2227], [2227, 2242], [2242, 2281], [2281, 2316], [2316, 2380], [2380, 2384], [2384, 2412], [2412, 2420], [2420, 2469], [2469, 2496], [2496, 2521], [2521, 2529], [2529, 2570], [2570, 2598], [2598, 2647], [2647, 2663], [2663, 2703], [2703, 2718], [2718, 2729], [2729, 2773], [2773, 2780], [2780, 2792], [2792, 2859], [2859, 2893], [2893, 2915], [2915, 2948], [2948, 3001], [3001, 3024], [3024, 3061], [3061, 3078], [3078, 3097], [3097, 3103], [3103, 3109], [3109, 3114], [3114, 3160], [3160, 3167], [3167, 3188], [3188, 3237], [3237, 3266], [3266, 3283], [3283, 3285], [3285, 3303], [3303, 3313], [3313, 3353], [3353, 3370], [3370, 3400], [3400, 3412], [3412, 3429], [3429, 3445], [3445, 3458], [3458, 3468], [3468, 3501], [3501, 3523], [3523, 3538], [3538, 3555], [3555, 3570], [3570, 3644], [3644, 3648], [3648, 3671], [3671, 3691], [3691, 3720], [3720, 3731], [3731, 3751], [3751, 3770], [3770, 3793], [3793, 3798], [3798, 3838], [3838, 3913], [3913, 3917], [3917, 3930], [3930, 3945], [3945, 3952], [3952, 3970], [3970, 3997], [3997, 4009], [4009, 4017], [4017, 4030], [4030, 4069], [4069, 4073], [4073, 4088], [4088, 4104], [4104, 4119], [4119, 4123], [4123, 4131], [4131, 4164], [4164, 4185], [4185, 4215], [4215, 4223], [4223, 4229], [4229, 4242], [4242, 4251], [4251, 4275], [4275, 4324], [4324, 4349], [4349, 4363], [4363, 4367], [4367, 4371], [4371, 4392], [4392, 4425], [4425, 4465], [4465, 4489], [4489, 4507], [4507, 4529], [4529, 4546], [4546, 4587], [4587, 4632], [4632, 4646], [4646, 4668], [4668, 4689], [4689, 4726], [4726, 4730], [4730, 4747], [4747, 4785], [4785, 4786], [4786, 4808], [4808, 4809], [4809, 4859], [4859, 4892], [4892, 4900], [4900, 4922], [4922, 4933], [4933, 4935], [4935, 4956], [4956, 4965], [4965, 4987], [4987, 5008], [5008, 5041], [5041, 5074], [5074, 5087], [5087, 5117], [5117, 5132], [5132, 5169], [5169, 5201], [5201, 5227], [5227, 5237], [5237, 5265], [5265, 5273], [5273, 5291], [5291, 5329], [5329, 5367], [5367, 5390], [5390, 5407], [5407, 5413], [5413, 5434], [5434, 5466], [5466, 5482], [5482, 5490], [5490, 5514], [5514, 5545], [5545, 5558], [5558, 5597], [5597, 5601], [5601, 5625], [5625, 5634], [5634, 5647], [5647, 5656], [5656, 5697], [5697, 5727], [5727, 5749], [5749, 5756], [5756, 5759], [5759, 5788], [5788, 5816], [5816, 5826], [5826, 5848], [5848, 5851], [5851, 5863], [5863, 5866]], "words": ["document", ":", "Neural", "Natural", "Language", "Inference", "Models", "Enhanced", "with", "External", "Knowledge", "Modeling", "natural", "language", "inference", "is", "a", "very", "challenging", "task", ".", "With", "the", "availability", "of", "large", "annotated", "data", ",", "it", "has", "recently", "become", "feasible", "to", "train", "complex", "models", "such", "as", "neural", "-", "network", "-", "based", "inference", "models", ",", "which", "have", "shown", "to", "achieve", "the", "state", "-", "of", "-", "the", "-", "art", "performance", ".", "Although", "there", "exist", "relatively", "large", "annotated", "data", ",", "can", "machines", "learn", "all", "knowledge", "needed", "to", "perform", "natural", "language", "inference", "(", "NLI", ")", "from", "these", "data", "?", "If", "not", ",", "how", "can", "neural", "-", "network", "-", "based", "NLI", "models", "benefit", "from", "external", "knowledge", "and", "how", "to", "build", "NLI", "models", "to", "leverage", "it", "?", "In", "this", "paper", ",", "we", "enrich", "the", "state", "-", "of", "-", "the", "-", "art", "neural", "natural", "language", "inference", "models", "with", "external", "knowledge", ".", "We", "demonstrate", "that", "the", "proposed", "models", "improve", "neural", "NLI", "models", "to", "achieve", "the", "state", "-", "of", "-", "the", "-", "art", "performance", "on", "the", "SNLI", "and", "MultiNLI", "datasets", ".", "[", "enumerate", ",", "1", "]", "1", "o", "section", ":", "Introduction", "Reasoning", "and", "inference", "are", "central", "to", "both", "human", "and", "artificial", "intelligence", ".", "Natural", "language", "inference", "(", "NLI", ")", ",", "also", "known", "as", "recognizing", "textual", "entailment", "(", "RTE", ")", ",", "is", "an", "important", "NLP", "problem", "concerned", "with", "determining", "inferential", "relationship", "(", "e.g.", ",", "entailment", ",", "contradiction", ",", "or", "neutral", ")", "between", "a", "premise", "p", "and", "a", "hypothesis", "h", ".", "In", "general", ",", "modeling", "informal", "inference", "in", "language", "is", "a", "very", "challenging", "and", "basic", "problem", "towards", "achieving", "true", "natural", "language", "understanding", ".", "In", "the", "last", "several", "years", ",", "larger", "annotated", "datasets", "were", "made", "available", ",", "e.g.", ",", "the", "SNLI", "DBLP", ":", "conf", "/", "emnlp", "/", "BowmanAPM15", "and", "MultiNLI", "datasets", "DBLP", ":", "journals", "/", "corr", "/", "WilliamsNB17", ",", "which", "made", "it", "feasible", "to", "train", "rather", "complicated", "neural", "-", "network", "-", "based", "models", "that", "fit", "a", "large", "set", "of", "parameters", "to", "better", "model", "NLI", ".", "Such", "models", "have", "shown", "to", "achieve", "the", "state", "-", "of", "-", "the", "-", "art", "performance", "DBLP", ":", "conf", "/", "emnlp", "/", "BowmanAPM15", ",", "DBLP", ":", "conf", "/", "acl", "/", "BowmanGRGMP16", ",", "DBLP", ":", "conf", "/", "eacl", "/", "YuM17", ",", "DBLP", ":", "conf", "/", "emnlp", "/", "ParikhT0U16", ",", "DBLP", ":", "conf", "/", "coling", "/", "ShaCSL16", ",", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", ",", "DBLP", ":", "conf", "/", "repeval", "/", "ChenZLWJI17", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1801", "-", "00102", ".", "While", "neural", "networks", "have", "been", "shown", "to", "be", "very", "effective", "in", "modeling", "NLI", "with", "large", "training", "data", ",", "they", "have", "often", "focused", "on", "end", "-", "to", "-", "end", "training", "by", "assuming", "that", "all", "inference", "knowledge", "is", "learnable", "from", "the", "provided", "training", "data", ".", "In", "this", "paper", ",", "we", "relax", "this", "assumption", "and", "explore", "whether", "external", "knowledge", "can", "further", "help", "NLI", ".", "Consider", "an", "example", ":", "p", ":", "A", "lady", "standing", "in", "a", "wheat", "field", ".", "h", ":", "A", "person", "standing", "in", "a", "corn", "field", ".", "In", "this", "simplified", "example", ",", "when", "computers", "are", "asked", "to", "predict", "the", "relation", "between", "these", "two", "sentences", "and", "if", "training", "data", "do", "not", "provide", "the", "knowledge", "of", "relationship", "between", "\u201c", "wheat", "\u201d", "and", "\u201c", "corn", "\u201d", "(", "e.g.", ",", "if", "one", "of", "the", "two", "words", "does", "not", "appear", "in", "the", "training", "data", "or", "they", "are", "not", "paired", "in", "any", "premise", "-", "hypothesis", "pairs", ")", ",", "it", "will", "be", "hard", "for", "computers", "to", "correctly", "recognize", "that", "the", "premise", "contradicts", "the", "hypothesis", ".", "In", "general", ",", "although", "in", "many", "tasks", "learning", "tabula", "rasa", "achieved", "state", "-", "of", "-", "the", "-", "art", "performance", ",", "we", "believe", "complicated", "NLP", "problems", "such", "as", "NLI", "could", "benefit", "from", "leveraging", "knowledge", "accumulated", "by", "humans", ",", "particularly", "in", "a", "foreseeable", "future", "when", "machines", "are", "unable", "to", "learn", "it", "by", "themselves", ".", "In", "this", "paper", "we", "enrich", "neural", "-", "network", "-", "based", "NLI", "models", "with", "external", "knowledge", "in", "co", "-", "attention", ",", "local", "inference", "collection", ",", "and", "inference", "composition", "components", ".", "We", "show", "the", "proposed", "model", "improves", "the", "state", "-", "of", "-", "the", "-", "art", "NLI", "models", "to", "achieve", "better", "performances", "on", "the", "SNLI", "and", "MultiNLI", "datasets", ".", "The", "advantage", "of", "using", "external", "knowledge", "is", "more", "significant", "when", "the", "size", "of", "training", "data", "is", "restricted", ",", "suggesting", "that", "if", "more", "knowledge", "can", "be", "obtained", ",", "it", "may", "bring", "more", "benefit", ".", "In", "addition", "to", "attaining", "the", "state", "-", "of", "-", "the", "-", "art", "performance", ",", "we", "are", "also", "interested", "in", "understanding", "how", "external", "knowledge", "contributes", "to", "the", "major", "components", "of", "typical", "neural", "-", "network", "-", "based", "NLI", "models", ".", "section", ":", "Related", "Work", "Early", "research", "on", "natural", "language", "inference", "and", "recognizing", "textual", "entailment", "has", "been", "performed", "on", "relatively", "small", "datasets", "(", "refer", "to", "MacCartneyThesis", "for", "a", "good", "literature", "survey", ")", ",", "which", "includes", "a", "large", "bulk", "of", "contributions", "made", "under", "the", "name", "of", "RTE", ",", "such", "as", "Dagan2005ThePR", ",", "Iftene", ":", "W07", "-", "1421", ",", "among", "many", "others", ".", "More", "recently", "the", "availability", "of", "much", "larger", "annotated", "data", ",", "e.g.", ",", "SNLI", "DBLP", ":", "conf", "/", "emnlp", "/", "BowmanAPM15", "and", "MultiNLI", "DBLP", ":", "journals", "/", "corr", "/", "WilliamsNB17", ",", "has", "made", "it", "possible", "to", "train", "more", "complex", "models", ".", "These", "models", "mainly", "fall", "into", "two", "types", "of", "approaches", ":", "sentence", "-", "encoding", "-", "based", "models", "and", "models", "using", "also", "inter", "-", "sentence", "attention", ".", "Sentence", "-", "encoding", "-", "based", "models", "use", "Siamese", "architecture", "DBLP", ":", "conf", "/", "nips", "/", "BromleyGLSS93", ".", "The", "parameter", "-", "tied", "neural", "networks", "are", "applied", "to", "encode", "both", "the", "premise", "and", "the", "hypothesis", ".", "Then", "a", "neural", "network", "classifier", "is", "applied", "to", "decide", "relationship", "between", "the", "two", "sentences", ".", "Different", "neural", "networks", "have", "been", "utilized", "for", "sentence", "encoding", ",", "such", "as", "LSTM", "DBLP", ":", "conf", "/", "emnlp", "/", "BowmanAPM15", ",", "GRU", "DBLP", ":", "journals", "/", "corr", "/", "VendrovKFU15", ",", "CNN", "DBLP", ":", "conf", "/", "acl", "/", "MouMLX0YJ16", ",", "BiLSTM", "and", "its", "variants", "DBLP", ":", "journals", "/", "corr", "/", "LiuSLW16", ",", "DBLP", ":", "journals", "/", "corr", "/", "LinFSYXZB17", ",", "DBLP", ":", "conf", "/", "repeval", "/", "ChenZLWJI17", ",", "DBLP", ":", "conf", "/", "repeval", "/", "NieB17", ",", "self", "-", "attention", "network", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1709", "-", "04696", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1801", "-", "10296", ",", "and", "more", "complicated", "neural", "networks", "DBLP", ":", "conf", "/", "acl", "/", "BowmanGRGMP16", ",", "DBLP", ":", "conf", "/", "eacl", "/", "YuM17a", ",", "DBLP", ":", "conf", "/", "eacl", "/", "YuM17", ",", "DBLP", ":", "journals", "/", "corr", "/", "ChoiYL17", ".", "Sentence", "-", "encoding", "-", "based", "models", "transform", "sentences", "into", "fixed", "-", "length", "vector", "representations", ",", "which", "may", "help", "a", "wide", "range", "of", "tasks", "DBLP", ":", "conf", "/", "emnlp", "/", "ConneauKSBB17", ".", "The", "second", "set", "of", "models", "use", "inter", "-", "sentence", "attention", "DBLP", ":", "journals", "/", "corr", "/", "RocktaschelGHKB15", ",", "DBLP", ":", "conf", "/", "naacl", "/", "WangJ16", ",", "DBLP", ":", "conf", "/", "emnlp", "/", "0001DL16", ",", "DBLP", ":", "conf", "/", "emnlp", "/", "ParikhT0U16", ",", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", ".", "Among", "them", ",", "DBLP", ":", "journals", "/", "corr", "/", "RocktaschelGHKB15", "were", "among", "the", "first", "to", "propose", "neural", "attention", "-", "based", "models", "for", "NLI", ".", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", "proposed", "an", "enhanced", "sequential", "inference", "model", "(", "ESIM", ")", ",", "which", "is", "one", "of", "the", "best", "models", "so", "far", "and", "is", "used", "as", "one", "of", "our", "baselines", "in", "this", "paper", ".", "In", "this", "paper", "we", "enrich", "neural", "-", "network", "-", "based", "NLI", "models", "with", "external", "knowledge", ".", "Unlike", "early", "work", "on", "NLI", "Jijkoun2005RecognizingTE", ",", "DBLP", ":", "conf", "/", "emnlp", "/", "MacCartneyGM08", ",", "MacCartneyThesis", "that", "explores", "external", "knowledge", "in", "conventional", "NLI", "models", "on", "relatively", "small", "NLI", "datasets", ",", "we", "aim", "to", "merge", "the", "advantage", "of", "powerful", "modeling", "ability", "of", "neural", "networks", "with", "extra", "external", "inference", "knowledge", ".", "We", "show", "that", "the", "proposed", "model", "improves", "the", "state", "-", "of", "-", "the", "-", "art", "neural", "NLI", "models", "to", "achieve", "better", "performances", "on", "the", "SNLI", "and", "MultiNLI", "datasets", ".", "The", "advantage", "of", "using", "external", "knowledge", "is", "more", "significant", "when", "the", "size", "of", "training", "data", "is", "restricted", ",", "suggesting", "that", "if", "more", "knowledge", "can", "be", "obtained", ",", "it", "may", "have", "more", "benefit", ".", "In", "addition", "to", "attaining", "the", "state", "-", "of", "-", "the", "-", "art", "performance", ",", "we", "are", "also", "interested", "in", "understanding", "how", "external", "knowledge", "affect", "major", "components", "of", "neural", "-", "network", "-", "based", "NLI", "models", ".", "In", "general", ",", "external", "knowledge", "has", "shown", "to", "be", "effective", "in", "neural", "networks", "for", "other", "NLP", "tasks", ",", "including", "word", "embedding", "DBLP", ":", "conf", "/", "acl", "/", "ChenLCCWJZ15", ",", "DBLP", ":", "conf", "/", "naacl", "/", "FaruquiDJDHS15", ",", "DBLP", ":", "conf", "/", "acl", "/", "Liu0WLH15", ",", "DBLP", ":", "journals", "/", "tacl", "/", "WietingBGL15", ",", "DBLP", ":", "journals", "/", "corr", "/", "MrksicVSLRGKY17", ",", "machine", "translation", "DBLP", ":", "conf", "/", "acl", "/", "ShiLRFLZSW16", ",", "DBLP", ":", "conf", "/", "apsipa", "/", "ZhangMWH17", ",", "language", "modeling", "DBLP", ":", "journals", "/", "corr", "/", "AhnCPB16", ",", "and", "dialogue", "systems", "DBLP", ":", "journals", "/", "corr", "/", "ChenHTCGD16", ".", "section", ":", "Neural", "-", "Network", "-", "Based", "NLI", "Models", "with", "External", "Knowledge", "In", "this", "section", "we", "propose", "neural", "-", "network", "-", "based", "NLI", "models", "to", "incorporate", "external", "inference", "knowledge", ",", "which", ",", "as", "we", "will", "show", "later", "in", "Section", "[", "reference", "]", ",", "achieve", "the", "state", "-", "of", "-", "the", "-", "art", "performance", ".", "In", "addition", "to", "attaining", "the", "leading", "performance", "we", "are", "also", "interested", "in", "investigating", "the", "effects", "of", "external", "knowledge", "on", "major", "components", "of", "neural", "-", "network", "-", "based", "NLI", "modeling", ".", "Figure", "[", "reference", "]", "shows", "a", "high", "-", "level", "general", "view", "of", "the", "proposed", "framework", ".", "While", "specific", "NLI", "systems", "vary", "in", "their", "implementation", ",", "typical", "state", "-", "of", "-", "the", "-", "art", "NLI", "models", "contain", "the", "main", "components", "(", "or", "equivalents", ")", "of", "representing", "premise", "and", "hypothesis", "sentences", ",", "collecting", "local", "(", "e.g.", ",", "lexical", ")", "inference", "information", ",", "and", "aggregating", "and", "composing", "local", "information", "to", "make", "the", "global", "decision", "at", "the", "sentence", "level", ".", "We", "incorporate", "and", "investigate", "external", "knowledge", "accordingly", "in", "these", "major", "NLI", "components", ":", "computing", "co", "-", "attention", ",", "collecting", "local", "inference", "information", ",", "and", "composing", "inference", "to", "make", "final", "decision", ".", "subsection", ":", "External", "Knowledge", "As", "discussed", "above", ",", "although", "there", "exist", "relatively", "large", "annotated", "data", "for", "NLI", ",", "can", "machines", "learn", "all", "inference", "knowledge", "needed", "to", "perform", "NLI", "from", "the", "data", "?", "If", "not", ",", "how", "can", "neural", "network", "-", "based", "NLI", "models", "benefit", "from", "external", "knowledge", "and", "how", "to", "build", "NLI", "models", "to", "leverage", "it", "?", "We", "study", "the", "incorporation", "of", "external", ",", "inference", "-", "related", "knowledge", "in", "major", "components", "of", "neural", "networks", "for", "natural", "language", "inference", ".", "For", "example", ",", "intuitively", "knowledge", "about", "synonymy", ",", "antonymy", ",", "hypernymy", "and", "hyponymy", "between", "given", "words", "may", "help", "model", "soft", "-", "alignment", "between", "premises", "and", "hypotheses", ";", "knowledge", "about", "hypernymy", "and", "hyponymy", "may", "help", "capture", "entailment", ";", "knowledge", "about", "antonymy", "and", "co", "-", "hyponyms", "(", "words", "sharing", "the", "same", "hypernym", ")", "may", "benefit", "the", "modeling", "of", "contradiction", ".", "In", "this", "section", ",", "we", "discuss", "the", "incorporation", "of", "basic", ",", "lexical", "-", "level", "semantic", "knowledge", "into", "neural", "NLI", "components", ".", "Specifically", ",", "we", "consider", "external", "lexical", "-", "level", "inference", "knowledge", "between", "word", "and", ",", "which", "is", "represented", "as", "a", "vector", "and", "is", "incorporated", "into", "three", "specific", "components", "shown", "in", "Figure", "[", "reference", "]", ".", "We", "will", "discuss", "the", "details", "of", "how", "is", "constructed", "later", "in", "the", "experiment", "setup", "section", "(", "Section", "[", "reference", "]", ")", "but", "instead", "focus", "on", "the", "proposed", "model", "in", "this", "section", ".", "Note", "that", "while", "we", "study", "lexical", "-", "level", "inference", "knowledge", "in", "the", "paper", ",", "if", "inference", "knowledge", "about", "larger", "pieces", "of", "text", "pairs", "(", "e.g.", ",", "inference", "relations", "between", "phrases", ")", "are", "available", ",", "the", "proposed", "model", "can", "be", "easily", "extended", "to", "handle", "that", ".", "In", "this", "paper", ",", "we", "instead", "let", "the", "NLI", "models", "to", "compose", "lexical", "-", "level", "knowledge", "to", "obtain", "inference", "relations", "between", "larger", "pieces", "of", "texts", ".", "subsection", ":", "Encoding", "Premise", "and", "Hypothesis", "Same", "as", "much", "previous", "work", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", ",", "DBLP", ":", "conf", "/", "repeval", "/", "ChenZLWJI17", ",", "we", "encode", "the", "premise", "and", "the", "hypothesis", "with", "bidirectional", "LSTMs", "(", "BiLSTMs", ")", ".", "The", "premise", "is", "represented", "as", "and", "the", "hypothesis", "is", ",", "where", "and", "are", "the", "lengths", "of", "the", "sentences", ".", "Then", "and", "are", "embedded", "into", "-", "dimensional", "vectors", "and", "using", "the", "embedding", "matrix", ",", "where", "is", "the", "vocabulary", "size", "and", "can", "be", "initialized", "with", "the", "pre", "-", "trained", "word", "embedding", ".", "To", "represent", "words", "in", "its", "context", ",", "the", "premise", "and", "the", "hypothesis", "are", "fed", "into", "BiLSTM", "encoders", "DBLP", ":", "journals", "/", "neco", "/", "HochreiterS97", "to", "obtain", "context", "-", "dependent", "hidden", "states", "and", ":", "where", "and", "indicate", "the", "-", "th", "word", "in", "the", "premise", "and", "the", "-", "th", "word", "in", "the", "hypothesis", ",", "respectively", ".", "subsection", ":", "Knowledge", "-", "Enriched", "Co", "-", "Attention", "As", "discussed", "above", ",", "soft", "-", "alignment", "of", "word", "pairs", "between", "the", "premise", "and", "the", "hypothesis", "may", "benefit", "from", "knowledge", "-", "enriched", "co", "-", "attention", "mechanism", ".", "Given", "the", "relation", "features", "between", "the", "premise", "\u2019s", "-", "th", "word", "and", "the", "hypothesis", "\u2019s", "-", "th", "word", "derived", "from", "the", "external", "knowledge", ",", "the", "co", "-", "attention", "is", "calculated", "as", ":", "The", "function", "can", "be", "any", "non", "-", "linear", "or", "linear", "functions", ".", "In", "this", "paper", ",", "we", "use", ",", "where", "is", "a", "hyper", "-", "parameter", "tuned", "on", "the", "development", "set", "and", "is", "the", "indication", "function", "as", "follows", ":", "Intuitively", ",", "word", "pairs", "with", "semantic", "relationship", ",", "e.g.", ",", "synonymy", ",", "antonymy", ",", "hypernymy", ",", "hyponymy", "and", "co", "-", "hyponyms", ",", "are", "probably", "aligned", "together", ".", "We", "will", "discuss", "how", "we", "construct", "external", "knowledge", "later", "in", "Section", "[", "reference", "]", ".", "We", "have", "also", "tried", "a", "two", "-", "layer", "MLP", "as", "a", "universal", "function", "approximator", "in", "function", "to", "learn", "the", "underlying", "combination", "function", "but", "did", "not", "observe", "further", "improvement", "over", "the", "best", "performance", "we", "obtained", "on", "the", "development", "datasets", ".", "Soft", "-", "alignment", "is", "determined", "by", "the", "co", "-", "attention", "matrix", "computed", "in", "Equation", "(", "[", "reference", "]", ")", ",", "which", "is", "used", "to", "obtain", "the", "local", "relevance", "between", "the", "premise", "and", "the", "hypothesis", ".", "For", "the", "hidden", "state", "of", "the", "-", "th", "word", "in", "the", "premise", ",", "i.e.", ",", "(", "already", "encoding", "the", "word", "itself", "and", "its", "context", ")", ",", "the", "relevant", "semantics", "in", "the", "hypothesis", "is", "identified", "into", "a", "context", "vector", "using", ",", "more", "specifically", "with", "Equation", "(", "[", "reference", "]", ")", ".", "where", "and", "are", "the", "normalized", "attention", "weight", "matrices", "with", "respect", "to", "the", "-", "axis", "and", "-", "axis", ".", "The", "same", "calculation", "is", "performed", "for", "each", "word", "in", "the", "hypothesis", ",", "i.e.", ",", ",", "with", "Equation", "(", "[", "reference", "]", ")", "to", "obtain", "the", "context", "vector", ".", "subsection", ":", "Local", "Inference", "Collection", "with", "External", "Knowledge", "By", "way", "of", "comparing", "the", "inference", "-", "related", "semantic", "relation", "between", "(", "individual", "word", "representation", "in", "premise", ")", "and", "(", "context", "representation", "from", "hypothesis", "which", "is", "align", "to", "word", ")", ",", "we", "can", "model", "local", "inference", "(", "i.e.", ",", "word", "-", "level", "inference", ")", "between", "aligned", "word", "pairs", ".", "Intuitively", ",", "for", "example", ",", "knowledge", "about", "hypernymy", "or", "hyponymy", "may", "help", "model", "entailment", "and", "knowledge", "about", "antonymy", "and", "co", "-", "hyponyms", "may", "help", "model", "contradiction", ".", "Through", "comparing", "and", ",", "in", "addition", "to", "their", "relation", "from", "external", "knowledge", ",", "we", "can", "obtain", "word", "-", "level", "inference", "information", "for", "each", "word", ".", "The", "same", "calculation", "is", "performed", "for", "and", ".", "Thus", ",", "we", "collect", "knowledge", "-", "enriched", "local", "inference", "information", ":", "where", "a", "heuristic", "matching", "trick", "with", "difference", "and", "element", "-", "wise", "product", "is", "used", "DBLP", ":", "conf", "/", "acl", "/", "MouMLX0YJ16", ",", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", ".", "The", "last", "terms", "in", "Equation", "(", "[", "reference", "]", ")", "(", "[", "reference", "]", ")", "are", "used", "to", "obtain", "word", "-", "level", "inference", "information", "from", "external", "knowledge", ".", "Take", "Equation", "(", "[", "reference", "]", ")", "as", "example", ",", "is", "the", "relation", "feature", "between", "the", "-", "th", "word", "in", "the", "premise", "and", "the", "-", "th", "word", "in", "the", "hypothesis", ",", "but", "we", "care", "more", "about", "semantic", "relation", "between", "aligned", "word", "pairs", "between", "the", "premise", "and", "the", "hypothesis", ".", "Thus", ",", "we", "use", "a", "soft", "-", "aligned", "version", "through", "the", "soft", "-", "alignment", "weight", ".", "For", "the", "-", "th", "word", "in", "the", "premise", ",", "the", "last", "term", "in", "Equation", "(", "[", "reference", "]", ")", "is", "a", "word", "-", "level", "inference", "information", "based", "on", "external", "knowledge", "between", "the", "-", "th", "word", "and", "the", "aligned", "word", ".", "The", "same", "calculation", "for", "hypothesis", "is", "performed", "in", "Equation", "(", "[", "reference", "]", ")", ".", "is", "a", "non", "-", "linear", "mapping", "function", "to", "reduce", "dimensionality", ".", "Specifically", ",", "we", "use", "a", "1", "-", "layer", "feed", "-", "forward", "neural", "network", "with", "the", "ReLU", "activation", "function", "with", "a", "shortcut", "connection", ",", "i.e.", ",", "concatenate", "the", "hidden", "states", "after", "ReLU", "with", "the", "input", "(", "or", ")", "as", "the", "output", "(", "or", ")", ".", "subsection", ":", "Knowledge", "-", "Enhanced", "Inference", "Composition", "In", "this", "component", ",", "we", "introduce", "knowledge", "-", "enriched", "inference", "composition", ".", "To", "determine", "the", "overall", "inference", "relationship", "between", "the", "premise", "and", "the", "hypothesis", ",", "we", "need", "to", "explore", "a", "composition", "layer", "to", "compose", "the", "local", "inference", "vectors", "(", "and", ")", "collected", "above", ":", "Here", ",", "we", "also", "use", "BiLSTMs", "as", "building", "blocks", "for", "the", "composition", "layer", ",", "but", "the", "responsibility", "of", "BiLSTMs", "in", "the", "inference", "composition", "layer", "is", "completely", "different", "from", "that", "in", "the", "input", "encoding", "layer", ".", "The", "BiLSTMs", "here", "read", "local", "inference", "vectors", "(", "and", ")", "and", "learn", "to", "judge", "the", "types", "of", "local", "inference", "relationship", "and", "distinguish", "crucial", "local", "inference", "vectors", "for", "overall", "sentence", "-", "level", "inference", "relationship", ".", "Intuitively", ",", "the", "final", "prediction", "is", "likely", "to", "depend", "on", "word", "pairs", "appearing", "in", "external", "knowledge", "that", "have", "some", "semantic", "relation", ".", "Our", "inference", "model", "converts", "the", "output", "hidden", "vectors", "of", "BiLSTMs", "to", "the", "fixed", "-", "length", "vector", "with", "pooling", "operations", "and", "puts", "it", "into", "the", "final", "classifier", "to", "determine", "the", "overall", "inference", "class", ".", "Particularly", ",", "in", "addition", "to", "using", "mean", "pooling", "and", "max", "pooling", "similarly", "to", "ESIM", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", ",", "we", "propose", "to", "use", "weighted", "pooling", "based", "on", "external", "knowledge", "to", "obtain", "a", "fixed", "-", "length", "vector", "as", "in", "Equation", "(", "[", "reference", "]", ")", "(", "[", "reference", "]", ")", ".", "In", "our", "experiments", ",", "we", "regard", "the", "function", "as", "a", "1", "-", "layer", "feed", "-", "forward", "neural", "network", "with", "ReLU", "activation", "function", ".", "We", "concatenate", "all", "pooling", "vectors", ",", "i.e.", ",", "mean", ",", "max", ",", "and", "weighted", "pooling", ",", "into", "the", "fixed", "-", "length", "vector", "and", "then", "put", "the", "vector", "into", "the", "final", "multilayer", "perceptron", "(", "MLP", ")", "classifier", ".", "The", "MLP", "has", "one", "hidden", "layer", "with", "tanh", "activation", "and", "softmax", "output", "layer", "in", "our", "experiments", ".", "The", "entire", "model", "is", "trained", "end", "-", "to", "-", "end", ",", "through", "minimizing", "the", "cross", "-", "entropy", "loss", ".", "section", ":", "Experiment", "Set", "-", "Up", "subsection", ":", "Representation", "of", "External", "Knowledge", "paragraph", ":", "Lexical", "Semantic", "Relations", "As", "described", "in", "Section", "[", "reference", "]", ",", "to", "incorporate", "external", "knowledge", "(", "as", "a", "knowledge", "vector", ")", "to", "the", "state", "-", "of", "-", "the", "-", "art", "neural", "network", "-", "based", "NLI", "models", ",", "we", "first", "explore", "semantic", "relations", "in", "WordNet", "DBLP", ":", "journals", "/", "cacm", "/", "Miller95", ",", "motivated", "by", "MacCartneyThesis", ".", "Specifically", ",", "the", "relations", "of", "lexical", "pairs", "are", "derived", "as", "described", "in", "[", "reference", "]", "-", "[", "reference", "]", "below", ".", "Instead", "of", "using", "Jiang", "-", "Conrath", "WordNet", "distance", "metric", "DBLP", ":", "conf", "/", "rocling", "/", "JiangC97", ",", "which", "does", "not", "improve", "the", "performance", "of", "our", "models", "on", "the", "development", "sets", ",", "we", "add", "a", "new", "feature", ",", "i.e.", ",", "co", "-", "hyponyms", ",", "which", "consistently", "benefit", "our", "models", ".", "Synonymy", ":", "It", "takes", "the", "value", "if", "the", "words", "in", "the", "pair", "are", "synonyms", "in", "WordNet", "(", "i.e.", ",", "belong", "to", "the", "same", "synset", ")", ",", "and", "otherwise", ".", "For", "example", ",", "[", "felicitous", ",", "good", "]", "=", ",", "[", "dog", ",", "wolf", "]", "=", ".", "Antonymy", ":", "It", "takes", "the", "value", "if", "the", "words", "in", "the", "pair", "are", "antonyms", "in", "WordNet", ",", "and", "otherwise", ".", "For", "example", ",", "[", "wet", ",", "dry", "]", "=", ".", "Hypernymy", ":", "It", "takes", "the", "value", "if", "one", "word", "is", "a", "(", "direct", "or", "indirect", ")", "hypernym", "of", "the", "other", "word", "in", "WordNet", ",", "where", "is", "the", "number", "of", "edges", "between", "the", "two", "words", "in", "hierarchies", ",", "and", "otherwise", ".", "Note", "that", "we", "ignore", "pairs", "in", "the", "hierarchy", "which", "have", "more", "than", "8", "edges", "in", "between", ".", "For", "example", ",", "[", "dog", ",", "canid", "]", "=", ",", "[", "wolf", ",", "canid", "]", "=", ",", "[", "dog", ",", "carnivore", "]", "=", ",", "[", "canid", ",", "dog", "]", "=", "Hyponymy", ":", "It", "is", "simply", "the", "inverse", "of", "the", "hypernymy", "feature", ".", "For", "example", ",", "[", "canid", ",", "dog", "]", "=", ",", "[", "dog", ",", "canid", "]", "=", ".", "Co", "-", "hyponyms", ":", "It", "takes", "the", "value", "if", "the", "two", "words", "have", "the", "same", "hypernym", "but", "they", "do", "not", "belong", "to", "the", "same", "synset", ",", "and", "otherwise", ".", "For", "example", ",", "[", "dog", ",", "wolf", "]", "=", ".", "As", "discussed", "above", ",", "we", "expect", "features", "like", "synonymy", ",", "antonymy", ",", "hypernymy", ",", "hyponymy", "and", "co", "-", "hyponyms", "would", "help", "model", "co", "-", "attention", "alignment", "between", "the", "premise", "and", "the", "hypothesis", ".", "Knowledge", "of", "hypernymy", "and", "hyponymy", "may", "help", "capture", "entailment", ";", "knowledge", "of", "antonymy", "and", "co", "-", "hyponyms", "may", "help", "model", "contradiction", ".", "Their", "final", "contributions", "will", "be", "learned", "in", "end", "-", "to", "-", "end", "model", "training", ".", "We", "regard", "the", "vector", "as", "the", "relation", "feature", "derived", "from", "external", "knowledge", ",", "where", "is", "here", ".", "In", "addition", ",", "Table", "[", "reference", "]", "reports", "some", "key", "statistics", "of", "these", "features", ".", "In", "addition", "to", "the", "above", "relations", ",", "we", "also", "use", "more", "relation", "features", "in", "WordNet", ",", "including", "instance", ",", "instance", "of", ",", "same", "instance", ",", "entailment", ",", "member", "meronym", ",", "member", "holonym", ",", "substance", "meronym", ",", "substance", "holonym", ",", "part", "meronym", ",", "part", "holonym", ",", "summing", "up", "to", "15", "features", ",", "but", "these", "additional", "features", "do", "not", "bring", "further", "improvement", "on", "the", "development", "dataset", ",", "as", "also", "discussed", "in", "Section", "[", "reference", "]", ".", "paragraph", ":", "Relation", "Embeddings", "In", "the", "most", "recent", "years", "graph", "embedding", "has", "been", "widely", "employed", "to", "learn", "representation", "for", "vertexes", "and", "their", "relations", "in", "a", "graph", ".", "In", "our", "work", "here", ",", "we", "also", "capture", "the", "relation", "between", "any", "two", "words", "in", "WordNet", "through", "relation", "embedding", ".", "Specifically", ",", "we", "employed", "TransE", "DBLP", ":", "conf", "/", "nips", "/", "BordesUGWY13", ",", "a", "widely", "used", "graph", "embedding", "methods", ",", "to", "capture", "relation", "embedding", "between", "any", "two", "words", ".", "We", "used", "two", "typical", "approaches", "to", "obtaining", "the", "relation", "embedding", ".", "The", "first", "directly", "uses", "18", "relation", "embeddings", "pretrained", "on", "the", "WN18", "dataset", "DBLP", ":", "conf", "/", "nips", "/", "BordesUGWY13", ".", "Specifically", ",", "if", "a", "word", "pair", "has", "a", "certain", "type", "relation", ",", "we", "take", "the", "corresponding", "relation", "embedding", ".", "Sometimes", ",", "if", "a", "word", "pair", "has", "multiple", "relations", "among", "the", "18", "types", ";", "we", "take", "an", "average", "of", "the", "relation", "embedding", ".", "The", "second", "approach", "uses", "TransE", "\u2019s", "word", "embedding", "(", "trained", "on", "WordNet", ")", "to", "obtain", "relation", "embedding", ",", "through", "the", "objective", "function", "used", "in", "TransE", ",", "i.e.", ",", ",", "where", "indicates", "relation", "embedding", ",", "indicates", "tail", "entity", "embedding", ",", "and", "indicates", "head", "entity", "embedding", ".", "Note", "that", "in", "addition", "to", "relation", "embedding", "trained", "on", "WordNet", ",", "other", "relational", "embedding", "resources", "exist", ";", "e.g.", ",", "that", "trained", "on", "Freebase", "(", "WikiData", ")", "DBLP", ":", "conf", "/", "aaai", "/", "BollackerCT07", ",", "but", "such", "knowledge", "resources", "are", "mainly", "about", "facts", "(", "e.g.", ",", "relationship", "between", "Bill", "Gates", "and", "Microsoft", ")", "and", "are", "less", "for", "commonsense", "knowledge", "used", "in", "general", "natural", "language", "inference", "(", "e.g.", ",", "the", "color", "yellow", "potentially", "contradicts", "red", ")", ".", "subsection", ":", "NLI", "Datasets", "In", "our", "experiments", ",", "we", "use", "Stanford", "Natural", "Language", "Inference", "(", "SNLI", ")", "dataset", "DBLP", ":", "conf", "/", "emnlp", "/", "BowmanAPM15", "and", "Multi", "-", "Genre", "Natural", "Language", "Inference", "(", "MultiNLI", ")", "DBLP", ":", "journals", "/", "corr", "/", "WilliamsNB17", "dataset", ",", "which", "focus", "on", "three", "basic", "relations", "between", "a", "premise", "and", "a", "potential", "hypothesis", ":", "the", "premise", "entails", "the", "hypothesis", "(", "entailment", ")", ",", "they", "contradict", "each", "other", "(", "contradiction", ")", ",", "or", "they", "are", "not", "related", "(", "neutral", ")", ".", "We", "use", "the", "same", "data", "split", "as", "in", "previous", "work", "DBLP", ":", "conf", "/", "emnlp", "/", "BowmanAPM15", ",", "DBLP", ":", "journals", "/", "corr", "/", "WilliamsNB17", "and", "classification", "accuracy", "as", "the", "evaluation", "metric", ".", "In", "addition", ",", "we", "test", "our", "models", "(", "trained", "on", "the", "SNLI", "training", "set", ")", "on", "a", "new", "test", "set", "glockner_acl18", ",", "which", "assesses", "the", "lexical", "inference", "abilities", "of", "NLI", "systems", "and", "consists", "of", "8", ",", "193", "samples", ".", "WordNet", "3.0", "DBLP", ":", "journals", "/", "cacm", "/", "Miller95", "is", "used", "to", "extract", "semantic", "relation", "features", "between", "words", ".", "The", "words", "are", "lemmatized", "using", "Stanford", "CoreNLP", "3.7.0", "DBLP", ":", "conf", "/", "acl", "/", "ManningSBFBM14", ".", "The", "premise", "and", "the", "hypothesis", "sentences", "fed", "into", "the", "input", "encoding", "layer", "are", "tokenized", ".", "subsection", ":", "Training", "Details", "For", "duplicability", ",", "we", "release", "our", "code", ".", "All", "our", "models", "were", "strictly", "selected", "on", "the", "development", "set", "of", "the", "SNLI", "data", "and", "the", "in", "-", "domain", "development", "set", "of", "MultiNLI", "and", "were", "then", "tested", "on", "the", "corresponding", "test", "set", ".", "The", "main", "training", "details", "are", "as", "follows", ":", "the", "dimension", "of", "the", "hidden", "states", "of", "LSTMs", "and", "word", "embeddings", "are", ".", "The", "word", "embeddings", "are", "initialized", "by", "300D", "GloVe", "840B", "DBLP", ":", "conf", "/", "emnlp", "/", "PenningtonSM14", ",", "and", "out", "-", "of", "-", "vocabulary", "words", "among", "them", "are", "initialized", "randomly", ".", "All", "word", "embeddings", "are", "updated", "during", "training", ".", "Adam", "DBLP", ":", "journals", "/", "corr", "/", "KingmaB14", "is", "used", "for", "optimization", "with", "an", "initial", "learning", "rate", "of", ".", "The", "mini", "-", "batch", "size", "is", "set", "to", ".", "Note", "that", "the", "above", "hyperparameter", "settings", "are", "same", "as", "those", "used", "in", "the", "baseline", "ESIM", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", "model", ".", "ESIM", "is", "a", "strong", "NLI", "baseline", "framework", "with", "the", "source", "code", "made", "available", "at", "https:", "//", "github.com", "/", "lukecq1231", "/", "nli", "(", "the", "ESIM", "core", "code", "has", "also", "been", "adapted", "to", "summarization", "DBLP", ":", "conf", "/", "ijcai", "/", "ChenZLWJ16", "and", "question", "-", "answering", "tasks", "Zhang", ":", "qa:2017", ")", ".", "The", "trade", "-", "off", "for", "calculating", "co", "-", "attention", "in", "Equation", "(", "[", "reference", "]", ")", "is", "selected", "in", "based", "on", "the", "development", "set", ".", "When", "training", "TransE", "for", "WordNet", ",", "relations", "are", "represented", "with", "vectors", "of", "dimension", ".", "section", ":", "Experimental", "Results", "subsection", ":", "Overall", "Performance", "Table", "[", "reference", "]", "shows", "the", "results", "of", "state", "-", "of", "-", "the", "-", "art", "models", "on", "the", "SNLI", "dataset", ".", "Among", "them", ",", "ESIM", "DBLP", ":", "conf", "/", "acl", "/", "ChenZLWJI17", "is", "one", "of", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "systems", "with", "an", "88.0", "%", "test", "-", "set", "accuracy", ".", "The", "proposed", "model", ",", "namely", "Knowledge", "-", "based", "Inference", "Model", "(", "KIM", ")", ",", "which", "enriches", "ESIM", "with", "external", "knowledge", ",", "obtains", "an", "accuracy", "of", "88.6", "%", ",", "the", "best", "single", "-", "model", "performance", "reported", "on", "the", "SNLI", "dataset", ".", "The", "difference", "between", "ESIM", "and", "KIM", "is", "statistically", "significant", "under", "the", "one", "-", "tailed", "paired", "-", "test", "at", "the", "99", "%", "significance", "level", ".", "Note", "that", "the", "KIM", "model", "reported", "here", "uses", "five", "semantic", "relations", "described", "in", "Section", "[", "reference", "]", ".", "In", "addition", "to", "that", ",", "we", "also", "use", "15", "semantic", "relation", "features", ",", "which", "does", "not", "bring", "additional", "gains", "in", "performance", ".", "These", "results", "highlight", "the", "effectiveness", "of", "the", "five", "semantic", "relations", "described", "in", "Section", "[", "reference", "]", ".", "To", "further", "investigate", "external", "knowledge", ",", "we", "add", "TransE", "relation", "embedding", ",", "and", "again", "no", "further", "improvement", "is", "observed", "on", "both", "the", "development", "and", "test", "sets", "when", "TransE", "relation", "embedding", "is", "used", "(", "concatenated", ")", "with", "the", "semantic", "relation", "vectors", ".", "We", "consider", "this", "is", "due", "to", "the", "fact", "that", "TransE", "embedding", "is", "not", "specifically", "sensitive", "to", "inference", "information", ";", "e.g.", ",", "it", "does", "not", "model", "co", "-", "hyponyms", "features", ",", "and", "its", "potential", "benefit", "has", "already", "been", "covered", "by", "the", "semantic", "relation", "features", "used", ".", "Table", "[", "reference", "]", "shows", "the", "performance", "of", "models", "on", "the", "MultiNLI", "dataset", ".", "The", "baseline", "ESIM", "achieves", "76.8", "%", "and", "75.8", "%", "on", "in", "-", "domain", "and", "cross", "-", "domain", "test", "set", ",", "respectively", ".", "If", "we", "extend", "the", "ESIM", "with", "external", "knowledge", ",", "we", "achieve", "significant", "gains", "to", "77.2", "%", "and", "76.4", "%", "respectively", ".", "Again", ",", "the", "gains", "are", "consistent", "on", "SNLI", "and", "MultiNLI", ",", "and", "we", "expect", "they", "would", "be", "orthogonal", "to", "other", "factors", "when", "external", "knowledge", "is", "added", "into", "other", "state", "-", "of", "-", "the", "-", "art", "models", ".", "subsection", ":", "Ablation", "Results", "Figure", "[", "reference", "]", "displays", "the", "ablation", "analysis", "of", "different", "components", "when", "using", "the", "external", "knowledge", ".", "To", "compare", "the", "effects", "of", "external", "knowledge", "under", "different", "training", "data", "scales", ",", "we", "randomly", "sample", "different", "ratios", "of", "the", "entire", "training", "set", ",", "i.e.", ",", "0.8", "%", ",", "4", "%", ",", "20", "%", "and", "100", "%", ".", "\u201c", "A", "\u201d", "indicates", "adding", "external", "knowledge", "in", "calculating", "the", "co", "-", "attention", "matrix", "as", "in", "Equation", "(", "[", "reference", "]", ")", ",", "\u201c", "I", "\u201d", "indicates", "adding", "external", "knowledge", "in", "collecting", "local", "inference", "information", "as", "in", "Equation", "(", "[", "reference", "]", ")", "(", "[", "reference", "]", ")", ",", "and", "\u201c", "C", "\u201d", "indicates", "adding", "external", "knowledge", "in", "composing", "inference", "as", "in", "Equation", "(", "[", "reference", "]", ")", "(", "[", "reference", "]", ")", ".", "When", "we", "only", "have", "restricted", "training", "data", ",", "i.e.", ",", "0.8", "%", "training", "set", "(", "about", "4", ",", "000", "samples", ")", ",", "the", "baseline", "ESIM", "has", "a", "poor", "accuracy", "of", "62.4", "%", ".", "When", "we", "only", "add", "external", "knowledge", "in", "calculating", "co", "-", "attention", "(", "\u201c", "A", "\u201d", ")", ",", "the", "accuracy", "increases", "to", "66.6", "%", "(", "+", "absolute", "4.2", "%", ")", ".", "When", "we", "only", "utilize", "external", "knowledge", "in", "collecting", "local", "inference", "information", "(", "\u201c", "I", "\u201d", ")", ",", "the", "accuracy", "has", "a", "significant", "gain", ",", "to", "70.3", "%", "(", "+", "absolute", "7.9", "%", ")", ".", "When", "we", "only", "add", "external", "knowledge", "in", "inference", "composition", "(", "\u201c", "C", "\u201d", ")", ",", "the", "accuracy", "gets", "a", "smaller", "gain", "to", "63.4", "%", "(", "+", "absolute", "1.0", "%", ")", ".", "The", "comparison", "indicates", "that", "\u201c", "I", "\u201d", "plays", "the", "most", "important", "role", "among", "the", "three", "components", "in", "using", "external", "knowledge", ".", "Moreover", ",", "when", "we", "compose", "the", "three", "components", "(", "\u201c", "A", ",", "I", ",", "C", "\u201d", ")", ",", "we", "obtain", "the", "best", "result", "of", "72.6", "%", "(", "+", "absolute", "10.2", "%", ")", ".", "When", "we", "use", "more", "training", "data", ",", "i.e.", ",", "4", "%", ",", "20", "%", ",", "100", "%", "of", "the", "training", "set", ",", "only", "\u201c", "I", "\u201d", "achieves", "a", "significant", "gain", ",", "but", "\u201c", "A", "\u201d", "or", "\u201c", "C", "\u201d", "does", "not", "bring", "any", "significant", "improvement", ".", "The", "results", "indicate", "that", "external", "semantic", "knowledge", "only", "helps", "co", "-", "attention", "and", "composition", "when", "limited", "training", "data", "is", "limited", ",", "but", "always", "helps", "in", "collecting", "local", "inference", "information", ".", "Meanwhile", ",", "for", "less", "training", "data", ",", "is", "usually", "set", "to", "a", "larger", "value", ".", "For", "example", ",", "the", "optimal", "on", "the", "development", "set", "is", "for", "0.8", "%", "training", "set", ",", "for", "the", "4", "%", "training", "set", ",", "for", "the", "20", "%", "training", "set", "and", "for", "the", "100", "%", "training", "set", ".", "Figure", "[", "reference", "]", "displays", "the", "results", "of", "using", "different", "ratios", "of", "external", "knowledge", "(", "randomly", "keep", "different", "percentages", "of", "whole", "lexical", "semantic", "relations", ")", "under", "different", "sizes", "of", "training", "data", ".", "Note", "that", "here", "we", "only", "use", "external", "knowledge", "in", "collecting", "local", "inference", "information", "as", "it", "always", "works", "well", "for", "different", "scale", "of", "the", "training", "set", ".", "Better", "accuracies", "are", "achieved", "when", "using", "more", "external", "knowledge", ".", "Especially", "under", "the", "condition", "of", "restricted", "training", "data", "(", "0.8", "%", ")", ",", "the", "model", "obtains", "a", "large", "gain", "when", "using", "more", "than", "half", "of", "external", "knowledge", ".", "subsection", ":", "Analysis", "on", "the", "glockner_acl18", "Test", "Set", "In", "addition", ",", "Table", "[", "reference", "]", "shows", "the", "results", "on", "a", "newly", "published", "test", "set", "glockner_acl18", ".", "Compared", "with", "the", "performance", "on", "the", "SNLI", "test", "set", ",", "the", "performance", "of", "the", "three", "baseline", "models", "dropped", "substantially", "on", "the", "glockner_acl18", "test", "set", ",", "with", "the", "differences", "ranging", "from", "22.3", "%", "to", "32.8", "%", "in", "accuracy", ".", "Instead", ",", "the", "proposed", "KIM", "achieves", "83.5", "%", "on", "this", "test", "set", "(", "with", "only", "a", "5.1", "%", "drop", "in", "performance", ")", ",", "which", "demonstrates", "its", "better", "ability", "of", "utilizing", "lexical", "level", "inference", "and", "hence", "better", "generalizability", ".", "Figure", "[", "reference", "]", "displays", "the", "accuracy", "of", "ESIM", "and", "KIM", "in", "each", "replacement", "-", "word", "category", "of", "the", "glockner_acl18", "test", "set", ".", "KIM", "outperforms", "ESIM", "in", "13", "out", "of", "14", "categories", ",", "and", "only", "performs", "worse", "on", "synonyms", ".", "subsection", ":", "Analysis", "by", "Inference", "Categories", "We", "perform", "more", "analysis", "(", "Table", "[", "reference", "]", ")", "using", "the", "supplementary", "annotations", "provided", "by", "the", "MultiNLI", "dataset", "DBLP", ":", "journals", "/", "corr", "/", "WilliamsNB17", ",", "which", "have", "495", "samples", "(", "about", "1", "/", "20", "of", "the", "entire", "development", "set", ")", "for", "both", "in", "-", "domain", "and", "out", "-", "domain", "set", ".", "We", "compare", "against", "the", "model", "outputs", "of", "the", "ESIM", "model", "across", "13", "categories", "of", "inference", ".", "Table", "[", "reference", "]", "reports", "the", "results", ".", "We", "can", "see", "that", "KIM", "outperforms", "ESIM", "on", "overall", "accuracies", "on", "both", "in", "-", "domain", "and", "cross", "-", "domain", "subset", "of", "development", "set", ".", "KIM", "outperforms", "or", "equals", "ESIM", "in", "10", "out", "of", "13", "categories", "on", "the", "cross", "-", "domain", "setting", ",", "while", "only", "7", "out", "of", "13", "categories", "on", "in", "-", "domain", "setting", ".", "It", "indicates", "that", "external", "knowledge", "helps", "more", "in", "cross", "-", "domain", "setting", ".", "Especially", ",", "for", "antonym", "category", "in", "cross", "-", "domain", "set", ",", "KIM", "outperform", "ESIM", "significantly", "(", "+", "absolute", "5.0", "%", ")", "as", "expected", ",", "because", "antonym", "feature", "captured", "by", "external", "knowledge", "would", "help", "unseen", "cross", "-", "domain", "samples", ".", "subsection", ":", "Case", "Study", "Table", "[", "reference", "]", "includes", "some", "examples", "from", "the", "SNLI", "test", "set", ",", "where", "KIM", "successfully", "predicts", "the", "inference", "relation", "and", "ESIM", "fails", ".", "In", "the", "first", "example", ",", "the", "premise", "is", "\u201c", "An", "African", "person", "standing", "in", "a", "wheat", "field", "\u201d", "and", "the", "hypothesis", "\u201c", "A", "person", "standing", "in", "a", "corn", "field", "\u201d", ".", "As", "the", "KIM", "model", "knows", "that", "\u201c", "wheat", "\u201d", "and", "\u201c", "corn", "\u201d", "are", "both", "a", "kind", "of", "cereal", ",", "i.e", ",", "the", "co", "-", "hyponyms", "relationship", "in", "our", "relation", "features", ",", "KIM", "therefore", "predicts", "the", "premise", "contradicts", "the", "hypothesis", ".", "However", ",", "the", "baseline", "ESIM", "can", "not", "learn", "the", "relationship", "between", "\u201c", "wheat", "\u201d", "and", "\u201c", "corn", "\u201d", "effectively", "due", "to", "lack", "of", "enough", "samples", "in", "the", "training", "sets", ".", "With", "the", "help", "of", "external", "knowledge", ",", "i.e.", ",", "\u201c", "wheat", "\u201d", "and", "\u201c", "corn", "\u201d", "having", "the", "same", "hypernym", "\u201c", "cereal", "\u201d", ",", "KIM", "predicts", "contradiction", "correctly", ".", "section", ":", "Conclusions", "Our", "neural", "-", "network", "-", "based", "model", "for", "natural", "language", "inference", "with", "external", "knowledge", ",", "namely", "KIM", ",", "achieves", "the", "state", "-", "of", "-", "the", "-", "art", "accuracies", ".", "The", "model", "is", "equipped", "with", "external", "knowledge", "in", "its", "main", "components", ",", "specifically", ",", "in", "calculating", "co", "-", "attention", ",", "collecting", "local", "inference", ",", "and", "composing", "inference", ".", "We", "provide", "detailed", "analyses", "on", "our", "model", "and", "results", ".", "The", "proposed", "model", "of", "infusing", "neural", "networks", "with", "external", "knowledge", "may", "also", "help", "shed", "some", "light", "on", "tasks", "other", "than", "NLI", ".", "section", ":", "Acknowledgments", "We", "thank", "Yibo", "Sun", "and", "Bing", "Qin", "for", "early", "helpful", "discussion", ".", "bibliography", ":", "References"]}