{"coref": {"BLEU_score": [[115, 117], [161, 163], [661, 663], [2757, 2759], [2780, 2782], [2863, 2865], [2871, 2873], [3081, 3083], [3145, 3147], [3807, 3809], [3845, 3847], [2732, 2734]], "Machine_Translation": [[6, 8], [17, 20], [78, 81], [247, 249], [476, 479], [704, 707], [708, 710]], "WMT2014_English-French": [[2705, 2710], [178, 183], [503, 508], [2770, 2775]], "WMT2014_English-German": [[2690, 2699]], "Weighted_Transformer": [[2, 5], [97, 99], [101, 102], [419, 421], [482, 484], [521, 523], [576, 578], [581, 583], [611, 613], [803, 805], [821, 823], [894, 896], [963, 965], [967, 969], [1336, 1338], [1522, 1524], [1698, 1700], [1755, 1757], [1771, 1773], [1990, 1992], [2123, 2125], [2198, 2199], [2402, 2404], [2486, 2488], [2492, 2494], [2531, 2533], [2728, 2730], [2831, 2833], [2891, 2893], [2925, 2927], [3045, 3047], [3049, 3051], [3068, 3070], [3430, 3432], [3486, 3488], [3725, 3727], [3737, 3739], [3751, 3753], [3796, 3798]], "Weighted_Transformer__large_": [], "large": [[564, 566]]}, "coref_non_salient": {"0": [[2921, 2923], [2933, 2935]], "1": [[673, 679], [2682, 2688], [3786, 3792]], "10": [[3626, 3633], [3685, 3689]], "11": [[1156, 1161], [1206, 1210], [1236, 1241]], "12": [[1582, 1583], [1604, 1605]], "13": [[1414, 1416], [2132, 2134], [2310, 2312]], "14": [[712, 713], [715, 716], [1116, 1117], [3036, 3039]], "15": [[3251, 3253]], "16": [[54, 62], [1606, 1610], [1656, 1660]], "17": [[825, 830], [1452, 1459], [1739, 1746], [1777, 1781], [2004, 2008]], "18": [[2180, 2182], [2632, 2634]], "19": [[997, 1001], [1143, 1147], [1340, 1344], [1783, 1787]], "2": [[2992, 2995], [3325, 3329], [3534, 3540]], "20": [[375, 377], [606, 608], [982, 984], [2102, 2104]], "21": [[936, 938], [2240, 2242]], "22": [[3756, 3760]], "23": [[453, 458], [1132, 1137], [1149, 1154], [1221, 1226], [2091, 2096]], "24": [[746, 748], [1311, 1313], [2585, 2587]], "25": [[3580, 3581]], "26": [[2375, 2377], [2411, 2413]], "27": [[2318, 2320]], "28": [[2986, 2990]], "29": [[1736, 1737]], "3": [[447, 449], [1031, 1033], [2129, 2131]], "30": [[1356, 1358]], "31": [[2226, 2228]], "32": [[1812, 1814]], "33": [[2746, 2752], [2820, 2826], [3019, 3025], [3194, 3199], [3490, 3495], [547, 552]], "34": [[2355, 2357], [2372, 2373]], "35": [[1469, 1471], [3184, 3186]], "36": [[2322, 2324], [2331, 2333]], "37": [[1761, 1763], [3060, 3062]], "38": [[2261, 2264]], "39": [[3585, 3589]], "4": [[1315, 1319], [1477, 1485], [1966, 1970]], "40": [[383, 385], [1674, 1676]], "41": [[2565, 2567]], "42": [[1409, 1410]], "43": [[2909, 2913]], "44": [[665, 672], [2674, 2681], [3778, 3785], [165, 172], [495, 502]], "45": [[3556, 3557]], "46": [[2452, 2454]], "47": [[834, 839]], "48": [[1641, 1644]], "49": [[2805, 2807]], "5": [[189, 192], [325, 327]], "50": [[2365, 2371], [2394, 2398]], "51": [[3334, 3336]], "52": [[852, 854]], "53": [[1596, 1598], [1615, 1617], [1633, 1635], [1665, 1667]], "54": [[334, 339]], "55": [[1062, 1064]], "56": [[958, 960]], "57": [[2266, 2267]], "58": [[2644, 2647]], "59": [[1526, 1529]], "6": [[639, 641], [1549, 1551]], "60": [[750, 754]], "61": [[33, 36]], "62": [[1350, 1354]], "63": [[1405, 1406]], "64": [[1533, 1537]], "65": [[219, 223]], "66": [[1688, 1690]], "67": [[1732, 1734], [1791, 1793]], "68": [[3573, 3575]], "69": [[799, 801]], "7": [[2513, 2515], [2929, 2931], [3072, 3074], [3823, 3825], [2847, 2849], [3090, 3092]], "70": [[2275, 2280], [2996, 3001], [2172, 2177]], "71": [[3605, 3607]], "72": [[193, 194], [224, 225], [253, 254]], "73": [[240, 242]], "74": [[393, 395]], "75": [[948, 950]], "76": [[1748, 1752]], "77": [[626, 628]], "78": [[1364, 1367]], "79": [[2609, 2613]], "8": [[858, 862], [2033, 2037], [2235, 2239], [3353, 3357]], "80": [[971, 975]], "81": [[1218, 1220]], "82": [[2340, 2341]], "83": [[916, 919]], "84": [[719, 723]], "85": [[279, 281]], "86": [[198, 204]], "87": [[463, 465]], "88": [[2326, 2328]], "89": [[329, 331]], "9": [[205, 206], [725, 726]], "90": [[2010, 2014]], "91": [[2464, 2466]], "92": [[22, 29]], "93": [[1026, 1029]], "94": [[1255, 1259]], "95": [[103, 106]], "96": [[234, 236]], "97": [[469, 470]]}, "doc_id": "3861ae2a6bdd2a759c2d901a6583e63a216bc2fc", "method_subrelations": {"Weighted_Transformer__large_": [[[0, 20], "Weighted_Transformer"], [[22, 27], "large"]]}, "n_ary_relations": [{"Material": "WMT2014_English-French", "Method": "Weighted_Transformer__large_", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "41.4"}, {"Material": "WMT2014_English-German", "Method": "Weighted_Transformer__large_", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "28.9"}], "ner": [[2, 5, "Method"], [6, 8, "Task"], [17, 20, "Task"], [22, 29, "Method"], [33, 36, "Method"], [54, 62, "Method"], [78, 81, "Task"], [97, 99, "Method"], [101, 102, "Method"], [103, 106, "Method"], [115, 117, "Metric"], [161, 163, "Metric"], [189, 192, "Method"], [193, 194, "Method"], [198, 204, "Method"], [205, 206, "Method"], [219, 223, "Task"], [224, 225, "Method"], [234, 236, "Method"], [240, 242, "Task"], [247, 249, "Task"], [253, 254, "Method"], [279, 281, "Method"], [325, 327, "Method"], [329, 331, "Method"], [334, 339, "Method"], [375, 377, "Method"], [383, 385, "Method"], [393, 395, "Task"], [419, 421, "Method"], [447, 449, "Method"], [453, 458, "Method"], [463, 465, "Task"], [469, 470, "Metric"], [476, 479, "Task"], [482, 484, "Method"], [521, 523, "Method"], [564, 566, "Method"], [576, 578, "Method"], [581, 583, "Method"], [606, 608, "Method"], [611, 613, "Method"], [626, 628, "Method"], [639, 641, "Metric"], [661, 663, "Metric"], [665, 672, "Task"], [673, 679, "Task"], [704, 707, "Task"], [708, 710, "Task"], [712, 713, "Method"], [715, 716, "Method"], [719, 723, "Method"], [725, 726, "Method"], [746, 748, "Metric"], [750, 754, "Method"], [799, 801, "Method"], [803, 805, "Method"], [821, 823, "Method"], [825, 830, "Method"], [834, 839, "Method"], [852, 854, "Method"], [858, 862, "Method"], [894, 896, "Method"], [916, 919, "Method"], [936, 938, "Task"], [948, 950, "Method"], [958, 960, "Task"], [963, 965, "Method"], [967, 969, "Method"], [971, 975, "Method"], [982, 984, "Method"], [997, 1001, "Method"], [1026, 1029, "Method"], [1031, 1033, "Method"], [1062, 1064, "Method"], [1116, 1117, "Method"], [1132, 1137, "Method"], [1143, 1147, "Method"], [1149, 1154, "Method"], [1156, 1161, "Method"], [1206, 1210, "Method"], [1218, 1220, "Metric"], [1221, 1226, "Method"], [1236, 1241, "Method"], [1255, 1259, "Method"], [1311, 1313, "Metric"], [1315, 1319, "Task"], [1336, 1338, "Method"], [1340, 1344, "Method"], [1350, 1354, "Method"], [1356, 1358, "Method"], [1364, 1367, "Method"], [1405, 1406, "Task"], [1409, 1410, "Task"], [1414, 1416, "Method"], [1452, 1459, "Method"], [1469, 1471, "Method"], [1477, 1485, "Task"], [1522, 1524, "Method"], [1526, 1529, "Task"], [1533, 1537, "Method"], [1549, 1551, "Metric"], [1582, 1583, "Metric"], [1596, 1598, "Method"], [1604, 1605, "Metric"], [1606, 1610, "Method"], [1615, 1617, "Method"], [1633, 1635, "Method"], [1641, 1644, "Method"], [1656, 1660, "Method"], [1665, 1667, "Method"], [1674, 1676, "Method"], [1688, 1690, "Method"], [1698, 1700, "Method"], [1732, 1734, "Method"], [1736, 1737, "Method"], [1739, 1746, "Method"], [1748, 1752, "Method"], [1755, 1757, "Method"], [1761, 1763, "Method"], [1771, 1773, "Method"], [1777, 1781, "Method"], [1783, 1787, "Method"], [1791, 1793, "Method"], [1812, 1814, "Method"], [1966, 1970, "Task"], [1990, 1992, "Method"], [2004, 2008, "Method"], [2010, 2014, "Method"], [2033, 2037, "Method"], [2091, 2096, "Method"], [2102, 2104, "Method"], [2123, 2125, "Method"], [2129, 2131, "Method"], [2180, 2182, "Method"], [2198, 2199, "Method"], [2226, 2228, "Task"], [2235, 2239, "Method"], [2240, 2242, "Task"], [2261, 2264, "Method"], [2266, 2267, "Task"], [2275, 2280, "Method"], [2318, 2320, "Method"], [2322, 2324, "Method"], [2326, 2328, "Method"], [2331, 2333, "Method"], [2340, 2341, "Method"], [2355, 2357, "Method"], [2365, 2371, "Method"], [2372, 2373, "Method"], [2375, 2377, "Metric"], [2394, 2398, "Method"], [2402, 2404, "Method"], [2411, 2413, "Metric"], [2452, 2454, "Method"], [2464, 2466, "Method"], [2486, 2488, "Method"], [2492, 2494, "Method"], [2513, 2515, "Method"], [2531, 2533, "Method"], [2565, 2567, "Method"], [2585, 2587, "Metric"], [2609, 2613, "Method"], [2644, 2647, "Method"], [2674, 2681, "Task"], [2682, 2688, "Task"], [2690, 2699, "Material"], [2705, 2710, "Material"], [2728, 2730, "Method"], [2746, 2752, "Task"], [2757, 2759, "Metric"], [2780, 2782, "Metric"], [2805, 2807, "Task"], [2820, 2826, "Task"], [2831, 2833, "Method"], [2863, 2865, "Metric"], [2871, 2873, "Metric"], [2891, 2893, "Method"], [2909, 2913, "Method"], [2921, 2923, "Metric"], [2925, 2927, "Method"], [2929, 2931, "Method"], [2933, 2935, "Metric"], [2986, 2990, "Method"], [2992, 2995, "Method"], [2996, 3001, "Method"], [3019, 3025, "Task"], [3036, 3039, "Method"], [3045, 3047, "Method"], [3049, 3051, "Method"], [3060, 3062, "Method"], [3068, 3070, "Method"], [3072, 3074, "Method"], [3081, 3083, "Metric"], [3145, 3147, "Metric"], [3184, 3186, "Method"], [3194, 3199, "Task"], [3251, 3253, "Method"], [3325, 3329, "Method"], [3334, 3336, "Method"], [3353, 3357, "Method"], [3430, 3432, "Method"], [3486, 3488, "Method"], [3490, 3495, "Task"], [3534, 3540, "Method"], [3556, 3557, "Task"], [3573, 3575, "Method"], [3580, 3581, "Method"], [3585, 3589, "Method"], [3605, 3607, "Method"], [3626, 3633, "Method"], [3685, 3689, "Method"], [3725, 3727, "Method"], [3737, 3739, "Method"], [3751, 3753, "Method"], [3756, 3760, "Method"], [3778, 3785, "Task"], [3786, 3792, "Task"], [3796, 3798, "Method"], [3807, 3809, "Metric"], [3823, 3825, "Method"], [3845, 3847, "Metric"], [165, 172, "Task"], [178, 183, "Material"], [495, 502, "Task"], [503, 508, "Material"], [547, 552, "Task"], [2132, 2134, "Method"], [2172, 2177, "Method"], [2310, 2312, "Method"], [2632, 2634, "Method"], [2732, 2734, "Metric"], [2770, 2775, "Material"], [2847, 2849, "Method"], [3090, 3092, "Method"]], "sections": [[0, 186], [186, 697], [697, 961], [961, 1685], [1685, 2283], [2283, 2286], [2286, 2660], [2660, 3002], [3002, 3310], [3310, 3554], [3554, 3719], [3719, 3862], [3862, 3865]], "sentences": [[0, 8], [8, 37], [37, 49], [49, 63], [63, 95], [95, 122], [122, 148], [148, 186], [186, 189], [189, 224], [224, 249], [249, 253], [253, 278], [278, 293], [293, 323], [323, 351], [351, 375], [375, 412], [412, 442], [442, 459], [459, 471], [471, 521], [521, 541], [541, 570], [570, 598], [598, 625], [625, 642], [642, 682], [682, 697], [697, 701], [701, 738], [738, 762], [762, 802], [802, 817], [817, 840], [840, 855], [855, 876], [876, 897], [897, 924], [924, 961], [961, 965], [965, 1002], [1002, 1009], [1009, 1030], [1030, 1059], [1059, 1087], [1087, 1105], [1105, 1148], [1148, 1181], [1181, 1202], [1202, 1221], [1221, 1260], [1260, 1287], [1287, 1305], [1305, 1328], [1328, 1345], [1345, 1359], [1359, 1386], [1386, 1404], [1404, 1432], [1432, 1472], [1472, 1517], [1517, 1538], [1538, 1572], [1572, 1599], [1599, 1618], [1618, 1636], [1636, 1645], [1645, 1668], [1668, 1685], [1685, 1690], [1690, 1715], [1715, 1728], [1728, 1753], [1753, 1789], [1789, 1811], [1811, 1825], [1825, 1850], [1850, 1870], [1870, 1890], [1890, 1904], [1904, 1928], [1928, 1950], [1950, 1976], [1976, 1993], [1993, 2015], [2015, 2041], [2041, 2047], [2047, 2060], [2060, 2078], [2078, 2100], [2100, 2126], [2126, 2147], [2147, 2159], [2159, 2184], [2184, 2196], [2196, 2217], [2217, 2229], [2229, 2259], [2259, 2268], [2268, 2283], [2283, 2286], [2286, 2290], [2290, 2306], [2306, 2331], [2331, 2348], [2348, 2361], [2361, 2390], [2390, 2420], [2420, 2435], [2435, 2448], [2448, 2467], [2467, 2489], [2489, 2510], [2510, 2528], [2528, 2543], [2543, 2558], [2558, 2580], [2580, 2604], [2604, 2620], [2620, 2638], [2638, 2660], [2660, 2667], [2667, 2689], [2689, 2704], [2704, 2715], [2715, 2727], [2727, 2764], [2764, 2794], [2794, 2827], [2827, 2854], [2854, 2881], [2881, 2894], [2894, 2914], [2914, 2941], [2941, 2959], [2959, 2977], [2977, 3002], [3002, 3006], [3006, 3026], [3026, 3052], [3052, 3084], [3084, 3129], [3129, 3148], [3148, 3168], [3168, 3202], [3202, 3229], [3229, 3249], [3249, 3291], [3291, 3310], [3310, 3314], [3314, 3332], [3332, 3358], [3358, 3367], [3367, 3381], [3381, 3394], [3394, 3468], [3468, 3511], [3511, 3529], [3529, 3554], [3554, 3557], [3557, 3590], [3590, 3621], [3621, 3636], [3636, 3661], [3661, 3683], [3683, 3700], [3700, 3719], [3719, 3722], [3722, 3740], [3740, 3772], [3772, 3814], [3814, 3826], [3826, 3862], [3862, 3865]], "words": ["document", ":", "Weighted", "Transformer", "Network", "for", "Machine", "Translation", "State", "-", "of", "-", "the", "-", "art", "results", "on", "neural", "machine", "translation", "often", "use", "attentional", "sequence", "-", "to", "-", "sequence", "models", "with", "some", "form", "of", "convolution", "or", "recursion", ".", "vaswani2017attention", "propose", "a", "new", "architecture", "that", "avoids", "recurrence", "and", "convolution", "completely", ".", "Instead", ",", "it", "uses", "only", "self", "-", "attention", "and", "feed", "-", "forward", "layers", ".", "While", "the", "proposed", "architecture", "achieves", "state", "-", "of", "-", "the", "-", "art", "results", "on", "several", "machine", "translation", "tasks", ",", "it", "requires", "a", "large", "number", "of", "parameters", "and", "training", "iterations", "to", "converge", ".", "We", "propose", "Weighted", "Transformer", ",", "a", "Transformer", "with", "modified", "attention", "layers", ",", "that", "not", "only", "outperforms", "the", "baseline", "network", "in", "BLEU", "score", "but", "also", "converges", "faster", ".", "Specifically", ",", "we", "replace", "the", "multi", "-", "head", "attention", "by", "multiple", "self", "-", "attention", "branches", "that", "the", "model", "learns", "to", "combine", "during", "the", "training", "process", ".", "Our", "model", "improves", "the", "state", "-", "of", "-", "the", "-", "art", "performance", "by", "BLEU", "points", "on", "the", "WMT", "2014", "English", "-", "to", "-", "German", "translation", "task", "and", "by", "on", "the", "English", "-", "to", "-", "French", "translation", "task", ".", "section", ":", "Introduction", "Recurrent", "neural", "networks", "(", "RNNs", ")", ",", "such", "as", "long", "short", "-", "term", "memory", "networks", "(", "LSTMs", ")", "hochreiter1997long", ",", "form", "an", "important", "building", "block", "for", "many", "tasks", "that", "require", "modeling", "of", "sequential", "data", ".", "RNNs", "have", "been", "successfully", "employed", "for", "several", "such", "tasks", "including", "language", "modeling", "melis2017state", ",", "merity2017regularizing", ",", "speech", "recognition", "xiong2017microsoft", ",", "graves2013speech", ",", "and", "machine", "translation", "wu2016google", ",", "bahdanau2014neural", ".", "RNNs", "make", "output", "predictions", "at", "each", "time", "step", "by", "computing", "a", "hidden", "state", "vector", "based", "on", "the", "current", "input", "token", "and", "the", "previous", "states", ".", "This", "sequential", "computation", "underlies", "their", "ability", "to", "map", "arbitrary", "input", "-", "output", "sequence", "pairs", ".", "However", ",", "because", "of", "their", "auto", "-", "regressive", "property", "of", "requiring", "previous", "hidden", "states", "to", "be", "computed", "before", "the", "current", "time", "step", ",", "they", "can", "not", "benefit", "from", "parallelization", ".", "Variants", "of", "recurrent", "networks", "that", "use", "strided", "convolutions", "eschew", "the", "traditional", "time", "-", "step", "based", "computation", "kaiser2016can", ",", "lei2017training", ",", "bradbury2016quasi", ",", "gehring2016convenc", ",", "gehring2017convs2s", ",", "kalchbrenner2016neural", ".", "However", ",", "in", "these", "models", ",", "the", "operations", "needed", "to", "learn", "dependencies", "between", "distant", "positions", "can", "be", "difficult", "to", "learn", "hochreiter2001gradient", ",", "hochreiter1998vanishing", ".", "Attention", "mechanisms", ",", "often", "used", "in", "conjunction", "with", "recurrent", "models", ",", "have", "become", "an", "integral", "part", "of", "complex", "sequential", "tasks", "because", "they", "facilitate", "learning", "of", "such", "dependencies", "luong2015effective", ",", "bahdanau2014neural", ",", "parikh2016decomposable", ",", "paulus2017deep", ",", "kim2017structured", ".", "In", "vaswani2017attention", ",", "the", "authors", "introduce", "the", "Transformer", "network", ",", "a", "novel", "architecture", "that", "avoids", "the", "recurrence", "equation", "and", "maps", "the", "input", "sequences", "into", "hidden", "states", "solely", "using", "attention", ".", "Specifically", ",", "the", "authors", "use", "positional", "encodings", "in", "conjunction", "with", "a", "multi", "-", "head", "attention", "mechanism", ".", "This", "allows", "for", "increased", "parallel", "computation", "and", "reduces", "time", "to", "convergence", ".", "The", "authors", "report", "results", "for", "neural", "machine", "translation", "that", "show", "the", "Transformer", "networks", "achieves", "state", "-", "of", "-", "the", "-", "art", "performance", "on", "the", "WMT", "2014", "English", "-", "to", "-", "German", "and", "English", "-", "to", "-", "French", "tasks", "while", "being", "orders", "-", "of", "-", "magnitude", "faster", "than", "prior", "approaches", ".", "Transformer", "networks", "still", "require", "a", "large", "number", "of", "parameters", "to", "achieve", "state", "-", "of", "-", "the", "-", "art", "performance", ".", "In", "the", "case", "of", "the", "newstest2013", "English", "-", "to", "-", "German", "translation", "task", ",", "the", "base", "model", "required", "M", "parameters", ",", "and", "the", "large", "model", "required", "M", "parameters", ".", "We", "propose", "a", "variant", "of", "the", "Transformer", "network", "which", "we", "call", "Weighted", "Transformer", "that", "uses", "self", "-", "attention", "branches", "in", "lieu", "of", "the", "multi", "-", "head", "attention", ".", "The", "branches", "replace", "the", "multiple", "heads", "in", "the", "attention", "mechanism", "of", "the", "original", "Transformer", "network", ",", "and", "the", "model", "learns", "to", "combine", "these", "branches", "during", "training", ".", "This", "branched", "architecture", "enables", "the", "network", "to", "achieve", "comparable", "performance", "at", "a", "significantly", "lower", "computational", "cost", ".", "Indeed", ",", "through", "this", "modification", ",", "we", "improve", "the", "state", "-", "of", "-", "the", "-", "art", "performance", "by", "and", "BLEU", "scores", "on", "the", "WMT", "2014", "English", "-", "to", "-", "German", "and", "English", "-", "to", "-", "French", "tasks", ",", "respectively", ".", "Finally", ",", "we", "present", "evidence", "that", "suggests", "a", "regularizing", "effect", "of", "the", "proposed", "architecture", ".", "section", ":", "Related", "Work", "Most", "architectures", "for", "neural", "machine", "translation", "(", "NMT", ")", "use", "an", "encoder", "and", "a", "decoder", "that", "rely", "on", "deep", "recurrent", "neural", "networks", "like", "the", "LSTM", "luong2015effective", ",", "sutskever2014sequence", ",", "bahdanau2014neural", ",", "wu2016google", ",", "barone2017deep", ",", "cho2014learning", ".", "Several", "architectures", "have", "been", "proposed", "to", "reduce", "the", "computational", "load", "associated", "with", "recurrence", "-", "based", "computation", "gehring2016convenc", ",", "gehring2017convs2s", ",", "kaiser2016can", ",", "kalchbrenner2016neural", ".", "Self", "-", "attention", ",", "which", "relies", "on", "dot", "-", "products", "between", "elements", "of", "the", "input", "sequence", "to", "compute", "a", "weighted", "sum", "lin2017structured", ",", "bahdanau2014neural", ",", "parikh2016decomposable", ",", "kim2017structured", ",", "has", "also", "been", "a", "critical", "ingredient", "in", "modern", "NMT", "architectures", ".", "The", "Transformer", "network", "vaswani2017attention", "avoids", "the", "recurrence", "completely", "and", "uses", "only", "self", "-", "attention", ".", "We", "propose", "a", "modified", "Transformer", "network", "wherein", "the", "multi", "-", "head", "attention", "layer", "is", "replaced", "by", "a", "branched", "self", "-", "attention", "layer", ".", "The", "contributions", "of", "the", "various", "branches", "is", "learned", "as", "part", "of", "the", "training", "procedure", ".", "The", "idea", "of", "multi", "-", "branch", "networks", "has", "been", "explored", "in", "several", "domains", "ahmed2017branchconnect", ",", "gastaldi2017shake", ",", "shazeer2017outrageously", ",", "xie2016aggregated", ".", "To", "the", "best", "of", "our", "knowledge", ",", "this", "is", "the", "first", "model", "using", "a", "branched", "structure", "in", "the", "Transformer", "network", ".", "In", "shazeer2017outrageously", ",", "the", "authors", "use", "a", "large", "network", ",", "with", "billions", "of", "weights", ",", "in", "conjunction", "with", "a", "sparse", "expert", "model", "to", "achieve", "competitive", "performance", ".", "ahmed2017branchconnect", "analyze", "learned", "branching", ",", "through", "gates", ",", "in", "the", "context", "of", "computer", "vision", "while", "in", "gastaldi2017shake", ",", "the", "author", "analyzes", "a", "two", "-", "branch", "model", "with", "randomly", "sampled", "weights", "in", "the", "context", "of", "image", "classification", ".", "subsection", ":", "Transformer", "Network", "The", "original", "Transformer", "network", "uses", "an", "encoder", "-", "decoder", "architecture", "with", "each", "layer", "consisting", "of", "a", "novel", "attention", "mechanism", ",", "which", "the", "authors", "call", "multi", "-", "head", "attention", ",", "followed", "by", "a", "feed", "-", "forward", "network", ".", "We", "describe", "both", "these", "components", "below", ".", "From", "the", "source", "tokens", ",", "learned", "embeddings", "of", "dimension", "are", "generated", "which", "are", "then", "modified", "by", "an", "additive", "positional", "encoding", ".", "The", "positional", "encoding", "is", "necessary", "since", "the", "network", "does", "not", "otherwise", "possess", "any", "means", "of", "leveraging", "the", "order", "of", "the", "sequence", "since", "it", "contains", "no", "recurrence", "or", "convolution", ".", "The", "authors", "use", "additive", "encoding", "which", "is", "defined", "as", ":", "where", "is", "the", "position", "of", "a", "word", "in", "the", "sentence", "and", "is", "the", "dimension", "of", "the", "vector", ".", "The", "authors", "also", "experiment", "with", "learned", "embeddings", "gehring2016convenc", ",", "gehring2017convs2s", "but", "found", "no", "benefit", "in", "doing", "so", ".", "The", "encoded", "word", "embeddings", "are", "then", "used", "as", "input", "to", "the", "encoder", "which", "consists", "of", "layers", "each", "containing", "two", "sub", "-", "layers", ":", "(", "a", ")", "a", "multi", "-", "head", "attention", "mechanism", ",", "and", "(", "b", ")", "a", "feed", "-", "forward", "network", ".", "A", "multi", "-", "head", "attention", "mechanism", "builds", "upon", "scaled", "dot", "-", "product", "attention", ",", "which", "operates", "on", "a", "query", ",", "key", "and", "a", "value", ":", "where", "is", "the", "dimension", "of", "the", "key", ".", "In", "the", "first", "layer", ",", "the", "inputs", "are", "concatenated", "such", "that", "each", "of", "is", "equal", "to", "the", "word", "vector", "matrix", ".", "This", "is", "identical", "to", "dot", "-", "product", "attention", "except", "for", "the", "scaling", "factor", ",", "which", "improves", "numerical", "stability", ".", "Multi", "-", "head", "attention", "mechanisms", "obtain", "different", "representations", "of", "(", ",", ",", ")", ",", "compute", "scaled", "dot", "-", "product", "attention", "for", "each", "representation", ",", "concatenate", "the", "results", ",", "and", "project", "the", "concatenation", "with", "a", "feed", "-", "forward", "layer", ".", "This", "can", "be", "expressed", "in", "the", "same", "notation", "as", "Equation", "(", "[", "reference", "]", ")", ":", "where", "the", "and", "are", "parameter", "projection", "matrices", "that", "are", "learned", ".", "Note", "that", ",", ",", "and", "where", "denotes", "the", "number", "of", "heads", "in", "the", "multi", "-", "head", "attention", ".", "vaswani2017attention", "proportionally", "reduce", "so", "that", "the", "computational", "load", "of", "the", "multi", "-", "head", "attention", "is", "the", "same", "as", "simple", "self", "-", "attention", ".", "The", "second", "component", "of", "each", "layer", "of", "the", "Transformer", "network", "is", "a", "feed", "-", "forward", "network", ".", "The", "authors", "propose", "using", "a", "two", "-", "layered", "network", "with", "a", "ReLU", "activation", ".", "Given", "trainable", "weights", ",", "the", "sub", "-", "layer", "is", "defined", "as", ":", "The", "dimension", "of", "the", "inner", "layer", "is", "which", "is", "set", "to", "in", "their", "experiments", ".", "For", "the", "sake", "of", "brevity", ",", "we", "refer", "the", "reader", "to", "for", "additional", "details", "regarding", "the", "architecture", ".", "For", "regularization", "and", "ease", "of", "training", ",", "the", "network", "uses", "layer", "normalization", "ba2016layer", "after", "each", "sub", "-", "layer", "and", "a", "residual", "connection", "around", "each", "full", "layer", "he2016deep", ".", "Analogously", ",", "each", "layer", "of", "the", "decoder", "contains", "the", "two", "sub", "-", "layers", "mentioned", "above", "as", "well", "as", "an", "additional", "multi", "-", "head", "attention", "sub", "-", "layer", "that", "receives", "as", "inputs", "from", "the", "output", "of", "the", "corresponding", "encoding", "layer", ".", "In", "the", "case", "of", "the", "decoder", "multi", "-", "head", "attention", "sub", "-", "layers", ",", "the", "scaled", "dot", "-", "product", "attention", "is", "masked", "to", "prevent", "future", "positions", "from", "being", "attended", "to", ",", "or", "in", "other", "words", ",", "to", "prevent", "illegal", "leftward", "-", "ward", "information", "flow", ".", "One", "natural", "question", "regarding", "the", "Transformer", "network", "is", "why", "self", "-", "attention", "should", "be", "preferred", "to", "recurrent", "or", "convolutional", "models", ".", "vaswani2017attention", "state", "three", "reasons", "for", "the", "preference", ":", "(", "a", ")", "computational", "complexity", "of", "each", "layer", ",", "(", "b", ")", "concurrency", ",", "and", "(", "c", ")", "path", "length", "between", "long", "-", "range", "dependencies", ".", "Assuming", "a", "sequence", "length", "of", "and", "vector", "dimension", ",", "the", "complexity", "of", "each", "layer", "is", "for", "self", "-", "attention", "layers", "while", "it", "is", "for", "recurrent", "layers", ".", "Given", "that", "typically", ",", "the", "complexity", "of", "self", "-", "attention", "layers", "is", "lower", "than", "that", "of", "recurrent", "layers", ".", "Further", ",", "the", "number", "of", "sequential", "computations", "is", "for", "self", "-", "attention", "layers", "and", "for", "recurrent", "layers", ".", "This", "helps", "improved", "utilization", "of", "parallel", "computing", "architectures", ".", "Finally", ",", "the", "maximum", "path", "length", "between", "dependencies", "is", "for", "the", "self", "-", "attention", "layer", "while", "it", "is", "for", "the", "recurrent", "layer", ".", "This", "difference", "is", "instrumental", "in", "impeding", "recurrent", "models", "\u2019", "ability", "to", "learn", "long", "-", "range", "dependencies", ".", "section", ":", "Proposed", "Network", "Architecture", "We", "now", "describe", "the", "proposed", "architecture", ",", "the", "Weighted", "Transformer", ",", "which", "is", "more", "efficient", "to", "train", "and", "makes", "better", "use", "of", "representational", "power", ".", "In", "Equations", "(", "[", "reference", "]", ")", "and", "(", "[", "reference", "]", ")", ",", "we", "described", "the", "attention", "layer", "proposed", "in", "vaswani2017attention", "comprising", "the", "multi", "-", "head", "attention", "sub", "-", "layer", "and", "a", "FFN", "sub", "-", "layer", ".", "For", "the", "Weighted", "Transformer", ",", "we", "propose", "a", "branched", "attention", "that", "modifies", "the", "entire", "attention", "layer", "in", "the", "Transformer", "network", "(", "including", "both", "the", "multi", "-", "head", "attention", "and", "the", "feed", "-", "forward", "network", ")", ".", "The", "proposed", "attention", "layer", "can", "be", "described", "as", ":", "where", "denotes", "the", "total", "number", "of", "branches", ",", "are", "learned", "parameters", "and", ".", "The", "FFN", "function", "above", "is", "identical", "to", "Equation", "(", "[", "reference", "]", ")", ".", "Further", ",", "we", "require", "that", "and", "so", "that", "Equation", "(", "[", "reference", "]", ")", "is", "a", "weighted", "sum", "of", "the", "individual", "branch", "attention", "values", ".", "In", "the", "equations", "above", ",", "can", "be", "interpreted", "as", "a", "learned", "concatenation", "weight", "and", "as", "the", "learned", "addition", "weight", ".", "Indeed", ",", "scales", "the", "contribution", "of", "the", "various", "branches", "before", "is", "used", "to", "sum", "them", "in", "a", "weighted", "fashion", ".", "We", "ensure", "that", "all", "bounds", "are", "respected", "during", "each", "training", "step", "by", "projection", ".", "While", "it", "is", "possible", "that", "and", "could", "be", "merged", "into", "one", "variable", "and", "trained", ",", "we", "found", "better", "training", "outcomes", "by", "separating", "them", ".", "It", "also", "improves", "the", "interpretability", "of", "the", "models", "gives", "that", "can", "be", "thought", "of", "as", "probability", "masses", "on", "the", "various", "branches", ".", "It", "can", "be", "shown", "that", "if", "and", "for", "all", ",", "we", "recover", "the", "equation", "for", "the", "multi", "-", "head", "attention", "(", "[", "reference", "]", ")", ".", "However", ",", "given", "the", "and", "bounds", ",", "these", "values", "are", "not", "permissible", "in", "the", "Weighted", "Transformer", ".", "One", "interpretation", "of", "our", "proposed", "architecture", "is", "that", "it", "replaces", "the", "multi", "-", "head", "attention", "by", "a", "multi", "-", "branch", "attention", ".", "Rather", "than", "concatenating", "the", "contributions", "of", "the", "different", "heads", ",", "they", "are", "instead", "treated", "as", "branches", "that", "a", "multi", "-", "branch", "network", "learns", "to", "combine", ".", "This", "mechanism", "adds", "trainable", "weights", ".", "This", "is", "an", "insignificant", "increase", "compared", "to", "the", "total", "number", "of", "weights", ".", "Indeed", ",", "in", "our", "experiments", ",", "the", "proposed", "mechanism", "added", "weights", "to", "a", "model", "containing", "weights", "already", ".", "Without", "these", "additional", "trainable", "weights", ",", "the", "proposed", "mechanism", "is", "identical", "to", "the", "multi", "-", "head", "attention", "mechanism", "in", "the", "Transformer", ".", "The", "proposed", "attention", "mechanism", "is", "used", "in", "both", "the", "encoder", "and", "decoder", "layers", "and", "is", "masked", "in", "the", "decoder", "layers", "as", "in", "the", "Transformer", "network", ".", "Similarly", ",", "the", "positional", "encoding", ",", "layer", "normalization", ",", "and", "residual", "connections", "in", "the", "encoder", "-", "decoder", "layers", "are", "retained", ".", "We", "eliminate", "these", "details", "from", "Figure", "[", "reference", "]", "for", "clarity", ".", "Instead", "of", "using", "learned", "weights", ",", "it", "is", "possible", "to", "also", "use", "a", "mixture", "-", "of", "-", "experts", "normalization", "via", "a", "softmax", "layer", "shazeer2017outrageously", ".", "However", ",", "we", "found", "this", "to", "perform", "worse", "than", "our", "proposal", ".", "Unlike", "the", "Transformer", ",", "which", "weighs", "all", "heads", "equally", ",", "the", "proposed", "mechanism", "allows", "for", "ascribing", "importance", "to", "different", "heads", ".", "This", "in", "turn", "prioritizes", "their", "gradients", "and", "eases", "the", "optimization", "process", ".", "Further", ",", "as", "is", "known", "from", "multi", "-", "branch", "networks", "in", "computer", "vision", "gastaldi2017shake", ",", "such", "mechanisms", "tend", "to", "cause", "the", "branches", "to", "learn", "decorrelated", "input", "-", "output", "mappings", ".", "This", "reduces", "co", "-", "adaptation", "and", "improves", "generalization", ".", "This", "observation", "also", "forms", "the", "basis", "for", "mixture", "-", "of", "-", "experts", "models", "shazeer2017outrageously", ".", "section", ":", "Experiments", "subsection", ":", "Training", "Details", "The", "weights", "and", "are", "initialized", "randomly", ",", "as", "with", "the", "rest", "of", "the", "Transformer", "weights", ".", "In", "addition", "to", "the", "layer", "normalization", "and", "residual", "connections", ",", "we", "use", "label", "smoothing", "with", ",", "attention", "dropout", ",", "and", "residual", "dropout", "with", "probability", ".", "Attention", "dropout", "randomly", "drops", "out", "elements", "srivastava2014dropout", "from", "the", "softmax", "in", "(", "[", "reference", "]", ")", ".", "As", "in", "vaswani2017attention", ",", "we", "used", "the", "Adam", "optimizer", "kingma2014adam", "with", "and", ".", "We", "also", "use", "the", "learning", "rate", "warm", "-", "up", "strategy", "for", "Adam", "wherein", "the", "learning", "rate", "takes", "on", "the", "form", ":", "for", "the", "all", "parameters", "except", "and", "for", ".", "This", "corresponds", "to", "the", "warm", "-", "up", "strategy", "used", "for", "the", "original", "Transformer", "network", "except", "that", "we", "use", "a", "larger", "peak", "learning", "rate", "for", "to", "compensate", "for", "their", "bounds", ".", "Further", ",", "we", "found", "that", "freezing", "the", "weights", "in", "the", "last", "iterations", "aids", "convergence", ".", "During", "this", "time", ",", "we", "continue", "training", "the", "rest", "of", "the", "network", ".", "We", "hypothesize", "that", "this", "freezing", "process", "helps", "stabilize", "the", "rest", "of", "the", "network", "weights", "given", "the", "weighting", "scheme", ".", "We", "note", "that", "the", "number", "of", "iterations", "required", "for", "convergence", "to", "the", "final", "score", "is", "substantially", "reduced", "for", "the", "Weighted", "Transformer", ".", "We", "found", "that", "Weighted", "Transformer", "converges", "\u2013", "faster", "as", "measured", "by", "the", "total", "number", "of", "iterations", "to", "achieve", "optimal", "performance", ".", "We", "train", "the", "baseline", "model", "for", "K", "steps", "for", "the", "smaller", "variant", "and", "K", "for", "the", "larger", ".", "We", "train", "the", "Weighted", "Transformer", "for", "the", "respective", "variants", "for", "K", "and", "K", "iterations", ".", "We", "found", "that", "the", "objective", "did", "not", "significantly", "improve", "by", "running", "it", "for", "longer", ".", "Further", ",", "we", "do", "not", "use", "any", "averaging", "strategies", "employed", "in", "vaswani2017attention", "and", "simply", "return", "the", "final", "model", "for", "testing", "purposes", ".", "In", "order", "to", "reduce", "the", "computational", "load", "associated", "with", "padding", ",", "sentences", "were", "batched", "such", "that", "they", "were", "approximately", "of", "the", "same", "length", ".", "All", "sentences", "were", "encoded", "using", "byte", "-", "pair", "encoding", "sennrich2015neural", "and", "shared", "a", "common", "vocabulary", ".", "Weights", "for", "word", "embeddings", "were", "tied", "to", "corresponding", "entries", "in", "the", "final", "softmax", "layer", "inan2016tying", ",", "press2016using", ".", "We", "trained", "all", "our", "networks", "on", "NVIDIA", "K80", "GPUs", "with", "a", "batch", "containing", "roughly", "25", ",", "000", "source", "and", "target", "tokens", ".", "subsection", ":", "Results", "on", "Benchmark", "Data", "Sets", "We", "benchmark", "our", "proposed", "architecture", "on", "the", "WMT", "2014", "English", "-", "to", "-", "German", "and", "English", "-", "to", "-", "French", "tasks", ".", "The", "WMT", "2014", "English", "-", "to", "-", "German", "data", "set", "contains", "M", "sentence", "pairs", ".", "The", "English", "-", "to", "-", "French", "contains", "M", "sentence", "pairs", ".", "Results", "of", "our", "experiments", "are", "summarized", "in", "Table", "[", "reference", "]", ".", "The", "Weighted", "Transformer", "achieves", "a", "BLEU", "score", "improvement", "over", "the", "state", "-", "of", "-", "the", "-", "art", "on", "the", "English", "-", "to", "-", "German", "task", "for", "the", "smaller", "network", "and", "BLEU", "improvement", "for", "the", "larger", "network", ".", "In", "the", "case", "of", "the", "larger", "English", "-", "to", "-", "French", "task", ",", "we", "note", "a", "BLEU", "improvement", "for", "the", "smaller", "model", "and", "a", "improvement", "for", "the", "larger", "model", ".", "Also", ",", "note", "that", "the", "performance", "of", "the", "smaller", "model", "for", "Weighted", "Transformer", "is", "close", "to", "that", "of", "the", "larger", "baseline", "model", ",", "especially", "for", "the", "English", "-", "to", "-", "German", "task", ".", "This", "suggests", "that", "the", "Weighted", "Transformer", "better", "utilizes", "available", "model", "capacity", "since", "it", "needs", "only", "of", "the", "parameters", "as", "the", "baseline", "transformer", "for", "matching", "its", "performance", ".", "Our", "relative", "improvements", "do", "not", "hinge", "on", "using", "the", "BLEU", "scores", "for", "comparison", ";", "experiments", "with", "the", "GLEU", "score", "proposed", "in", "wu2016google", "also", "yielded", "similar", "improvements", ".", "Finally", ",", "we", "comment", "on", "the", "regularizing", "effect", "of", "the", "Weighted", "Transformer", ".", "Given", "the", "improved", "results", ",", "a", "natural", "question", "is", "whether", "the", "results", "stem", "from", "improved", "regularization", "of", "the", "model", ".", "To", "investigate", "this", ",", "we", "report", "the", "testing", "loss", "of", "the", "Weighted", "Transformer", "and", "the", "baseline", "Transformer", "against", "the", "training", "loss", "in", "Figure", "[", "reference", "]", ".", "Models", "which", "have", "a", "regularizing", "effect", "tend", "to", "have", "lower", "testing", "losses", "for", "the", "same", "training", "loss", ".", "We", "see", "this", "effect", "in", "our", "experiments", "suggesting", "that", "the", "proposed", "architecture", "may", "have", "better", "regularizing", "properties", ".", "This", "is", "not", "unexpected", "given", "similar", "outcomes", "for", "other", "branching", "-", "based", "strategies", "such", "as", "Shake", "-", "Shake", "and", "mixture", "-", "of", "-", "experts", ".", "subsection", ":", "Sensitivity", "Analysis", "In", "Table", "[", "reference", "]", ",", "we", "report", "sensitivity", "results", "on", "the", "newstest2013", "English", "-", "to", "-", "German", "task", ".", "Specifically", ",", "we", "vary", "the", "number", "of", "layers", "in", "the", "encoder", "/", "decoder", "and", "compare", "the", "performance", "of", "the", "Weighted", "Transformer", "and", "the", "Transformer", "baseline", ".", "The", "results", "clearly", "demonstrate", "the", "benefit", "of", "the", "branched", "attention", ";", "for", "every", "experiment", ",", "the", "Weighted", "Transformer", "outperforms", "the", "baseline", "transformer", ",", "in", "some", "cases", "by", "up", "to", "BLEU", "points", ".", "As", "in", "the", "case", "of", "the", "baseline", "Transformer", ",", "increasing", "the", "number", "of", "layers", "does", "not", "necessarily", "improve", "performance", ";", "a", "modest", "improvement", "is", "seen", "when", "the", "number", "of", "layers", "is", "increased", "from", "to", "and", "to", "but", "the", "performance", "degrades", "when", "is", "increased", "to", ".", "Increasing", "the", "number", "of", "heads", "from", "to", "in", "configuration", "(", "A", ")", "yielded", "an", "even", "better", "BLEU", "score", ".", "However", ",", "preliminary", "experiments", "with", "and", ",", "like", "in", "the", "case", "with", ",", "degrade", "the", "performance", "of", "the", "model", ".", "In", "Figure", "[", "reference", "]", ",", "we", "present", "the", "behavior", "of", "the", "weights", "for", "the", "second", "encoder", "layer", "of", "the", "configuration", "(", "C", ")", "for", "the", "English", "-", "to", "-", "German", "newstest2013", "task", ".", "The", "figure", "shows", "that", ",", "in", "terms", "of", "relative", "weights", ",", "the", "network", "does", "prioritize", "some", "branches", "more", "than", "others", ";", "circumstantially", "by", "as", "much", "as", ".", "Further", ",", "the", "relative", "ordering", "of", "the", "branches", "changes", "over", "time", "suggesting", "that", "the", "network", "is", "not", "purely", "exploitative", ".", "A", "purely", "exploitative", "network", ",", "which", "would", "learn", "to", "exploit", "a", "subset", "of", "the", "branches", "at", "the", "expense", "of", "the", "rest", ",", "would", "not", "be", "preferred", "since", "it", "would", "effectively", "reduce", "the", "number", "of", "available", "parameters", "and", "limit", "the", "representational", "power", ".", "Similar", "results", "are", "seen", "for", "other", "layers", ",", "including", "the", "decoder", "layers", ";", "we", "omit", "them", "for", "brevity", ".", "subsection", ":", "Randomization", "Baseline", "The", "proposed", "modification", "can", "also", "be", "interpreted", "as", "a", "form", "of", "Shake", "-", "Shake", "regularization", "proposed", "in", ".", "In", "this", "regularization", "strategy", ",", "random", "weights", "are", "sampled", "during", "forward", "and", "backward", "passes", "for", "weighing", "the", "various", "branches", "in", "a", "multi", "-", "branch", "network", ".", "During", "test", "time", ",", "they", "are", "weighed", "equally", ".", "In", "our", "strategy", ",", "the", "weights", "are", "learned", "instead", "of", "being", "sampled", "randomly", ".", "Consequently", ",", "no", "changes", "to", "the", "model", "are", "required", "during", "test", "time", ".", "In", "order", "to", "better", "understand", "whether", "the", "network", "benefits", "from", "the", "learned", "weights", "or", "if", ",", "at", "test", "time", ",", "random", "or", "uniform", "weights", "suffice", ",", "we", "propose", "the", "following", "experiment", ":", "the", "weights", "for", "the", "Weighted", "Transformer", ",", "including", "are", "trained", "as", "before", ",", "but", ",", "during", "test", "time", ",", "we", "replace", "them", "with", "(", "a", ")", "randomly", "sampled", "weights", ",", "and", "(", "b", ")", "where", "is", "the", "number", "of", "incoming", "branches", ".", "In", "Table", "[", "reference", "]", ",", "we", "report", "experimental", "results", "on", "the", "configuration", "(", "C", ")", "of", "the", "Weighted", "Transformer", "on", "the", "English", "-", "to", "-", "German", "newstest2013", "data", "set", "(", "see", "Table", "[", "reference", "]", "for", "details", "regarding", "the", "configuration", ")", ".", "It", "is", "evident", "that", "random", "or", "uniform", "weights", "can", "not", "replace", "the", "learned", "weights", "during", "test", "time", ".", "Preliminary", "experiments", "suggest", "that", "a", "Shake", "-", "Shake", "-", "like", "strategy", "where", "the", "weights", "are", "sampled", "randomly", "during", "training", "also", "leads", "to", "inferior", "performance", ".", "subsection", ":", "Gating", "In", "order", "to", "analyze", "whether", "a", "hard", "(", "discrete", ")", "choice", "through", "gating", "will", "outperform", "our", "normalization", "strategy", ",", "we", "experimented", "with", "using", "gates", "instead", "of", "the", "proposed", "concatenation", "-", "addition", "strategy", ".", "Specifically", ",", "we", "replaced", "the", "summation", "in", "Equation", "(", "[", "reference", "]", ")", "by", "a", "gating", "structure", "that", "sums", "up", "the", "contributions", "of", "the", "top", "branches", "with", "the", "highest", "probabilities", ".", "This", "is", "similar", "to", "the", "sparsely", "-", "gated", "mixture", "of", "experts", "model", "in", "shazeer2017outrageously", ".", "Despite", "significant", "hyper", "-", "parameter", "tuning", "of", "and", ",", "we", "found", "that", "this", "strategy", "performs", "worse", "than", "our", "proposed", "mechanism", "by", "a", "large", "margin", ".", "We", "hypothesize", "that", "this", "is", "due", "to", "the", "fact", "that", "the", "number", "of", "branches", "is", "low", ",", "typically", "less", "than", "16", ".", "Hence", ",", "sparsely", "-", "gated", "models", "lose", "representational", "power", "due", "to", "reduced", "capacity", "in", "the", "model", ".", "We", "plan", "to", "investigate", "the", "setup", "with", "a", "large", "number", "of", "branches", "and", "sparse", "gates", "in", "future", "work", ".", "section", ":", "Conclusions", "We", "present", "the", "Weighted", "Transformer", "that", "trains", "faster", "and", "achieves", "better", "performance", "than", "the", "original", "Transformer", "network", ".", "The", "proposed", "architecture", "replaces", "the", "multi", "-", "head", "attention", "in", "the", "Transformer", "network", "by", "a", "multiple", "self", "-", "attention", "branches", "whose", "contributions", "are", "learned", "as", "a", "part", "of", "the", "training", "process", ".", "We", "report", "numerical", "results", "on", "the", "WMT", "2014", "English", "-", "to", "-", "German", "and", "English", "-", "to", "-", "French", "tasks", "and", "show", "that", "the", "Weighted", "Transformer", "improves", "the", "state", "-", "of", "-", "the", "-", "art", "BLEU", "scores", "by", "and", "points", "respectively", ".", "Further", ",", "our", "proposed", "architecture", "trains", "faster", "than", "the", "baseline", "Transformer", ".", "Finally", ",", "we", "present", "evidence", "suggesting", "the", "regularizing", "effect", "of", "the", "proposal", "and", "emphasize", "that", "the", "relative", "improvement", "in", "BLEU", "score", "is", "observed", "across", "various", "hyper", "-", "parameter", "settings", "for", "both", "small", "and", "large", "models", ".", "bibliography", ":", "References"]}