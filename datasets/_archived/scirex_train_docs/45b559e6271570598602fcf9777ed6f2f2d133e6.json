{"coref": {"10_conv__4_FC_layers": [], "Deep_CNN": [[3, 8], [84, 88], [1042, 1045], [2737, 2741]], "Deep_CNN__10_conv__4_FC_layers___multi-scale_feature_maps": [], "Percentage_error": [[214, 217], [1978, 1981], [2354, 2357]], "Speech_Recognition": [[9, 10], [30, 35], [36, 37], [44, 45], [76, 77], [159, 163], [281, 283], [340, 342], [374, 376], [480, 481], [624, 625], [664, 665], [682, 683], [858, 861], [870, 871], [2731, 2732], [2784, 2788], [1057, 1058]], "Switchboard___Hub500": [[198, 200], [206, 211], [452, 454], [710, 714], [726, 727], [2342, 2350], [2851, 2852], [2358, 2359]], "__multi-scale_feature_maps": []}, "coref_non_salient": {"0": [[2537, 2539], [2929, 2931]], "1": [[1083, 1085], [1637, 1639]], "10": [[326, 328], [799, 802]], "11": [[788, 791], [1299, 1302]], "12": [[872, 876], [1665, 1669]], "13": [[792, 794], [880, 882]], "14": [[2947, 2952], [2970, 2971], [2976, 2977], [2983, 2984]], "15": [[221, 225], [2132, 2136], [2614, 2618], [2685, 2689], [2907, 2911]], "16": [[14, 15], [42, 43], [74, 75], [179, 180], [240, 241], [293, 294], [430, 431], [435, 436], [621, 622], [818, 819], [2216, 2217], [2421, 2422], [2729, 2730], [2905, 2906], [124, 125], [195, 196], [356, 357], [365, 366], [477, 478], [680, 681], [759, 760], [977, 978], [985, 986], [1080, 1081], [1495, 1496], [1708, 1709], [2154, 2155], [2316, 2317], [2322, 2323], [2336, 2337], [2630, 2631], [2644, 2645], [2670, 2671], [2712, 2713], [2776, 2777]], "17": [[114, 118], [513, 515], [535, 537], [660, 663], [1052, 1055], [2758, 2762]], "18": [[541, 543], [1608, 1609]], "19": [[10, 13], [273, 275], [289, 292], [1038, 1040], [1291, 1293]], "2": [[1090, 1092], [1539, 1541]], "20": [[1750, 1753], [1945, 1946], [2158, 2160]], "21": [[98, 100], [484, 486], [609, 610], [629, 631], [1066, 1068], [1094, 1095], [1122, 1124], [1336, 1337], [1375, 1377], [1406, 1408], [1612, 1614], [1642, 1644], [2651, 2653], [2748, 2750]], "22": [[360, 361], [369, 370]], "23": [[1168, 1169]], "24": [[2512, 2514]], "25": [[332, 333], [862, 864]], "26": [[1877, 1879], [2920, 2922]], "27": [[56, 59], [594, 596]], "28": [[1730, 1731], [1804, 1805], [2474, 2475], [2524, 2525], [2548, 2549], [2596, 2597]], "29": [[169, 170], [229, 230], [716, 717], [2213, 2214], [2228, 2232], [2247, 2249], [2573, 2574], [2601, 2602], [2683, 2684], [2814, 2815], [2833, 2834], [2867, 2868], [2876, 2877], [2891, 2892]], "3": [[424, 426], [778, 783]], "30": [[174, 176], [2818, 2821]], "31": [[71, 73], [2726, 2728]], "32": [[1544, 1549], [1554, 1559], [1578, 1583]], "33": [[646, 649]], "34": [[1249, 1251]], "35": [[102, 104], [367, 368], [1148, 1150], [2752, 2754]], "36": [[146, 148], [839, 841], [1567, 1569], [1651, 1653], [1690, 1692], [2441, 2443], [2804, 2806]], "37": [[2858, 2862]], "38": [[1218, 1223]], "39": [[2325, 2327]], "4": [[815, 817], [1949, 1955]], "40": [[1235, 1237], [1239, 1241]], "41": [[2195, 2198]], "42": [[1086, 1087], [1170, 1171]], "43": [[904, 905], [1698, 1699], [1711, 1712], [1798, 1799], [2137, 2139], [2472, 2473], [2491, 2492]], "44": [[1991, 1995]], "45": [[417, 418], [2010, 2012]], "46": [[527, 530]], "47": [[284, 286]], "48": [[488, 490]], "49": [[917, 924]], "5": [[888, 889], [1030, 1034], [2384, 2386], [2496, 2497], [2917, 2919]], "50": [[1997, 1999]], "51": [[337, 339]], "52": [[2397, 2399], [2368, 2370]], "53": [[2932, 2934]], "54": [[1491, 1492]], "55": [[1808, 1810]], "56": [[2000, 2003]], "57": [[1723, 1725], [2483, 2484]], "58": [[2067, 2068]], "59": [[865, 868]], "6": [[409, 414], [2165, 2171]], "60": [[1968, 1971]], "61": [[276, 280]], "62": [[323, 325], [523, 524]], "63": [[466, 468]], "64": [[884, 886]], "65": [[1106, 1113]], "66": [[1770, 1773]], "67": [[304, 308]], "68": [[329, 331]], "69": [[732, 737]], "7": [[1115, 1120], [1394, 1397], [1419, 1422], [1472, 1475]], "70": [[551, 553]], "71": [[545, 550]], "72": [[906, 907], [1700, 1701], [2470, 2471], [2493, 2494]], "73": [[393, 394]], "74": [[385, 392]], "75": [[1820, 1822]], "76": [[2500, 2502]], "77": [[343, 348]], "78": [[901, 903]], "79": [[319, 322]], "8": [[2064, 2065], [2072, 2073]], "80": [[1477, 1481]], "81": [[2923, 2924]], "82": [[774, 777], [1318, 1322]], "83": [[1201, 1202]], "84": [[379, 382]], "9": [[156, 158], [1310, 1312], [2047, 2049], [2125, 2127], [1734, 1736], [1961, 1963]]}, "doc_id": "45b559e6271570598602fcf9777ed6f2f2d133e6", "method_subrelations": {"Deep_CNN__10_conv__4_FC_layers___multi-scale_feature_maps": [[[0, 8], "Deep_CNN"], [[10, 30], "10_conv__4_FC_layers"], [[31, 57], "__multi-scale_feature_maps"]]}, "n_ary_relations": [{"Material": "Switchboard___Hub500", "Method": "Deep_CNN__10_conv__4_FC_layers___multi-scale_feature_maps", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 12.2}], "ner": [[3, 8, "Method"], [9, 10, "Task"], [10, 13, "Method"], [14, 15, "Method"], [30, 35, "Task"], [36, 37, "Task"], [42, 43, "Method"], [44, 45, "Task"], [56, 59, "Method"], [71, 73, "Method"], [74, 75, "Method"], [76, 77, "Task"], [84, 88, "Method"], [98, 100, "Method"], [102, 104, "Method"], [114, 118, "Method"], [146, 148, "Metric"], [156, 158, "Task"], [159, 163, "Task"], [169, 170, "Metric"], [174, 176, "Method"], [179, 180, "Method"], [198, 200, "Material"], [206, 211, "Material"], [214, 217, "Metric"], [221, 225, "Method"], [229, 230, "Metric"], [240, 241, "Method"], [273, 275, "Method"], [276, 280, "Task"], [281, 283, "Task"], [284, 286, "Task"], [289, 292, "Method"], [293, 294, "Method"], [304, 308, "Task"], [319, 322, "Task"], [323, 325, "Task"], [326, 328, "Task"], [329, 331, "Task"], [332, 333, "Task"], [337, 339, "Method"], [340, 342, "Task"], [343, 348, "Method"], [360, 361, "Method"], [367, 368, "Method"], [369, 370, "Method"], [374, 376, "Task"], [379, 382, "Method"], [385, 392, "Method"], [393, 394, "Method"], [409, 414, "Method"], [417, 418, "Method"], [424, 426, "Method"], [430, 431, "Method"], [435, 436, "Method"], [452, 454, "Material"], [466, 468, "Method"], [480, 481, "Task"], [484, 486, "Method"], [488, 490, "Method"], [513, 515, "Method"], [523, 524, "Task"], [527, 530, "Task"], [535, 537, "Method"], [541, 543, "Method"], [545, 550, "Method"], [551, 553, "Method"], [594, 596, "Method"], [609, 610, "Method"], [621, 622, "Method"], [624, 625, "Task"], [629, 631, "Method"], [646, 649, "Method"], [660, 663, "Method"], [664, 665, "Task"], [682, 683, "Task"], [710, 714, "Material"], [716, 717, "Metric"], [726, 727, "Material"], [732, 737, "Task"], [774, 777, "Method"], [778, 783, "Method"], [788, 791, "Method"], [792, 794, "Method"], [799, 802, "Task"], [815, 817, "Task"], [818, 819, "Method"], [839, 841, "Metric"], [858, 861, "Task"], [862, 864, "Task"], [865, 868, "Task"], [870, 871, "Task"], [872, 876, "Method"], [880, 882, "Method"], [884, 886, "Method"], [888, 889, "Task"], [901, 903, "Method"], [904, 905, "Method"], [906, 907, "Method"], [917, 924, "Method"], [1030, 1034, "Task"], [1042, 1045, "Method"], [1052, 1055, "Method"], [1066, 1068, "Method"], [1083, 1085, "Method"], [1086, 1087, "Method"], [1090, 1092, "Method"], [1094, 1095, "Method"], [1106, 1113, "Method"], [1115, 1120, "Method"], [1122, 1124, "Method"], [1148, 1150, "Method"], [1168, 1169, "Method"], [1170, 1171, "Method"], [1201, 1202, "Method"], [1218, 1223, "Method"], [1235, 1237, "Method"], [1239, 1241, "Method"], [1249, 1251, "Method"], [1299, 1302, "Method"], [1310, 1312, "Task"], [1318, 1322, "Method"], [1336, 1337, "Method"], [1375, 1377, "Method"], [1394, 1397, "Method"], [1406, 1408, "Method"], [1419, 1422, "Method"], [1472, 1475, "Method"], [1477, 1481, "Method"], [1491, 1492, "Method"], [1539, 1541, "Method"], [1544, 1549, "Task"], [1554, 1559, "Task"], [1567, 1569, "Metric"], [1578, 1583, "Task"], [1608, 1609, "Method"], [1612, 1614, "Method"], [1637, 1639, "Method"], [1642, 1644, "Method"], [1651, 1653, "Metric"], [1665, 1669, "Method"], [1690, 1692, "Metric"], [1698, 1699, "Method"], [1700, 1701, "Method"], [1711, 1712, "Method"], [1723, 1725, "Task"], [1730, 1731, "Method"], [1750, 1753, "Material"], [1770, 1773, "Method"], [1798, 1799, "Method"], [1804, 1805, "Method"], [1808, 1810, "Metric"], [1820, 1822, "Task"], [1877, 1879, "Method"], [1945, 1946, "Material"], [1949, 1955, "Task"], [1968, 1971, "Method"], [1978, 1981, "Metric"], [1991, 1995, "Task"], [1997, 1999, "Task"], [2000, 2003, "Method"], [2010, 2012, "Method"], [2047, 2049, "Task"], [2064, 2065, "Material"], [2067, 2068, "Material"], [2072, 2073, "Material"], [2125, 2127, "Task"], [2132, 2136, "Method"], [2137, 2139, "Method"], [2158, 2160, "Material"], [2165, 2171, "Method"], [2195, 2198, "Method"], [2213, 2214, "Metric"], [2216, 2217, "Method"], [2228, 2232, "Metric"], [2247, 2249, "Metric"], [2325, 2327, "Method"], [2342, 2350, "Material"], [2354, 2357, "Metric"], [2384, 2386, "Task"], [2397, 2399, "Task"], [2421, 2422, "Method"], [2441, 2443, "Metric"], [2470, 2471, "Method"], [2472, 2473, "Method"], [2474, 2475, "Method"], [2483, 2484, "Task"], [2491, 2492, "Method"], [2493, 2494, "Method"], [2496, 2497, "Task"], [2500, 2502, "Method"], [2512, 2514, "Metric"], [2524, 2525, "Method"], [2537, 2539, "Method"], [2548, 2549, "Method"], [2573, 2574, "Metric"], [2596, 2597, "Method"], [2601, 2602, "Metric"], [2614, 2618, "Method"], [2651, 2653, "Method"], [2683, 2684, "Metric"], [2685, 2689, "Method"], [2726, 2728, "Method"], [2729, 2730, "Method"], [2731, 2732, "Task"], [2737, 2741, "Method"], [2748, 2750, "Method"], [2752, 2754, "Method"], [2758, 2762, "Method"], [2784, 2788, "Task"], [2804, 2806, "Metric"], [2814, 2815, "Metric"], [2818, 2821, "Method"], [2833, 2834, "Metric"], [2851, 2852, "Material"], [2858, 2862, "Material"], [2867, 2868, "Metric"], [2876, 2877, "Metric"], [2891, 2892, "Metric"], [2905, 2906, "Method"], [2907, 2911, "Method"], [2917, 2919, "Task"], [2920, 2922, "Method"], [2923, 2924, "Method"], [2929, 2931, "Method"], [2932, 2934, "Method"], [2947, 2952, "Method"], [2970, 2971, "Method"], [2976, 2977, "Method"], [2983, 2984, "Method"], [124, 125, "Method"], [195, 196, "Method"], [356, 357, "Method"], [365, 366, "Method"], [477, 478, "Method"], [680, 681, "Method"], [759, 760, "Method"], [977, 978, "Method"], [985, 986, "Method"], [1038, 1040, "Method"], [1057, 1058, "Task"], [1080, 1081, "Method"], [1291, 1293, "Method"], [1495, 1496, "Method"], [1708, 1709, "Method"], [1734, 1736, "Task"], [1961, 1963, "Task"], [2154, 2155, "Method"], [2316, 2317, "Method"], [2322, 2323, "Method"], [2336, 2337, "Method"], [2358, 2359, "Material"], [2368, 2370, "Task"], [2630, 2631, "Method"], [2644, 2645, "Method"], [2670, 2671, "Method"], [2712, 2713, "Method"], [2776, 2777, "Method"]], "sections": [[0, 286], [286, 1028], [1028, 1034], [1034, 1288], [1288, 1542], [1542, 1693], [1693, 1932], [1932, 1936], [1936, 2328], [2328, 2715], [2715, 2935], [2935, 3129], [3129, 3132]], "sentences": [[0, 10], [10, 40], [40, 63], [63, 78], [78, 95], [95, 119], [119, 130], [130, 149], [149, 189], [189, 245], [245, 272], [272, 286], [286, 289], [289, 334], [334, 362], [362, 383], [383, 419], [419, 435], [435, 455], [455, 487], [487, 507], [507, 531], [531, 589], [589, 616], [616, 650], [650, 666], [666, 684], [684, 728], [728, 754], [754, 770], [770, 803], [803, 820], [820, 842], [842, 869], [869, 887], [887, 914], [914, 951], [951, 961], [961, 1009], [1009, 1028], [1028, 1034], [1034, 1040], [1040, 1070], [1070, 1082], [1082, 1100], [1100, 1121], [1121, 1147], [1147, 1166], [1166, 1226], [1226, 1242], [1242, 1259], [1259, 1288], [1288, 1293], [1293, 1313], [1313, 1338], [1338, 1369], [1369, 1409], [1409, 1425], [1425, 1459], [1459, 1493], [1493, 1522], [1522, 1542], [1542, 1549], [1549, 1570], [1570, 1608], [1608, 1631], [1631, 1660], [1660, 1674], [1674, 1693], [1693, 1696], [1696, 1710], [1710, 1717], [1717, 1763], [1763, 1784], [1784, 1794], [1794, 1811], [1811, 1830], [1830, 1854], [1854, 1867], [1867, 1886], [1886, 1908], [1908, 1932], [1932, 1936], [1936, 1939], [1939, 1959], [1959, 1976], [1976, 2013], [2013, 2033], [2033, 2083], [2083, 2110], [2110, 2128], [2128, 2149], [2149, 2181], [2181, 2191], [2191, 2209], [2209, 2246], [2246, 2259], [2259, 2296], [2296, 2328], [2328, 2332], [2332, 2367], [2367, 2380], [2380, 2423], [2423, 2453], [2453, 2464], [2464, 2503], [2503, 2515], [2515, 2537], [2537, 2550], [2550, 2562], [2562, 2594], [2594, 2609], [2609, 2632], [2632, 2654], [2654, 2700], [2700, 2715], [2715, 2718], [2718, 2733], [2733, 2763], [2763, 2772], [2772, 2789], [2789, 2807], [2807, 2846], [2846, 2912], [2912, 2935], [2935, 2938], [2938, 2989], [2989, 3026], [3026, 3045], [3045, 3047], [3047, 3089], [3089, 3098], [3098, 3129], [3129, 3132]], "words": ["document", ":", "Very", "Deep", "Multilingual", "Convolutional", "Neural", "Networks", "for", "LVCSR", "Convolutional", "neural", "networks", "(", "CNNs", ")", "are", "a", "standard", "component", "of", "many", "current", "state", "-", "of", "-", "the", "-", "art", "Large", "Vocabulary", "Continuous", "Speech", "Recognition", "(", "LVCSR", ")", "systems", ".", "However", ",", "CNNs", "in", "LVCSR", "have", "not", "kept", "pace", "with", "recent", "advances", "in", "other", "domains", "where", "deeper", "neural", "networks", "provide", "superior", "performance", ".", "In", "this", "paper", "we", "propose", "a", "number", "of", "architectural", "advances", "in", "CNNs", "for", "LVCSR", ".", "First", ",", "we", "introduce", "a", "very", "deep", "convolutional", "network", "architecture", "with", "up", "to", "14", "weight", "layers", ".", "There", "are", "multiple", "convolutional", "layers", "before", "each", "pooling", "layer", ",", "with", "small", "3", "3", "kernels", ",", "inspired", "by", "the", "VGG", "Imagenet", "2014", "architecture", ".", "Then", ",", "we", "introduce", "multilingual", "CNNs", "with", "multiple", "untied", "layers", ".", "Finally", ",", "we", "introduce", "multi", "-", "scale", "input", "features", "aimed", "at", "exploiting", "more", "context", "at", "negligible", "computational", "cost", ".", "We", "evaluate", "the", "improvements", "first", "on", "a", "Babel", "task", "for", "low", "resource", "speech", "recognition", ",", "obtaining", "an", "absolute", "5.77", "%", "WER", "improvement", "over", "the", "baseline", "PLP", "DNN", "by", "training", "our", "CNN", "on", "the", "combined", "data", "of", "six", "different", "languages", ".", "We", "then", "evaluate", "the", "very", "deep", "CNNs", "on", "the", "Hub5\u201900", "benchmark", "(", "using", "the", "262", "hours", "of", "SWB", "-", "1", "training", "data", ")", "achieving", "a", "word", "error", "rate", "of", "11.8", "%", "after", "cross", "-", "entropy", "training", ",", "a", "1.4", "%", "WER", "improvement", "(", "10.6", "%", "relative", ")", "over", "the", "best", "published", "CNN", "result", "so", "far", ".", "TomSercu", "ChristianPuhrsch", "BrianKingsbury", "YannLeCun", "CenterforDataScience", ",", "CourantInstituteofMathematicalSciences", ",", "NewYorkUniversity", "IBMT.J.WatsonResearchCenter", ",", "YorktownHeights", ",", "NY", ",", "10598", ",", "U.S.A.", "{", "tsercu", ",", "bedk", "}", "@us.ibm.com", ",", "1cpuhrsch@nyu.edu", ",", "yann@cs.nyu.edu", "Convolutional", "Networks", ",", "Multilingual", ",", "Acoustic", "Modeling", ",", "Speech", "Recognition", ",", "Neural", "Networks", "section", ":", "INTRODUCTION", "Convolutional", "Neural", "Networks", "(", "CNNs", ")", "have", "recently", "pushed", "the", "state", "of", "the", "art", "on", "large", "-", "scale", "tasks", "in", "many", "domains", "dealing", "with", "natural", "data", ",", "most", "notably", "in", "computer", "vision", "tasks", "like", "image", "classification", ",", "object", "detection", ",", "object", "localization", "and", "segmentation", ".", "Early", "applications", "of", "neural", "nets", "to", "speech", "recognition", "used", "Time", "-", "Delay", "Neural", "Nets", "which", "can", "be", "seen", "as", "simple", "forms", "of", "CNNs", "without", "pooling", "or", "subsampling", ".", "Full", "-", "fledged", "CNNs", "with", "pooling", "and", "subsampling", "were", "soon", "applied", "to", "speech", "recognition", "and", "combined", "with", "dynamic", "time", "warping", ".", "While", "the", "globally", "-", "trained", "combination", "of", "neural", "nets", "and", "HMMs", "for", "speech", "and", "handwriting", "goes", "back", "to", "the", "1990s", ",", "only", "due", "to", "recent", "developments", "HMM", "/", "DNN", "hybrid", "modeling", "became", "dominant", "in", "ASR", ".", "In", "the", "context", "of", "these", "hybrid", "models", ",", "the", "use", "of", "CNNs", "is", "relatively", "recent", ".", "CNNs", "were", "shown", "to", "achieve", "state", "of", "the", "art", "performance", "on", "the", "benchmark", "datasets", "Broadcast", "News", "and", "Switchboard", "300", ".", "However", ",", "in", "contrast", "to", "the", "trend", "in", "other", "domains", "where", "deeper", "architectures", "are", "often", "shown", "to", "gain", "performance", ",", "the", "classical", "CNN", "architecture", "in", "LVCSR", "has", "only", "two", "convolutional", "layers", ".", "Our", "network", "architecture", "(", "Section", "[", "reference", "]", ")", "is", "strongly", "inspired", "by", "the", "work", "of", "Simonyan", "et", "al", ".", "(", "subsequently", "referred", "to", "as", "\u201c", "VGG", "Net", "\u201d", ")", "which", "obtained", "second", "place", "in", "the", "classification", "section", "of", "the", "Imagenet", "2014", "competition", ".", "The", "central", "idea", "of", "VGG", "Net", "is", "to", "replace", "large", "convolutional", "kernels", "by", "a", "stack", "of", "3", "3", "kernels", "with", "ReLU", "nonlinearities", "without", "pooling", "between", "these", "layers", ";", "The", "authors", "argue", "the", "advantage", "of", "this", "is", "twofold", ":", "(", "1", ")", "additional", "nonlinearity", "hence", "more", "expressive", "power", ",", "and", "(", "2", ")", "a", "reduced", "number", "of", "parameters", ".", "Using", "these", "principles", ",", "very", "deep", "networks", "are", "trained", "with", "up", "to", "19", "weight", "layers", "(", "of", "which", "16", "are", "convolutional", "and", "3", "fully", "connected", ")", ".", "By", "contrast", ",", "the", "classical", "CNNs", "deployed", "in", "LVCSR", "have", "typically", "only", "two", "convolutional", "layers", ",", "use", "large", "(", "9", "9", ")", "kernels", "in", "the", "first", "layer", ",", "and", "use", "sigmoid", "activation", "functions", ".", "The", "first", "goal", "of", "this", "work", "is", "to", "adapt", "the", "VGG", "Net", "architecture", "to", "LVCSR", ".", "Most", "closely", "related", "to", "this", "is", ",", "which", "also", "uses", "VGG", "Net", "-", "inspired", "CNNs", "for", "LVCSR", ".", "In", "contrast", "to", "our", "work", ",", "the", "architectures", "investigated", "in", "are", "quite", "different", "and", "the", "paper", "only", "provides", "results", "from", "training", "on", "a", "non", "-", "standard", "Switchboard", "-", "51h", "dataset", ",", "with", "WER", "not", "close", "to", "state", "of", "the", "art", "performance", "on", "Hub5\u201900", ".", "In", "the", "context", "of", "low", "-", "resource", "language", "tasks", ",", "it", "can", "be", "crucial", "to", "leverage", "training", "data", "in", "languages", "other", "than", "the", "target", "language", ".", "Therefore", "we", "trained", "multilingual", "deep", "CNNs", ",", "which", "we", "describe", "in", "Section", "[", "reference", "]", ".", "This", "is", "related", "to", "multilingual", "neural", "networks", "in", "hybrid", "NN", "-", "HMM", "systems", "which", "have", "been", "extended", "to", "multilingual", "bottleneck", "architectures", "for", "tandem", "models", "and", "have", "proven", "valuable", "for", "spoken", "term", "detection", ".", "To", "our", "knowledge", ",", "no", "work", "has", "been", "published", "that", "extends", "the", "multilingual", "setup", "to", "CNNs", ".", "The", "multi", "-", "scale", "features", "described", "in", "Section", "[", "reference", "]", "aim", "at", "exploiting", "more", "context", "at", "very", "low", "computational", "cost", ".", "They", "are", "inspired", "by", "the", "recent", "success", "of", "combining", "information", "at", "multiple", "scales", "in", "tasks", "like", "traffic", "sign", "recognition", ",", "semantic", "segmentation", "and", "depth", "map", "prediction", ".", "In", "LVCSR", "the", "multi", "-", "scale", "idea", "has", "been", "explored", "in", "tandem", "systems", "and", "the", "CLDNN", "architecture", ".", "As", "training", "becomes", "more", "challenging", "with", "increasing", "depth", ",", "we", "used", "two", "recently", "proposed", "optimization", "algorithms", ",", "Adadelta", "and", "Adam", "(", "Section", "[", "reference", "]", ")", ".", "Both", "algorithms", "are", "first", "order", "gradient", "-", "based", "optimization", "methods", ",", "which", "keep", "track", "of", "an", "estimate", "of", "the", "first", "and", "second", "order", "moment", "of", "the", "gradient", "to", "tune", "the", "step", "size", "of", "each", "weight", "separately", ".", "The", "rest", "of", "the", "paper", "is", "organized", "as", "follows", ".", "In", "Section", "[", "reference", "]", "we", "introduce", "the", "novel", "aspects", "of", "our", "work", ":", "very", "deep", "CNN", "architectures", "in", "[", "reference", "]", ",", "multilingual", "CNN", "training", "in", "[", "reference", "]", ",", "multi", "-", "scale", "features", "in", "[", "reference", "]", ",", "and", "training", "details", "in", "[", "reference", "]", ".", "We", "then", "show", "experimental", "results", "on", "Babel", "in", "[", "reference", "]", "and", "on", "Switchboard", "in", "[", "reference", "]", ".", "section", ":", "ARCHITECTURAL", "AND", "TRAINING", "NOVELTIES", "subsection", ":", "Very", "Deep", "Convolutional", "Networks", "The", "very", "deep", "convolutional", "networks", "we", "describe", "here", "are", "adaptations", "of", "the", "VGG", "Net", "architecture", "to", "the", "LVCSR", "domain", ",", "where", "until", "now", "networks", "with", "two", "convolutional", "layers", "dominated", ".", "Table", "[", "reference", "]", "shows", "the", "configurations", "of", "the", "deep", "CNNs", ".", "The", "deepest", "configuration", ",", "WDX", ",", "has", "14", "weight", "layers", ":", "10", "convolutional", "and", "4", "fully", "connected", ".", "As", "in", ",", "we", "omit", "the", "Rectified", "Linear", "Unit", "(", "ReLU", ")", "layers", "following", "every", "convolutional", "and", "fully", "connected", "layer", ".", "The", "convolutional", "layers", "are", "written", "as", "conv", "(", "{input", "feature", "maps}\u2013{output", "feature", "maps", "}", ")", "where", "each", "kernel", "is", "understood", "to", "be", "size", "3", "3", ".", "The", "pooling", "layers", "are", "written", "as", "(", "time", "x", "frequency", ")", "with", "stride", "equal", "to", "the", "pool", "size", ".", "For", "architectures", "VDX", "and", "WDX", ",", "we", "apply", "zero", "padding", "of", "size", "1", "at", "every", "side", "before", "every", "convolution", ",", "while", "for", "architecture", "VC", "(", "X", ")", "and", "VB", "(", "X", ")", "we", "use", "the", "convolutions", "to", "reduce", "the", "size", "of", "the", "feature", "maps", ",", "hence", "only", "in", "the", "higher", "layers", "of", "VC", "(", "X", ")", "padding", "is", "applied", ".", "In", "contrast", "to", ",", "we", "do", "not", "reinitialize", "the", "deeper", "models", "with", "the", "shallower", "models", ".", "Each", "model", "is", "trained", "from", "scratch", "with", "random", "initialization", "from", "a", "uniform", "distribution", "in", "the", "range", ".", "This", "follows", "the", "argument", "of", "to", "initialize", "the", "weights", "such", "that", "the", "variance", "of", "the", "activations", "on", "each", "layer", "does", "not", "explode", "or", "vanish", "during", "the", "forward", "pass", ".", "subsection", ":", "Multilingual", "Convolutional", "Networks", "Figure", "[", "reference", "]", "shows", "a", "multilingual", "VBX", "network", ",", "which", "we", "used", "for", "most", "of", "our", "Babel", "experiments", ".", "It", "is", "similar", "to", "previous", "multilingual", "deep", "neural", "networks", ",", "with", "the", "main", "difference", "that", "the", "shared", "lower", "layers", "of", "the", "network", "are", "convolutional", ".", "A", "second", "difference", "is", "that", "we", "untie", "more", "than", "only", "the", "last", "layer", ",", "meaning", "that", "the", "weights", "and", "biases", "of", "multiple", "fully", "connected", "layers", "are", "different", "for", "each", "language", ".", "Since", "the", "output", "dimension", "of", "the", "convolutional", "stages", "is", "typically", "large", "when", "using", "large", "context", "windows", ",", "most", "of", "the", "weights", "are", "in", "the", "first", "fully", "connected", "layer", ",", "which", "acts", "on", "the", "flattened", "output", "of", "the", "convolutional", "stages", ".", "This", "is", "an", "argument", "to", "share", "this", "large", ",", "first", "fully", "connected", "layer", "across", "languages", ".", "We", "experimentally", "confirmed", "that", "for", "all", "architectures", ",", "untying", "all", "fully", "connected", "layers", "except", "the", "lowest", "one", "gives", "optimal", "performance", ",", "with", "strong", "degradation", "if", "the", "first", "fully", "connected", "layer", "is", "also", "untied", ".", "This", "untying", "corresponds", "to", "a", "view", "of", "the", "shared", "layers", "and", "the", "first", "fully", "connected", "layer", "as", "a", "shared", "multilingual", "feature", "extractor", ",", "while", "the", "fully", "connected", "layers", "higher", "up", "form", "the", "classifier", ".", "The", "multilingual", "CNN", "is", "trained", "in", "a", "round", "-", "robin", "fashion", ":", "we", "process", "a", "mini", "-", "batch", "for", "each", "language", "before", "making", "an", "update", "to", "the", "weights", ".", "In", "the", "shared", "part", "of", "the", "network", "the", "gradients", "of", "all", "mini", "-", "batches", "are", "accumulated", "between", "weight", "updates", ".", "subsection", ":", "Multi", "-", "scale", "feature", "maps", "The", "main", "goal", "of", "constructing", "multi", "-", "scale", "feature", "maps", "is", "to", "add", "more", "context", "without", "increasing", "the", "computational", "cost", ".", "Figure", "[", "reference", "]", "illustrates", "the", "concept", "of", "multi", "-", "scale", "feature", "maps", ",", "where", "additional", "input", "feature", "maps", "contain", "a", "larger", "view", "of", "the", "context", "of", "the", "frame", "by", "downsampling", "larger", "context", "windows", "with", "different", "strides", ".", "Kernels", "on", "the", "first", "convolutional", "layer", "are", "able", "to", "combine", "information", "from", "multiple", "scales", ",", "i.e.", "different", "distances", "from", "the", "central", "frame", ".", "Because", "the", "only", "difference", "for", "the", "convnet", "configuration", "is", "the", "first", "convolutional", "layer", "having", "more", "feature", "maps", ",", "the", "additional", "computational", "cost", "and", "number", "of", "parameters", "is", "small", ".", "We", "found", "this", "style", "of", "multi", "-", "scale", "training", "to", "give", "small", "gains", ".", "Increasing", "the", "context", "size", "had", "a", "stronger", "positive", "impact", ",", "though", "at", "the", "expense", "of", "increased", "computational", "cost", ".", "subsection", ":", "Training", "We", "use", "Adadelta", "and", "Adam", "to", "do", "initial", "training", "of", "the", "deep", "CNNs", ".", "Using", "Adadelta", "has", "two", "main", "advantages", ".", "Firstly", ",", "in", "our", "experience", "the", "optimization", "problem", "converges", "much", "faster", "than", "with", "SGD", ";", "for", "the", "Babel", "experiments", "we", "typically", "see", "convergence", "after", "about", "40", "million", "frames", "using", "the", "18", "hours", "of", "Babel", "training", "data", "(", "after", "silence", "removal", "about", "5.8", "million", "frames", ")", ".", "Secondly", ",", "the", "optimal", "working", "point", "of", "Adadelta", "\u2019s", "hyperparameters", "and", "was", "stable", "across", "architectures", ",", "always", "giving", "optimal", "performance", ".", "This", "was", "crucial", "in", "order", "to", "explore", "architectural", "variations", ".", "After", "initial", "training", "with", "Adadelta", ",", "we", "fine", "tune", "using", "SGD", "with", "a", "small", "learning", "rate", ".", "Another", "aspect", "of", "training", "that", "improved", "our", "results", "is", "data", "balancing", "(", "something", "similar", "was", "done", "in", ")", ".", "We", "construct", "batches", "on", "the", "fly", "by", "sampling", "target", "with", "probability", ",", "where", "is", "related", "to", "the", "frequency", "of", "context", "dependent", "state", "as", ".", "After", "sampling", ",", "we", "sample", "uniformly", "across", "all", "frames", "with", "that", "target", ".", "The", "exponent", "takes", "values", "between", "balanced", "training", "(", ")", "and", "unbalanced", "training", "using", "the", "natural", "frequencies", "(", ")", ".", "In", "our", "experiments", "on", "Babel", "it", "proved", "optimal", "to", "start", "with", "and", "raise", "it", "during", "training", "to", "its", "final", "value", "of", ".", "In", "our", "experiments", "on", "switchboard", "we", "varied", "typically", "from", "0.4", "to", "0.8", "and", "decoded", "with", "HMM", "priors", "adjusted", "to", "match", "the", "final", "distribution", ".", "section", ":", "EXPERIMENTAL", "RESULTS", "subsection", ":", "Babel", "Our", "first", "set", "of", "experiments", "on", "Babel", "focuses", "on", "the", "multilingual", "and", "multi", "-", "scale", "aspects", "of", "this", "work", ".", "The", "IARPA", "Babel", "program", "is", "aimed", "at", "developing", "robust", "keyword", "search", "technology", "for", "low", "resource", "languages", ".", "Though", "the", "word", "error", "rates", "reported", "here", "are", "too", "high", "to", "be", "useful", "for", "simple", "speech", "to", "text", "applications", ",", "useful", "keyword", "search", "(", "KWS", ")", "systems", "can", "still", "be", "built", "based", "on", "these", "ASR", "models", ".", "As", "training", "data", "we", "use", "a", "combination", "of", "6", "languages", ",", "with", "3", "hours", "of", "training", "data", "per", "language", ".", "The", "languages", "used", "for", "training", "are", "languages", "from", "the", "second", "Option", "Period", "of", "the", "Babel", "program", ",", "i.e.", "Kurmanji", "(", "KUR", ")", ",", "Tok", "Pisin", "(", "TOK", ")", ",", "Cebuano", "(", "CEB", ")", ",", "Kazakh", "(", "KAZ", ")", ",", "Telugu", "(", "TEL", ")", ",", "and", "Lithuanian", "(", "LIT", ")", ".", "The", "features", "used", "in", "these", "experiments", "are", "standard", "log", "-", "Mel", "features", ",", "standardized", "with", "a", "global", "mean", "and", "variance", "shared", "across", "the", "speakers", "and", "langauges", ".", "Unless", "explicitly", "mentioned", ",", "we", "use", "multi", "-", "scale", "features", "with", "context", "20", "in", "the", "Babel", "experiments", ".", "We", "report", "results", "after", "cross", "-", "entropy", "training", "with", "adadelta", "(", ",", ")", ",", "and", "varying", "from", "0", "to", "1", ".", "We", "trained", "the", "multilingual", "deep", "CNN", "architecture", "on", "6", "Babel", "languages", "using", "alignments", "from", "6", "baseline", "speaker", "independent", "HMM", "/", "DNN", "systems", "using", "PLP", "features", ",", "with", "1000", "context", "dependent", "states", ".", "The", "context", "dependent", "states", "are", "specific", "to", "each", "language", ".", "Each", "baseline", "system", "is", "cross", "-", "entropy", "trained", "on", "a", "single", "language", "with", "3", "hours", "of", "data", ".", "We", "will", "report", "the", "WER", "of", "the", "CNNs", "compared", "to", "the", "baseline", "DNN", ",", "and", "summarize", "this", "in", "the", "average", "absolute", "WER", "improvement", "over", "the", "baseline", "DNN", ",", "which", "gives", "one", "number", "to", "compare", "different", "models", ".", "The", "WER", "improvements", "over", "the", "baseline", "DNN", "are", "fairly", "consistent", "across", "languages", ".", "Tables", "[", "reference", "]", "through", "[", "reference", "]", "show", "the", "results", "outlining", "the", "performance", "gains", "from", "the", "different", "architectural", "improvements", "discussed", "in", "Section", "[", "reference", "]", ",", "[", "reference", "]", ",", "and", "[", "reference", "]", "respectively", ".", "From", "table", "[", "reference", "]", "note", "that", "even", "in", "the", "monolingual", "case", "(", "3", "hours", "of", "data", ")", "the", "VBX", "CNN", "architecture", "outperforms", "both", "the", "classical", "CNN", "and", "the", "baseline", "DNN", ".", "subsection", ":", "Switchboard", "300", "We", "evaluate", "our", "deep", "CNN", "architecture", "by", "training", "on", "the", "262", "-", "hour", "SWB", "-", "1", "training", "data", ",", "and", "report", "the", "Word", "Error", "Rates", "on", "Hub5\u201900", "SWB", "(", "table", "[", "reference", "]", ")", ".", "The", "Switchboard", "experiments", "focus", "on", "the", "very", "deep", "aspect", "of", "our", "work", ".", "Apart", "from", "not", "involving", "multilingual", "training", ",", "we", "did", "not", "use", "multi", "-", "scale", "features", "in", "the", "Switchboard", "experiments", ",", "but", "did", "use", "speaker", "-", "dependent", "VTLN", "and", "deltas", "and", "double", "deltas", "as", "this", "is", "shown", "to", "help", "performance", "for", "classical", "CNNs", ".", "In", "the", "switchboard", "experiments", ",", "using", "a", "large", "context", "only", "gave", "marginal", "gains", "which", "were", "not", "worth", "the", "computational", "cost", ",", "so", "we", "worked", "with", "context", "windows", "of", "8", ".", "We", "use", "a", "data", "balancing", "value", "of", ",", "chosen", "from", ".", "After", "training", "with", "multiple", "combinations", "of", "Adam", ",", "Adadelta", "and", "SGD", ",", "we", "settled", "on", "two", "possible", "strategies", "for", "optimization", ":", "the", "first", "strategy", "is", "to", "use", "Adadelta", "or", "Adam", "for", "initial", "training", ",", "followed", "by", "SGD", "finetuning", ".", "This", "way", "one", "can", "typically", "achieve", "good", "performance", "in", "minimal", "time", ".", "The", "second", "strategy", ",", "training", "from", "scratch", "using", "only", "SGD", ",", "requires", "more", "training", ",", "however", "the", "performance", "is", "slightly", "superior", ".", "Classical", "momentum", "yielded", "no", "gains", "and", "sometimes", "slight", "degradation", "over", "plain", "SGD", ".", "We", "provide", "the", "results", "and", "total", "number", "of", "frames", "until", "convergence", ".", "Note", "that", "with", "the", "first", "strategy", ",", "we", "achieve", "12.2", "%", "WER", "after", "140", "M", "frames", ",", "i.e.", "only", "1.5", "passes", "through", "the", "dataset", "(", "which", "has", "92.1", "M", "frames", ")", ".", "Using", "just", "SGD", "we", "achieve", "11.8", "%", "WER", "in", "3.5", "passes", "through", "the", "data", ".", "We", "only", "present", "results", "after", "cross", "-", "entropy", "training", ",", "so", "we", "compare", "against", "the", "best", "published", "cross", "-", "entropy", "trained", "CNNs", ".", "The", "baseline", "is", "the", "work", "of", "Soltau", "et", "al", ".", "using", "classical", "CNNs", "with", "512", "feature", "maps", "on", "both", "convolutional", "layers", ".", "A", "second", "baseline", "is", "the", "work", "of", "Saon", "et", "al", ".", "which", "introduces", "annealed", "dropout", "maxout", "CNN", "\u2019s", "with", "a", "large", "number", "of", "HMM", "states", ",", "achieving", "12.6", "%", "WER", "after", "cross", "-", "entropy", "training", "(", "not", "in", "the", "paper", ",", "from", "personal", "communication", ")", ".", "Note", "that", "these", "improvements", "could", "readily", "be", "integrated", "with", "our", "very", "deep", "CNN", "architectures", ".", "section", ":", "DISCUSSION", "In", "this", "paper", "we", "proposed", "a", "number", "of", "architectural", "advances", "in", "CNNs", "for", "LVCSR", ".", "We", "introduced", "a", "very", "deep", "convolutional", "network", "architecture", "with", "small", "3", "3", "kernels", "and", "multiple", "convolutional", "layers", "before", "each", "pooling", "layer", ",", "inspired", "by", "the", "VGG", "Imagenet", "2014", "architecture", ".", "Our", "best", "performing", "model", "has", "14", "weight", "layers", ".", "We", "also", "introduced", "multilingual", "CNNs", "which", "proved", "valuable", "in", "the", "context", "of", "low", "resource", "speech", "recognition", ".", "We", "introduced", "multi", "-", "scale", "input", "features", "aimed", "at", "exploiting", "more", "acoustic", "context", "with", "minimal", "computational", "increase", ".", "We", "showed", "an", "improvement", "of", "2.50", "%", "WER", "over", "a", "standard", "DNN", "PLP", "baseline", "using", "3", "hours", "of", "data", ",", "and", "an", "improvement", "of", "5.77", "%", "WER", "by", "combining", "six", "languages", "to", "train", "on", "18", "hours", "of", "data", ".", "We", "then", "showed", "results", "on", "Hub5\u201900", "after", "training", "on", "262", "hours", "of", "SWB", "-", "1", "data", "where", "we", "get", "11.8", "%", "WER", ",", "which", "is", "an", "improvement", "of", "2.0", "%", "WER", "(", "14.5", "%", "relative", ")", "over", "our", "own", "baseline", ",", "and", "a", "1.4", "%", "WER", "(", "10.6", "%", "relative", ")", "improvement", "over", "the", "best", "result", "published", "on", "classical", "CNNs", "after", "cross", "-", "entropy", "training", ".", "We", "expect", "additional", "gains", "from", "sequence", "training", ",", "joint", "training", "with", "DNNs", ",", "and", "integrating", "improvements", "like", "annealed", "dropout", "and", "maxout", "nonlinearities", ".", "section", ":", "ACKNOWLEDGEMENT", "This", "effort", "uses", "the", "very", "limited", "language", "packs", "from", "IARPA", "Babel", "Program", "language", "collections", "IARPA", "-", "babel205b", "-", "v1.0a", ",", "IARPA", "-", "babel207b", "-", "v1.0e", ",", "IARPA", "-", "babel301b", "-", "v2.0b", ",", "IARPA", "-", "babel302b", "-", "v1.0a", ",", "IARPA", "-", "babel303b", "-", "v1.0a", ",", "and", "IARPA", "-", "babel304b", "-", "v1.0b", ".", "This", "work", "is", "supported", "by", "the", "Intelligence", "Advanced", "Research", "Projects", "Activity", "(", "IARPA", ")", "via", "Department", "of", "Defense", "U.S.", "Army", "Research", "Laboratory", "(", "DoD", "/", "ARL", ")", "contract", "number", "W911NF", "-", "12", "-", "C", "-", "0012", ".", "The", "U.S.", "Government", "is", "authorized", "to", "reproduce", "and", "distribute", "reprints", "for", "Governmental", "purposes", "notwithstanding", "any", "copyright", "annotation", "thereon", ".", "Disclaimer", ":", "The", "views", "and", "conclusions", "contained", "herein", "are", "those", "of", "the", "authors", "and", "should", "not", "be", "interpreted", "as", "necessarily", "representing", "the", "official", "policies", "or", "endorsements", ",", "either", "expressed", "or", "implied", ",", "of", "IARPA", ",", "DoD", "/", "ARL", ",", "or", "the", "U.S.", "Government", ".", "We", "gratefully", "acknowledge", "the", "support", "of", "NVIDIA", "Corporation", ".", "The", "authors", "would", "like", "to", "thank", "Pierre", "Sermanet", "for", "the", "initial", "code", "base", ",", "George", "Saon", ",", "Vaibhava", "Goel", ",", "Etienne", "Marcheret", "and", "Xiaodong", "Cui", "for", "valuable", "discussions", "and", "comments", ".", "bibliography", ":", "References"]}