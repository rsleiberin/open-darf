{"coref": {"Accented_Speech_Recognition": [[1733, 1734]], "Bi-RNN": [[1275, 1280], [4958, 4960]], "CHiME_clean": [], "CHiME_real": [], "CNN": [[24, 27], [947, 950]], "CNN___Bi-RNN___CTC__speech_to_letters_": [], "CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB": [], "CTC": [[4898, 4901], [4903, 4904], [1402, 1403], [4931, 4932]], "Deep_Speech": [[2, 4], [169, 171], [193, 195], [255, 257], [357, 359], [876, 878], [1018, 1020], [4188, 4190], [3976, 3978], [3999, 4001], [4118, 4120], [4588, 4590]], "Deep_Speech___FSH": [[4143, 4147]], "FSH": [[4143, 4147]], "Noisy_Speech_Recognition": [[381, 385]], "Percentage_error": [[186, 187], [372, 373], [916, 919], [930, 931], [3603, 3606], [3607, 3608], [3689, 3692], [4128, 4130], [4151, 4152], [4760, 4761], [4764, 4765], [4790, 4791], [4798, 4799], [880, 881], [1407, 1408]], "Speech_Recognition": [[7, 14], [282, 286], [482, 484], [5202, 5204], [5327, 5328]], "Switchboard___Hub500": [[180, 182], [3615, 3620], [3637, 3638], [3712, 3713], [3714, 3715], [3732, 3733], [3751, 3752], [3801, 3802], [3892, 3893], [3951, 3952], [4098, 4101], [5244, 5245], [365, 366], [366, 367], [888, 889], [889, 890], [3652, 3653], [3703, 3704], [3744, 3745], [3884, 3885]], "VoxForge_American-Canadian": [], "VoxForge_Commonwealth": [], "VoxForge_European": [], "VoxForge_Indian": [], "__25_9__WER_if_trainedonlyon_SWB": [], "speech_to_letters": [], "swb_hub_500_WER_fullSWBCH": []}, "coref_non_salient": {"0": [[301, 304], [515, 518], [957, 961], [1050, 1053]], "1": [[1220, 1224], [1333, 1338]], "10": [[138, 139], [309, 310], [2266, 2267], [2329, 2330], [2379, 2380], [2391, 2392], [2395, 2396], [2573, 2574], [2787, 2788], [2836, 2837], [5035, 5036], [5054, 5055], [5120, 5123]], "100": [[681, 687]], "101": [[3119, 3123]], "102": [[667, 669], [5111, 5113]], "103": [[232, 237], [393, 395], [5447, 5449]], "104": [[1350, 1352]], "105": [[4639, 4642]], "106": [[3293, 3295]], "107": [[4785, 4787]], "108": [[3819, 3824], [4448, 4453]], "109": [[4066, 4070]], "11": [[2297, 2301], [2447, 2449]], "110": [[3830, 3832]], "111": [[1666, 1668]], "112": [[978, 980]], "113": [[405, 408]], "114": [[4166, 4167]], "115": [[2944, 2950]], "116": [[3583, 3584]], "117": [[3408, 3411]], "118": [[3587, 3591]], "119": [[4612, 4614]], "12": [[245, 252], [440, 442], [852, 859]], "120": [[1464, 1469]], "121": [[52, 54]], "122": [[4772, 4775]], "123": [[1452, 1455]], "124": [[989, 991]], "125": [[2171, 2176]], "126": [[2937, 2938]], "127": [[527, 530]], "128": [[791, 792]], "129": [[2494, 2500]], "13": [[1063, 1066], [2967, 2969]], "130": [[1519, 1526]], "131": [[329, 331]], "132": [[643, 644]], "133": [[335, 337]], "134": [[701, 705]], "135": [[1251, 1254]], "136": [[1700, 1702]], "137": [[77, 81]], "138": [[3566, 3570]], "139": [[1567, 1569]], "14": [[992, 994], [5302, 5304]], "15": [[1636, 1638], [3268, 3270], [4943, 4945], [4952, 4954], [5282, 5284]], "16": [[4074, 4075], [4059, 4060], [4080, 4081]], "17": [[344, 345], [565, 566], [830, 831]], "18": [[29, 36], [260, 262], [457, 459], [507, 514]], "19": [[44, 46], [213, 215], [220, 223], [387, 389], [638, 640], [3416, 3419], [4594, 4596], [4879, 4881]], "2": [[3934, 3939], [4525, 4530]], "20": [[3929, 3930], [4532, 4533]], "21": [[579, 586], [5021, 5023]], "22": [[962, 964], [4937, 4939]], "23": [[4320, 4321], [4332, 4333]], "24": [[2246, 2248], [2424, 2426]], "25": [[1293, 1295], [1553, 1557]], "26": [[1412, 1413], [1470, 1471], [1662, 1663], [2160, 2162], [2525, 2526], [2580, 2581], [4512, 4513]], "27": [[5092, 5093], [1599, 1600], [2190, 2191]], "28": [[5344, 5350]], "29": [[3719, 3720], [4003, 4004]], "3": [[2929, 2931], [5090, 5091]], "30": [[1697, 1699], [5176, 5178], [5315, 5317]], "31": [[5229, 5233]], "32": [[5426, 5431]], "33": [[3875, 3877], [4205, 4207]], "34": [[4175, 4180]], "35": [[332, 334], [3866, 3868]], "36": [[4831, 4833]], "37": [[5013, 5015], [5051, 5053]], "38": [[2222, 2226], [5384, 5388]], "39": [[1504, 1506], [2303, 2305], [2742, 2744], [2856, 2858], [2878, 2880]], "4": [[1928, 1933], [1961, 1966], [2096, 2100]], "40": [[5390, 5392], [3484, 3486]], "41": [[1614, 1615]], "42": [[226, 227], [467, 469], [866, 867]], "43": [[2553, 2555]], "44": [[3964, 3967]], "45": [[5198, 5200]], "46": [[3985, 3987], [4014, 4016]], "47": [[401, 403], [447, 449]], "48": [[5081, 5084]], "49": [[4605, 4608]], "5": [[873, 875], [4837, 4839]], "50": [[2659, 2661], [2663, 2665]], "51": [[2912, 2914], [5079, 5080]], "52": [[5102, 5104]], "53": [[3717, 3718], [3803, 3805], [3949, 3950], [3764, 3765], [4156, 4157], [5257, 5258]], "54": [[3826, 3828], [4463, 4465]], "55": [[435, 438], [4882, 4885]], "56": [[5042, 5046]], "57": [[4475, 4476]], "58": [[4849, 4852]], "59": [[4712, 4715]], "6": [[147, 150], [3036, 3038]], "60": [[270, 272], [1796, 1798], [2022, 2024], [2027, 2029], [3576, 3578], [4537, 4539]], "61": [[4767, 4770]], "62": [[3023, 3024]], "63": [[4496, 4499]], "64": [[4313, 4319]], "65": [[4908, 4910]], "66": [[5246, 5248]], "67": [[2559, 2562]], "68": [[4825, 4829]], "69": [[3852, 3858]], "7": [[3900, 3902], [4412, 4414]], "70": [[409, 410]], "71": [[1680, 1684]], "72": [[4859, 4862]], "73": [[2109, 2112]], "74": [[3816, 3817]], "75": [[718, 721]], "76": [[5019, 5020]], "77": [[786, 789]], "78": [[2520, 2522]], "79": [[3959, 3961]], "8": [[305, 306], [1054, 1055], [1132, 1133], [1784, 1785], [1836, 1837], [1850, 1851], [1999, 2000], [2072, 2073], [2514, 2515], [2683, 2684], [2860, 2861], [4010, 4011], [4346, 4347], [4725, 4726], [4912, 4913], [132, 133], [729, 730], [773, 774], [1040, 1041], [1154, 1155], [1474, 1475], [1809, 1810], [2018, 2019], [2900, 2901]], "80": [[2852, 2854]], "81": [[1255, 1256], [1591, 1592]], "82": [[1527, 1528], [1534, 1535], [4916, 4917], [4996, 4997]], "83": [[499, 500]], "84": [[635, 637]], "85": [[3597, 3600]], "86": [[3639, 3640]], "87": [[1677, 1678]], "88": [[2153, 2157]], "89": [[3860, 3865]], "9": [[2830, 2832], [2839, 2841]], "90": [[1355, 1357]], "91": [[4946, 4950]], "92": [[2653, 2655]], "93": [[4618, 4620]], "94": [[757, 761]], "95": [[417, 419]], "96": [[1573, 1575]], "97": [[2060, 2063]], "98": [[2873, 2874]], "99": [[4600, 4601]]}, "doc_id": "2329a46590b2036d508097143e65c1b77e571e8c", "method_subrelations": {"CNN___Bi-RNN___CTC__speech_to_letters_": [[[0, 3], "CNN"], [[6, 12], "Bi-RNN"], [[15, 18], "CTC"], [[20, 37], "speech_to_letters"]], "CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB": [[[0, 3], "CNN"], [[6, 12], "Bi-RNN"], [[15, 18], "CTC"], [[20, 37], "speech_to_letters"], [[38, 70], "__25_9__WER_if_trainedonlyon_SWB"]], "Deep_Speech": [[[0, 11], "Deep_Speech"]], "Deep_Speech___FSH": [[[0, 11], "Deep_Speech"], [[14, 17], "FSH"]]}, "n_ary_relations": [{"Material": "CHiME_clean", "Method": "CNN___Bi-RNN___CTC__speech_to_letters_", "Metric": "Percentage_error", "Task": "Noisy_Speech_Recognition", "score": 6.3}, {"Material": "CHiME_real", "Method": "CNN___Bi-RNN___CTC__speech_to_letters_", "Metric": "Percentage_error", "Task": "Noisy_Speech_Recognition", "score": 67.94}, {"Material": "Switchboard___Hub500", "Method": "CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 12.6}, {"Material": "Switchboard___Hub500", "Method": "Deep_Speech", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 20}, {"Material": "Switchboard___Hub500", "Method": "Deep_Speech___FSH", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 12.6}, {"Material": "VoxForge_American-Canadian", "Method": "Deep_Speech", "Metric": "Percentage_error", "Task": "Accented_Speech_Recognition", "score": "15.01"}, {"Material": "VoxForge_Commonwealth", "Method": "Deep_Speech", "Metric": "Percentage_error", "Task": "Accented_Speech_Recognition", "score": "28.46"}, {"Material": "VoxForge_European", "Method": "Deep_Speech", "Metric": "Percentage_error", "Task": "Accented_Speech_Recognition", "score": "31.20"}, {"Material": "VoxForge_Indian", "Method": "Deep_Speech", "Metric": "Percentage_error", "Task": "Accented_Speech_Recognition", "score": "45.35"}, {"Material": "swb_hub_500_WER_fullSWBCH", "Method": "CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 16}], "ner": [[2, 4, "Method"], [7, 14, "Task"], [24, 27, "Method"], [29, 36, "Method"], [44, 46, "Method"], [52, 54, "Method"], [77, 81, "Method"], [138, 139, "Method"], [147, 150, "Method"], [169, 171, "Method"], [180, 182, "Material"], [186, 187, "Metric"], [193, 195, "Method"], [213, 215, "Method"], [220, 223, "Method"], [226, 227, "Method"], [232, 237, "Method"], [245, 252, "Task"], [255, 257, "Method"], [260, 262, "Method"], [270, 272, "Method"], [282, 286, "Task"], [301, 304, "Method"], [305, 306, "Method"], [309, 310, "Method"], [329, 331, "Method"], [332, 334, "Task"], [335, 337, "Task"], [344, 345, "Metric"], [357, 359, "Method"], [372, 373, "Metric"], [381, 385, "Task"], [387, 389, "Method"], [393, 395, "Method"], [401, 403, "Method"], [405, 408, "Method"], [409, 410, "Method"], [417, 419, "Method"], [435, 438, "Method"], [440, 442, "Task"], [447, 449, "Method"], [457, 459, "Method"], [467, 469, "Method"], [482, 484, "Task"], [499, 500, "Task"], [507, 514, "Method"], [515, 518, "Method"], [527, 530, "Method"], [565, 566, "Metric"], [579, 586, "Task"], [635, 637, "Task"], [638, 640, "Method"], [643, 644, "Task"], [667, 669, "Method"], [681, 687, "Task"], [701, 705, "Method"], [718, 721, "Method"], [757, 761, "Method"], [786, 789, "Method"], [791, 792, "Task"], [830, 831, "Metric"], [852, 859, "Task"], [866, 867, "Method"], [873, 875, "Task"], [876, 878, "Method"], [916, 919, "Metric"], [930, 931, "Metric"], [947, 950, "Method"], [957, 961, "Method"], [962, 964, "Method"], [978, 980, "Task"], [989, 991, "Method"], [992, 994, "Method"], [1018, 1020, "Method"], [1050, 1053, "Method"], [1054, 1055, "Method"], [1063, 1066, "Material"], [1132, 1133, "Method"], [1220, 1224, "Method"], [1251, 1254, "Method"], [1255, 1256, "Method"], [1275, 1280, "Method"], [1293, 1295, "Method"], [1333, 1338, "Method"], [1350, 1352, "Method"], [1355, 1357, "Method"], [1412, 1413, "Task"], [1452, 1455, "Method"], [1464, 1469, "Method"], [1470, 1471, "Task"], [1504, 1506, "Method"], [1519, 1526, "Method"], [1527, 1528, "Method"], [1553, 1557, "Method"], [1567, 1569, "Metric"], [1573, 1575, "Method"], [1614, 1615, "Task"], [1636, 1638, "Method"], [1662, 1663, "Task"], [1666, 1668, "Metric"], [1677, 1678, "Method"], [1680, 1684, "Method"], [1697, 1699, "Task"], [1700, 1702, "Task"], [1733, 1734, "Task"], [1784, 1785, "Method"], [1796, 1798, "Method"], [1836, 1837, "Method"], [1850, 1851, "Method"], [1928, 1933, "Method"], [1961, 1966, "Method"], [1999, 2000, "Method"], [2022, 2024, "Method"], [2027, 2029, "Method"], [2060, 2063, "Metric"], [2072, 2073, "Method"], [2096, 2100, "Method"], [2109, 2112, "Method"], [2153, 2157, "Task"], [2160, 2162, "Task"], [2171, 2176, "Method"], [2222, 2226, "Method"], [2246, 2248, "Task"], [2266, 2267, "Method"], [2297, 2301, "Method"], [2303, 2305, "Method"], [2329, 2330, "Method"], [2379, 2380, "Method"], [2391, 2392, "Method"], [2395, 2396, "Method"], [2424, 2426, "Task"], [2447, 2449, "Method"], [2494, 2500, "Method"], [2514, 2515, "Method"], [2520, 2522, "Method"], [2525, 2526, "Task"], [2553, 2555, "Method"], [2559, 2562, "Metric"], [2573, 2574, "Method"], [2580, 2581, "Task"], [2653, 2655, "Method"], [2659, 2661, "Method"], [2663, 2665, "Method"], [2683, 2684, "Method"], [2742, 2744, "Method"], [2787, 2788, "Method"], [2830, 2832, "Task"], [2836, 2837, "Method"], [2839, 2841, "Task"], [2852, 2854, "Metric"], [2856, 2858, "Method"], [2860, 2861, "Method"], [2873, 2874, "Task"], [2878, 2880, "Method"], [2912, 2914, "Method"], [2929, 2931, "Method"], [2937, 2938, "Method"], [2944, 2950, "Task"], [2967, 2969, "Material"], [3023, 3024, "Task"], [3036, 3038, "Method"], [3119, 3123, "Method"], [3268, 3270, "Method"], [3293, 3295, "Method"], [3408, 3411, "Task"], [3416, 3419, "Method"], [3566, 3570, "Task"], [3576, 3578, "Method"], [3583, 3584, "Method"], [3587, 3591, "Task"], [3597, 3600, "Metric"], [3603, 3606, "Metric"], [3607, 3608, "Metric"], [3615, 3620, "Material"], [3637, 3638, "Material"], [3639, 3640, "Material"], [3689, 3692, "Metric"], [3712, 3713, "Material"], [3714, 3715, "Material"], [3717, 3718, "Material"], [3719, 3720, "Material"], [3732, 3733, "Material"], [3751, 3752, "Material"], [3801, 3802, "Material"], [3803, 3805, "Material"], [3816, 3817, "Method"], [3819, 3824, "Method"], [3826, 3828, "Method"], [3830, 3832, "Method"], [3852, 3858, "Method"], [3860, 3865, "Method"], [3866, 3868, "Task"], [3875, 3877, "Method"], [3892, 3893, "Material"], [3900, 3902, "Method"], [3929, 3930, "Task"], [3934, 3939, "Method"], [3949, 3950, "Material"], [3951, 3952, "Material"], [3959, 3961, "Metric"], [3964, 3967, "Method"], [3985, 3987, "Method"], [4010, 4011, "Method"], [4014, 4016, "Method"], [4066, 4070, "Method"], [4074, 4075, "Method"], [4098, 4101, "Material"], [4128, 4130, "Metric"], [4143, 4147, "Method"], [4151, 4152, "Metric"], [4166, 4167, "Method"], [4175, 4180, "Method"], [4188, 4190, "Method"], [4205, 4207, "Method"], [4313, 4319, "Metric"], [4320, 4321, "Metric"], [4332, 4333, "Metric"], [4346, 4347, "Method"], [4412, 4414, "Method"], [4448, 4453, "Method"], [4463, 4465, "Method"], [4475, 4476, "Method"], [4496, 4499, "Metric"], [4512, 4513, "Task"], [4525, 4530, "Method"], [4532, 4533, "Task"], [4537, 4539, "Method"], [4594, 4596, "Method"], [4600, 4601, "Method"], [4605, 4608, "Method"], [4612, 4614, "Method"], [4618, 4620, "Method"], [4639, 4642, "Task"], [4712, 4715, "Method"], [4725, 4726, "Method"], [4760, 4761, "Metric"], [4764, 4765, "Metric"], [4767, 4770, "Method"], [4772, 4775, "Method"], [4785, 4787, "Method"], [4790, 4791, "Metric"], [4798, 4799, "Metric"], [4825, 4829, "Method"], [4831, 4833, "Method"], [4837, 4839, "Task"], [4849, 4852, "Method"], [4859, 4862, "Method"], [4882, 4885, "Method"], [4898, 4901, "Method"], [4903, 4904, "Method"], [4908, 4910, "Task"], [4912, 4913, "Method"], [4937, 4939, "Method"], [4943, 4945, "Method"], [4946, 4950, "Method"], [4952, 4954, "Method"], [4958, 4960, "Method"], [5013, 5015, "Method"], [5019, 5020, "Task"], [5021, 5023, "Task"], [5035, 5036, "Method"], [5042, 5046, "Task"], [5051, 5053, "Method"], [5054, 5055, "Method"], [5079, 5080, "Method"], [5081, 5084, "Method"], [5090, 5091, "Method"], [5092, 5093, "Method"], [5102, 5104, "Method"], [5111, 5113, "Method"], [5120, 5123, "Method"], [5176, 5178, "Task"], [5198, 5200, "Task"], [5202, 5204, "Task"], [5229, 5233, "Material"], [5244, 5245, "Material"], [5246, 5248, "Material"], [5282, 5284, "Method"], [5302, 5304, "Method"], [5315, 5317, "Task"], [5327, 5328, "Task"], [5344, 5350, "Method"], [5384, 5388, "Method"], [5390, 5392, "Method"], [5426, 5431, "Method"], [5447, 5449, "Method"], [132, 133, "Method"], [365, 366, "Material"], [366, 367, "Material"], [729, 730, "Method"], [773, 774, "Method"], [880, 881, "Metric"], [888, 889, "Material"], [889, 890, "Material"], [1040, 1041, "Method"], [1154, 1155, "Method"], [1402, 1403, "Method"], [1407, 1408, "Metric"], [1474, 1475, "Method"], [1534, 1535, "Method"], [1591, 1592, "Method"], [1599, 1600, "Method"], [1809, 1810, "Method"], [2018, 2019, "Method"], [2190, 2191, "Method"], [2900, 2901, "Method"], [3484, 3486, "Method"], [3652, 3653, "Material"], [3703, 3704, "Material"], [3744, 3745, "Material"], [3764, 3765, "Material"], [3884, 3885, "Material"], [3976, 3978, "Method"], [3999, 4001, "Method"], [4003, 4004, "Material"], [4059, 4060, "Method"], [4080, 4081, "Method"], [4118, 4120, "Method"], [4156, 4157, "Material"], [4588, 4590, "Method"], [4879, 4881, "Method"], [4916, 4917, "Method"], [4931, 4932, "Method"], [4996, 4997, "Method"], [5257, 5258, "Material"]], "sections": [[0, 216], [216, 1038], [1038, 1612], [1612, 1794], [1794, 2134], [2134, 2244], [2244, 2518], [2518, 2843], [2843, 2940], [2940, 3021], [3021, 3406], [3406, 3525], [3525, 3610], [3610, 4208], [4208, 4810], [4810, 5332], [5332, 5477], [5477, 5536], [5536, 5539]], "sentences": [[0, 14], [14, 37], [37, 69], [69, 105], [105, 122], [122, 123], [123, 165], [165, 193], [193, 216], [216, 219], [219, 238], [238, 267], [267, 292], [292, 317], [317, 338], [338, 386], [386, 412], [412, 432], [432, 450], [450, 470], [470, 501], [501, 519], [519, 541], [541, 575], [575, 631], [631, 651], [651, 679], [679, 706], [706, 740], [740, 752], [752, 769], [769, 793], [793, 818], [818, 843], [843, 876], [876, 898], [898, 932], [932, 951], [951, 1001], [1001, 1038], [1038, 1043], [1043, 1067], [1067, 1080], [1080, 1104], [1104, 1128], [1128, 1153], [1153, 1165], [1165, 1184], [1184, 1192], [1192, 1218], [1218, 1233], [1233, 1270], [1270, 1281], [1281, 1330], [1330, 1349], [1349, 1391], [1391, 1411], [1411, 1433], [1433, 1462], [1462, 1472], [1472, 1484], [1484, 1531], [1531, 1551], [1551, 1570], [1570, 1612], [1612, 1615], [1615, 1629], [1629, 1649], [1649, 1661], [1661, 1675], [1675, 1692], [1692, 1727], [1727, 1774], [1774, 1794], [1794, 1798], [1798, 1821], [1821, 1845], [1845, 1872], [1872, 1889], [1889, 1920], [1920, 1945], [1945, 1994], [1994, 2038], [2038, 2085], [2085, 2101], [2101, 2134], [2134, 2137], [2137, 2164], [2164, 2193], [2193, 2220], [2220, 2244], [2244, 2248], [2248, 2263], [2263, 2273], [2273, 2289], [2289, 2328], [2328, 2369], [2369, 2415], [2415, 2424], [2424, 2450], [2450, 2488], [2488, 2518], [2518, 2522], [2522, 2525], [2525, 2563], [2563, 2582], [2582, 2618], [2618, 2635], [2635, 2651], [2651, 2678], [2678, 2709], [2709, 2738], [2738, 2778], [2778, 2803], [2803, 2824], [2824, 2843], [2843, 2846], [2846, 2870], [2870, 2907], [2907, 2926], [2926, 2940], [2940, 2944], [2944, 2957], [2957, 2980], [2980, 3003], [3003, 3021], [3021, 3026], [3026, 3056], [3056, 3076], [3076, 3106], [3106, 3124], [3124, 3135], [3135, 3172], [3172, 3205], [3205, 3216], [3216, 3247], [3247, 3286], [3286, 3336], [3336, 3367], [3367, 3406], [3406, 3411], [3411, 3445], [3445, 3466], [3466, 3503], [3503, 3525], [3525, 3528], [3528, 3539], [3539, 3571], [3571, 3610], [3610, 3620], [3620, 3642], [3642, 3673], [3673, 3693], [3693, 3734], [3734, 3753], [3753, 3770], [3770, 3799], [3799, 3829], [3829, 3842], [3842, 3866], [3866, 3886], [3886, 3913], [3913, 3928], [3928, 3954], [3954, 3975], [3975, 3998], [3998, 4028], [4028, 4037], [4037, 4042], [4042, 4051], [4051, 4058], [4058, 4090], [4090, 4108], [4108, 4135], [4135, 4142], [4142, 4161], [4161, 4181], [4181, 4208], [4208, 4212], [4212, 4241], [4241, 4273], [4273, 4305], [4305, 4364], [4364, 4393], [4393, 4409], [4409, 4423], [4423, 4444], [4444, 4466], [4466, 4477], [4477, 4514], [4514, 4534], [4534, 4566], [4566, 4585], [4585, 4621], [4621, 4632], [4632, 4668], [4668, 4689], [4689, 4706], [4706, 4746], [4746, 4777], [4777, 4810], [4810, 4814], [4814, 4825], [4825, 4844], [4844, 4863], [4863, 4886], [4886, 4927], [4927, 4951], [4951, 4976], [4976, 4999], [4999, 5016], [5016, 5048], [5048, 5060], [5060, 5096], [5096, 5124], [5124, 5151], [5151, 5170], [5170, 5201], [5201, 5242], [5242, 5250], [5250, 5274], [5274, 5309], [5309, 5332], [5332, 5335], [5335, 5378], [5378, 5417], [5417, 5455], [5455, 5477], [5477, 5480], [5480, 5509], [5509, 5536], [5536, 5539]], "words": ["document", ":", "Deep", "Speech", ":", "Scaling", "up", "end", "-", "to", "-", "end", "speech", "recognition", "We", "present", "a", "state", "-", "of", "-", "the", "-", "art", "speech", "recognition", "system", "developed", "using", "end", "-", "to", "-", "end", "deep", "learning", ".", "Our", "architecture", "is", "significantly", "simpler", "than", "traditional", "speech", "systems", ",", "which", "rely", "on", "laboriously", "engineered", "processing", "pipelines", ";", "these", "traditional", "systems", "also", "tend", "to", "perform", "poorly", "when", "used", "in", "noisy", "environments", ".", "In", "contrast", ",", "our", "system", "does", "not", "need", "hand", "-", "designed", "components", "to", "model", "background", "noise", ",", "reverberation", ",", "or", "speaker", "variation", ",", "but", "instead", "directly", "learns", "a", "function", "that", "is", "robust", "to", "such", "effects", ".", "We", "do", "not", "need", "a", "phoneme", "dictionary", ",", "nor", "even", "the", "concept", "of", "a", "\u201c", "phoneme", ".", "\u201d", "Key", "to", "our", "approach", "is", "a", "well", "-", "optimized", "RNN", "training", "system", "that", "uses", "multiple", "GPUs", ",", "as", "well", "as", "a", "set", "of", "novel", "data", "synthesis", "techniques", "that", "allow", "us", "to", "efficiently", "obtain", "a", "large", "amount", "of", "varied", "data", "for", "training", ".", "Our", "system", ",", "called", "Deep", "Speech", ",", "outperforms", "previously", "published", "results", "on", "the", "widely", "studied", "Switchboard", "Hub5\u201900", ",", "achieving", "16.0", "%", "error", "on", "the", "full", "test", "set", ".", "Deep", "Speech", "also", "handles", "challenging", "noisy", "environments", "better", "than", "widely", "used", ",", "state", "-", "of", "-", "the", "-", "art", "commercial", "speech", "systems", ".", "section", ":", "Introduction", "Top", "speech", "recognition", "systems", "rely", "on", "sophisticated", "pipelines", "composed", "of", "multiple", "algorithms", "and", "hand", "-", "engineered", "processing", "stages", ".", "In", "this", "paper", ",", "we", "describe", "an", "end", "-", "to", "-", "end", "speech", "system", ",", "called", "\u201c", "Deep", "Speech", "\u201d", ",", "where", "deep", "learning", "supersedes", "these", "processing", "stages", ".", "Combined", "with", "a", "language", "model", ",", "this", "approach", "achieves", "higher", "performance", "than", "traditional", "methods", "on", "hard", "speech", "recognition", "tasks", "while", "also", "being", "much", "simpler", ".", "These", "results", "are", "made", "possible", "by", "training", "a", "large", "recurrent", "neural", "network", "(", "RNN", ")", "using", "multiple", "GPUs", "and", "thousands", "of", "hours", "of", "data", ".", "Because", "this", "system", "learns", "directly", "from", "data", ",", "we", "do", "not", "require", "specialized", "components", "for", "speaker", "adaptation", "or", "noise", "filtering", ".", "In", "fact", ",", "in", "settings", "where", "robustness", "to", "speaker", "variation", "and", "noise", "are", "critical", ",", "our", "system", "excels", ":", "Deep", "Speech", "outperforms", "previously", "published", "methods", "on", "the", "Switchboard", "Hub5\u201900", "corpus", ",", "achieving", "16.0", "%", "error", ",", "and", "performs", "better", "than", "commercial", "systems", "in", "noisy", "speech", "recognition", "tests", ".", "Traditional", "speech", "systems", "use", "many", "heavily", "engineered", "processing", "stages", ",", "including", "specialized", "input", "features", ",", "acoustic", "models", ",", "and", "Hidden", "Markov", "Models", "(", "HMMs", ")", ".", "To", "improve", "these", "pipelines", ",", "domain", "experts", "must", "invest", "a", "great", "deal", "of", "effort", "tuning", "their", "features", "and", "models", ".", "The", "introduction", "of", "deep", "learning", "algorithms", "has", "improved", "speech", "system", "performance", ",", "usually", "by", "improving", "acoustic", "models", ".", "While", "this", "improvement", "has", "been", "significant", ",", "deep", "learning", "still", "plays", "only", "a", "limited", "role", "in", "traditional", "speech", "pipelines", ".", "As", "a", "result", ",", "to", "improve", "performance", "on", "a", "task", "such", "as", "recognizing", "speech", "in", "a", "noisy", "environment", ",", "one", "must", "laboriously", "engineer", "the", "rest", "of", "the", "system", "for", "robustness", ".", "In", "contrast", ",", "our", "system", "applies", "deep", "learning", "end", "-", "to", "-", "end", "using", "recurrent", "neural", "networks", ".", "We", "take", "advantage", "of", "the", "capacity", "provided", "by", "deep", "learning", "systems", "to", "learn", "from", "large", "datasets", "to", "improve", "our", "overall", "performance", ".", "Our", "model", "is", "trained", "end", "-", "to", "-", "end", "to", "produce", "transcriptions", "and", "thus", ",", "with", "sufficient", "data", "and", "computing", "power", ",", "can", "learn", "robustness", "to", "noise", "or", "speaker", "variation", "on", "its", "own", ".", "Tapping", "the", "benefits", "of", "end", "-", "to", "-", "end", "deep", "learning", ",", "however", ",", "poses", "several", "challenges", ":", "(", "i", ")", "we", "must", "find", "innovative", "ways", "to", "build", "large", ",", "labeled", "training", "sets", "and", "(", "ii", ")", "we", "must", "be", "able", "to", "train", "networks", "that", "are", "large", "enough", "to", "effectively", "utilize", "all", "of", "this", "data", ".", "One", "challenge", "for", "handling", "labeled", "data", "in", "speech", "systems", "is", "finding", "the", "alignment", "of", "text", "transcripts", "with", "input", "speech", ".", "This", "problem", "has", "been", "addressed", "by", "Graves", ",", "Fern\u00e1ndez", ",", "Gomez", "and", "Schmidhuber", ",", "thus", "enabling", "neural", "networks", "to", "easily", "consume", "unaligned", ",", "transcribed", "audio", "during", "training", ".", "Meanwhile", ",", "rapid", "training", "of", "large", "neural", "networks", "has", "been", "tackled", "by", "Coates", "et", "al", ".", ",", "demonstrating", "the", "speed", "advantages", "of", "multi", "-", "GPU", "computation", ".", "We", "aim", "to", "leverage", "these", "insights", "to", "fulfill", "the", "vision", "of", "a", "generic", "learning", "system", ",", "based", "on", "large", "speech", "datasets", "and", "scalable", "RNN", "training", ",", "that", "can", "surpass", "more", "complicated", "traditional", "methods", ".", "This", "vision", "is", "inspired", "partly", "by", "the", "work", "of", "Lee", "et", ".", "al", ".", "who", "applied", "early", "unsupervised", "feature", "learning", "techniques", "to", "replace", "hand", "-", "built", "speech", "features", ".", "We", "have", "chosen", "our", "RNN", "model", "specifically", "to", "map", "well", "to", "GPUs", "and", "we", "use", "a", "novel", "model", "partition", "scheme", "to", "improve", "parallelization", ".", "Additionally", ",", "we", "propose", "a", "process", "for", "assembling", "large", "quantities", "of", "labeled", "speech", "data", "exhibiting", "the", "distortions", "that", "our", "system", "should", "learn", "to", "handle", ".", "Using", "a", "combination", "of", "collected", "and", "synthesized", "data", ",", "our", "system", "learns", "robustness", "to", "realistic", "noise", "and", "speaker", "variation", "(", "including", "Lombard", "Effect", ")", ".", "Taken", "together", ",", "these", "ideas", "suffice", "to", "build", "an", "end", "-", "to", "-", "end", "speech", "system", "that", "is", "at", "once", "simpler", "than", "traditional", "pipelines", "yet", "also", "performs", "better", "on", "difficult", "speech", "tasks", ".", "Deep", "Speech", "achieves", "an", "error", "rate", "of", "16.0", "%", "on", "the", "full", "Switchboard", "Hub5\u201900", "test", "set", "\u2014", "the", "best", "published", "result", ".", "Further", ",", "on", "a", "new", "noisy", "speech", "recognition", "dataset", "of", "our", "own", "construction", ",", "our", "system", "achieves", "a", "word", "error", "rate", "of", "19.1", "%", "where", "the", "best", "commercial", "systems", "achieve", "30.5", "%", "error", ".", "In", "the", "remainder", "of", "this", "paper", ",", "we", "will", "introduce", "the", "key", "ideas", "behind", "our", "speech", "recognition", "system", ".", "We", "begin", "by", "describing", "the", "basic", "recurrent", "neural", "network", "model", "and", "training", "framework", "that", "we", "use", "in", "Section", "[", "reference", "]", ",", "followed", "by", "a", "discussion", "of", "GPU", "optimizations", "(", "Section", "[", "reference", "]", ")", ",", "and", "our", "data", "capture", "and", "synthesis", "strategy", "(", "Section", "[", "reference", "]", ")", ".", "We", "conclude", "with", "our", "experimental", "results", "demonstrating", "the", "state", "-", "of", "-", "the", "-", "art", "performance", "of", "Deep", "Speech", "(", "Section", "[", "reference", "]", ")", ",", "followed", "by", "a", "discussion", "of", "related", "work", "and", "our", "conclusions", ".", "section", ":", "RNN", "Training", "Setup", "The", "core", "of", "our", "system", "is", "a", "recurrent", "neural", "network", "(", "RNN", ")", "trained", "to", "ingest", "speech", "spectrograms", "and", "generate", "English", "text", "transcriptions", ".", "Let", "a", "single", "utterance", "and", "label", "be", "sampled", "from", "a", "training", "set", ".", "Each", "utterance", ",", ",", "is", "a", "time", "-", "series", "of", "length", "where", "every", "time", "-", "slice", "is", "a", "vector", "of", "audio", "features", ",", ".", "We", "use", "spectrograms", "as", "our", "features", ",", "so", "denotes", "the", "power", "of", "the", "\u2019", "th", "frequency", "bin", "in", "the", "audio", "frame", "at", "time", ".", "The", "goal", "of", "our", "RNN", "is", "to", "convert", "an", "input", "sequence", "into", "a", "sequence", "of", "character", "probabilities", "for", "the", "transcription", ",", "with", ",", "where", ".", "Our", "RNN", "model", "is", "composed", "of", "5", "layers", "of", "hidden", "units", ".", "For", "an", "input", ",", "the", "hidden", "units", "at", "layer", "are", "denoted", "with", "the", "convention", "that", "is", "the", "input", ".", "The", "first", "three", "layers", "are", "not", "recurrent", ".", "For", "the", "first", "layer", ",", "at", "each", "time", ",", "the", "output", "depends", "on", "the", "spectrogram", "frame", "along", "with", "a", "context", "of", "frames", "on", "each", "side", ".", "The", "remaining", "non", "-", "recurrent", "layers", "operate", "on", "independent", "data", "for", "each", "time", "step", ".", "Thus", ",", "for", "each", "time", ",", "the", "first", "3", "layers", "are", "computed", "by", ":", "where", "is", "the", "clipped", "rectified", "-", "linear", "(", "ReLu", ")", "activation", "function", "and", "are", "the", "weight", "matrix", "and", "bias", "parameters", "for", "layer", ".", "The", "fourth", "layer", "is", "a", "bi", "-", "directional", "recurrent", "layer", ".", "This", "layer", "includes", "two", "sets", "of", "hidden", "units", ":", "a", "set", "with", "forward", "recurrence", ",", ",", "and", "a", "set", "with", "backward", "recurrence", ":", "Note", "that", "must", "be", "computed", "sequentially", "from", "to", "for", "the", "\u2019", "th", "utterance", ",", "while", "the", "units", "must", "be", "computed", "sequentially", "in", "reverse", "from", "to", ".", "The", "fifth", "(", "non", "-", "recurrent", ")", "layer", "takes", "both", "the", "forward", "and", "backward", "units", "as", "inputs", "where", ".", "The", "output", "layer", "is", "a", "standard", "softmax", "function", "that", "yields", "the", "predicted", "character", "probabilities", "for", "each", "time", "slice", "and", "character", "in", "the", "alphabet", ":", "Here", "and", "denote", "the", "\u2019", "th", "column", "of", "the", "weight", "matrix", "and", "\u2019", "th", "bias", ",", "respectively", ".", "Once", "we", "have", "computed", "a", "prediction", "for", ",", "we", "compute", "the", "CTC", "loss", "to", "measure", "the", "error", "in", "prediction", ".", "During", "training", ",", "we", "can", "evaluate", "the", "gradient", "with", "respect", "to", "the", "network", "outputs", "given", "the", "ground", "-", "truth", "character", "sequence", ".", "From", "this", "point", ",", "computing", "the", "gradient", "with", "respect", "to", "all", "of", "the", "model", "parameters", "may", "be", "done", "via", "back", "-", "propagation", "through", "the", "rest", "of", "the", "network", ".", "We", "use", "Nesterov", "\u2019s", "Accelerated", "gradient", "method", "for", "training", ".", "The", "complete", "RNN", "model", "is", "illustrated", "in", "Figure", "[", "reference", "]", ".", "Note", "that", "its", "structure", "is", "considerably", "simpler", "than", "related", "models", "from", "the", "literature", "\u2014we", "have", "limited", "ourselves", "to", "a", "single", "recurrent", "layer", "(", "which", "is", "the", "hardest", "to", "parallelize", ")", "and", "we", "do", "not", "use", "Long", "-", "Short", "-", "Term", "-", "Memory", "(", "LSTM", ")", "circuits", ".", "One", "disadvantage", "of", "LSTM", "cells", "is", "that", "they", "require", "computing", "and", "storing", "multiple", "gating", "neuron", "responses", "at", "each", "step", ".", "Since", "the", "forward", "and", "backward", "recurrences", "are", "sequential", ",", "this", "small", "additional", "cost", "can", "become", "a", "computational", "bottleneck", ".", "By", "using", "a", "homogeneous", "model", "we", "have", "made", "the", "computation", "of", "the", "recurrent", "activations", "as", "efficient", "as", "possible", ":", "computing", "the", "ReLu", "outputs", "involves", "only", "a", "few", "highly", "optimized", "BLAS", "operations", "on", "the", "GPU", "and", "a", "single", "point", "-", "wise", "nonlinearity", ".", "subsection", ":", "Regularization", "While", "we", "have", "gone", "to", "significant", "lengths", "to", "expand", "our", "datasets", "(", "c.f", ".", "Section", "[", "reference", "]", ")", ",", "the", "recurrent", "networks", "we", "use", "are", "still", "adept", "at", "fitting", "the", "training", "data", ".", "In", "order", "to", "reduce", "variance", "further", ",", "we", "use", "several", "techniques", ".", "During", "training", "we", "apply", "a", "dropout", "rate", "between", "5", "%", "-", "10", "%", ".", "We", "apply", "dropout", "in", "the", "feed", "-", "forward", "layers", "but", "not", "to", "the", "recurrent", "hidden", "activations", ".", "A", "commonly", "employed", "technique", "in", "computer", "vision", "during", "network", "evaluation", "is", "to", "randomly", "jitter", "inputs", "by", "translations", "or", "reflections", ",", "feed", "each", "jittered", "version", "through", "the", "network", ",", "and", "vote", "or", "average", "the", "results", ".", "Such", "jittering", "is", "not", "common", "in", "ASR", ",", "however", "we", "found", "it", "beneficial", "to", "translate", "the", "raw", "audio", "files", "by", "5ms", "(", "half", "the", "filter", "bank", "step", "size", ")", "to", "the", "left", "and", "right", ",", "then", "forward", "propagate", "the", "recomputed", "features", "and", "average", "the", "output", "probabilities", ".", "At", "test", "time", "we", "also", "use", "an", "ensemble", "of", "several", "RNNs", ",", "averaging", "their", "outputs", "in", "the", "same", "way", ".", "subsection", ":", "Language", "Model", "When", "trained", "from", "large", "quantities", "of", "labeled", "speech", "data", ",", "the", "RNN", "model", "can", "learn", "to", "produce", "readable", "character", "-", "level", "transcriptions", ".", "Indeed", "for", "many", "of", "the", "transcriptions", ",", "the", "most", "likely", "character", "sequence", "predicted", "by", "the", "RNN", "is", "exactly", "correct", "without", "external", "language", "constraints", ".", "The", "errors", "made", "by", "the", "RNN", "in", "this", "case", "tend", "to", "be", "phonetically", "plausible", "renderings", "of", "English", "words", "\u2014", "Table", "[", "reference", "]", "shows", "some", "examples", ".", "Many", "of", "the", "errors", "occur", "on", "words", "that", "rarely", "or", "never", "appear", "in", "our", "training", "set", ".", "In", "practice", ",", "this", "is", "hard", "to", "avoid", ":", "training", "from", "enough", "speech", "data", "to", "hear", "all", "of", "the", "words", "or", "language", "constructions", "we", "might", "need", "to", "know", "is", "impractical", ".", "Therefore", ",", "we", "integrate", "our", "system", "with", "an", "N", "-", "gram", "language", "model", "since", "these", "models", "are", "easily", "trained", "from", "huge", "unlabeled", "text", "corpora", ".", "For", "comparison", ",", "while", "our", "speech", "datasets", "typically", "include", "up", "to", "3", "million", "utterances", ",", "the", "N", "-", "gram", "language", "model", "used", "for", "the", "experiments", "in", "Section", "[", "reference", "]", "is", "trained", "from", "a", "corpus", "of", "220", "million", "phrases", ",", "supporting", "a", "vocabulary", "of", "495", ",", "000", "words", ".", "Given", "the", "output", "of", "our", "RNN", "we", "perform", "a", "search", "to", "find", "the", "sequence", "of", "characters", "that", "is", "most", "probable", "according", "to", "both", "the", "RNN", "output", "and", "the", "language", "model", "(", "where", "the", "language", "model", "interprets", "the", "string", "of", "characters", "as", "words", ")", ".", "Specifically", ",", "we", "aim", "to", "find", "a", "sequence", "that", "maximizes", "the", "combined", "objective", ":", "where", "and", "are", "tunable", "parameters", "(", "set", "by", "cross", "-", "validation", ")", "that", "control", "the", "trade", "-", "off", "between", "the", "RNN", ",", "the", "language", "model", "constraint", "and", "the", "length", "of", "the", "sentence", ".", "The", "term", "denotes", "the", "probability", "of", "the", "sequence", "according", "to", "the", "N", "-", "gram", "model", ".", "We", "maximize", "this", "objective", "using", "a", "highly", "optimized", "beam", "search", "algorithm", ",", "with", "a", "typical", "beam", "size", "in", "the", "range", "1000", "-", "8000\u2014similar", "to", "the", "approach", "described", "by", "Hannun", "et", "al", ".", ".", "section", ":", "Optimizations", "As", "noted", "above", ",", "we", "have", "made", "several", "design", "decisions", "to", "make", "our", "networks", "amenable", "to", "high", "-", "speed", "execution", "(", "and", "thus", "fast", "training", ")", ".", "For", "example", ",", "we", "have", "opted", "for", "homogeneous", "rectified", "-", "linear", "networks", "that", "are", "simple", "to", "implement", "and", "depend", "on", "just", "a", "few", "highly", "-", "optimized", "BLAS", "calls", ".", "When", "fully", "unrolled", ",", "our", "networks", "include", "almost", "5", "billion", "connections", "for", "a", "typical", "utterance", "and", "thus", "efficient", "computation", "is", "critical", "to", "make", "our", "experiments", "feasible", ".", "We", "use", "multi", "-", "GPU", "training", "to", "accelerate", "our", "experiments", ",", "but", "doing", "this", "effectively", "requires", "some", "additional", "work", ",", "as", "we", "explain", ".", "subsection", ":", "Data", "parallelism", "In", "order", "to", "process", "data", "efficiently", ",", "we", "use", "two", "levels", "of", "data", "parallelism", ".", "First", ",", "each", "GPU", "processes", "many", "examples", "in", "parallel", ".", "This", "is", "done", "in", "the", "usual", "way", "by", "concatenating", "many", "examples", "into", "a", "single", "matrix", ".", "For", "instance", ",", "rather", "than", "performing", "a", "single", "matrix", "-", "vector", "multiplication", "in", "the", "recurrent", "layer", ",", "we", "prefer", "to", "do", "many", "in", "parallel", "by", "computing", "where", "(", "where", "corresponds", "to", "the", "\u2019", "th", "example", "at", "time", ")", ".", "The", "GPU", "is", "most", "efficient", "when", "is", "relatively", "wide", "(", "e.g.", ",", "1000", "examples", "or", "more", ")", "and", "thus", "we", "prefer", "to", "process", "as", "many", "examples", "on", "one", "GPU", "as", "possible", "(", "up", "to", "the", "limit", "of", "GPU", "memory", ")", ".", "When", "we", "wish", "to", "use", "larger", "minibatches", "than", "a", "single", "GPU", "can", "support", "on", "its", "own", "we", "use", "data", "parallelism", "across", "multiple", "GPUs", ",", "with", "each", "GPU", "processing", "a", "separate", "minibatch", "of", "examples", "and", "then", "combining", "its", "computed", "gradient", "with", "its", "peers", "during", "each", "iteration", ".", "We", "typically", "use", "or", "data", "parallelism", "across", "GPUs", ".", "Data", "parallelism", "is", "not", "easily", "implemented", ",", "however", ",", "when", "utterances", "have", "different", "lengths", "since", "they", "can", "not", "be", "combined", "into", "a", "single", "matrix", "multiplication", ".", "We", "resolve", "the", "problem", "by", "sorting", "our", "training", "examples", "by", "length", "and", "combining", "only", "similarly", "-", "sized", "utterances", "into", "minibatches", ",", "padding", "with", "silence", "when", "necessary", "so", "that", "all", "utterances", "in", "a", "batch", "have", "the", "same", "length", ".", "This", "solution", "is", "inspired", "by", "the", "ITPACK", "/", "ELLPACK", "sparse", "matrix", "format", ";", "a", "similar", "solution", "was", "used", "by", "the", "Sutskever", "et", "al", ".", "to", "accelerate", "RNNs", "for", "text", ".", "subsection", ":", "Model", "parallelism", "Data", "parallelism", "yields", "training", "speedups", "for", "modest", "multiples", "of", "the", "minibatch", "size", "(", "e.g.", ",", "2", "to", "4", ")", ",", "but", "faces", "diminishing", "returns", "as", "batching", "more", "examples", "into", "a", "single", "gradient", "update", "fails", "to", "improve", "the", "training", "convergence", "rate", ".", "That", "is", ",", "processing", "as", "many", "examples", "on", "as", "many", "GPUs", "fails", "to", "yield", "a", "speedup", "in", "training", ".", "It", "is", "also", "inefficient", "to", "fix", "the", "total", "minibatch", "size", "but", "spread", "out", "the", "examples", "to", "as", "many", "GPUs", ":", "as", "the", "minibatch", "within", "each", "GPU", "shrinks", ",", "most", "operations", "become", "memory", "-", "bandwidth", "limited", ".", "To", "scale", "further", ",", "we", "parallelize", "by", "partitioning", "the", "model", "(", "\u201c", "model", "parallelism", "\u201d", ")", ".", "Our", "model", "is", "challenging", "to", "parallelize", "due", "to", "the", "sequential", "nature", "of", "the", "recurrent", "layers", ".", "Since", "the", "bidirectional", "layer", "is", "comprised", "of", "a", "forward", "computation", "and", "a", "backward", "computation", "that", "are", "independent", ",", "we", "can", "perform", "the", "two", "computations", "in", "parallel", ".", "Unfortunately", ",", "naively", "splitting", "the", "RNN", "to", "place", "and", "on", "separate", "GPUs", "commits", "us", "to", "significant", "data", "transfers", "when", "we", "go", "to", "compute", "(", "which", "depends", "on", "both", "and", ")", ".", "Thus", ",", "we", "have", "chosen", "a", "different", "partitioning", "of", "work", "that", "requires", "less", "communication", "for", "our", "models", ":", "we", "divide", "the", "model", "in", "half", "along", "the", "time", "dimension", ".", "All", "layers", "except", "the", "recurrent", "layer", "can", "be", "trivially", "decomposed", "along", "the", "time", "dimension", ",", "with", "the", "first", "half", "of", "the", "time", "-", "series", ",", "from", "to", ",", "assigned", "to", "one", "GPU", "and", "the", "second", "half", "to", "another", "GPU", ".", "When", "computing", "the", "recurrent", "layer", "activations", ",", "the", "first", "GPU", "begins", "computing", "the", "forward", "activations", ",", "while", "the", "second", "begins", "computing", "the", "backward", "activations", ".", "At", "the", "mid", "-", "point", "(", ")", ",", "the", "two", "GPUs", "exchange", "the", "intermediate", "activations", ",", "and", "and", "swap", "roles", ".", "The", "first", "GPU", "then", "finishes", "the", "backward", "computation", "of", "and", "the", "second", "GPU", "finishes", "the", "forward", "computation", "of", ".", "subsection", ":", "Striding", "We", "have", "worked", "to", "minimize", "the", "running", "time", "of", "the", "recurrent", "layers", "of", "our", "RNN", ",", "since", "these", "are", "the", "hardest", "to", "parallelize", ".", "As", "a", "final", "optimization", ",", "we", "shorten", "the", "recurrent", "layers", "by", "taking", "\u201c", "steps", "\u201d", "(", "or", "strides", ")", "of", "size", "2", "in", "the", "original", "input", "so", "that", "the", "unrolled", "RNN", "has", "half", "as", "many", "steps", ".", "This", "is", "similar", "to", "a", "convolutional", "network", "with", "a", "step", "-", "size", "of", "2", "in", "the", "first", "layer", ".", "We", "use", "the", "cuDNN", "library", "to", "implement", "this", "first", "layer", "of", "convolution", "efficiently", ".", "section", ":", "Training", "Data", "Large", "-", "scale", "deep", "learning", "systems", "require", "an", "abundance", "of", "labeled", "data", ".", "For", "our", "system", "we", "need", "many", "recorded", "utterances", "and", "corresponding", "English", "transcriptions", ",", "but", "there", "are", "few", "public", "datasets", "of", "sufficient", "scale", ".", "To", "train", "our", "largest", "models", "we", "have", "thus", "collected", "an", "extensive", "dataset", "consisting", "of", "5000", "hours", "of", "read", "speech", "from", "9600", "speakers", ".", "For", "comparison", ",", "we", "have", "summarized", "the", "labeled", "datasets", "available", "to", "us", "in", "Table", "[", "reference", "]", ".", "subsection", ":", "Synthesis", "by", "superposition", "To", "expand", "our", "potential", "training", "data", "even", "further", "we", "use", "data", "synthesis", ",", "which", "has", "been", "successfully", "applied", "in", "other", "contexts", "to", "amplify", "the", "effective", "number", "of", "training", "samples", ".", "In", "our", "work", ",", "the", "goal", "is", "primarily", "to", "improve", "performance", "in", "noisy", "environments", "where", "existing", "systems", "break", "down", ".", "Capturing", "labeled", "data", "(", "e.g.", ",", "read", "speech", ")", "from", "noisy", "environments", "is", "not", "practical", ",", "however", ",", "and", "thus", "we", "must", "find", "other", "ways", "to", "generate", "such", "data", ".", "To", "a", "first", "order", ",", "audio", "signals", "are", "generated", "through", "a", "process", "of", "superposition", "of", "source", "signals", ".", "We", "can", "use", "this", "fact", "to", "synthesize", "noisy", "training", "data", ".", "For", "example", ",", "if", "we", "have", "a", "speech", "audio", "track", "and", "a", "\u201c", "noise", "\u201d", "audio", "track", ",", "then", "we", "can", "form", "the", "\u201c", "noisy", "speech", "\u201d", "track", "to", "simulate", "audio", "captured", "in", "a", "noisy", "environment", ".", "If", "necessary", ",", "we", "can", "add", "reverberations", ",", "echoes", "or", "other", "forms", "of", "damping", "to", "the", "power", "spectrum", "of", "or", "and", "then", "simply", "add", "them", "together", "to", "make", "fairly", "realistic", "audio", "scenes", ".", "There", "are", ",", "however", ",", "some", "risks", "in", "this", "approach", ".", "For", "example", ",", "in", "order", "to", "take", "1000", "hours", "of", "clean", "speech", "and", "create", "1000", "hours", "of", "noisy", "speech", ",", "we", "will", "need", "unique", "noise", "tracks", "spanning", "roughly", "1000", "hours", ".", "We", "can", "not", "settle", "for", ",", "say", ",", "10", "hours", "of", "repeating", "noise", ",", "since", "it", "may", "become", "possible", "for", "the", "recurrent", "network", "to", "memorize", "the", "noise", "track", "and", "\u201c", "subtract", "\u201d", "it", "out", "of", "the", "synthesized", "data", ".", "Thus", ",", "instead", "of", "using", "a", "single", "noise", "source", "with", "a", "length", "of", "1000", "hours", ",", "we", "use", "a", "large", "number", "of", "shorter", "clips", "(", "which", "are", "easier", "to", "collect", "from", "public", "video", "sources", ")", "and", "treat", "them", "as", "separate", "sources", "of", "noise", "before", "superimposing", "all", "of", "them", ":", ".", "When", "superimposing", "many", "signals", "collected", "from", "video", "clips", ",", "we", "can", "end", "up", "with", "\u201c", "noise", "\u201d", "sounds", "that", "are", "different", "from", "the", "kinds", "of", "noise", "recorded", "in", "real", "environments", ".", "To", "ensure", "a", "good", "match", "between", "our", "synthetic", "data", "and", "real", "data", ",", "we", "rejected", "any", "candidate", "noise", "clips", "where", "the", "average", "power", "in", "each", "frequency", "band", "differed", "significantly", "from", "the", "average", "power", "observed", "in", "real", "noisy", "recordings", ".", "subsection", ":", "Capturing", "Lombard", "Effect", "One", "challenging", "effect", "encountered", "by", "speech", "recognition", "systems", "in", "noisy", "environments", "is", "the", "\u201c", "Lombard", "Effect", "\u201d", ":", "speakers", "actively", "change", "the", "pitch", "or", "inflections", "of", "their", "voice", "to", "overcome", "noise", "around", "them", ".", "This", "(", "involuntary", ")", "effect", "does", "not", "show", "up", "in", "recorded", "speech", "datasets", "since", "they", "are", "collected", "in", "quiet", "environments", ".", "To", "ensure", "that", "the", "effect", "is", "represented", "in", "our", "training", "data", "we", "induce", "the", "Lombard", "effect", "intentionally", "during", "data", "collection", "by", "playing", "loud", "background", "noise", "through", "headphones", "worn", "by", "a", "person", "as", "they", "record", "an", "utterance", ".", "The", "noise", "induces", "them", "to", "inflect", "their", "voice", ",", "thus", "allowing", "us", "to", "capture", "the", "Lombard", "effect", "in", "our", "training", "data", ".", "section", ":", "Experiments", "We", "performed", "two", "sets", "of", "experiments", "to", "evaluate", "our", "system", ".", "In", "both", "cases", "we", "use", "the", "model", "described", "in", "Section", "[", "reference", "]", "trained", "from", "a", "selection", "of", "the", "datasets", "in", "Table", "[", "reference", "]", "to", "predict", "character", "-", "level", "transcriptions", ".", "The", "predicted", "probability", "vectors", "and", "language", "model", "are", "then", "fed", "into", "our", "decoder", "to", "yield", "a", "word", "-", "level", "transcription", ",", "which", "is", "compared", "with", "the", "ground", "truth", "transcription", "to", "yield", "the", "word", "error", "rate", "(", "WER", ")", ".", "subsection", ":", "Conversational", "speech", ":", "Switchboard", "Hub5\u201900", "(", "full", ")", "To", "compare", "our", "system", "to", "prior", "research", "we", "use", "an", "accepted", "but", "highly", "challenging", "test", "set", ",", "Hub5\u201900", "(", "LDC2002S23", ")", ".", "Some", "researchers", "split", "this", "set", "into", "\u201c", "easy", "\u201d", "(", "Switchboard", ")", "and", "\u201c", "hard", "\u201d", "(", "CallHome", ")", "instances", ",", "often", "reporting", "new", "results", "on", "the", "easier", "portion", "alone", ".", "We", "use", "the", "full", "set", ",", "which", "is", "the", "most", "challenging", "case", "and", "report", "the", "overall", "word", "error", "rate", ".", "We", "evaluate", "our", "system", "trained", "on", "only", "the", "300", "hour", "Switchboard", "conversational", "telephone", "speech", "dataset", "and", "trained", "on", "both", "Switchboard", "(", "SWB", ")", "and", "Fisher", "(", "FSH", ")", ",", "a", "2000", "hour", "corpus", "collected", "in", "a", "similar", "manner", "as", "Switchboard", ".", "Many", "researchers", "evaluate", "models", "trained", "only", "with", "300", "hours", "from", "Switchboard", "conversational", "telephone", "speech", "when", "testing", "on", "Hub5\u201900", ".", "In", "part", "this", "is", "because", "training", "on", "the", "full", "2000", "hour", "Fisher", "corpus", "is", "computationally", "difficult", ".", "Using", "the", "techniques", "mentioned", "in", "Section", "[", "reference", "]", "our", "system", "is", "able", "perform", "a", "full", "pass", "over", "the", "2300", "hours", "of", "data", "in", "just", "a", "few", "hours", ".", "Since", "the", "Switchboard", "and", "Fisher", "corpora", "are", "distributed", "at", "a", "sample", "rate", "of", "8kHz", ",", "we", "compute", "spectrograms", "of", "80", "linearly", "spaced", "log", "filter", "banks", "and", "an", "energy", "term", ".", "The", "filter", "banks", "are", "computed", "over", "windows", "of", "20ms", "strided", "by", "10ms", ".", "We", "did", "not", "evaluate", "more", "sophisticated", "features", "such", "as", "the", "mel", "-", "scale", "log", "filter", "banks", "or", "the", "mel", "-", "frequency", "cepstral", "coefficients", ".", "Speaker", "adaptation", "is", "critical", "to", "the", "success", "of", "current", "ASR", "systems", ",", "particularly", "when", "trained", "on", "300", "hour", "Switchboard", ".", "For", "the", "models", "we", "test", "on", "Hub5\u201900", ",", "we", "apply", "a", "simple", "form", "of", "speaker", "adaptation", "by", "normalizing", "the", "spectral", "features", "on", "a", "per", "speaker", "basis", ".", "Other", "than", "this", ",", "we", "do", "not", "modify", "the", "input", "features", "in", "any", "way", ".", "For", "decoding", ",", "we", "use", "a", "4", "-", "gram", "language", "model", "with", "a", "30", ",", "000", "word", "vocabulary", "trained", "on", "the", "Fisher", "and", "Switchboard", "transcriptions", ".", "Again", ",", "hyperparameters", "for", "the", "decoding", "objective", "are", "chosen", "via", "cross", "-", "validation", "on", "a", "held", "-", "out", "development", "set", ".", "The", "Deep", "Speech", "SWB", "model", "is", "a", "network", "of", "5", "hidden", "layers", "each", "with", "2048", "neurons", "trained", "on", "only", "300", "hour", "switchboard", ".", "The", "Deep", "Speech", "SWB", "+", "FSH", "model", "is", "an", "ensemble", "of", "4", "RNNs", "each", "with", "5", "hidden", "layers", "of", "2304", "neurons", "trained", "on", "the", "full", "2300", "hour", "combined", "corpus", ".", "All", "networks", "are", "trained", "on", "inputs", "of", "+", "/-", "9", "frames", "of", "context", ".", "We", "report", "results", "in", "Table", "[", "reference", "]", ".", "The", "model", "from", "Vesely", "et", "al", ".", "(", "DNN", "-", "GMM", "sMBR", ")", "uses", "a", "sequence", "based", "loss", "function", "on", "top", "of", "a", "DNN", "after", "using", "a", "typical", "hybrid", "DNN", "-", "HMM", "system", "to", "realign", "the", "training", "set", ".", "The", "performance", "of", "this", "model", "on", "the", "combined", "Hub5\u201900", "test", "set", "is", "the", "best", "previously", "published", "result", ".", "When", "trained", "on", "the", "combined", "2300", "hours", "of", "data", "the", "Deep", "Speech", "system", "improves", "upon", "this", "baseline", "by", "2.4", "%", "absolute", "WER", "and", "13.0", "%", "relative", ".", "The", "model", "from", "Maas", "et", "al", ".", "(", "DNN", "-", "HMM", "FSH", ")", "achieves", "19.9", "%", "WER", "when", "trained", "on", "the", "Fisher", "2000", "hour", "corpus", ".", "That", "system", "was", "built", "using", "Kaldi", ",", "state", "-", "of", "-", "the", "-", "art", "open", "source", "speech", "recognition", "software", ".", "We", "include", "this", "result", "to", "demonstrate", "that", "Deep", "Speech", ",", "when", "trained", "on", "a", "comparable", "amount", "of", "data", "is", "competitive", "with", "the", "best", "existing", "ASR", "systems", ".", "subsection", ":", "Noisy", "speech", "Few", "standards", "exist", "for", "testing", "noisy", "speech", "performance", ",", "so", "we", "constructed", "our", "own", "evaluation", "set", "of", "100", "noisy", "and", "100", "noise", "-", "free", "utterances", "from", "10", "speakers", ".", "The", "noise", "environments", "included", "a", "background", "radio", "or", "TV", ";", "washing", "dishes", "in", "a", "sink", ";", "a", "crowded", "cafeteria", ";", "a", "restaurant", ";", "and", "inside", "a", "car", "driving", "in", "the", "rain", ".", "The", "utterance", "text", "came", "primarily", "from", "web", "search", "queries", "and", "text", "messages", ",", "as", "well", "as", "news", "clippings", ",", "phone", "conversations", ",", "Internet", "comments", ",", "public", "speeches", ",", "and", "movie", "scripts", ".", "We", "did", "not", "have", "precise", "control", "over", "the", "signal", "-", "to", "-", "noise", "ratio", "(", "SNR", ")", "of", "the", "noisy", "samples", ",", "but", "we", "aimed", "for", "an", "SNR", "between", "2", "and", "6", "dB.", "For", "the", "following", "experiments", ",", "we", "train", "our", "RNNs", "on", "all", "the", "datasets", "(", "more", "than", "7000", "hours", ")", "listed", "in", "Table", "[", "reference", "]", ".", "Since", "we", "train", "for", "15", "to", "20", "epochs", "with", "newly", "synthesized", "noise", "in", "each", "pass", ",", "our", "model", "learns", "from", "over", "100", ",", "000", "hours", "of", "novel", "data", ".", "We", "use", "an", "ensemble", "of", "6", "networks", "each", "with", "5", "hidden", "layers", "of", "2560", "neurons", ".", "No", "form", "of", "speaker", "adaptation", "is", "applied", "to", "the", "training", "or", "evaluation", "sets", ".", "We", "normalize", "training", "examples", "on", "a", "per", "utterance", "basis", "in", "order", "to", "make", "the", "total", "power", "of", "each", "example", "consistent", ".", "The", "features", "are", "160", "linearly", "spaced", "log", "filter", "banks", "computed", "over", "windows", "of", "20ms", "strided", "by", "10ms", "and", "an", "energy", "term", ".", "Audio", "files", "are", "resampled", "to", "16kHz", "prior", "to", "the", "featurization", ".", "Finally", ",", "from", "each", "frequency", "bin", "we", "remove", "the", "global", "mean", "over", "the", "training", "set", "and", "divide", "by", "the", "global", "standard", "deviation", ",", "primarily", "so", "the", "inputs", "are", "well", "scaled", "during", "the", "early", "stages", "of", "training", ".", "As", "described", "in", "Section", "[", "reference", "]", ",", "we", "use", "a", "5", "-", "gram", "language", "model", "for", "the", "decoding", ".", "We", "train", "the", "language", "model", "on", "220", "million", "phrases", "of", "the", "Common", "Crawl", ",", "selected", "such", "that", "at", "least", "95", "%", "of", "the", "characters", "of", "each", "phrase", "are", "in", "the", "alphabet", ".", "Only", "the", "most", "common", "495", ",", "000", "words", "are", "kept", ",", "the", "rest", "remapped", "to", "an", "UNKNOWN", "token", ".", "We", "compared", "the", "Deep", "Speech", "system", "to", "several", "commercial", "speech", "systems", ":", "(", "1", ")", "wit.ai", ",", "(", "2", ")", "Google", "Speech", "API", ",", "(", "3", ")", "Bing", "Speech", "and", "(", "4", ")", "Apple", "Dictation", ".", "Our", "test", "is", "designed", "to", "benchmark", "performance", "in", "noisy", "environments", ".", "This", "situation", "creates", "challenges", "for", "evaluating", "the", "web", "speech", "APIs", ":", "these", "systems", "will", "give", "no", "result", "at", "all", "when", "the", "SNR", "is", "too", "low", "or", "in", "some", "cases", "when", "the", "utterance", "is", "too", "long", ".", "Therefore", "we", "restrict", "our", "comparison", "to", "the", "subset", "of", "utterances", "for", "which", "all", "systems", "returned", "a", "non", "-", "empty", "result", ".", "The", "results", "of", "evaluating", "each", "system", "on", "our", "test", "files", "appear", "in", "Table", "[", "reference", "]", ".", "To", "evaluate", "the", "efficacy", "of", "the", "noise", "synthesis", "techniques", "described", "in", "Section", "[", "reference", "]", ",", "we", "trained", "two", "RNNs", ",", "one", "on", "5000", "hours", "of", "raw", "data", "and", "the", "other", "trained", "on", "the", "same", "5000", "hours", "plus", "noise", ".", "On", "the", "100", "clean", "utterances", "both", "models", "perform", "about", "the", "same", ",", "9.2", "%", "WER", "and", "9.0", "%", "WER", "for", "the", "clean", "trained", "model", "and", "the", "noise", "trained", "model", "respectively", ".", "However", ",", "on", "the", "100", "noisy", "utterances", "the", "noisy", "model", "achieves", "22.6", "%", "WER", "over", "the", "clean", "model", "\u2019s", "28.7", "%", "WER", ",", "a", "6.1", "%", "absolute", "and", "21.3", "%", "relative", "improvement", ".", "section", ":", "Related", "Work", "Several", "parts", "of", "our", "work", "are", "inspired", "by", "previous", "results", ".", "Neural", "network", "acoustic", "models", "and", "other", "connectionist", "approaches", "were", "first", "introduced", "to", "speech", "pipelines", "in", "the", "early", "1990s", ".", "These", "systems", ",", "similar", "to", "DNN", "acoustic", "models", ",", "replace", "only", "one", "stage", "of", "the", "speech", "recognition", "pipeline", ".", "Mechanically", ",", "our", "system", "is", "similar", "to", "other", "efforts", "to", "build", "end", "-", "to", "-", "end", "speech", "systems", "from", "deep", "learning", "algorithms", ".", "For", "example", ",", "Graves", "et", "al", ".", "have", "previously", "introduced", "the", "\u201c", "Connectionist", "Temporal", "Classification", "\u201d", "(", "CTC", ")", "loss", "function", "for", "scoring", "transcriptions", "produced", "by", "RNNs", "and", ",", "with", "LSTM", "networks", ",", "have", "previously", "applied", "this", "approach", "to", "speech", ".", "We", "similarly", "adopt", "the", "CTC", "loss", "for", "part", "of", "our", "training", "procedure", "but", "use", "much", "simpler", "recurrent", "networks", "with", "rectified", "-", "linear", "activations", ".", "Our", "recurrent", "network", "is", "similar", "to", "the", "bidirectional", "RNN", "used", "by", "Hannun", "et", "al", ".", ",", "but", "with", "multiple", "changes", "to", "enhance", "its", "scalability", ".", "By", "focusing", "on", "scalability", ",", "we", "have", "shown", "that", "these", "simpler", "networks", "can", "be", "effective", "even", "without", "the", "more", "complex", "LSTM", "machinery", ".", "Our", "work", "is", "certainly", "not", "the", "first", "to", "exploit", "scalability", "to", "improve", "performance", "of", "DL", "algorithms", ".", "The", "value", "of", "scalability", "in", "deep", "learning", "is", "well", "-", "studied", "and", "the", "use", "of", "parallel", "processors", "(", "including", "GPUs", ")", "has", "been", "instrumental", "to", "recent", "large", "-", "scale", "DL", "results", ".", "Early", "ports", "of", "DL", "algorithms", "to", "GPUs", "revealed", "significant", "speed", "gains", ".", "Researchers", "have", "also", "begun", "choosing", "designs", "that", "map", "well", "to", "GPU", "hardware", "to", "gain", "even", "more", "efficiency", ",", "including", "convolutional", "and", "locally", "connected", "networks", ",", "especially", "when", "optimized", "libraries", "like", "cuDNN", "and", "BLAS", "are", "available", ".", "Indeed", ",", "using", "high", "-", "performance", "computing", "infrastructure", ",", "it", "is", "possible", "today", "to", "train", "neural", "networks", "with", "more", "than", "10", "billion", "connections", "using", "clusters", "of", "GPUs", ".", "These", "results", "inspired", "us", "to", "focus", "first", "on", "making", "scalable", "design", "choices", "to", "efficiently", "utilize", "many", "GPUs", "before", "trying", "to", "engineer", "the", "algorithms", "and", "models", "themselves", ".", "With", "the", "potential", "to", "train", "large", "models", ",", "there", "is", "a", "need", "for", "large", "training", "sets", "as", "well", ".", "In", "other", "fields", ",", "such", "as", "computer", "vision", ",", "large", "labeled", "training", "sets", "have", "enabled", "significant", "leaps", "in", "performance", "as", "they", "are", "used", "to", "feed", "larger", "and", "larger", "DL", "systems", ".", "In", "speech", "recognition", ",", "however", ",", "such", "large", "training", "sets", "are", "less", "common", ",", "with", "typical", "benchmarks", "having", "training", "sets", "ranging", "from", "tens", "of", "hours", "(", "e.g.", "the", "Wall", "Street", "Journal", "corpus", "with", "80", "hours", ")", "to", "several", "hundreds", "of", "hours", "(", "e.g.", "Switchboard", "and", "Broadcast", "News", ")", ".", "Larger", "benchmark", "datasets", ",", "such", "as", "the", "Fisher", "corpus", "with", "2000", "hours", "of", "transcribed", "speech", ",", "are", "rare", "and", "only", "recently", "being", "studied", ".", "To", "fully", "utilize", "the", "expressive", "power", "of", "the", "recurrent", "networks", "available", "to", "us", ",", "we", "rely", "not", "only", "on", "large", "sets", "of", "labeled", "utterances", ",", "but", "also", "on", "synthesis", "techniques", "to", "generate", "novel", "examples", ".", "This", "approach", "is", "well", "known", "in", "computer", "vision", "but", "we", "have", "found", "this", "especially", "convenient", "and", "effective", "for", "speech", "when", "done", "properly", ".", "section", ":", "Conclusion", "We", "have", "presented", "an", "end", "-", "to", "-", "end", "deep", "learning", "-", "based", "speech", "system", "capable", "of", "outperforming", "existing", "state", "-", "of", "-", "the", "-", "art", "recognition", "pipelines", "in", "two", "challenging", "scenarios", ":", "clear", ",", "conversational", "speech", "and", "speech", "in", "noisy", "environments", ".", "Our", "approach", "is", "enabled", "particularly", "by", "multi", "-", "GPU", "training", "and", "by", "data", "collection", "and", "synthesis", "strategies", "to", "build", "large", "training", "sets", "exhibiting", "the", "distortions", "our", "system", "must", "handle", "(", "such", "as", "background", "noise", "and", "Lombard", "effect", ")", ".", "Combined", ",", "these", "solutions", "enable", "us", "to", "build", "a", "data", "-", "driven", "speech", "system", "that", "is", "at", "once", "better", "performing", "than", "existing", "methods", "while", "no", "longer", "relying", "on", "the", "complex", "processing", "stages", "that", "had", "stymied", "further", "progress", ".", "We", "believe", "this", "approach", "will", "continue", "to", "improve", "as", "we", "capitalize", "on", "increased", "computing", "power", "and", "dataset", "sizes", "in", "the", "future", ".", "section", ":", "Acknowledgments", "We", "are", "grateful", "to", "Jia", "Lei", ",", "whose", "work", "on", "DL", "for", "speech", "at", "Baidu", "has", "spurred", "us", "forward", ",", "for", "his", "advice", "and", "support", "throughout", "this", "project", ".", "We", "also", "thank", "Ian", "Lane", ",", "Dan", "Povey", ",", "Dan", "Jurafsky", ",", "Dario", "Amodei", ",", "Andrew", "Maas", ",", "Calisa", "Cole", "and", "Li", "Wei", "for", "helpful", "conversations", ".", "bibliography", ":", "References"]}