{"coref": {"BLEU_score": [[98, 99], [124, 125], [153, 155], [3373, 3375], [3423, 3424], [3435, 3437], [3506, 3508], [3822, 3823]], "Constituency_Parsing": [[196, 199], [3944, 3947], [3961, 3964]], "F1_score": [], "IWSLT2015_English-German": [], "IWSLT2015_German-English": [], "Machine_Translation": [[71, 74], [250, 252], [2721, 2723], [3381, 3383], [4299, 4301]], "Penn_Treebank": [[4035, 4037]], "Transformer": [[53, 54], [185, 186], [460, 461], [485, 486], [616, 617], [775, 776], [813, 814], [915, 916], [1762, 1763], [2040, 2042], [2051, 2053], [3719, 3720], [3951, 3952], [4019, 4023], [4238, 4239], [4266, 4267], [4303, 4304], [4386, 4387], [3400, 3401], [3538, 3539]], "Transformer_Base": [[3124, 3126], [3145, 3147], [3326, 3328], [3465, 3467], [3559, 3561], [3724, 3726], [3939, 3941], [4143, 4151]], "Transformer_Big": [[3161, 3163], [3183, 3185], [3396, 3399], [3502, 3504], [3586, 3588]], "WMT2014_English-French": [[3043, 3046], [3052, 3058], [3545, 3550]], "WMT2014_English-German": [[2999, 3005]]}, "coref_non_salient": {"0": [[210, 213], [4220, 4224]], "1": [[247, 249], [754, 763]], "10": [[3268, 3269], [3340, 3341]], "100": [[839, 843]], "101": [[547, 550]], "102": [[742, 746]], "103": [[706, 708]], "104": [[4011, 4015]], "105": [[3371, 3372]], "106": [[3736, 3742]], "107": [[1840, 1841]], "108": [[1346, 1348]], "109": [[2548, 2553]], "11": [[1410, 1417], [1418, 1422], [1462, 1466], [1503, 1506], [3876, 3878]], "110": [[1443, 1447]], "111": [[3857, 3859]], "112": [[988, 995]], "113": [[8, 12]], "114": [[532, 535]], "115": [[214, 219]], "116": [[414, 417]], "12": [[407, 409], [779, 781], [2107, 2110], [4270, 4273]], "13": [[272, 276], [845, 849], [4286, 4290]], "14": [[660, 663], [690, 693], [1161, 1167], [1849, 1853], [1856, 1860], [2445, 2449], [2494, 2497], [2651, 2655], [2681, 2685], [2895, 2899]], "15": [[171, 173], [3480, 3482], [3523, 3525], [3654, 3656], [3680, 3682]], "16": [[1450, 1452], [2479, 2481]], "17": [[1563, 1567], [1567, 1576], [1668, 1672]], "18": [[2191, 2193], [2322, 2324], [3923, 3926]], "19": [[1288, 1293], [1299, 1304], [1959, 1964]], "2": [[243, 245], [2559, 2562]], "20": [[831, 832], [1844, 1845]], "21": [[1745, 1749], [3816, 3820]], "22": [[816, 819], [2748, 2751]], "23": [[24, 25], [27, 28], [36, 37], [38, 39], [858, 859], [876, 877], [938, 939], [957, 958], [966, 967], [1088, 1089], [1113, 1114], [1138, 1139], [1803, 1804], [1847, 1848], [1886, 1887], [2014, 2017], [963, 964]], "24": [[1030, 1032], [1154, 1156]], "25": [[971, 975], [1094, 1098]], "26": [[537, 538], [599, 600]], "27": [[501, 503], [3651, 3653], [3829, 3830]], "28": [[63, 66], [802, 803], [2077, 2078], [2201, 2202]], "29": [[1003, 1012], [2019, 2025]], "3": [[404, 406], [1832, 1838], [3989, 3996], [4228, 4235]], "30": [[1405, 1407], [1435, 1437], [1500, 1502], [4276, 4277]], "31": [[1485, 1488]], "32": [[41, 43], [58, 60], [395, 397], [443, 445], [473, 475], [671, 673], [1826, 1831]], "33": [[4083, 4086], [4175, 4177]], "34": [[362, 364], [1738, 1740]], "35": [[3628, 3629], [4153, 4154]], "36": [[3911, 3914]], "37": [[829, 830]], "38": [[2168, 2173]], "39": [[466, 467], [2451, 2452], [16, 17], [222, 223], [268, 269], [451, 452], [737, 738], [2669, 2670], [2688, 2689], [2856, 2857], [4280, 4281], [4313, 4314]], "4": [[1459, 1461], [2506, 2508], [2678, 2680], [2872, 2873], [2883, 2884]], "40": [[1218, 1220], [1402, 1404], [1581, 1583], [1635, 1637]], "41": [[2453, 2455], [2794, 2796], [2849, 2851], [2816, 2818], [4315, 4317]], "42": [[3928, 3930]], "43": [[3344, 3349]], "44": [[2864, 2866], [2886, 2888]], "45": [[119, 120], [3418, 3419]], "46": [[2726, 2729]], "47": [[4051, 4055], [4095, 4099], [4179, 4183]], "48": [[3337, 3339]], "49": [[2901, 2908]], "5": [[921, 925], [1507, 1508]], "50": [[1286, 1287]], "51": [[4471, 4473]], "52": [[4368, 4372]], "53": [[4241, 4242]], "54": [[2261, 2263], [2284, 2286], [3314, 3316]], "55": [[3598, 3600], [3751, 3753]], "56": [[1109, 1111], [1788, 1790]], "57": [[959, 961]], "58": [[4421, 4422]], "59": [[3266, 3267]], "6": [[101, 110], [128, 137], [3385, 3394], [3491, 3500], [4320, 4327], [4328, 4337]], "60": [[540, 541], [595, 596]], "61": [[3276, 3277], [3305, 3306], [3905, 3906], [3551, 3552], [4111, 4112]], "62": [[896, 899]], "63": [[1016, 1018]], "64": [[240, 242]], "65": [[3486, 3488]], "66": [[49, 51]], "67": [[490, 491]], "68": [[2484, 2489]], "69": [[926, 933]], "7": [[3853, 3856], [4127, 4129]], "70": [[870, 872]], "71": [[388, 390], [524, 526]], "72": [[2731, 2734], [3017, 3020]], "73": [[2410, 2412]], "74": [[1842, 1843]], "75": [[463, 465]], "76": [[703, 705]], "77": [[2101, 2102]], "78": [[720, 721]], "79": [[514, 516], [3118, 3121], [3460, 3462]], "8": [[864, 866], [2708, 2710]], "80": [[666, 669]], "81": [[282, 284]], "82": [[633, 635]], "83": [[3203, 3205]], "84": [[369, 371]], "85": [[2093, 2096]], "86": [[797, 801]], "87": [[712, 718]], "88": [[709, 711]], "89": [[3762, 3764]], "9": [[3213, 3215], [3231, 3233], [4124, 4126]], "90": [[1993, 2000]], "91": [[1775, 1781]], "92": [[4400, 4405]], "93": [[827, 828]], "94": [[164, 165]], "95": [[726, 733]], "96": [[2103, 2104]], "97": [[2739, 2740]], "98": [[3260, 3261]], "99": [[365, 367]]}, "doc_id": "0b0cf7e00e7532e38238a9164f0a8db2574be2ea", "method_subrelations": {"Transformer": [[[0, 11], "Transformer"]], "Transformer_Base": [[[0, 16], "Transformer_Base"]], "Transformer_Big": [[[0, 15], "Transformer_Big"]]}, "n_ary_relations": [{"Material": "IWSLT2015_English-German", "Method": "Transformer", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "28.23"}, {"Material": "IWSLT2015_German-English", "Method": "Transformer", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "34.44"}, {"Material": "Penn_Treebank", "Method": "Transformer", "Metric": "F1_score", "Task": "Constituency_Parsing", "score": "92.7"}, {"Material": "WMT2014_English-French", "Method": "Transformer_Base", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "38.1"}, {"Material": "WMT2014_English-French", "Method": "Transformer_Big", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "41.0"}, {"Material": "WMT2014_English-German", "Method": "Transformer_Base", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "27.3"}, {"Material": "WMT2014_English-German", "Method": "Transformer_Big", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "28.4"}], "ner": [[8, 12, "Method"], [24, 25, "Method"], [27, 28, "Method"], [36, 37, "Method"], [38, 39, "Method"], [41, 43, "Method"], [49, 51, "Method"], [53, 54, "Method"], [58, 60, "Method"], [63, 66, "Method"], [71, 74, "Task"], [98, 99, "Metric"], [101, 110, "Task"], [119, 120, "Method"], [124, 125, "Metric"], [128, 137, "Task"], [153, 155, "Metric"], [164, 165, "Method"], [171, 173, "Metric"], [185, 186, "Method"], [196, 199, "Task"], [210, 213, "Method"], [214, 219, "Method"], [240, 242, "Task"], [243, 245, "Task"], [247, 249, "Task"], [250, 252, "Task"], [272, 276, "Method"], [282, 284, "Method"], [362, 364, "Metric"], [365, 367, "Method"], [369, 371, "Method"], [388, 390, "Task"], [395, 397, "Method"], [404, 406, "Method"], [407, 409, "Method"], [414, 417, "Task"], [443, 445, "Method"], [460, 461, "Method"], [463, 465, "Method"], [466, 467, "Method"], [473, 475, "Method"], [485, 486, "Method"], [490, 491, "Task"], [501, 503, "Metric"], [514, 516, "Method"], [524, 526, "Task"], [532, 535, "Method"], [537, 538, "Method"], [540, 541, "Method"], [547, 550, "Method"], [595, 596, "Method"], [599, 600, "Method"], [616, 617, "Method"], [633, 635, "Metric"], [660, 663, "Method"], [666, 669, "Method"], [671, 673, "Method"], [690, 693, "Method"], [703, 705, "Task"], [706, 708, "Task"], [709, 711, "Task"], [712, 718, "Task"], [720, 721, "Method"], [726, 733, "Method"], [742, 746, "Method"], [754, 763, "Task"], [775, 776, "Method"], [779, 781, "Method"], [797, 801, "Method"], [802, 803, "Method"], [813, 814, "Method"], [816, 819, "Task"], [827, 828, "Method"], [829, 830, "Method"], [831, 832, "Method"], [839, 843, "Method"], [845, 849, "Method"], [858, 859, "Method"], [864, 866, "Method"], [870, 872, "Method"], [876, 877, "Method"], [896, 899, "Method"], [915, 916, "Method"], [921, 925, "Method"], [926, 933, "Method"], [938, 939, "Method"], [957, 958, "Method"], [959, 961, "Method"], [966, 967, "Method"], [971, 975, "Method"], [988, 995, "Method"], [1003, 1012, "Method"], [1016, 1018, "Method"], [1030, 1032, "Method"], [1088, 1089, "Method"], [1094, 1098, "Method"], [1109, 1111, "Method"], [1113, 1114, "Method"], [1138, 1139, "Method"], [1154, 1156, "Method"], [1161, 1167, "Method"], [1218, 1220, "Method"], [1286, 1287, "Method"], [1288, 1293, "Method"], [1299, 1304, "Method"], [1346, 1348, "Method"], [1402, 1404, "Method"], [1405, 1407, "Method"], [1410, 1417, "Method"], [1418, 1422, "Method"], [1435, 1437, "Method"], [1443, 1447, "Method"], [1450, 1452, "Method"], [1459, 1461, "Metric"], [1462, 1466, "Method"], [1485, 1488, "Method"], [1500, 1502, "Method"], [1503, 1506, "Method"], [1507, 1508, "Method"], [1563, 1567, "Task"], [1567, 1576, "Task"], [1581, 1583, "Method"], [1635, 1637, "Method"], [1668, 1672, "Task"], [1738, 1740, "Metric"], [1745, 1749, "Method"], [1762, 1763, "Method"], [1775, 1781, "Method"], [1788, 1790, "Method"], [1803, 1804, "Method"], [1826, 1831, "Method"], [1832, 1838, "Method"], [1840, 1841, "Method"], [1842, 1843, "Method"], [1844, 1845, "Method"], [1847, 1848, "Method"], [1849, 1853, "Method"], [1856, 1860, "Method"], [1886, 1887, "Method"], [1959, 1964, "Method"], [1993, 2000, "Method"], [2014, 2017, "Method"], [2019, 2025, "Method"], [2040, 2042, "Method"], [2051, 2053, "Method"], [2077, 2078, "Method"], [2093, 2096, "Method"], [2101, 2102, "Task"], [2103, 2104, "Task"], [2107, 2110, "Method"], [2168, 2173, "Method"], [2191, 2193, "Method"], [2201, 2202, "Method"], [2261, 2263, "Method"], [2284, 2286, "Method"], [2322, 2324, "Method"], [2410, 2412, "Method"], [2445, 2449, "Method"], [2451, 2452, "Method"], [2453, 2455, "Method"], [2479, 2481, "Method"], [2484, 2489, "Method"], [2494, 2497, "Method"], [2506, 2508, "Metric"], [2548, 2553, "Task"], [2559, 2562, "Task"], [2651, 2655, "Method"], [2678, 2680, "Metric"], [2681, 2685, "Method"], [2708, 2710, "Method"], [2721, 2723, "Task"], [2726, 2729, "Method"], [2731, 2734, "Method"], [2739, 2740, "Task"], [2748, 2751, "Task"], [2794, 2796, "Method"], [2849, 2851, "Method"], [2864, 2866, "Method"], [2872, 2873, "Metric"], [2883, 2884, "Metric"], [2886, 2888, "Method"], [2895, 2899, "Method"], [2901, 2908, "Method"], [2999, 3005, "Material"], [3043, 3046, "Material"], [3052, 3058, "Material"], [3118, 3121, "Method"], [3124, 3126, "Method"], [3145, 3147, "Method"], [3161, 3163, "Method"], [3183, 3185, "Method"], [3203, 3205, "Method"], [3213, 3215, "Metric"], [3231, 3233, "Metric"], [3260, 3261, "Task"], [3266, 3267, "Method"], [3268, 3269, "Task"], [3276, 3277, "Method"], [3305, 3306, "Method"], [3314, 3316, "Method"], [3326, 3328, "Method"], [3337, 3339, "Method"], [3340, 3341, "Task"], [3344, 3349, "Method"], [3371, 3372, "Metric"], [3373, 3375, "Metric"], [3381, 3383, "Task"], [3385, 3394, "Task"], [3396, 3399, "Method"], [3418, 3419, "Method"], [3423, 3424, "Metric"], [3435, 3437, "Metric"], [3460, 3462, "Method"], [3465, 3467, "Method"], [3480, 3482, "Metric"], [3486, 3488, "Method"], [3491, 3500, "Task"], [3502, 3504, "Method"], [3506, 3508, "Metric"], [3523, 3525, "Metric"], [3545, 3550, "Material"], [3559, 3561, "Method"], [3586, 3588, "Method"], [3598, 3600, "Method"], [3628, 3629, "Task"], [3651, 3653, "Metric"], [3654, 3656, "Metric"], [3680, 3682, "Metric"], [3719, 3720, "Method"], [3724, 3726, "Method"], [3736, 3742, "Task"], [3751, 3753, "Method"], [3762, 3764, "Method"], [3816, 3820, "Method"], [3822, 3823, "Metric"], [3829, 3830, "Metric"], [3853, 3856, "Metric"], [3857, 3859, "Metric"], [3876, 3878, "Method"], [3905, 3906, "Method"], [3911, 3914, "Task"], [3923, 3926, "Method"], [3928, 3930, "Method"], [3939, 3941, "Method"], [3944, 3947, "Task"], [3951, 3952, "Method"], [3961, 3964, "Task"], [3989, 3996, "Method"], [4011, 4015, "Task"], [4019, 4023, "Method"], [4035, 4037, "Material"], [4051, 4055, "Task"], [4083, 4086, "Task"], [4095, 4099, "Task"], [4124, 4126, "Metric"], [4127, 4129, "Metric"], [4143, 4151, "Method"], [4153, 4154, "Task"], [4175, 4177, "Task"], [4179, 4183, "Task"], [4220, 4224, "Method"], [4228, 4235, "Method"], [4238, 4239, "Method"], [4241, 4242, "Method"], [4266, 4267, "Method"], [4270, 4273, "Method"], [4276, 4277, "Method"], [4286, 4290, "Method"], [4299, 4301, "Task"], [4303, 4304, "Method"], [4320, 4327, "Task"], [4328, 4337, "Task"], [4368, 4372, "Method"], [4386, 4387, "Method"], [4400, 4405, "Method"], [4421, 4422, "Task"], [4471, 4473, "Method"], [16, 17, "Method"], [222, 223, "Method"], [268, 269, "Method"], [451, 452, "Method"], [737, 738, "Method"], [963, 964, "Method"], [2669, 2670, "Method"], [2688, 2689, "Method"], [2816, 2818, "Method"], [2856, 2857, "Method"], [3017, 3020, "Method"], [3400, 3401, "Method"], [3538, 3539, "Method"], [3551, 3552, "Method"], [4111, 4112, "Method"], [4280, 4281, "Method"], [4313, 4314, "Method"], [4315, 4317, "Method"]], "sections": [[0, 207], [207, 517], [517, 833], [833, 955], [955, 961], [961, 1083], [1083, 1214], [1214, 1286], [1286, 1561], [1561, 1753], [1753, 1991], [1991, 2099], [2099, 2189], [2189, 2431], [2431, 2975], [2975, 2988], [2988, 3104], [3104, 3197], [3197, 3258], [3258, 3270], [3270, 3335], [3335, 3376], [3376, 3379], [3379, 3706], [3706, 3942], [3942, 4256], [4256, 4445], [4445, 4466], [4466, 4469], [4469, 4473]], "sentences": [[0, 7], [7, 29], [29, 44], [44, 68], [68, 94], [94, 126], [126, 181], [181, 207], [207, 210], [210, 252], [252, 254], [254, 258], [258, 276], [276, 282], [282, 298], [298, 329], [329, 355], [355, 384], [384, 395], [395, 432], [432, 454], [454, 484], [484, 517], [517, 520], [520, 567], [567, 601], [601, 614], [614, 660], [660, 690], [690, 726], [726, 765], [765, 804], [804, 833], [833, 837], [837, 855], [855, 873], [873, 890], [890, 914], [914, 955], [955, 961], [961, 965], [965, 976], [976, 984], [984, 1013], [1013, 1018], [1018, 1033], [1033, 1057], [1057, 1083], [1083, 1087], [1087, 1099], [1099, 1135], [1135, 1157], [1157, 1180], [1180, 1214], [1214, 1217], [1217, 1253], [1253, 1286], [1286, 1293], [1293, 1312], [1312, 1327], [1327, 1356], [1356, 1377], [1377, 1389], [1389, 1418], [1418, 1435], [1435, 1453], [1453, 1489], [1489, 1520], [1520, 1549], [1549, 1561], [1561, 1567], [1567, 1576], [1576, 1619], [1619, 1646], [1646, 1668], [1668, 1688], [1688, 1698], [1698, 1708], [1708, 1720], [1720, 1727], [1727, 1753], [1753, 1761], [1761, 1805], [1805, 1822], [1822, 1846], [1846, 1854], [1854, 1888], [1888, 1906], [1906, 1936], [1936, 1954], [1954, 1985], [1985, 1991], [1991, 2000], [2000, 2036], [2036, 2049], [2049, 2069], [2069, 2083], [2083, 2099], [2099, 2104], [2104, 2128], [2128, 2151], [2151, 2178], [2178, 2189], [2189, 2193], [2193, 2236], [2236, 2260], [2260, 2279], [2279, 2292], [2292, 2315], [2315, 2329], [2329, 2338], [2338, 2374], [2374, 2407], [2407, 2431], [2431, 2437], [2437, 2490], [2490, 2502], [2502, 2511], [2511, 2533], [2533, 2548], [2548, 2563], [2563, 2590], [2590, 2618], [2618, 2642], [2642, 2675], [2675, 2737], [2737, 2772], [2772, 2781], [2781, 2792], [2792, 2810], [2810, 2849], [2849, 2864], [2864, 2877], [2877, 2917], [2917, 2930], [2930, 2946], [2946, 2975], [2975, 2978], [2978, 2988], [2988, 2994], [2994, 3013], [3013, 3042], [3042, 3075], [3075, 3085], [3085, 3104], [3104, 3109], [3109, 3122], [3122, 3142], [3142, 3159], [3159, 3182], [3182, 3197], [3197, 3200], [3200, 3210], [3210, 3255], [3255, 3258], [3258, 3261], [3261, 3270], [3270, 3274], [3274, 3300], [3300, 3324], [3324, 3335], [3335, 3339], [3339, 3356], [3356, 3376], [3376, 3379], [3379, 3383], [3383, 3439], [3439, 3456], [3456, 3463], [3463, 3489], [3489, 3537], [3537, 3557], [3557, 3584], [3584, 3596], [3596, 3610], [3610, 3621], [3621, 3641], [3641, 3664], [3664, 3706], [3706, 3710], [3710, 3749], [3749, 3765], [3765, 3775], [3775, 3815], [3815, 3838], [3838, 3860], [3860, 3882], [3882, 3915], [3915, 3942], [3942, 3947], [3947, 3965], [3965, 3987], [3987, 4016], [4016, 4045], [4045, 4073], [4073, 4100], [4100, 4152], [4152, 4166], [4166, 4184], [4184, 4225], [4225, 4256], [4256, 4259], [4259, 4298], [4298, 4318], [4318, 4347], [4347, 4361], [4361, 4381], [4381, 4420], [4420, 4431], [4431, 4445], [4445, 4448], [4448, 4466], [4466, 4469], [4469, 4473]], "words": ["document", ":", "Attention", "Is", "All", "You", "Need", "The", "dominant", "sequence", "transduction", "models", "are", "based", "on", "complex", "recurrent", "or", "convolutional", "neural", "networks", "that", "include", "an", "encoder", "and", "a", "decoder", ".", "The", "best", "performing", "models", "also", "connect", "the", "encoder", "and", "decoder", "through", "an", "attention", "mechanism", ".", "We", "propose", "a", "new", "simple", "network", "architecture", ",", "the", "Transformer", ",", "based", "solely", "on", "attention", "mechanisms", ",", "dispensing", "with", "recurrence", "and", "convolutions", "entirely", ".", "Experiments", "on", "two", "machine", "translation", "tasks", "show", "these", "models", "to", "be", "superior", "in", "quality", "while", "being", "more", "parallelizable", "and", "requiring", "significantly", "less", "time", "to", "train", ".", "Our", "model", "achieves", "28.4", "BLEU", "on", "the", "WMT", "2014", "English", "-", "to", "-", "German", "translation", "task", ",", "improving", "over", "the", "existing", "best", "results", ",", "including", "ensembles", ",", "by", "over", "2", "BLEU", ".", "On", "the", "WMT", "2014", "English", "-", "to", "-", "French", "translation", "task", ",", "our", "model", "establishes", "a", "new", "single", "-", "model", "state", "-", "of", "-", "the", "-", "art", "BLEU", "score", "of", "41.8", "after", "training", "for", "3.5", "days", "on", "eight", "GPUs", ",", "a", "small", "fraction", "of", "the", "training", "costs", "of", "the", "best", "models", "from", "the", "literature", ".", "We", "show", "that", "the", "Transformer", "generalizes", "well", "to", "other", "tasks", "by", "applying", "it", "successfully", "to", "English", "constituency", "parsing", "both", "with", "large", "and", "limited", "training", "data", ".", "section", ":", "Introduction", "Recurrent", "neural", "networks", ",", "long", "short", "-", "term", "memory", "hochreiter1997", "and", "gated", "recurrent", "gruEval14", "neural", "networks", "in", "particular", ",", "have", "been", "firmly", "established", "as", "state", "of", "the", "art", "approaches", "in", "sequence", "modeling", "and", "transduction", "problems", "such", "as", "language", "modeling", "and", "machine", "translation", "sutskever14", ",", "bahdanau2014neural", ",", "cho2014learning", ".", "Numerous", "efforts", "have", "since", "continued", "to", "push", "the", "boundaries", "of", "recurrent", "language", "models", "and", "encoder", "-", "decoder", "architectures", "wu2016google", ",", "luong2015effective", ",", "jozefowicz2016exploring", ".", "Recurrent", "models", "typically", "factor", "computation", "along", "the", "symbol", "positions", "of", "the", "input", "and", "output", "sequences", ".", "Aligning", "the", "positions", "to", "steps", "in", "computation", "time", ",", "they", "generate", "a", "sequence", "of", "hidden", "states", ",", "as", "a", "function", "of", "the", "previous", "hidden", "state", "and", "the", "input", "for", "position", ".", "This", "inherently", "sequential", "nature", "precludes", "parallelization", "within", "training", "examples", ",", "which", "becomes", "critical", "at", "longer", "sequence", "lengths", ",", "as", "memory", "constraints", "limit", "batching", "across", "examples", ".", "Recent", "work", "has", "achieved", "significant", "improvements", "in", "computational", "efficiency", "through", "factorization", "tricks", "Kuchaiev2017Factorization", "and", "conditional", "computation", "shazeer2017outrageously", ",", "while", "also", "improving", "model", "performance", "in", "case", "of", "the", "latter", ".", "The", "fundamental", "constraint", "of", "sequential", "computation", ",", "however", ",", "remains", ".", "Attention", "mechanisms", "have", "become", "an", "integral", "part", "of", "compelling", "sequence", "modeling", "and", "transduction", "models", "in", "various", "tasks", ",", "allowing", "modeling", "of", "dependencies", "without", "regard", "to", "their", "distance", "in", "the", "input", "or", "output", "sequences", "bahdanau2014neural", ",", "structuredAttentionNetworks", ".", "In", "all", "but", "a", "few", "cases", "decomposableAttnModel", ",", "however", ",", "such", "attention", "mechanisms", "are", "used", "in", "conjunction", "with", "a", "recurrent", "network", ".", "In", "this", "work", "we", "propose", "the", "Transformer", ",", "a", "model", "architecture", "eschewing", "recurrence", "and", "instead", "relying", "entirely", "on", "an", "attention", "mechanism", "to", "draw", "global", "dependencies", "between", "input", "and", "output", ".", "The", "Transformer", "allows", "for", "significantly", "more", "parallelization", "and", "can", "reach", "a", "new", "state", "of", "the", "art", "in", "translation", "quality", "after", "being", "trained", "for", "as", "little", "as", "twelve", "hours", "on", "eight", "P100", "GPUs", ".", "section", ":", "Background", "The", "goal", "of", "reducing", "sequential", "computation", "also", "forms", "the", "foundation", "of", "the", "Extended", "Neural", "GPU", "extendedngpu", ",", "ByteNet", "NalBytenet2017", "and", "ConvS2S", "JonasFaceNet2017", ",", "all", "of", "which", "use", "convolutional", "neural", "networks", "as", "basic", "building", "block", ",", "computing", "hidden", "representations", "in", "parallel", "for", "all", "input", "and", "output", "positions", ".", "In", "these", "models", ",", "the", "number", "of", "operations", "required", "to", "relate", "signals", "from", "two", "arbitrary", "input", "or", "output", "positions", "grows", "in", "the", "distance", "between", "positions", ",", "linearly", "for", "ConvS2S", "and", "logarithmically", "for", "ByteNet", ".", "This", "makes", "it", "more", "difficult", "to", "learn", "dependencies", "between", "distant", "positions", "hochreiter2001gradient", ".", "In", "the", "Transformer", "this", "is", "reduced", "to", "a", "constant", "number", "of", "operations", ",", "albeit", "at", "the", "cost", "of", "reduced", "effective", "resolution", "due", "to", "averaging", "attention", "-", "weighted", "positions", ",", "an", "effect", "we", "counteract", "with", "Multi", "-", "Head", "Attention", "as", "described", "in", "section", "[", "reference", "]", ".", "Self", "-", "attention", ",", "sometimes", "called", "intra", "-", "attention", "is", "an", "attention", "mechanism", "relating", "different", "positions", "of", "a", "single", "sequence", "in", "order", "to", "compute", "a", "representation", "of", "the", "sequence", ".", "Self", "-", "attention", "has", "been", "used", "successfully", "in", "a", "variety", "of", "tasks", "including", "reading", "comprehension", ",", "abstractive", "summarization", ",", "textual", "entailment", "and", "learning", "task", "-", "independent", "sentence", "representations", "cheng2016long", ",", "decomposableAttnModel", ",", "paulus2017deep", ",", "lin2017structured", ".", "End", "-", "to", "-", "end", "memory", "networks", "are", "based", "on", "a", "recurrent", "attention", "mechanism", "instead", "of", "sequence", "-", "aligned", "recurrence", "and", "have", "been", "shown", "to", "perform", "well", "on", "simple", "-", "language", "question", "answering", "and", "language", "modeling", "tasks", "sukhbaatar2015", ".", "To", "the", "best", "of", "our", "knowledge", ",", "however", ",", "the", "Transformer", "is", "the", "first", "transduction", "model", "relying", "entirely", "on", "self", "-", "attention", "to", "compute", "representations", "of", "its", "input", "and", "output", "without", "using", "sequence", "-", "aligned", "RNNs", "or", "convolution", ".", "In", "the", "following", "sections", ",", "we", "will", "describe", "the", "Transformer", ",", "motivate", "self", "-", "attention", "and", "discuss", "its", "advantages", "over", "models", "such", "as", "neural_gpu", ",", "NalBytenet2017", "and", "JonasFaceNet2017", ".", "section", ":", "Model", "Architecture", "Most", "competitive", "neural", "sequence", "transduction", "models", "have", "an", "encoder", "-", "decoder", "structure", "cho2014learning", ",", "bahdanau2014neural", ",", "sutskever14", ".", "Here", ",", "the", "encoder", "maps", "an", "input", "sequence", "of", "symbol", "representations", "to", "a", "sequence", "of", "continuous", "representations", ".", "Given", ",", "the", "decoder", "then", "generates", "an", "output", "sequence", "of", "symbols", "one", "element", "at", "a", "time", ".", "At", "each", "step", "the", "model", "is", "auto", "-", "regressive", "graves2013generating", ",", "consuming", "the", "previously", "generated", "symbols", "as", "additional", "input", "when", "generating", "the", "next", ".", "The", "Transformer", "follows", "this", "overall", "architecture", "using", "stacked", "self", "-", "attention", "and", "point", "-", "wise", ",", "fully", "connected", "layers", "for", "both", "the", "encoder", "and", "decoder", ",", "shown", "in", "the", "left", "and", "right", "halves", "of", "Figure", "[", "reference", "]", ",", "respectively", ".", "subsection", ":", "Encoder", "and", "Decoder", "Stacks", "paragraph", ":", "Encoder", ":", "The", "encoder", "is", "composed", "of", "a", "stack", "of", "identical", "layers", ".", "Each", "layer", "has", "two", "sub", "-", "layers", ".", "The", "first", "is", "a", "multi", "-", "head", "self", "-", "attention", "mechanism", ",", "and", "the", "second", "is", "a", "simple", ",", "position", "-", "wise", "fully", "connected", "feed", "-", "forward", "network", ".", "We", "employ", "a", "residual", "connection", "he2016deep", "around", "each", "of", "the", "two", "sub", "-", "layers", ",", "followed", "by", "layer", "normalization", ".", "That", "is", ",", "the", "output", "of", "each", "sub", "-", "layer", "is", ",", "where", "is", "the", "function", "implemented", "by", "the", "sub", "-", "layer", "itself", ".", "To", "facilitate", "these", "residual", "connections", ",", "all", "sub", "-", "layers", "in", "the", "model", ",", "as", "well", "as", "the", "embedding", "layers", ",", "produce", "outputs", "of", "dimension", ".", "paragraph", ":", "Decoder", ":", "The", "decoder", "is", "also", "composed", "of", "a", "stack", "of", "identical", "layers", ".", "In", "addition", "to", "the", "two", "sub", "-", "layers", "in", "each", "encoder", "layer", ",", "the", "decoder", "inserts", "a", "third", "sub", "-", "layer", ",", "which", "performs", "multi", "-", "head", "attention", "over", "the", "output", "of", "the", "encoder", "stack", ".", "Similar", "to", "the", "encoder", ",", "we", "employ", "residual", "connections", "around", "each", "of", "the", "sub", "-", "layers", ",", "followed", "by", "layer", "normalization", ".", "We", "also", "modify", "the", "self", "-", "attention", "sub", "-", "layer", "in", "the", "decoder", "stack", "to", "prevent", "positions", "from", "attending", "to", "subsequent", "positions", ".", "This", "masking", ",", "combined", "with", "fact", "that", "the", "output", "embeddings", "are", "offset", "by", "one", "position", ",", "ensures", "that", "the", "predictions", "for", "position", "can", "depend", "only", "on", "the", "known", "outputs", "at", "positions", "less", "than", ".", "subsection", ":", "Attention", "An", "attention", "function", "can", "be", "described", "as", "mapping", "a", "query", "and", "a", "set", "of", "key", "-", "value", "pairs", "to", "an", "output", ",", "where", "the", "query", ",", "keys", ",", "values", ",", "and", "output", "are", "all", "vectors", ".", "The", "output", "is", "computed", "as", "a", "weighted", "sum", "of", "the", "values", ",", "where", "the", "weight", "assigned", "to", "each", "value", "is", "computed", "by", "a", "compatibility", "function", "of", "the", "query", "with", "the", "corresponding", "key", ".", "subsubsection", ":", "Scaled", "Dot", "-", "Product", "Attention", "We", "call", "our", "particular", "attention", "\"", "Scaled", "Dot", "-", "Product", "Attention", "\"", "(", "Figure", "[", "reference", "]", ")", ".", "The", "input", "consists", "of", "queries", "and", "keys", "of", "dimension", ",", "and", "values", "of", "dimension", ".", "We", "compute", "the", "dot", "products", "of", "the", "query", "with", "all", "keys", ",", "divide", "each", "by", ",", "and", "apply", "a", "softmax", "function", "to", "obtain", "the", "weights", "on", "the", "values", ".", "In", "practice", ",", "we", "compute", "the", "attention", "function", "on", "a", "set", "of", "queries", "simultaneously", ",", "packed", "together", "into", "a", "matrix", ".", "The", "keys", "and", "values", "are", "also", "packed", "together", "into", "matrices", "and", ".", "We", "compute", "the", "matrix", "of", "outputs", "as", ":", "The", "two", "most", "commonly", "used", "attention", "functions", "are", "additive", "attention", "bahdanau2014neural", ",", "and", "dot", "-", "product", "(", "multiplicative", ")", "attention", ".", "Dot", "-", "product", "attention", "is", "identical", "to", "our", "algorithm", ",", "except", "for", "the", "scaling", "factor", "of", ".", "Additive", "attention", "computes", "the", "compatibility", "function", "using", "a", "feed", "-", "forward", "network", "with", "a", "single", "hidden", "layer", ".", "While", "the", "two", "are", "similar", "in", "theoretical", "complexity", ",", "dot", "-", "product", "attention", "is", "much", "faster", "and", "more", "space", "-", "efficient", "in", "practice", ",", "since", "it", "can", "be", "implemented", "using", "highly", "optimized", "matrix", "multiplication", "code", ".", "While", "for", "small", "values", "of", "the", "two", "mechanisms", "perform", "similarly", ",", "additive", "attention", "outperforms", "dot", "product", "attention", "without", "scaling", "for", "larger", "values", "of", "DBLP", ":", "journals", "/", "corr", "/", "BritzGLL17", ".", "We", "suspect", "that", "for", "large", "values", "of", ",", "the", "dot", "products", "grow", "large", "in", "magnitude", ",", "pushing", "the", "softmax", "function", "into", "regions", "where", "it", "has", "extremely", "small", "gradients", ".", "To", "counteract", "this", "effect", ",", "we", "scale", "the", "dot", "products", "by", ".", "subsubsection", ":", "Multi", "-", "Head", "Attention", "Scaled", "Dot", "-", "Product", "Attention", "Multi", "-", "Head", "Attention", "Instead", "of", "performing", "a", "single", "attention", "function", "with", "-", "dimensional", "keys", ",", "values", "and", "queries", ",", "we", "found", "it", "beneficial", "to", "linearly", "project", "the", "queries", ",", "keys", "and", "values", "times", "with", "different", ",", "learned", "linear", "projections", "to", ",", "and", "dimensions", ",", "respectively", ".", "On", "each", "of", "these", "projected", "versions", "of", "queries", ",", "keys", "and", "values", "we", "then", "perform", "the", "attention", "function", "in", "parallel", ",", "yielding", "-", "dimensional", "output", "values", ".", "These", "are", "concatenated", "and", "once", "again", "projected", ",", "resulting", "in", "the", "final", "values", ",", "as", "depicted", "in", "Figure", "[", "reference", "]", ".", "Multi", "-", "head", "attention", "allows", "the", "model", "to", "jointly", "attend", "to", "information", "from", "different", "representation", "subspaces", "at", "different", "positions", ".", "With", "a", "single", "attention", "head", ",", "averaging", "inhibits", "this", ".", "Where", "the", "projections", "are", "parameter", "matrices", ",", ",", "and", ".", "In", "this", "work", "we", "employ", "parallel", "attention", "layers", ",", "or", "heads", ".", "For", "each", "of", "these", "we", "use", ".", "Due", "to", "the", "reduced", "dimension", "of", "each", "head", ",", "the", "total", "computational", "cost", "is", "similar", "to", "that", "of", "single", "-", "head", "attention", "with", "full", "dimensionality", ".", "subsubsection", ":", "Applications", "of", "Attention", "in", "our", "Model", "The", "Transformer", "uses", "multi", "-", "head", "attention", "in", "three", "different", "ways", ":", "In", "\"", "encoder", "-", "decoder", "attention", "\"", "layers", ",", "the", "queries", "come", "from", "the", "previous", "decoder", "layer", ",", "and", "the", "memory", "keys", "and", "values", "come", "from", "the", "output", "of", "the", "encoder", ".", "This", "allows", "every", "position", "in", "the", "decoder", "to", "attend", "over", "all", "positions", "in", "the", "input", "sequence", ".", "This", "mimics", "the", "typical", "encoder", "-", "decoder", "attention", "mechanisms", "in", "sequence", "-", "to", "-", "sequence", "models", "such", "as", "wu2016google", ",", "bahdanau2014neural", ",", "JonasFaceNet2017", ".", "The", "encoder", "contains", "self", "-", "attention", "layers", ".", "In", "a", "self", "-", "attention", "layer", "all", "of", "the", "keys", ",", "values", "and", "queries", "come", "from", "the", "same", "place", ",", "in", "this", "case", ",", "the", "output", "of", "the", "previous", "layer", "in", "the", "encoder", ".", "Each", "position", "in", "the", "encoder", "can", "attend", "to", "all", "positions", "in", "the", "previous", "layer", "of", "the", "encoder", ".", "Similarly", ",", "self", "-", "attention", "layers", "in", "the", "decoder", "allow", "each", "position", "in", "the", "decoder", "to", "attend", "to", "all", "positions", "in", "the", "decoder", "up", "to", "and", "including", "that", "position", ".", "We", "need", "to", "prevent", "leftward", "information", "flow", "in", "the", "decoder", "to", "preserve", "the", "auto", "-", "regressive", "property", ".", "We", "implement", "this", "inside", "of", "scaled", "dot", "-", "product", "attention", "by", "masking", "out", "(", "setting", "to", ")", "all", "values", "in", "the", "input", "of", "the", "softmax", "which", "correspond", "to", "illegal", "connections", ".", "See", "Figure", "[", "reference", "]", ".", "subsection", ":", "Position", "-", "wise", "Feed", "-", "Forward", "Networks", "In", "addition", "to", "attention", "sub", "-", "layers", ",", "each", "of", "the", "layers", "in", "our", "encoder", "and", "decoder", "contains", "a", "fully", "connected", "feed", "-", "forward", "network", ",", "which", "is", "applied", "to", "each", "position", "separately", "and", "identically", ".", "This", "consists", "of", "two", "linear", "transformations", "with", "a", "ReLU", "activation", "in", "between", ".", "While", "the", "linear", "transformations", "are", "the", "same", "across", "different", "positions", ",", "they", "use", "different", "parameters", "from", "layer", "to", "layer", ".", "Another", "way", "of", "describing", "this", "is", "as", "two", "convolutions", "with", "kernel", "size", "1", ".", "The", "dimensionality", "of", "input", "and", "output", "is", ",", "and", "the", "inner", "-", "layer", "has", "dimensionality", ".", "subsection", ":", "Embeddings", "and", "Softmax", "Similarly", "to", "other", "sequence", "transduction", "models", ",", "we", "use", "learned", "embeddings", "to", "convert", "the", "input", "tokens", "and", "output", "tokens", "to", "vectors", "of", "dimension", ".", "We", "also", "use", "the", "usual", "learned", "linear", "transformation", "and", "softmax", "function", "to", "convert", "the", "decoder", "output", "to", "predicted", "next", "-", "token", "probabilities", ".", "In", "our", "model", ",", "we", "share", "the", "same", "weight", "matrix", "between", "the", "two", "embedding", "layers", "and", "the", "pre", "-", "softmax", "linear", "transformation", ",", "similar", "to", "press2016using", ".", "In", "the", "embedding", "layers", ",", "we", "multiply", "those", "weights", "by", ".", "subsection", ":", "Positional", "Encoding", "Since", "our", "model", "contains", "no", "recurrence", "and", "no", "convolution", ",", "in", "order", "for", "the", "model", "to", "make", "use", "of", "the", "order", "of", "the", "sequence", ",", "we", "must", "inject", "some", "information", "about", "the", "relative", "or", "absolute", "position", "of", "the", "tokens", "in", "the", "sequence", ".", "To", "this", "end", ",", "we", "add", "\"", "positional", "encodings", "\"", "to", "the", "input", "embeddings", "at", "the", "bottoms", "of", "the", "encoder", "and", "decoder", "stacks", ".", "The", "positional", "encodings", "have", "the", "same", "dimension", "as", "the", "embeddings", ",", "so", "that", "the", "two", "can", "be", "summed", ".", "There", "are", "many", "choices", "of", "positional", "encodings", ",", "learned", "and", "fixed", "JonasFaceNet2017", ".", "In", "this", "work", ",", "we", "use", "sine", "and", "cosine", "functions", "of", "different", "frequencies", ":", "where", "is", "the", "position", "and", "is", "the", "dimension", ".", "That", "is", ",", "each", "dimension", "of", "the", "positional", "encoding", "corresponds", "to", "a", "sinusoid", ".", "The", "wavelengths", "form", "a", "geometric", "progression", "from", "to", ".", "We", "chose", "this", "function", "because", "we", "hypothesized", "it", "would", "allow", "the", "model", "to", "easily", "learn", "to", "attend", "by", "relative", "positions", ",", "since", "for", "any", "fixed", "offset", ",", "can", "be", "represented", "as", "a", "linear", "function", "of", ".", "We", "also", "experimented", "with", "using", "learned", "positional", "embeddings", "JonasFaceNet2017", "instead", ",", "and", "found", "that", "the", "two", "versions", "produced", "nearly", "identical", "results", "(", "see", "Table", "[", "reference", "]", "row", "(", "E", ")", ")", ".", "We", "chose", "the", "sinusoidal", "version", "because", "it", "may", "allow", "the", "model", "to", "extrapolate", "to", "sequence", "lengths", "longer", "than", "the", "ones", "encountered", "during", "training", ".", "section", ":", "Why", "Self", "-", "Attention", "In", "this", "section", "we", "compare", "various", "aspects", "of", "self", "-", "attention", "layers", "to", "the", "recurrent", "and", "convolutional", "layers", "commonly", "used", "for", "mapping", "one", "variable", "-", "length", "sequence", "of", "symbol", "representations", "to", "another", "sequence", "of", "equal", "length", ",", "with", ",", "such", "as", "a", "hidden", "layer", "in", "a", "typical", "sequence", "transduction", "encoder", "or", "decoder", ".", "Motivating", "our", "use", "of", "self", "-", "attention", "we", "consider", "three", "desiderata", ".", "One", "is", "the", "total", "computational", "complexity", "per", "layer", ".", "Another", "is", "the", "amount", "of", "computation", "that", "can", "be", "parallelized", ",", "as", "measured", "by", "the", "minimum", "number", "of", "sequential", "operations", "required", ".", "The", "third", "is", "the", "path", "length", "between", "long", "-", "range", "dependencies", "in", "the", "network", ".", "Learning", "long", "-", "range", "dependencies", "is", "a", "key", "challenge", "in", "many", "sequence", "transduction", "tasks", ".", "One", "key", "factor", "affecting", "the", "ability", "to", "learn", "such", "dependencies", "is", "the", "length", "of", "the", "paths", "forward", "and", "backward", "signals", "have", "to", "traverse", "in", "the", "network", ".", "The", "shorter", "these", "paths", "between", "any", "combination", "of", "positions", "in", "the", "input", "and", "output", "sequences", ",", "the", "easier", "it", "is", "to", "learn", "long", "-", "range", "dependencies", "hochreiter2001gradient", ".", "Hence", "we", "also", "compare", "the", "maximum", "path", "length", "between", "any", "two", "input", "and", "output", "positions", "in", "networks", "composed", "of", "the", "different", "layer", "types", ".", "As", "noted", "in", "Table", "[", "reference", "]", ",", "a", "self", "-", "attention", "layer", "connects", "all", "positions", "with", "a", "constant", "number", "of", "sequentially", "executed", "operations", ",", "whereas", "a", "recurrent", "layer", "requires", "sequential", "operations", ".", "In", "terms", "of", "computational", "complexity", ",", "self", "-", "attention", "layers", "are", "faster", "than", "recurrent", "layers", "when", "the", "sequence", "length", "is", "smaller", "than", "the", "representation", "dimensionality", ",", "which", "is", "most", "often", "the", "case", "with", "sentence", "representations", "used", "by", "state", "-", "of", "-", "the", "-", "art", "models", "in", "machine", "translations", ",", "such", "as", "word", "-", "piece", "wu2016google", "and", "byte", "-", "pair", "sennrich2015neural", "representations", ".", "To", "improve", "computational", "performance", "for", "tasks", "involving", "very", "long", "sequences", ",", "self", "-", "attention", "could", "be", "restricted", "to", "considering", "only", "a", "neighborhood", "of", "size", "in", "the", "input", "sequence", "centered", "around", "the", "respective", "output", "position", ".", "This", "would", "increase", "the", "maximum", "path", "length", "to", ".", "We", "plan", "to", "investigate", "this", "approach", "further", "in", "future", "work", ".", "A", "single", "convolutional", "layer", "with", "kernel", "width", "does", "not", "connect", "all", "pairs", "of", "input", "and", "output", "positions", ".", "Doing", "so", "requires", "a", "stack", "of", "convolutional", "layers", "in", "the", "case", "of", "contiguous", "kernels", ",", "or", "in", "the", "case", "of", "dilated", "convolutions", "NalBytenet2017", ",", "increasing", "the", "length", "of", "the", "longest", "paths", "between", "any", "two", "positions", "in", "the", "network", ".", "Convolutional", "layers", "are", "generally", "more", "expensive", "than", "recurrent", "layers", ",", "by", "a", "factor", "of", ".", "Separable", "convolutions", "xception2016", ",", "however", ",", "decrease", "the", "complexity", "considerably", ",", "to", ".", "Even", "with", ",", "however", ",", "the", "complexity", "of", "a", "separable", "convolution", "is", "equal", "to", "the", "combination", "of", "a", "self", "-", "attention", "layer", "and", "a", "point", "-", "wise", "feed", "-", "forward", "layer", ",", "the", "approach", "we", "take", "in", "our", "model", ".", "As", "side", "benefit", ",", "self", "-", "attention", "could", "yield", "more", "interpretable", "models", ".", "We", "inspect", "attention", "distributions", "from", "our", "models", "and", "present", "and", "discuss", "examples", "in", "the", "appendix", ".", "Not", "only", "do", "individual", "attention", "heads", "clearly", "learn", "to", "perform", "different", "tasks", ",", "many", "appear", "to", "exhibit", "behavior", "related", "to", "the", "syntactic", "and", "semantic", "structure", "of", "the", "sentences", ".", "section", ":", "Training", "This", "section", "describes", "the", "training", "regime", "for", "our", "models", ".", "subsection", ":", "Training", "Data", "and", "Batching", "We", "trained", "on", "the", "standard", "WMT", "2014", "English", "-", "German", "dataset", "consisting", "of", "about", "4.5", "million", "sentence", "pairs", ".", "Sentences", "were", "encoded", "using", "byte", "-", "pair", "encoding", "DBLP", ":", "journals", "/", "corr", "/", "BritzGLL17", ",", "which", "has", "a", "shared", "source", "-", "target", "vocabulary", "of", "about", "37000", "tokens", ".", "For", "English", "-", "French", ",", "we", "used", "the", "significantly", "larger", "WMT", "2014", "English", "-", "French", "dataset", "consisting", "of", "36", "M", "sentences", "and", "split", "tokens", "into", "a", "32000", "word", "-", "piece", "vocabulary", "wu2016google", ".", "Sentence", "pairs", "were", "batched", "together", "by", "approximate", "sequence", "length", ".", "Each", "training", "batch", "contained", "a", "set", "of", "sentence", "pairs", "containing", "approximately", "25000", "source", "tokens", "and", "25000", "target", "tokens", ".", "subsection", ":", "Hardware", "and", "Schedule", "We", "trained", "our", "models", "on", "one", "machine", "with", "8", "NVIDIA", "P100", "GPUs", ".", "For", "our", "base", "models", "using", "the", "hyperparameters", "described", "throughout", "the", "paper", ",", "each", "training", "step", "took", "about", "0.4", "seconds", ".", "We", "trained", "the", "base", "models", "for", "a", "total", "of", "100", ",", "000", "steps", "or", "12", "hours", ".", "For", "our", "big", "models", ",(", "described", "on", "the", "bottom", "line", "of", "table", "[", "reference", "]", ")", ",", "step", "time", "was", "1.0", "seconds", ".", "The", "big", "models", "were", "trained", "for", "300", ",", "000", "steps", "(", "3.5", "days", ")", ".", "subsection", ":", "Optimizer", "We", "used", "the", "Adam", "optimizer", "kingma2014adam", "with", ",", "and", ".", "We", "varied", "the", "learning", "rate", "over", "the", "course", "of", "training", ",", "according", "to", "the", "formula", ":", "This", "corresponds", "to", "increasing", "the", "learning", "rate", "linearly", "for", "the", "first", "training", "steps", ",", "and", "decreasing", "it", "thereafter", "proportionally", "to", "the", "inverse", "square", "root", "of", "the", "step", "number", ".", "We", "used", ".", "subsection", ":", "Regularization", "We", "employ", "three", "types", "of", "regularization", "during", "training", ":", "paragraph", ":", "Residual", "Dropout", "We", "apply", "dropout", "srivastava2014dropout", "to", "the", "output", "of", "each", "sub", "-", "layer", ",", "before", "it", "is", "added", "to", "the", "sub", "-", "layer", "input", "and", "normalized", ".", "In", "addition", ",", "we", "apply", "dropout", "to", "the", "sums", "of", "the", "embeddings", "and", "the", "positional", "encodings", "in", "both", "the", "encoder", "and", "decoder", "stacks", ".", "For", "the", "base", "model", ",", "we", "use", "a", "rate", "of", ".", "paragraph", ":", "Label", "Smoothing", "During", "training", ",", "we", "employed", "label", "smoothing", "of", "value", "DBLP", ":", "journals", "/", "corr", "/", "SzegedyVISW15", ".", "This", "hurts", "perplexity", ",", "as", "the", "model", "learns", "to", "be", "more", "unsure", ",", "but", "improves", "accuracy", "and", "BLEU", "score", ".", "section", ":", "Results", "subsection", ":", "Machine", "Translation", "On", "the", "WMT", "2014", "English", "-", "to", "-", "German", "translation", "task", ",", "the", "big", "transformer", "model", "(", "Transformer", "(", "big", ")", "in", "Table", "[", "reference", "]", ")", "outperforms", "the", "best", "previously", "reported", "models", "(", "including", "ensembles", ")", "by", "more", "than", "BLEU", ",", "establishing", "a", "new", "state", "-", "of", "-", "the", "-", "art", "BLEU", "score", "of", ".", "The", "configuration", "of", "this", "model", "is", "listed", "in", "the", "bottom", "line", "of", "Table", "[", "reference", "]", ".", "Training", "took", "days", "on", "P100", "GPUs", ".", "Even", "our", "base", "model", "surpasses", "all", "previously", "published", "models", "and", "ensembles", ",", "at", "a", "fraction", "of", "the", "training", "cost", "of", "any", "of", "the", "competitive", "models", ".", "On", "the", "WMT", "2014", "English", "-", "to", "-", "French", "translation", "task", ",", "our", "big", "model", "achieves", "a", "BLEU", "score", "of", ",", "outperforming", "all", "of", "the", "previously", "published", "single", "models", ",", "at", "less", "than", "the", "training", "cost", "of", "the", "previous", "state", "-", "of", "-", "the", "-", "art", "model", ".", "The", "Transformer", "(", "big", ")", "model", "trained", "for", "English", "-", "to", "-", "French", "used", "dropout", "rate", ",", "instead", "of", ".", "For", "the", "base", "models", ",", "we", "used", "a", "single", "model", "obtained", "by", "averaging", "the", "last", "5", "checkpoints", ",", "which", "were", "written", "at", "10", "-", "minute", "intervals", ".", "For", "the", "big", "models", ",", "we", "averaged", "the", "last", "20", "checkpoints", ".", "We", "used", "beam", "search", "with", "a", "beam", "size", "of", "and", "length", "penalty", "wu2016google", ".", "These", "hyperparameters", "were", "chosen", "after", "experimentation", "on", "the", "development", "set", ".", "We", "set", "the", "maximum", "output", "length", "during", "inference", "to", "input", "length", "+", ",", "but", "terminate", "early", "when", "possible", "wu2016google", ".", "Table", "[", "reference", "]", "summarizes", "our", "results", "and", "compares", "our", "translation", "quality", "and", "training", "costs", "to", "other", "model", "architectures", "from", "the", "literature", ".", "We", "estimate", "the", "number", "of", "floating", "point", "operations", "used", "to", "train", "a", "model", "by", "multiplying", "the", "training", "time", ",", "the", "number", "of", "GPUs", "used", ",", "and", "an", "estimate", "of", "the", "sustained", "single", "-", "precision", "floating", "-", "point", "capacity", "of", "each", "GPU", ".", "subsection", ":", "Model", "Variations", "To", "evaluate", "the", "importance", "of", "different", "components", "of", "the", "Transformer", ",", "we", "varied", "our", "base", "model", "in", "different", "ways", ",", "measuring", "the", "change", "in", "performance", "on", "English", "-", "to", "-", "German", "translation", "on", "the", "development", "set", ",", "newstest2013", ".", "We", "used", "beam", "search", "as", "described", "in", "the", "previous", "section", ",", "but", "no", "checkpoint", "averaging", ".", "We", "present", "these", "results", "in", "Table", "[", "reference", "]", ".", "In", "Table", "[", "reference", "]", "rows", "(", "A", ")", ",", "we", "vary", "the", "number", "of", "attention", "heads", "and", "the", "attention", "key", "and", "value", "dimensions", ",", "keeping", "the", "amount", "of", "computation", "constant", ",", "as", "described", "in", "Section", "[", "reference", "]", ".", "While", "single", "-", "head", "attention", "is", "0.9", "BLEU", "worse", "than", "the", "best", "setting", ",", "quality", "also", "drops", "off", "with", "too", "many", "heads", ".", "In", "Table", "[", "reference", "]", "rows", "(", "B", ")", ",", "we", "observe", "that", "reducing", "the", "attention", "key", "size", "hurts", "model", "quality", ".", "This", "suggests", "that", "determining", "compatibility", "is", "not", "easy", "and", "that", "a", "more", "sophisticated", "compatibility", "function", "than", "dot", "product", "may", "be", "beneficial", ".", "We", "further", "observe", "in", "rows", "(", "C", ")", "and", "(", "D", ")", "that", ",", "as", "expected", ",", "bigger", "models", "are", "better", ",", "and", "dropout", "is", "very", "helpful", "in", "avoiding", "over", "-", "fitting", ".", "In", "row", "(", "E", ")", "we", "replace", "our", "sinusoidal", "positional", "encoding", "with", "learned", "positional", "embeddings", "JonasFaceNet2017", ",", "and", "observe", "nearly", "identical", "results", "to", "the", "base", "model", ".", "subsection", ":", "English", "Constituency", "Parsing", "To", "evaluate", "if", "the", "Transformer", "can", "generalize", "to", "other", "tasks", "we", "performed", "experiments", "on", "English", "constituency", "parsing", ".", "This", "task", "presents", "specific", "challenges", ":", "the", "output", "is", "subject", "to", "strong", "structural", "constraints", "and", "is", "significantly", "longer", "than", "the", "input", ".", "Furthermore", ",", "RNN", "sequence", "-", "to", "-", "sequence", "models", "have", "not", "been", "able", "to", "attain", "state", "-", "of", "-", "the", "-", "art", "results", "in", "small", "-", "data", "regimes", ".", "We", "trained", "a", "4", "-", "layer", "transformer", "with", "on", "the", "Wall", "Street", "Journal", "(", "WSJ", ")", "portion", "of", "the", "Penn", "Treebank", "marcus1993building", ",", "about", "40", "K", "training", "sentences", ".", "We", "also", "trained", "it", "in", "a", "semi", "-", "supervised", "setting", ",", "using", "the", "larger", "high", "-", "confidence", "and", "BerkleyParser", "corpora", "from", "with", "approximately", "17", "M", "sentences", "KVparse15", ".", "We", "used", "a", "vocabulary", "of", "16", "K", "tokens", "for", "the", "WSJ", "only", "setting", "and", "a", "vocabulary", "of", "32", "K", "tokens", "for", "the", "semi", "-", "supervised", "setting", ".", "We", "performed", "only", "a", "small", "number", "of", "experiments", "to", "select", "the", "dropout", ",", "both", "attention", "and", "residual", "(", "section", "[", "reference", "]", ")", ",", "learning", "rates", "and", "beam", "size", "on", "the", "Section", "22", "development", "set", ",", "all", "other", "parameters", "remained", "unchanged", "from", "the", "English", "-", "to", "-", "German", "base", "translation", "model", ".", "During", "inference", ",", "we", "increased", "the", "maximum", "output", "length", "to", "input", "length", "+", ".", "We", "used", "a", "beam", "size", "of", "and", "for", "both", "WSJ", "only", "and", "the", "semi", "-", "supervised", "setting", ".", "Our", "results", "in", "Table", "[", "reference", "]", "show", "that", "despite", "the", "lack", "of", "task", "-", "specific", "tuning", "our", "model", "performs", "surprisingly", "well", ",", "yielding", "better", "results", "than", "all", "previously", "reported", "models", "with", "the", "exception", "of", "the", "Recurrent", "Neural", "Network", "Grammar", ".", "In", "contrast", "to", "RNN", "sequence", "-", "to", "-", "sequence", "models", "KVparse15", ",", "the", "Transformer", "outperforms", "the", "BerkeleyParser", "even", "when", "training", "only", "on", "the", "WSJ", "training", "set", "of", "40", "K", "sentences", ".", "section", ":", "Conclusion", "In", "this", "work", ",", "we", "presented", "the", "Transformer", ",", "the", "first", "sequence", "transduction", "model", "based", "entirely", "on", "attention", ",", "replacing", "the", "recurrent", "layers", "most", "commonly", "used", "in", "encoder", "-", "decoder", "architectures", "with", "multi", "-", "headed", "self", "-", "attention", ".", "For", "translation", "tasks", ",", "the", "Transformer", "can", "be", "trained", "significantly", "faster", "than", "architectures", "based", "on", "recurrent", "or", "convolutional", "layers", ".", "On", "both", "WMT", "2014", "English", "-", "to", "-", "German", "and", "WMT", "2014", "English", "-", "to", "-", "French", "translation", "tasks", ",", "we", "achieve", "a", "new", "state", "of", "the", "art", ".", "In", "the", "former", "task", "our", "best", "model", "outperforms", "even", "all", "previously", "reported", "ensembles", ".", "We", "are", "excited", "about", "the", "future", "of", "attention", "-", "based", "models", "and", "plan", "to", "apply", "them", "to", "other", "tasks", ".", "We", "plan", "to", "extend", "the", "Transformer", "to", "problems", "involving", "input", "and", "output", "modalities", "other", "than", "text", "and", "to", "investigate", "local", ",", "restricted", "attention", "mechanisms", "to", "efficiently", "handle", "large", "inputs", "and", "outputs", "such", "as", "images", ",", "audio", "and", "video", ".", "Making", "generation", "less", "sequential", "is", "another", "research", "goals", "of", "ours", ".", "The", "code", "we", "used", "to", "train", "and", "evaluate", "our", "models", "is", "available", "at", ".", "paragraph", ":", "Acknowledgements", "We", "are", "grateful", "to", "Nal", "Kalchbrenner", "and", "Stephan", "Gouws", "for", "their", "fruitful", "comments", ",", "corrections", "and", "inspiration", ".", "bibliography", ":", "References", "appendix", ":", "Attention", "Visualizations"]}