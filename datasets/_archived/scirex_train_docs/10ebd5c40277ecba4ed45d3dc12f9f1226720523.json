{"coref": {"Accuracy__trained_on_10k_": [], "Accuracy__trained_on_1k_": [], "End-To-End_Memory_Networks": [[2, 9], [128, 130], [494, 502], [1609, 1611], [1619, 1620], [2302, 2304], [2893, 2894], [3758, 3759], [4831, 4833], [3262, 3264]], "Mean_Error_Rate": [[3738, 3739], [587, 588], [4005, 4006]], "Question_Answering": [[110, 115], [215, 218], [420, 425], [2156, 2161], [2166, 2169], [2182, 2184], [2371, 2373], [2714, 2716], [3932, 3934], [4017, 4019], [4267, 4269], [4720, 4722], [4845, 4848], [4942, 4944]], "bAbi": [[2904, 2907]]}, "coref_non_salient": {"0": [[3326, 3330], [3938, 3941]], "1": [[1804, 1807], [2250, 2251]], "10": [[360, 361], [4715, 4716]], "100": [[3663, 3667]], "101": [[972, 974]], "102": [[1622, 1626]], "103": [[968, 970]], "104": [[16, 19]], "11": [[3243, 3244], [3586, 3588], [3607, 3608], [3631, 3632], [4463, 4465], [4493, 4494], [4527, 4528], [4558, 4559]], "12": [[1457, 1461], [1485, 1489]], "13": [[3282, 3284], [3351, 3353]], "14": [[3497, 3499], [3506, 3508], [3515, 3517]], "15": [[153, 154], [314, 315], [1142, 1143], [1280, 1281], [1288, 1289], [1385, 1386], [1455, 1456], [1514, 1515], [1564, 1565], [2070, 2071], [2108, 2109], [2132, 2133], [4087, 4088], [4431, 4432], [4476, 4477], [4499, 4500], [4648, 4649], [1318, 1319], [1700, 1701], [1844, 1845], [1849, 1850], [4787, 4788]], "16": [[1734, 1740], [1767, 1771]], "17": [[2935, 2937], [3116, 3118], [4338, 4340], [4352, 4354], [2998, 3000], [4317, 4319]], "18": [[4626, 4627], [4651, 4652]], "19": [[32, 34], [344, 346], [1234, 1236], [4729, 4731]], "2": [[2484, 2488], [2547, 2548], [3474, 3475], [3683, 3684], [3924, 3925]], "20": [[1170, 1172], [3926, 3928]], "21": [[682, 684], [703, 705], [772, 774]], "22": [[1862, 1864], [1883, 1885], [1914, 1916]], "23": [[12, 14], [4695, 4697], [1888, 1890], [2061, 2063]], "24": [[939, 940], [3430, 3432]], "25": [[4039, 4042], [1200, 1203], [1265, 1268], [1550, 1553]], "26": [[3252, 3253], [3331, 3332], [3602, 3603], [3626, 3627]], "27": [[738, 739], [887, 888], [3287, 3288]], "28": [[1072, 1074], [2119, 2121], [4124, 4126]], "29": [[4026, 4028]], "3": [[117, 119], [417, 419], [2029, 2031], [3839, 3841], [3845, 3847], [4093, 4096], [4679, 4681], [4723, 4725], [4779, 4782]], "30": [[155, 156], [2072, 2073], [2104, 2105], [3433, 3434], [4433, 4434], [4486, 4487], [4509, 4510], [3437, 3438]], "31": [[1213, 1215], [1801, 1803]], "32": [[2708, 2710]], "33": [[2011, 2013], [3310, 3312]], "34": [[2522, 2524], [2672, 2674], [3481, 3483]], "35": [[2647, 2651]], "36": [[58, 59], [3784, 3786], [4022, 4023], [4348, 4349]], "37": [[3306, 3309]], "38": [[3593, 3595], [3648, 3650]], "39": [[4467, 4468]], "4": [[1994, 1996], [2681, 2682], [3479, 3480], [3672, 3673], [3733, 3734]], "40": [[275, 277], [564, 566], [571, 573]], "41": [[3103, 3105], [3518, 3520], [3709, 3711], [3612, 3614], [3636, 3638]], "42": [[4435, 4439]], "43": [[4789, 4793]], "44": [[78, 79], [436, 437]], "45": [[4248, 4250]], "46": [[2783, 2785]], "47": [[612, 615], [859, 862]], "48": [[4139, 4141]], "49": [[4136, 4137]], "5": [[1939, 1941], [4703, 4706]], "50": [[4043, 4046]], "51": [[4851, 4853]], "52": [[1819, 1820]], "53": [[319, 320]], "54": [[3702, 3704]], "55": [[2924, 2926]], "56": [[3230, 3232]], "57": [[3425, 3427]], "58": [[2678, 2680], [3476, 3478], [3609, 3611], [3633, 3635], [3669, 3671]], "59": [[246, 248]], "6": [[638, 641], [754, 757]], "60": [[1855, 1857]], "61": [[4277, 4281]], "62": [[3106, 3107], [3521, 3522], [3712, 3713], [3742, 3743], [3111, 3112]], "63": [[1710, 1713]], "64": [[3321, 3324]], "65": [[1927, 1930]], "66": [[1633, 1635]], "67": [[4047, 4049]], "68": [[4937, 4941]], "69": [[855, 856], [3077, 3079]], "7": [[4440, 4443], [4478, 4479]], "70": [[2541, 2546], [3677, 3682]], "71": [[197, 200]], "72": [[1685, 1689]], "73": [[943, 946]], "74": [[4880, 4881]], "75": [[602, 604]], "76": [[2004, 2006]], "77": [[2843, 2846]], "78": [[4615, 4617]], "79": [[3993, 3997]], "8": [[4327, 4329], [4397, 4400]], "80": [[297, 301]], "81": [[2966, 2968]], "82": [[619, 623]], "83": [[733, 735]], "84": [[827, 830]], "85": [[929, 933]], "86": [[2969, 2971]], "87": [[244, 245]], "88": [[2633, 2637]], "89": [[1817, 1818]], "9": [[1823, 1824], [1730, 1731]], "90": [[3082, 3084]], "91": [[310, 313]], "92": [[483, 485]], "93": [[2020, 2022]], "94": [[2106, 2107]], "95": [[2252, 2253]], "96": [[4620, 4624]], "97": [[1725, 1728]], "98": [[2480, 2482]], "99": [[1859, 1860]]}, "doc_id": "10ebd5c40277ecba4ed45d3dc12f9f1226720523", "method_subrelations": {"End-To-End_Memory_Networks": [[[0, 26], "End-To-End_Memory_Networks"]]}, "n_ary_relations": [{"Material": "bAbi", "Method": "End-To-End_Memory_Networks", "Metric": "Accuracy__trained_on_10k_", "Task": "Question_Answering", "score": "93.4%"}, {"Material": "bAbi", "Method": "End-To-End_Memory_Networks", "Metric": "Accuracy__trained_on_1k_", "Task": "Question_Answering", "score": "86.1%"}, {"Material": "bAbi", "Method": "End-To-End_Memory_Networks", "Metric": "Mean_Error_Rate", "Task": "Question_Answering", "score": " 7.5%"}], "ner": [[2, 9, "Method"], [12, 14, "Method"], [16, 19, "Method"], [32, 34, "Method"], [58, 59, "Task"], [78, 79, "Method"], [110, 115, "Task"], [117, 119, "Task"], [128, 130, "Method"], [153, 154, "Method"], [155, 156, "Method"], [197, 200, "Task"], [215, 218, "Task"], [244, 245, "Task"], [246, 248, "Task"], [275, 277, "Method"], [297, 301, "Method"], [310, 313, "Method"], [314, 315, "Method"], [319, 320, "Method"], [360, 361, "Method"], [417, 419, "Task"], [420, 425, "Task"], [436, 437, "Method"], [483, 485, "Method"], [494, 502, "Method"], [564, 566, "Method"], [571, 573, "Method"], [602, 604, "Method"], [612, 615, "Task"], [619, 623, "Method"], [638, 641, "Method"], [682, 684, "Method"], [703, 705, "Method"], [733, 735, "Method"], [738, 739, "Method"], [754, 757, "Method"], [772, 774, "Method"], [827, 830, "Task"], [855, 856, "Task"], [859, 862, "Task"], [887, 888, "Method"], [929, 933, "Metric"], [939, 940, "Method"], [943, 946, "Method"], [968, 970, "Task"], [972, 974, "Method"], [1072, 1074, "Method"], [1142, 1143, "Method"], [1170, 1172, "Method"], [1213, 1215, "Method"], [1280, 1281, "Method"], [1288, 1289, "Method"], [1385, 1386, "Method"], [1455, 1456, "Method"], [1457, 1461, "Method"], [1485, 1489, "Method"], [1514, 1515, "Method"], [1564, 1565, "Method"], [1609, 1611, "Method"], [1619, 1620, "Method"], [1622, 1626, "Method"], [1633, 1635, "Method"], [1685, 1689, "Method"], [1710, 1713, "Method"], [1725, 1728, "Method"], [1734, 1740, "Method"], [1767, 1771, "Method"], [1801, 1803, "Method"], [1804, 1807, "Task"], [1817, 1818, "Task"], [1819, 1820, "Task"], [1823, 1824, "Method"], [1855, 1857, "Task"], [1859, 1860, "Method"], [1862, 1864, "Method"], [1883, 1885, "Method"], [1914, 1916, "Method"], [1927, 1930, "Task"], [1939, 1941, "Method"], [1994, 1996, "Method"], [2004, 2006, "Method"], [2011, 2013, "Method"], [2020, 2022, "Method"], [2029, 2031, "Task"], [2070, 2071, "Method"], [2072, 2073, "Method"], [2104, 2105, "Method"], [2106, 2107, "Method"], [2108, 2109, "Method"], [2119, 2121, "Method"], [2132, 2133, "Method"], [2156, 2161, "Task"], [2166, 2169, "Task"], [2182, 2184, "Task"], [2250, 2251, "Task"], [2252, 2253, "Task"], [2302, 2304, "Method"], [2371, 2373, "Task"], [2480, 2482, "Method"], [2484, 2488, "Method"], [2522, 2524, "Method"], [2541, 2546, "Method"], [2547, 2548, "Method"], [2633, 2637, "Method"], [2647, 2651, "Task"], [2672, 2674, "Method"], [2678, 2680, "Method"], [2681, 2682, "Method"], [2708, 2710, "Method"], [2714, 2716, "Task"], [2783, 2785, "Method"], [2843, 2846, "Task"], [2893, 2894, "Method"], [2904, 2907, "Material"], [2924, 2926, "Method"], [2935, 2937, "Metric"], [2966, 2968, "Method"], [2969, 2971, "Method"], [3077, 3079, "Task"], [3082, 3084, "Metric"], [3103, 3105, "Method"], [3106, 3107, "Method"], [3116, 3118, "Metric"], [3230, 3232, "Metric"], [3243, 3244, "Method"], [3252, 3253, "Method"], [3282, 3284, "Method"], [3287, 3288, "Method"], [3306, 3309, "Method"], [3310, 3312, "Method"], [3321, 3324, "Method"], [3326, 3330, "Method"], [3331, 3332, "Method"], [3351, 3353, "Method"], [3425, 3427, "Metric"], [3430, 3432, "Method"], [3433, 3434, "Method"], [3474, 3475, "Method"], [3476, 3478, "Method"], [3479, 3480, "Method"], [3481, 3483, "Method"], [3497, 3499, "Method"], [3506, 3508, "Method"], [3515, 3517, "Method"], [3518, 3520, "Method"], [3521, 3522, "Method"], [3586, 3588, "Method"], [3593, 3595, "Method"], [3602, 3603, "Method"], [3607, 3608, "Method"], [3609, 3611, "Method"], [3626, 3627, "Method"], [3631, 3632, "Method"], [3633, 3635, "Method"], [3648, 3650, "Method"], [3663, 3667, "Method"], [3669, 3671, "Method"], [3672, 3673, "Method"], [3677, 3682, "Method"], [3683, 3684, "Method"], [3702, 3704, "Task"], [3709, 3711, "Method"], [3712, 3713, "Method"], [3733, 3734, "Method"], [3738, 3739, "Metric"], [3742, 3743, "Method"], [3758, 3759, "Method"], [3784, 3786, "Task"], [3839, 3841, "Task"], [3845, 3847, "Task"], [3924, 3925, "Method"], [3926, 3928, "Method"], [3932, 3934, "Task"], [3938, 3941, "Method"], [3993, 3997, "Method"], [4017, 4019, "Task"], [4022, 4023, "Task"], [4026, 4028, "Method"], [4039, 4042, "Method"], [4043, 4046, "Method"], [4047, 4049, "Method"], [4087, 4088, "Method"], [4093, 4096, "Task"], [4124, 4126, "Method"], [4136, 4137, "Task"], [4139, 4141, "Method"], [4248, 4250, "Metric"], [4267, 4269, "Task"], [4277, 4281, "Task"], [4327, 4329, "Metric"], [4338, 4340, "Metric"], [4348, 4349, "Task"], [4352, 4354, "Metric"], [4397, 4400, "Metric"], [4431, 4432, "Method"], [4433, 4434, "Method"], [4435, 4439, "Method"], [4440, 4443, "Method"], [4463, 4465, "Method"], [4467, 4468, "Metric"], [4476, 4477, "Method"], [4478, 4479, "Method"], [4486, 4487, "Method"], [4493, 4494, "Method"], [4499, 4500, "Method"], [4509, 4510, "Method"], [4527, 4528, "Method"], [4558, 4559, "Method"], [4615, 4617, "Method"], [4620, 4624, "Method"], [4626, 4627, "Method"], [4648, 4649, "Method"], [4651, 4652, "Method"], [4679, 4681, "Task"], [4695, 4697, "Method"], [4703, 4706, "Method"], [4715, 4716, "Method"], [4720, 4722, "Task"], [4723, 4725, "Task"], [4779, 4782, "Task"], [4789, 4793, "Method"], [4831, 4833, "Method"], [4845, 4848, "Task"], [4851, 4853, "Method"], [4880, 4881, "Method"], [4937, 4941, "Task"], [4942, 4944, "Task"], [344, 346, "Method"], [587, 588, "Metric"], [1200, 1203, "Method"], [1234, 1236, "Method"], [1265, 1268, "Method"], [1318, 1319, "Method"], [1550, 1553, "Method"], [1700, 1701, "Method"], [1730, 1731, "Method"], [1844, 1845, "Method"], [1849, 1850, "Method"], [1888, 1890, "Method"], [2061, 2063, "Method"], [2998, 3000, "Metric"], [3111, 3112, "Method"], [3262, 3264, "Method"], [3437, 3438, "Method"], [3612, 3614, "Method"], [3636, 3638, "Method"], [4005, 4006, "Metric"], [4317, 4319, "Metric"], [4729, 4731, "Method"], [4787, 4788, "Method"]], "sections": [[0, 174], [174, 177], [177, 180], [180, 190], [190, 503], [503, 600], [600, 957], [957, 1434], [1434, 2154], [2154, 2468], [2468, 2896], [2896, 3233], [3233, 3460], [3460, 3837], [3837, 4253], [4253, 4420], [4420, 4682], [4682, 4886], [4886, 4925], [4925, 4928], [4928, 4935], [4935, 4944]], "sentences": [[0, 9], [9, 26], [26, 69], [69, 95], [95, 120], [120, 136], [136, 145], [145, 157], [157, 174], [174, 176], [176, 177], [177, 179], [179, 180], [180, 182], [182, 190], [190, 193], [193, 235], [235, 267], [267, 302], [302, 334], [334, 349], [349, 372], [372, 426], [426, 453], [453, 503], [503, 506], [506, 531], [531, 546], [546, 570], [570, 582], [582, 600], [600, 604], [604, 624], [624, 638], [638, 641], [641, 655], [655, 689], [689, 717], [717, 742], [742, 754], [754, 757], [757, 776], [776, 822], [822, 839], [839, 846], [846, 852], [852, 902], [902, 909], [909, 939], [939, 957], [957, 961], [961, 971], [971, 1024], [1024, 1043], [1043, 1067], [1067, 1078], [1078, 1097], [1097, 1138], [1138, 1141], [1141, 1159], [1159, 1162], [1162, 1183], [1183, 1206], [1206, 1220], [1220, 1227], [1227, 1259], [1259, 1295], [1295, 1316], [1316, 1326], [1326, 1335], [1335, 1356], [1356, 1365], [1365, 1380], [1380, 1412], [1412, 1434], [1434, 1438], [1438, 1462], [1462, 1484], [1484, 1505], [1505, 1523], [1523, 1527], [1527, 1547], [1547, 1602], [1602, 1640], [1640, 1652], [1652, 1664], [1664, 1671], [1671, 1675], [1675, 1678], [1678, 1703], [1703, 1718], [1718, 1729], [1729, 1772], [1772, 1796], [1796, 1825], [1825, 1835], [1835, 1838], [1838, 1858], [1858, 1912], [1912, 1924], [1924, 1931], [1931, 1958], [1958, 1985], [1985, 2023], [2023, 2037], [2037, 2051], [2051, 2055], [2055, 2081], [2081, 2117], [2117, 2154], [2154, 2161], [2161, 2180], [2180, 2216], [2216, 2235], [2235, 2254], [2254, 2262], [2262, 2300], [2300, 2309], [2309, 2342], [2342, 2364], [2364, 2394], [2394, 2429], [2429, 2440], [2440, 2468], [2468, 2472], [2472, 2489], [2489, 2522], [2522, 2537], [2537, 2564], [2564, 2567], [2567, 2583], [2583, 2608], [2608, 2624], [2624, 2638], [2638, 2671], [2671, 2694], [2694, 2708], [2708, 2711], [2711, 2752], [2752, 2782], [2782, 2798], [2798, 2805], [2805, 2817], [2817, 2843], [2843, 2865], [2865, 2884], [2884, 2896], [2896, 2900], [2900, 2929], [2929, 2951], [2951, 2959], [2959, 2971], [2971, 2973], [2973, 3010], [3010, 3047], [3047, 3080], [3080, 3098], [3098, 3110], [3110, 3122], [3122, 3135], [3135, 3165], [3165, 3177], [3177, 3208], [3208, 3233], [3233, 3236], [3236, 3252], [3252, 3254], [3254, 3269], [3269, 3279], [3279, 3304], [3304, 3321], [3321, 3325], [3325, 3343], [3343, 3399], [3399, 3433], [3433, 3435], [3435, 3454], [3454, 3460], [3460, 3463], [3463, 3511], [3511, 3547], [3547, 3576], [3576, 3623], [3623, 3654], [3654, 3668], [3668, 3708], [3708, 3723], [3723, 3749], [3749, 3784], [3784, 3791], [3791, 3800], [3800, 3814], [3814, 3827], [3827, 3837], [3837, 3842], [3842, 3862], [3862, 3876], [3876, 3890], [3890, 3908], [3908, 3935], [3935, 3947], [3947, 3958], [3958, 3973], [3973, 3992], [3992, 4020], [4020, 4037], [4037, 4071], [4071, 4121], [4121, 4151], [4151, 4161], [4161, 4184], [4184, 4191], [4191, 4193], [4193, 4213], [4213, 4231], [4231, 4253], [4253, 4257], [4257, 4275], [4275, 4307], [4307, 4314], [4314, 4348], [4348, 4364], [4364, 4376], [4376, 4401], [4401, 4420], [4420, 4423], [4423, 4439], [4439, 4449], [4449, 4462], [4462, 4491], [4491, 4515], [4515, 4548], [4548, 4551], [4551, 4554], [4554, 4566], [4566, 4583], [4583, 4628], [4628, 4642], [4642, 4669], [4669, 4682], [4682, 4688], [4688, 4726], [4726, 4754], [4754, 4778], [4778, 4794], [4794, 4810], [4810, 4819], [4819, 4849], [4849, 4867], [4867, 4886], [4886, 4889], [4889, 4925], [4925, 4928], [4928, 4935], [4935, 4944]], "words": ["document", ":", "End", "-", "To", "-", "End", "Memory", "Networks", "We", "introduce", "a", "neural", "network", "with", "a", "recurrent", "attention", "model", "over", "a", "possibly", "large", "external", "memory", ".", "The", "architecture", "is", "a", "form", "of", "Memory", "Network", "but", "unlike", "the", "model", "in", "that", "work", ",", "it", "is", "trained", "end", "-", "to", "-", "end", ",", "and", "hence", "requires", "significantly", "less", "supervision", "during", "training", ",", "making", "it", "more", "generally", "applicable", "in", "realistic", "settings", ".", "It", "can", "also", "be", "seen", "as", "an", "extension", "of", "RNNsearch", "to", "the", "case", "where", "multiple", "computational", "steps", "(", "hops", ")", "are", "performed", "per", "output", "symbol", ".", "The", "flexibility", "of", "the", "model", "allows", "us", "to", "apply", "it", "to", "tasks", "as", "diverse", "as", "(", "synthetic", ")", "question", "answering", "and", "to", "language", "modeling", ".", "For", "the", "former", "our", "approach", "is", "competitive", "with", "Memory", "Networks", ",", "but", "with", "less", "supervision", ".", "For", "the", "latter", ",", "on", "the", "Penn", "TreeBank", "and", "Text8", "datasets", "our", "approach", "demonstrates", "comparable", "performance", "to", "RNNs", "and", "LSTMs", ".", "In", "both", "cases", "we", "show", "that", "the", "key", "concept", "of", "multiple", "computational", "hops", "yields", "improved", "results", ".", "section", ":", "0pt0.5ex0.3ex", "subsection", ":", "0pt0.2ex0ex", "subsubsection", ":", "0pt0.1ex0ex", "[", "itemize", "]", "leftmargin=", "*", "tabular", "table", "section", ":", "Introduction", "Two", "grand", "challenges", "in", "artificial", "intelligence", "research", "have", "been", "to", "build", "models", "that", "can", "make", "multiple", "computational", "steps", "in", "the", "service", "of", "answering", "a", "question", "or", "completing", "a", "task", ",", "and", "models", "that", "can", "describe", "long", "term", "dependencies", "in", "sequential", "data", ".", "Recently", "there", "has", "been", "a", "resurgence", "in", "models", "of", "computation", "using", "explicit", "storage", "and", "a", "notion", "of", "attention", ";", "manipulating", "such", "a", "storage", "offers", "an", "approach", "to", "both", "of", "these", "challenges", ".", "In", ",", "the", "storage", "is", "endowed", "with", "a", "continuous", "representation", ";", "reads", "from", "and", "writes", "to", "the", "storage", ",", "as", "well", "as", "other", "processing", "steps", ",", "are", "modeled", "by", "the", "actions", "of", "neural", "networks", ".", "In", "this", "work", ",", "we", "present", "a", "novel", "recurrent", "neural", "network", "(", "RNN", ")", "architecture", "where", "the", "recurrence", "reads", "from", "a", "possibly", "large", "external", "memory", "multiple", "times", "before", "outputting", "a", "symbol", ".", "Our", "model", "can", "be", "considered", "a", "continuous", "form", "of", "the", "Memory", "Network", "implemented", "in", ".", "The", "model", "in", "that", "work", "was", "not", "easy", "to", "train", "via", "backpropagation", ",", "and", "required", "supervision", "at", "each", "layer", "of", "the", "network", ".", "The", "continuity", "of", "the", "model", "we", "present", "here", "means", "that", "it", "can", "be", "trained", "end", "-", "to", "-", "end", "from", "input", "-", "output", "pairs", ",", "and", "so", "is", "applicable", "to", "more", "tasks", ",", "i.e.", "tasks", "where", "such", "supervision", "is", "not", "available", ",", "such", "as", "in", "language", "modeling", "or", "realistically", "supervised", "question", "answering", "tasks", ".", "Our", "model", "can", "also", "be", "seen", "as", "a", "version", "of", "RNNsearch", "with", "multiple", "computational", "steps", "(", "which", "we", "term", "\u201c", "hops", "\u201d", ")", "per", "output", "symbol", ".", "We", "will", "show", "experimentally", "that", "the", "multiple", "hops", "over", "the", "long", "-", "term", "memory", "are", "crucial", "to", "good", "performance", "of", "our", "model", "on", "these", "tasks", ",", "and", "that", "training", "the", "memory", "representation", "can", "be", "integrated", "in", "a", "scalable", "manner", "into", "our", "end", "-", "to", "-", "end", "neural", "network", "model", ".", "section", ":", "Approach", "Our", "model", "takes", "a", "discrete", "set", "of", "inputs", "that", "are", "to", "be", "stored", "in", "the", "memory", ",", "a", "query", ",", "and", "outputs", "an", "answer", ".", "Each", "of", "the", ",", ",", "and", "contains", "symbols", "coming", "from", "a", "dictionary", "with", "words", ".", "The", "model", "writes", "all", "to", "the", "memory", "up", "to", "a", "fixed", "buffer", "size", ",", "and", "then", "finds", "a", "continuous", "representation", "for", "the", "and", ".", "The", "continuous", "representation", "is", "then", "processed", "via", "multiple", "hops", "to", "output", ".", "This", "allows", "backpropagation", "of", "the", "error", "signal", "through", "multiple", "memory", "accesses", "back", "to", "the", "input", "during", "training", ".", "subsection", ":", "Single", "Layer", "We", "start", "by", "describing", "our", "model", "in", "the", "single", "layer", "case", ",", "which", "implements", "a", "single", "memory", "hop", "operation", ".", "We", "then", "show", "it", "can", "be", "stacked", "to", "give", "multiple", "hops", "in", "memory", ".", "Input", "memory", "representation", ":", "Suppose", "we", "are", "given", "an", "input", "set", "to", "be", "stored", "in", "memory", ".", "The", "entire", "set", "of", "are", "converted", "into", "memory", "vectors", "of", "dimension", "computed", "by", "embedding", "each", "in", "a", "continuous", "space", ",", "in", "the", "simplest", "case", ",", "using", "an", "embedding", "matrix", "(", "of", "size", ")", ".", "The", "query", "is", "also", "embedded", "(", "again", ",", "in", "the", "simplest", "case", "via", "another", "embedding", "matrix", "with", "the", "same", "dimensions", "as", ")", "to", "obtain", "an", "internal", "state", ".", "In", "the", "embedding", "space", ",", "we", "compute", "the", "match", "between", "and", "each", "memory", "by", "taking", "the", "inner", "product", "followed", "by", "a", "softmax", ":", "where", ".", "Defined", "in", "this", "way", "is", "a", "probability", "vector", "over", "the", "inputs", ".", "Output", "memory", "representation", ":", "Each", "has", "a", "corresponding", "output", "vector", "(", "given", "in", "the", "simplest", "case", "by", "another", "embedding", "matrix", ")", ".", "The", "response", "vector", "from", "the", "memory", "is", "then", "a", "sum", "over", "the", "transformed", "inputs", ",", "weighted", "by", "the", "probability", "vector", "from", "the", "input", ":", "Because", "the", "function", "from", "input", "to", "output", "is", "smooth", ",", "we", "can", "easily", "compute", "gradients", "and", "back", "-", "propagate", "through", "it", ".", "Other", "recently", "proposed", "forms", "of", "memory", "or", "attention", "take", "this", "approach", ",", "notably", "Bahdanau", "et", "al", ".", "[", "]", "and", "Graves", "et", "al", ".", "[", "]", ",", "see", "also", ".", "Generating", "the", "final", "prediction", ":", "In", "the", "single", "layer", "case", ",", "the", "sum", "of", "the", "output", "vector", "and", "the", "input", "embedding", "is", "then", "passed", "through", "a", "final", "weight", "matrix", "(", "of", "size", ")", "and", "a", "softmax", "to", "produce", "the", "predicted", "label", ":", "The", "overall", "model", "is", "shown", "in", "Fig", ".", "[", "reference", "]", "(", "a", ")", ".", "During", "training", ",", "all", "three", "embedding", "matrices", ",", "and", ",", "as", "well", "as", "are", "jointly", "learned", "by", "minimizing", "a", "standard", "cross", "-", "entropy", "loss", "between", "and", "the", "true", "label", ".", "Training", "is", "performed", "using", "stochastic", "gradient", "descent", "(", "see", "Section", "[", "reference", "]", "for", "more", "details", ")", ".", "subsection", ":", "Multiple", "Layers", "We", "now", "extend", "our", "model", "to", "handle", "hop", "operations", ".", "The", "memory", "layers", "are", "stacked", "in", "the", "following", "way", ":", "The", "input", "to", "layers", "above", "the", "first", "is", "the", "sum", "of", "the", "output", "and", "the", "input", "from", "layer", "(", "different", "ways", "to", "combine", "and", "are", "proposed", "later", ")", ":", "Each", "layer", "has", "its", "own", "embedding", "matrices", ",", "used", "to", "embed", "the", "inputs", ".", "However", ",", "as", "discussed", "below", ",", "they", "are", "constrained", "to", "ease", "training", "and", "reduce", "the", "number", "of", "parameters", ".", "At", "the", "top", "of", "the", "network", ",", "the", "input", "to", "also", "combines", "the", "input", "and", "the", "output", "of", "the", "top", "memory", "layer", ":", ".", "We", "explore", "two", "types", "of", "weight", "tying", "within", "the", "model", ":", "Adjacent", ":", "the", "output", "embedding", "for", "one", "layer", "is", "the", "input", "embedding", "for", "the", "one", "above", ",", "i.e.", ".", "We", "also", "constrain", "(", "a", ")", "the", "answer", "prediction", "matrix", "to", "be", "the", "same", "as", "the", "final", "output", "embedding", ",", "i.e", ",", "and", "(", "b", ")", "the", "question", "embedding", "to", "match", "the", "input", "embedding", "of", "the", "first", "layer", ",", "i.e.", ".", "Layer", "-", "wise", "(", "RNN", "-", "like", ")", ":", "the", "input", "and", "output", "embeddings", "are", "the", "same", "across", "different", "layers", ",", "i.e.", "and", ".", "We", "have", "found", "it", "useful", "to", "add", "a", "linear", "mapping", "to", "the", "update", "of", "between", "hops", ";", "that", "is", ",", ".", "This", "mapping", "is", "learnt", "along", "with", "the", "rest", "of", "the", "parameters", "and", "used", "throughout", "our", "experiments", "for", "layer", "-", "wise", "weight", "tying", ".", "A", "three", "-", "layer", "version", "of", "our", "memory", "model", "is", "shown", "in", "Fig", ".", "[", "reference", "]", "(", "b", ")", ".", "Overall", ",", "it", "is", "similar", "to", "the", "Memory", "Network", "model", "in", ",", "except", "that", "the", "hard", "max", "operations", "within", "each", "layer", "have", "been", "replaced", "with", "a", "continuous", "weighting", "from", "the", "softmax", ".", "Note", "that", "if", "we", "use", "the", "layer", "-", "wise", "weight", "tying", "scheme", ",", "our", "model", "can", "be", "cast", "as", "a", "traditional", "RNN", "where", "we", "divide", "the", "outputs", "of", "the", "RNN", "into", "internal", "and", "external", "outputs", ".", "Emitting", "an", "internal", "output", "corresponds", "to", "considering", "a", "memory", ",", "and", "emitting", "an", "external", "output", "corresponds", "to", "predicting", "a", "label", ".", "From", "the", "RNN", "point", "of", "view", ",", "in", "Fig", ".", "[", "reference", "]", "(", "b", ")", "and", "Eqn", ".", "[", "reference", "]", "is", "a", "hidden", "state", ",", "and", "the", "model", "generates", "an", "internal", "output", "(", "attention", "weights", "in", "Fig", ".", "[", "reference", "]", "(", "a", ")", ")", "using", ".", "The", "model", "then", "ingests", "using", ",", "updates", "the", "hidden", "state", ",", "and", "so", "on", ".", "Here", ",", "unlike", "a", "standard", "RNN", ",", "we", "explicitly", "condition", "on", "the", "outputs", "stored", "in", "memory", "during", "the", "hops", ",", "and", "we", "keep", "these", "outputs", "soft", ",", "rather", "than", "sampling", "them", ".", "Thus", "our", "model", "makes", "several", "computational", "steps", "before", "producing", "an", "output", "meant", "to", "be", "seen", "by", "the", "\u201c", "outside", "world", "\u201d", ".", "section", ":", "Related", "Work", "A", "number", "of", "recent", "efforts", "have", "explored", "ways", "to", "capture", "long", "-", "term", "structure", "within", "sequences", "using", "RNNs", "or", "LSTM", "-", "based", "models", ".", "The", "memory", "in", "these", "models", "is", "the", "state", "of", "the", "network", ",", "which", "is", "latent", "and", "inherently", "unstable", "over", "long", "timescales", ".", "The", "LSTM", "-", "based", "models", "address", "this", "through", "local", "memory", "cells", "which", "lock", "in", "the", "network", "state", "from", "the", "past", ".", "In", "practice", ",", "the", "performance", "gains", "over", "carefully", "trained", "RNNs", "are", "modest", "(", "see", "Mikolov", "et", "al", ".", "[", "]", ")", ".", "Our", "model", "differs", "from", "these", "in", "that", "it", "uses", "a", "global", "memory", ",", "with", "shared", "read", "and", "write", "functions", ".", "However", ",", "with", "layer", "-", "wise", "weight", "tying", "our", "model", "can", "be", "viewed", "as", "a", "form", "of", "RNN", "which", "only", "produces", "an", "output", "after", "a", "fixed", "number", "of", "time", "steps", "(", "corresponding", "to", "the", "number", "of", "hops", ")", ",", "with", "the", "intermediary", "steps", "involving", "memory", "input", "/", "output", "operations", "that", "update", "the", "internal", "state", ".", "Some", "of", "the", "very", "early", "work", "on", "neural", "networks", "by", "Steinbuch", "and", "Piske", "and", "Taylor", "considered", "a", "memory", "that", "performed", "nearest", "-", "neighbor", "operations", "on", "stored", "input", "vectors", "and", "then", "fit", "parametric", "models", "to", "the", "retrieved", "sets", ".", "This", "has", "similarities", "to", "a", "single", "layer", "version", "of", "our", "model", ".", "Subsequent", "work", "in", "the", "1990", "\u2019s", "explored", "other", "types", "of", "memory", ".", "For", "example", ",", "Das", "et", "al", ".", "[", "]", "and", "Mozer", "et", "al", ".", "[", "]", "introduced", "an", "explicit", "stack", "with", "push", "and", "pop", "operations", "which", "has", "been", "revisited", "recently", "by", "in", "the", "context", "of", "an", "RNN", "model", ".", "Closely", "related", "to", "our", "model", "is", "the", "Neural", "Turing", "Machine", "of", "Graves", "et", "al", ".", "[", "]", ",", "which", "also", "uses", "a", "continuous", "memory", "representation", ".", "The", "NTM", "memory", "uses", "both", "content", "and", "address", "-", "based", "access", ",", "unlike", "ours", "which", "only", "explicitly", "allows", "the", "former", ",", "although", "the", "temporal", "features", "that", "we", "will", "introduce", "in", "Section", "[", "reference", "]", "allow", "a", "kind", "of", "address", "-", "based", "access", ".", "However", ",", "in", "part", "because", "we", "always", "write", "each", "memory", "sequentially", ",", "our", "model", "is", "somewhat", "simpler", ",", "not", "requiring", "operations", "like", "sharpening", ".", "Furthermore", ",", "we", "apply", "our", "memory", "model", "to", "textual", "reasoning", "tasks", ",", "which", "qualitatively", "differ", "from", "the", "more", "abstract", "operations", "of", "sorting", "and", "recall", "tackled", "by", "the", "NTM", ".", "Our", "model", "is", "also", "related", "to", "Bahdanau", "et", "al", ".", "[", "]", ".", "In", "that", "work", ",", "a", "bidirectional", "RNN", "based", "encoder", "and", "gated", "RNN", "based", "decoder", "were", "used", "for", "machine", "translation", ".", "The", "decoder", "uses", "an", "attention", "model", "that", "finds", "which", "hidden", "states", "from", "the", "encoding", "are", "most", "useful", "for", "outputting", "the", "next", "translated", "word", ";", "the", "attention", "model", "uses", "a", "small", "neural", "network", "that", "takes", "as", "input", "a", "concatenation", "of", "the", "current", "hidden", "state", "of", "the", "decoder", "and", "each", "of", "the", "encoders", "hidden", "states", ".", "A", "similar", "attention", "model", "is", "also", "used", "in", "Xu", "et", "al", ".", "[", "]", "for", "generating", "image", "captions", ".", "Our", "\u201c", "memory", "\u201d", "is", "analogous", "to", "their", "attention", "mechanism", ",", "although", "is", "only", "over", "a", "single", "sentence", "rather", "than", "many", ",", "as", "in", "our", "case", ".", "Furthermore", ",", "our", "model", "makes", "several", "hops", "on", "the", "memory", "before", "making", "an", "output", ";", "we", "will", "see", "below", "that", "this", "is", "important", "for", "good", "performance", ".", "There", "are", "also", "differences", "in", "the", "architecture", "of", "the", "small", "network", "used", "to", "score", "the", "memories", "compared", "to", "our", "scoring", "approach", ";", "we", "use", "a", "simple", "linear", "layer", ",", "whereas", "they", "use", "a", "more", "sophisticated", "gated", "architecture", ".", "We", "will", "apply", "our", "model", "to", "language", "modeling", ",", "an", "extensively", "studied", "task", ".", "Goodman", "showed", "simple", "but", "effective", "approaches", "which", "combine", "-", "grams", "with", "a", "cache", ".", "Bengio", "et", "al", ".", "[", "]", "ignited", "interest", "in", "using", "neural", "network", "based", "models", "for", "the", "task", ",", "with", "RNNs", "and", "LSTMs", "showing", "clear", "performance", "gains", "over", "traditional", "methods", ".", "Indeed", ",", "the", "current", "state", "-", "of", "-", "the", "-", "art", "is", "held", "by", "variants", "of", "these", "models", ",", "for", "example", "very", "large", "LSTMs", "with", "Dropout", "or", "RNNs", "with", "diagonal", "constraints", "on", "the", "weight", "matrix", ".", "With", "appropriate", "weight", "tying", ",", "our", "model", "can", "be", "regarded", "as", "a", "modified", "form", "of", "RNN", ",", "where", "the", "recurrence", "is", "indexed", "by", "memory", "lookups", "to", "the", "word", "sequence", "rather", "than", "indexed", "by", "the", "sequence", "itself", ".", "section", ":", "Synthetic", "Question", "and", "Answering", "Experiments", "We", "perform", "experiments", "on", "the", "synthetic", "QA", "tasks", "defined", "in", "(", "using", "version", "1.1", "of", "the", "dataset", ")", ".", "A", "given", "QA", "task", "consists", "of", "a", "set", "of", "statements", ",", "followed", "by", "a", "question", "whose", "answer", "is", "typically", "a", "single", "word", "(", "in", "a", "few", "tasks", ",", "answers", "are", "a", "set", "of", "words", ")", ".", "The", "answer", "is", "available", "to", "the", "model", "at", "training", "time", ",", "but", "must", "be", "predicted", "at", "test", "time", ".", "There", "are", "a", "total", "of", "20", "different", "types", "of", "tasks", "that", "probe", "different", "forms", "of", "reasoning", "and", "deduction", ".", "Here", "are", "samples", "of", "three", "of", "the", "tasks", ":", "Note", "that", "for", "each", "question", ",", "only", "some", "subset", "of", "the", "statements", "contain", "information", "needed", "for", "the", "answer", ",", "and", "the", "others", "are", "essentially", "irrelevant", "distractors", "(", "e.g.", "the", "first", "sentence", "in", "the", "first", "example", ")", ".", "In", "the", "Memory", "Networks", "of", "Weston", "et", "al", ".", "[", "]", ",", "this", "supporting", "subset", "was", "explicitly", "indicated", "to", "the", "model", "during", "training", "and", "the", "key", "difference", "between", "that", "work", "and", "this", "one", "is", "that", "this", "information", "is", "no", "longer", "provided", ".", "Hence", ",", "the", "model", "must", "deduce", "for", "itself", "at", "training", "and", "test", "time", "which", "sentences", "are", "relevant", "and", "which", "are", "not", ".", "Formally", ",", "for", "one", "of", "the", "20", "QA", "tasks", ",", "we", "are", "given", "example", "problems", ",", "each", "having", "a", "set", "of", "sentences", "where", ";", "a", "question", "sentence", "and", "answer", ".", "Let", "the", "th", "word", "of", "sentence", "be", ",", "represented", "by", "a", "one", "-", "hot", "vector", "of", "length", "(", "where", "the", "vocabulary", "is", "of", "size", ",", "reflecting", "the", "simplistic", "nature", "of", "the", "QA", "language", ")", ".", "The", "same", "representation", "is", "used", "for", "the", "question", "and", "answer", ".", "Two", "versions", "of", "the", "data", "are", "used", ",", "one", "that", "has", "1000", "training", "problems", "per", "task", "and", "a", "second", "larger", "one", "with", "10", ",", "000", "per", "task", ".", "subsection", ":", "Model", "Details", "Unless", "otherwise", "stated", ",", "all", "experiments", "used", "a", "hops", "model", "with", "the", "adjacent", "weight", "sharing", "scheme", ".", "For", "all", "tasks", "that", "output", "lists", "(", "i.e.", "the", "answers", "are", "multiple", "words", ")", ",", "we", "take", "each", "possible", "combination", "of", "possible", "outputs", "and", "record", "them", "as", "a", "separate", "answer", "vocabulary", "word", ".", "Sentence", "Representation", ":", "In", "our", "experiments", "we", "explore", "two", "different", "representations", "for", "the", "sentences", ".", "The", "first", "is", "the", "bag", "-", "of", "-", "words", "(", "BoW", ")", "representation", "that", "takes", "the", "sentence", ",", "embeds", "each", "word", "and", "sums", "the", "resulting", "vectors", ":", "e.g", "and", ".", "The", "input", "vector", "representing", "the", "question", "is", "also", "embedded", "as", "a", "bag", "of", "words", ":", ".", "This", "has", "the", "drawback", "that", "it", "can", "not", "capture", "the", "order", "of", "the", "words", "in", "the", "sentence", ",", "which", "is", "important", "for", "some", "tasks", ".", "We", "therefore", "propose", "a", "second", "representation", "that", "encodes", "the", "position", "of", "words", "within", "the", "sentence", ".", "This", "takes", "the", "form", ":", ",", "where", "is", "an", "element", "-", "wise", "multiplication", ".", "is", "a", "column", "vector", "with", "the", "structure", "(", "assuming", "1", "-", "based", "indexing", ")", ",", "with", "being", "the", "number", "of", "words", "in", "the", "sentence", ",", "and", "is", "the", "dimension", "of", "the", "embedding", ".", "This", "sentence", "representation", ",", "which", "we", "call", "position", "encoding", "(", "PE", ")", ",", "means", "that", "the", "order", "of", "the", "words", "now", "affects", ".", "The", "same", "representation", "is", "used", "for", "questions", ",", "memory", "inputs", "and", "memory", "outputs", ".", "Temporal", "Encoding", ":", "Many", "of", "the", "QA", "tasks", "require", "some", "notion", "of", "temporal", "context", ",", "i.e.", "in", "the", "first", "example", "of", "Section", "[", "reference", "]", ",", "the", "model", "needs", "to", "understand", "that", "Sam", "is", "in", "the", "bedroom", "after", "he", "is", "in", "the", "kitchen", ".", "To", "enable", "our", "model", "to", "address", "them", ",", "we", "modify", "the", "memory", "vector", "so", "that", ",", "where", "is", "the", "th", "row", "of", "a", "special", "matrix", "that", "encodes", "temporal", "information", ".", "The", "output", "embedding", "is", "augmented", "in", "the", "same", "way", "with", "a", "matrix", "(", "e.g.", ")", ".", "Both", "and", "are", "learned", "during", "training", ".", "They", "are", "also", "subject", "to", "the", "same", "sharing", "constraints", "as", "and", ".", "Note", "that", "sentences", "are", "indexed", "in", "reverse", "order", ",", "reflecting", "their", "relative", "distance", "from", "the", "question", "so", "that", "is", "the", "last", "sentence", "of", "the", "story", ".", "Learning", "time", "invariance", "by", "injecting", "random", "noise", ":", "we", "have", "found", "it", "helpful", "to", "add", "\u201c", "dummy", "\u201d", "memories", "to", "regularize", ".", "That", "is", ",", "at", "training", "time", "we", "can", "randomly", "add", "10", "%", "of", "empty", "memories", "to", "the", "stories", ".", "We", "refer", "to", "this", "approach", "as", "random", "noise", "(", "RN", ")", ".", "subsection", ":", "Training", "Details", "10", "%", "of", "the", "bAbI", "training", "set", "was", "held", "-", "out", "to", "form", "a", "validation", "set", ",", "which", "was", "used", "to", "select", "the", "optimal", "model", "architecture", "and", "hyperparameters", ".", "Our", "models", "were", "trained", "using", "a", "learning", "rate", "of", ",", "with", "anneals", "every", "25", "epochs", "by", "until", "100", "epochs", "were", "reached", ".", "No", "momentum", "or", "weight", "decay", "was", "used", ".", "The", "weights", "were", "initialized", "randomly", "from", "a", "Gaussian", "distribution", "with", "zero", "mean", "and", ".", "When", "trained", "on", "all", "tasks", "simultaneously", "with", "1k", "training", "samples", "(", "10k", "training", "samples", ")", ",", "60", "epochs", "(", "20", "epochs", ")", "were", "used", "with", "learning", "rate", "anneals", "of", "every", "15", "epochs", "(", "5", "epochs", ")", ".", "All", "training", "uses", "a", "batch", "size", "of", "32", "(", "but", "cost", "is", "not", "averaged", "over", "a", "batch", ")", ",", "and", "gradients", "with", "an", "norm", "larger", "than", "40", "are", "divided", "by", "a", "scalar", "to", "have", "norm", "40", ".", "In", "some", "of", "our", "experiments", ",", "we", "explored", "commencing", "training", "with", "the", "softmax", "in", "each", "memory", "layer", "removed", ",", "making", "the", "model", "entirely", "linear", "except", "for", "the", "final", "softmax", "for", "answer", "prediction", ".", "When", "the", "validation", "loss", "stopped", "decreasing", ",", "the", "softmax", "layers", "were", "re", "-", "inserted", "and", "training", "recommenced", ".", "We", "refer", "to", "this", "as", "linear", "start", "(", "LS", ")", "training", ".", "In", "LS", "training", ",", "the", "initial", "learning", "rate", "is", "set", "to", ".", "The", "capacity", "of", "memory", "is", "restricted", "to", "the", "most", "recent", "50", "sentences", ".", "Since", "the", "number", "of", "sentences", "and", "the", "number", "of", "words", "per", "sentence", "varied", "between", "problems", ",", "a", "null", "symbol", "was", "used", "to", "pad", "them", "all", "to", "a", "fixed", "size", ".", "The", "embedding", "of", "the", "null", "symbol", "was", "constrained", "to", "be", "zero", ".", "On", "some", "tasks", ",", "we", "observed", "a", "large", "variance", "in", "the", "performance", "of", "our", "model", "(", "i.e.", "sometimes", "failing", "badly", ",", "other", "times", "not", ",", "depending", "on", "the", "initialization", ")", ".", "To", "remedy", "this", ",", "we", "repeated", "each", "training", "10", "times", "with", "different", "random", "initializations", ",", "and", "picked", "the", "one", "with", "the", "lowest", "training", "error", ".", "subsection", ":", "Baselines", "We", "compare", "our", "approach", "(", "abbreviated", "to", "MemN2N", ")", "to", "a", "range", "of", "alternate", "models", ":", "MemNN", ":", "The", "strongly", "supervised", "AM", "+", "NG", "+", "NL", "Memory", "Networks", "approach", ",", "proposed", "in", ".", "This", "is", "the", "best", "reported", "approach", "in", "that", "paper", ".", "It", "uses", "a", "max", "operation", "(", "rather", "than", "softmax", ")", "at", "each", "layer", "which", "is", "trained", "directly", "with", "supporting", "facts", "(", "strong", "supervision", ")", ".", "It", "employs", "-", "gram", "modeling", ",", "nonlinear", "layers", "and", "an", "adaptive", "number", "of", "hops", "per", "query", ".", "MemNN", "-", "WSH", ":", "A", "weakly", "supervised", "heuristic", "version", "of", "MemNN", "where", "the", "supporting", "sentence", "labels", "are", "not", "used", "in", "training", ".", "Since", "we", "are", "unable", "to", "backpropagate", "through", "the", "max", "operations", "in", "each", "layer", ",", "we", "enforce", "that", "the", "first", "memory", "hop", "should", "share", "at", "least", "one", "word", "with", "the", "question", ",", "and", "that", "the", "second", "memory", "hop", "should", "share", "at", "least", "one", "word", "with", "the", "first", "hop", "and", "at", "least", "one", "word", "with", "the", "answer", ".", "All", "those", "memories", "that", "conform", "are", "called", "valid", "memories", ",", "and", "the", "goal", "during", "training", "is", "to", "rank", "them", "higher", "than", "invalid", "memories", "using", "the", "same", "ranking", "criteria", "as", "during", "strongly", "supervised", "training", ".", "LSTM", ":", "A", "standard", "LSTM", "model", ",", "trained", "using", "question", "/", "answer", "pairs", "only", "(", "i.e.", "also", "weakly", "supervised", ")", ".", "For", "more", "detail", ",", "see", ".", "subsection", ":", "Results", "We", "report", "a", "variety", "of", "design", "choices", ":", "(", "i", ")", "BoW", "vs", "Position", "Encoding", "(", "PE", ")", "sentence", "representation", ";", "(", "ii", ")", "training", "on", "all", "20", "tasks", "independently", "vs", "jointly", "training", "(", "joint", "training", "used", "an", "embedding", "dimension", "of", ",", "while", "independent", "training", "used", ")", ";", "(", "iii", ")", "two", "phase", "training", ":", "linear", "start", "(", "LS", ")", "where", "softmaxes", "are", "removed", "initially", "vs", "training", "with", "softmaxes", "from", "the", "start", ";", "(", "iv", ")", "varying", "memory", "hops", "from", "1", "to", "3", ".", "The", "results", "across", "all", "20", "tasks", "are", "given", "in", "Table", "[", "reference", "]", "for", "the", "1k", "training", "set", ",", "along", "with", "the", "mean", "performance", "for", "10k", "training", "set", ".", "They", "show", "a", "number", "of", "interesting", "points", ":", "The", "best", "MemN2N", "models", "are", "reasonably", "close", "to", "the", "supervised", "models", "(", "e.g.", "1k", ":", "6.7", "%", "for", "MemNN", "vs", "12.6", "%", "for", "MemN2N", "with", "position", "encoding", "+", "linear", "start", "+", "random", "noise", ",", "jointly", "trained", "and", "10k", ":", "3.2", "%", "for", "MemNN", "vs", "4.2", "%", "for", "MemN2N", "with", "position", "encoding", "+", "linear", "start", "+", "random", "noise", "+", "non", "-", "linearity", ",", "although", "the", "supervised", "models", "are", "still", "superior", ".", "All", "variants", "of", "our", "proposed", "model", "comfortably", "beat", "the", "weakly", "supervised", "baseline", "methods", ".", "The", "position", "encoding", "(", "PE", ")", "representation", "improves", "over", "bag", "-", "of", "-", "words", "(", "BoW", ")", ",", "as", "demonstrated", "by", "clear", "improvements", "on", "tasks", "4", ",", "5", ",", "15", "and", "18", ",", "where", "word", "ordering", "is", "particularly", "important", ".", "The", "linear", "start", "(", "LS", ")", "to", "training", "seems", "to", "help", "avoid", "local", "minima", ".", "See", "task", "16", "in", "Table", "[", "reference", "]", ",", "where", "PE", "alone", "gets", "53.6", "%", "error", ",", "while", "using", "LS", "reduces", "it", "to", "1.6", "%", ".", "Jittering", "the", "time", "index", "with", "random", "empty", "memories", "(", "RN", ")", "as", "described", "in", "Section", "[", "reference", "]", "gives", "a", "small", "but", "consistent", "boost", "in", "performance", ",", "especially", "for", "the", "smaller", "1k", "training", "set", ".", "Joint", "training", "on", "all", "tasks", "helps", ".", "Importantly", ",", "more", "computational", "hops", "give", "improved", "performance", ".", "We", "give", "examples", "of", "the", "hops", "performed", "(", "via", "the", "values", "of", "eq", ".", "(", "[", "reference", "]", ")", ")", "over", "some", "illustrative", "examples", "in", "Fig", ".", "[", "reference", "]", "and", "in", "Appendix", "[", "reference", "]", ".", "section", ":", "Language", "Modeling", "Experiments", "The", "goal", "in", "language", "modeling", "is", "to", "predict", "the", "next", "word", "in", "a", "text", "sequence", "given", "the", "previous", "words", ".", "We", "now", "explain", "how", "our", "model", "can", "easily", "be", "applied", "to", "this", "task", ".", "We", "now", "operate", "on", "word", "level", ",", "as", "opposed", "to", "the", "sentence", "level", ".", "Thus", "the", "previous", "words", "in", "the", "sequence", "(", "including", "the", "current", ")", "are", "embedded", "into", "memory", "separately", ".", "Each", "memory", "cell", "holds", "only", "a", "single", "word", ",", "so", "there", "is", "no", "need", "for", "the", "BoW", "or", "linear", "mapping", "representations", "used", "in", "the", "QA", "tasks", ".", "We", "employ", "the", "temporal", "embedding", "approach", "of", "Section", "[", "reference", "]", ".", "Since", "there", "is", "no", "longer", "any", "question", ",", "in", "Fig", ".", "[", "reference", "]", "is", "fixed", "to", "a", "constant", "vector", "0.1", "(", "without", "embedding", ")", ".", "The", "output", "softmax", "predicts", "which", "word", "in", "the", "vocabulary", "(", "of", "size", ")", "is", "next", "in", "the", "sequence", ".", "A", "cross", "-", "entropy", "loss", "is", "used", "to", "train", "model", "by", "backpropagating", "the", "error", "through", "multiple", "memory", "layers", ",", "in", "the", "same", "manner", "as", "the", "QA", "tasks", ".", "To", "aid", "training", ",", "we", "apply", "ReLU", "operations", "to", "half", "of", "the", "units", "in", "each", "layer", ".", "We", "use", "layer", "-", "wise", "(", "RNN", "-", "like", ")", "weight", "sharing", ",", "i.e.", "the", "query", "weights", "of", "each", "layer", "are", "the", "same", ";", "the", "output", "weights", "of", "each", "layer", "are", "the", "same", ".", "As", "noted", "in", "Section", "[", "reference", "]", ",", "this", "makes", "our", "architecture", "closely", "related", "to", "an", "RNN", "which", "is", "traditionally", "used", "for", "language", "modeling", "tasks", ";", "however", "here", "the", "\u201c", "sequence", "\u201d", "over", "which", "the", "network", "is", "recurrent", "is", "not", "in", "the", "text", ",", "but", "in", "the", "memory", "hops", ".", "Furthermore", ",", "the", "weight", "tying", "restricts", "the", "number", "of", "parameters", "in", "the", "model", ",", "helping", "generalization", "for", "the", "deeper", "models", "which", "we", "find", "to", "be", "effective", "for", "this", "task", ".", "We", "use", "two", "different", "datasets", ":", "Penn", "Tree", "Bank", ":", "This", "consists", "of", "929k", "/", "73k", "/", "82k", "train", "/", "validation", "/", "test", "words", ",", "distributed", "over", "a", "vocabulary", "of", "10k", "words", ".", "The", "same", "preprocessing", "as", "was", "used", ".", "Text8", ":", "This", "is", "a", "a", "pre", "-", "processed", "version", "of", "the", "first", "100", "M", "million", "characters", ",", "dumped", "from", "Wikipedia", ".", "This", "is", "split", "into", "93.3M", "/", "5.7M", "/", "1", "M", "character", "train", "/", "validation", "/", "test", "sets", ".", "All", "word", "occurring", "less", "than", "5", "times", "are", "replaced", "with", "the", "UNK", "token", ",", "resulting", "in", "a", "vocabulary", "size", "of", "44k", ".", "subsection", ":", "Training", "Details", "The", "training", "procedure", "we", "use", "is", "the", "same", "as", "the", "QA", "tasks", ",", "except", "for", "the", "following", ".", "For", "each", "mini", "-", "batch", "update", ",", "the", "norm", "of", "the", "whole", "gradient", "of", "all", "parameters", "is", "measured", "and", "if", "larger", "than", ",", "then", "it", "is", "scaled", "down", "to", "have", "norm", ".", "This", "was", "crucial", "for", "good", "performance", ".", "We", "use", "the", "learning", "rate", "annealing", "schedule", "from", ",", "namely", ",", "if", "the", "validation", "cost", "has", "not", "decreased", "after", "one", "epoch", ",", "then", "the", "learning", "rate", "is", "scaled", "down", "by", "a", "factor", "1.5", ".", "Training", "terminates", "when", "the", "learning", "rate", "drops", "below", ",", "i.e.", "after", "50", "epochs", "or", "so", ".", "Weights", "are", "initialized", "using", "and", "batch", "size", "is", "set", "to", "128", ".", "On", "the", "Penn", "tree", "dataset", ",", "we", "repeat", "each", "training", "10", "times", "with", "different", "random", "initializations", "and", "pick", "the", "one", "with", "smallest", "validation", "cost", ".", "However", ",", "we", "have", "done", "only", "a", "single", "training", "run", "on", "Text8", "dataset", "due", "to", "limited", "time", "constraints", ".", "subsection", ":", "Results", "Table", "[", "reference", "]", "compares", "our", "model", "to", "RNN", ",", "LSTM", "and", "Structurally", "Constrained", "Recurrent", "Nets", "(", "SCRN", ")", "baselines", "on", "the", "two", "benchmark", "datasets", ".", "Note", "that", "the", "baseline", "architectures", "were", "tuned", "in", "to", "give", "optimal", "perplexity", ".", "Our", "MemN2N", "approach", "achieves", "lower", "perplexity", "on", "both", "datasets", "(", "111", "vs", "115", "for", "RNN", "/", "SCRN", "on", "Penn", "and", "147", "vs", "154", "for", "LSTM", "on", "Text8", ")", ".", "Note", "that", "MemN2N", "has", "1.5x", "more", "parameters", "than", "RNNs", "with", "the", "same", "number", "of", "hidden", "units", ",", "while", "LSTM", "has", "4x", "more", "parameters", ".", "We", "also", "vary", "the", "number", "of", "hops", "and", "memory", "size", "of", "our", "MemN2N", ",", "showing", "the", "contribution", "of", "both", "to", "performance", ";", "note", "in", "particular", "that", "increasing", "the", "number", "of", "hops", "helps", ".", "In", "Fig", ".", "[", "reference", "]", ",", "we", "show", "how", "MemN2N", "operates", "on", "memory", "with", "multiple", "hops", ".", "It", "shows", "the", "average", "weight", "of", "the", "activation", "of", "each", "memory", "position", "over", "the", "test", "set", ".", "We", "can", "see", "that", "some", "hops", "concentrate", "only", "on", "recent", "words", ",", "while", "other", "hops", "have", "more", "broad", "attention", "over", "all", "memory", "locations", ",", "which", "is", "consistent", "with", "the", "idea", "that", "succesful", "language", "models", "consist", "of", "a", "smoothed", "-", "gram", "model", "and", "a", "cache", ".", "Interestingly", ",", "it", "seems", "that", "those", "two", "types", "of", "hops", "tend", "to", "alternate", ".", "Also", "note", "that", "unlike", "a", "traditional", "RNN", ",", "the", "cache", "does", "not", "decay", "exponentially", ":", "it", "has", "roughly", "the", "same", "average", "activation", "across", "the", "entire", "memory", ".", "This", "may", "be", "the", "source", "of", "the", "observed", "improvement", "in", "language", "modeling", ".", "section", ":", "Conclusions", "and", "Future", "Work", "In", "this", "work", "we", "showed", "that", "a", "neural", "network", "with", "an", "explicit", "memory", "and", "a", "recurrent", "attention", "mechanism", "for", "reading", "the", "memory", "can", "be", "successfully", "trained", "via", "backpropagation", "on", "diverse", "tasks", "from", "question", "answering", "to", "language", "modeling", ".", "Compared", "to", "the", "Memory", "Network", "implementation", "of", "there", "is", "no", "supervision", "of", "supporting", "facts", "and", "so", "our", "model", "can", "be", "used", "in", "a", "wider", "range", "of", "settings", ".", "Our", "model", "approaches", "the", "same", "performance", "of", "that", "model", ",", "and", "is", "significantly", "better", "than", "other", "baselines", "with", "the", "same", "level", "of", "supervision", ".", "On", "language", "modeling", "tasks", ",", "it", "slightly", "outperforms", "tuned", "RNNs", "and", "LSTMs", "of", "comparable", "complexity", ".", "On", "both", "tasks", "we", "can", "see", "that", "increasing", "the", "number", "of", "memory", "hops", "improves", "performance", ".", "However", ",", "there", "is", "still", "much", "to", "do", ".", "Our", "model", "is", "still", "unable", "to", "exactly", "match", "the", "performance", "of", "the", "memory", "networks", "trained", "with", "strong", "supervision", ",", "and", "both", "fail", "on", "several", "of", "the", "1k", "QA", "tasks", ".", "Furthermore", ",", "smooth", "lookups", "may", "not", "scale", "well", "to", "the", "case", "where", "a", "larger", "memory", "is", "required", ".", "For", "these", "settings", ",", "we", "plan", "to", "explore", "multiscale", "notions", "of", "attention", "or", "hashing", ",", "as", "proposed", "in", ".", "section", ":", "Acknowledgments", "The", "authors", "would", "like", "to", "thank", "Armand", "Joulin", ",", "Tomas", "Mikolov", ",", "Antoine", "Bordes", "and", "Sumit", "Chopra", "for", "useful", "comments", "and", "valuable", "discussions", ",", "and", "also", "the", "FAIR", "Infrastructure", "team", "for", "their", "help", "and", "support", ".", "bibliography", ":", "References", "section", ":", "Results", "on", "10k", "QA", "dataset", "section", ":", "Visualization", "of", "attention", "weights", "in", "QA", "problems"]}