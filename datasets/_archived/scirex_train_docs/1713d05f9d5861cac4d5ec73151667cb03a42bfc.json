{"coref": {"BLEU_score": [[3965, 3967], [4266, 4268]], "DCCL": [[6, 10]], "IWSLT2015_German-English": [[4011, 4013]], "Machine_Translation": [[170, 173], [1170, 1173], [2728, 2730], [2872, 2874], [3026, 3029], [3843, 3845], [3848, 3851], [3892, 3894], [5035, 5038]]}, "coref_non_salient": {"0": [[1636, 1638], [2380, 2382]], "1": [[2522, 2523], [3355, 3357]], "10": [[3347, 3349], [4118, 4120], [4128, 4130], [4152, 4155]], "100": [[1994, 1997], [2022, 2024]], "101": [[2513, 2515]], "102": [[1269, 1273]], "103": [[1265, 1268]], "104": [[4655, 4659]], "105": [[1086, 1088]], "106": [[3179, 3181]], "107": [[375, 377]], "108": [[3723, 3724]], "109": [[5102, 5104]], "11": [[1522, 1524], [1547, 1549], [1639, 1641], [3719, 3722]], "110": [[96, 101]], "111": [[1599, 1602]], "112": [[2888, 2890], [3324, 3326], [3365, 3367]], "113": [[4111, 4115]], "114": [[1234, 1236]], "115": [[418, 422]], "116": [[2037, 2039]], "117": [[1507, 1511]], "118": [[3002, 3005]], "12": [[745, 747], [1131, 1134], [1872, 1874], [1882, 1885], [2050, 2053], [2139, 2142], [2182, 2185], [2274, 2276], [2305, 2308], [3388, 3390], [4392, 4394], [4502, 4504], [4962, 4965], [5112, 5115]], "13": [[142, 149], [1064, 1071], [1193, 1200], [1693, 1696], [2608, 2615], [3034, 3038], [3065, 3071]], "14": [[2005, 2007], [2291, 2295]], "15": [[227, 229], [245, 248], [1789, 1791], [2861, 2863]], "16": [[961, 966], [1532, 1534]], "17": [[3137, 3139], [3476, 3478], [4258, 4260], [4284, 4286], [4308, 4310]], "18": [[14, 15], [241, 242], [3247, 3249], [39, 40], [442, 443], [2702, 2703], [2866, 2867]], "19": [[1658, 1661], [4442, 4444]], "2": [[1325, 1327], [1351, 1353]], "20": [[3866, 3874], [4168, 4170]], "21": [[1582, 1584], [4604, 4606]], "22": [[1775, 1781], [1847, 1851]], "23": [[4200, 4202], [4566, 4568]], "24": [[1785, 1787], [2540, 2542], [3337, 3339], [3392, 3394], [4406, 4408]], "25": [[946, 949], [1475, 1478]], "26": [[1454, 1458], [2821, 2824]], "27": [[1203, 1207], [2325, 2328], [2490, 2494], [2633, 2638]], "28": [[811, 813], [1544, 1546], [1895, 1897], [2197, 2199], [2832, 2834], [3405, 3407], [3782, 3784], [4074, 4075], [4790, 4792], [4813, 4815], [4859, 4861], [4864, 4866]], "29": [[42, 44], [413, 415]], "3": [[84, 86], [1306, 1307], [1382, 1384], [2090, 2092], [5129, 5131]], "30": [[4122, 4123]], "31": [[910, 911], [2718, 2719], [3021, 3022], [3257, 3258], [3292, 3293], [3378, 3379], [3679, 3680], [3731, 3732], [4626, 4627]], "32": [[4485, 4487]], "33": [[90, 92], [160, 162], [193, 195], [1175, 1180], [2854, 2860], [3703, 3705], [3772, 3778], [3805, 3807], [3825, 3827], [4461, 4466], [4506, 4511], [4544, 4546], [4675, 4677], [5048, 5053]], "34": [[1822, 1824], [3319, 3321], [4063, 4065]], "35": [[2083, 2085], [3637, 3639]], "36": [[313, 316], [3655, 3658]], "37": [[165, 168], [1162, 1165], [2869, 2871], [3008, 3011], [3205, 3207], [3210, 3212], [4529, 4532], [4631, 4634], [5029, 5033], [5180, 5183]], "38": [[1288, 1290], [1397, 1399], [3448, 3450]], "39": [[1551, 1553], [4693, 4695], [4968, 4970]], "4": [[2322, 2324], [2654, 2657], [2811, 2814], [2984, 2986], [3194, 3196], [3428, 3431], [3525, 3528], [4318, 4320], [4382, 4385], [4884, 4887]], "40": [[4131, 4133]], "41": [[3098, 3101], [3688, 3690]], "42": [[3649, 3651], [3683, 3684], [3710, 3712], [3815, 3817]], "43": [[3934, 3938], [4040, 4044]], "44": [[990, 994], [1561, 1566]], "45": [[4125, 4126]], "46": [[152, 156], [1075, 1080], [1738, 1742], [2456, 2461], [2568, 2572], [5008, 5012]], "47": [[969, 971], [973, 975], [1514, 1516], [1595, 1597], [1618, 1620], [1901, 1903]], "48": [[10, 13], [234, 240]], "49": [[781, 783], [3060, 3061], [3369, 3371], [3618, 3620], [5021, 5023]], "5": [[2345, 2347], [2452, 2454], [3490, 3491], [3501, 3503], [3530, 3533], [3594, 3596], [3644, 3646]], "50": [[5150, 5152]], "51": [[2, 5], [1890, 1893]], "52": [[3760, 3762]], "53": [[1366, 1368], [1394, 1396]], "54": [[4598, 4600], [5065, 5067]], "55": [[2224, 2226], [2317, 2319]], "56": [[1012, 1014], [1137, 1139], [2264, 2265], [2416, 2418], [4918, 4920]], "57": [[3452, 3455]], "58": [[4250, 4254]], "59": [[2256, 2258]], "6": [[828, 830], [2756, 2758]], "60": [[2363, 2366]], "61": [[2948, 2949]], "62": [[407, 412]], "63": [[1185, 1188]], "64": [[4052, 4059]], "65": [[1339, 1340]], "66": [[4330, 4331]], "67": [[2001, 2004]], "68": [[1302, 1304]], "69": [[3279, 3281]], "7": [[1216, 1218], [1486, 1487], [1651, 1655], [1706, 1707], [2040, 2041], [2427, 2430], [2644, 2646]], "70": [[1911, 1913], [2010, 2012]], "71": [[2349, 2351]], "72": [[1410, 1412], [1444, 1446], [1469, 1471], [3706, 3708], [4454, 4456]], "73": [[1237, 1239]], "74": [[4189, 4190]], "75": [[3328, 3330], [3383, 3385]], "76": [[4589, 4591]], "77": [[3838, 3840]], "78": [[1298, 1300]], "79": [[4236, 4241]], "8": [[4682, 4684], [4759, 4761], [4786, 4788]], "80": [[3897, 3900]], "81": [[2447, 2449]], "82": [[221, 223]], "83": [[1708, 1709], [2042, 2043]], "84": [[2374, 2377]], "85": [[1744, 1746]], "86": [[3186, 3188]], "87": [[2628, 2631]], "88": [[2892, 2896]], "89": [[103, 106]], "9": [[3855, 3864], [4161, 4163]], "90": [[4205, 4209]], "91": [[901, 905]], "92": [[907, 908]], "93": [[3039, 3040], [4080, 4081], [4345, 4346], [4389, 4390], [4552, 4553]], "94": [[367, 369], [383, 385], [477, 479], [1213, 1215], [1255, 1257], [1711, 1713], [2535, 2537], [2641, 2643], [2743, 2745], [2907, 2909], [5005, 5007]], "95": [[2945, 2946]], "96": [[2249, 2251]], "97": [[3131, 3133], [3468, 3470]], "98": [[1767, 1771]], "99": [[202, 206]]}, "doc_id": "1713d05f9d5861cac4d5ec73151667cb03a42bfc", "method_subrelations": {"DCCL": [[[0, 4], "DCCL"]]}, "n_ary_relations": [{"Material": "IWSLT2015_German-English", "Method": "DCCL", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "29.56"}], "ner": [[2, 5, "Task"], [6, 10, "Method"], [10, 13, "Method"], [14, 15, "Method"], [42, 44, "Task"], [84, 86, "Method"], [90, 92, "Metric"], [96, 101, "Method"], [103, 106, "Method"], [142, 149, "Method"], [152, 156, "Method"], [160, 162, "Metric"], [165, 168, "Task"], [170, 173, "Task"], [193, 195, "Metric"], [202, 206, "Task"], [221, 223, "Method"], [227, 229, "Method"], [234, 240, "Method"], [241, 242, "Method"], [245, 248, "Method"], [313, 316, "Method"], [367, 369, "Method"], [375, 377, "Metric"], [383, 385, "Method"], [407, 412, "Task"], [413, 415, "Task"], [418, 422, "Method"], [477, 479, "Method"], [745, 747, "Method"], [781, 783, "Method"], [811, 813, "Method"], [828, 830, "Method"], [901, 905, "Method"], [907, 908, "Method"], [910, 911, "Method"], [946, 949, "Task"], [961, 966, "Task"], [969, 971, "Task"], [973, 975, "Task"], [990, 994, "Task"], [1012, 1014, "Task"], [1064, 1071, "Method"], [1075, 1080, "Method"], [1086, 1088, "Method"], [1131, 1134, "Method"], [1137, 1139, "Task"], [1162, 1165, "Task"], [1170, 1173, "Task"], [1175, 1180, "Metric"], [1185, 1188, "Method"], [1193, 1200, "Method"], [1203, 1207, "Method"], [1213, 1215, "Method"], [1216, 1218, "Task"], [1234, 1236, "Method"], [1237, 1239, "Task"], [1255, 1257, "Method"], [1265, 1268, "Method"], [1269, 1273, "Method"], [1288, 1290, "Method"], [1298, 1300, "Task"], [1302, 1304, "Method"], [1306, 1307, "Method"], [1325, 1327, "Method"], [1339, 1340, "Method"], [1351, 1353, "Method"], [1366, 1368, "Method"], [1382, 1384, "Method"], [1394, 1396, "Method"], [1397, 1399, "Method"], [1410, 1412, "Method"], [1444, 1446, "Method"], [1454, 1458, "Method"], [1469, 1471, "Method"], [1475, 1478, "Task"], [1486, 1487, "Task"], [1507, 1511, "Task"], [1514, 1516, "Task"], [1522, 1524, "Method"], [1532, 1534, "Task"], [1544, 1546, "Method"], [1547, 1549, "Method"], [1551, 1553, "Metric"], [1561, 1566, "Task"], [1582, 1584, "Method"], [1595, 1597, "Task"], [1599, 1602, "Method"], [1618, 1620, "Task"], [1636, 1638, "Method"], [1639, 1641, "Method"], [1651, 1655, "Task"], [1658, 1661, "Method"], [1693, 1696, "Method"], [1706, 1707, "Task"], [1708, 1709, "Task"], [1711, 1713, "Method"], [1738, 1742, "Method"], [1744, 1746, "Task"], [1767, 1771, "Method"], [1775, 1781, "Method"], [1785, 1787, "Method"], [1789, 1791, "Method"], [1822, 1824, "Method"], [1847, 1851, "Method"], [1872, 1874, "Method"], [1882, 1885, "Method"], [1890, 1893, "Task"], [1895, 1897, "Method"], [1901, 1903, "Task"], [1911, 1913, "Method"], [1994, 1997, "Task"], [2001, 2004, "Task"], [2005, 2007, "Method"], [2010, 2012, "Method"], [2022, 2024, "Task"], [2037, 2039, "Method"], [2040, 2041, "Task"], [2042, 2043, "Task"], [2050, 2053, "Method"], [2083, 2085, "Metric"], [2090, 2092, "Method"], [2139, 2142, "Method"], [2182, 2185, "Method"], [2197, 2199, "Method"], [2224, 2226, "Metric"], [2249, 2251, "Method"], [2256, 2258, "Task"], [2264, 2265, "Task"], [2274, 2276, "Method"], [2291, 2295, "Method"], [2305, 2308, "Method"], [2317, 2319, "Metric"], [2322, 2324, "Method"], [2325, 2328, "Method"], [2345, 2347, "Metric"], [2349, 2351, "Metric"], [2363, 2366, "Method"], [2374, 2377, "Method"], [2380, 2382, "Method"], [2416, 2418, "Task"], [2427, 2430, "Task"], [2447, 2449, "Method"], [2452, 2454, "Metric"], [2456, 2461, "Method"], [2490, 2494, "Method"], [2513, 2515, "Method"], [2522, 2523, "Method"], [2535, 2537, "Method"], [2540, 2542, "Method"], [2568, 2572, "Method"], [2608, 2615, "Method"], [2628, 2631, "Method"], [2633, 2638, "Method"], [2641, 2643, "Method"], [2644, 2646, "Task"], [2654, 2657, "Method"], [2728, 2730, "Task"], [2743, 2745, "Method"], [2756, 2758, "Method"], [2811, 2814, "Method"], [2821, 2824, "Method"], [2832, 2834, "Method"], [2854, 2860, "Metric"], [2861, 2863, "Method"], [2869, 2871, "Task"], [2872, 2874, "Task"], [2888, 2890, "Method"], [2892, 2896, "Method"], [2907, 2909, "Method"], [2945, 2946, "Method"], [2948, 2949, "Method"], [2984, 2986, "Method"], [3002, 3005, "Method"], [3008, 3011, "Task"], [3026, 3029, "Task"], [3034, 3038, "Method"], [3039, 3040, "Method"], [3060, 3061, "Method"], [3065, 3071, "Method"], [3098, 3101, "Method"], [3131, 3133, "Method"], [3137, 3139, "Metric"], [3179, 3181, "Method"], [3186, 3188, "Method"], [3194, 3196, "Method"], [3205, 3207, "Task"], [3210, 3212, "Task"], [3247, 3249, "Method"], [3279, 3281, "Task"], [3319, 3321, "Method"], [3324, 3326, "Method"], [3328, 3330, "Method"], [3337, 3339, "Method"], [3347, 3349, "Method"], [3355, 3357, "Method"], [3365, 3367, "Method"], [3369, 3371, "Method"], [3383, 3385, "Method"], [3388, 3390, "Method"], [3392, 3394, "Method"], [3405, 3407, "Method"], [3428, 3431, "Method"], [3448, 3450, "Method"], [3452, 3455, "Method"], [3468, 3470, "Method"], [3476, 3478, "Metric"], [3490, 3491, "Metric"], [3501, 3503, "Metric"], [3525, 3528, "Method"], [3530, 3533, "Metric"], [3594, 3596, "Metric"], [3618, 3620, "Method"], [3637, 3639, "Metric"], [3644, 3646, "Metric"], [3649, 3651, "Metric"], [3655, 3658, "Method"], [3683, 3684, "Metric"], [3688, 3690, "Method"], [3703, 3705, "Metric"], [3706, 3708, "Method"], [3710, 3712, "Metric"], [3719, 3722, "Method"], [3723, 3724, "Method"], [3760, 3762, "Metric"], [3772, 3778, "Metric"], [3782, 3784, "Method"], [3805, 3807, "Metric"], [3815, 3817, "Metric"], [3825, 3827, "Metric"], [3838, 3840, "Method"], [3843, 3845, "Task"], [3848, 3851, "Task"], [3855, 3864, "Task"], [3866, 3874, "Task"], [3892, 3894, "Task"], [3897, 3900, "Method"], [3934, 3938, "Method"], [3965, 3967, "Metric"], [4011, 4013, "Material"], [4040, 4044, "Method"], [4052, 4059, "Method"], [4063, 4065, "Method"], [4074, 4075, "Method"], [4111, 4115, "Method"], [4118, 4120, "Method"], [4122, 4123, "Method"], [4125, 4126, "Method"], [4128, 4130, "Method"], [4131, 4133, "Method"], [4152, 4155, "Method"], [4161, 4163, "Task"], [4168, 4170, "Task"], [4189, 4190, "Method"], [4200, 4202, "Method"], [4205, 4209, "Method"], [4236, 4241, "Method"], [4250, 4254, "Method"], [4258, 4260, "Metric"], [4266, 4268, "Metric"], [4284, 4286, "Metric"], [4308, 4310, "Metric"], [4318, 4320, "Method"], [4330, 4331, "Method"], [4382, 4385, "Method"], [4392, 4394, "Method"], [4406, 4408, "Method"], [4442, 4444, "Method"], [4454, 4456, "Method"], [4461, 4466, "Metric"], [4485, 4487, "Metric"], [4502, 4504, "Method"], [4506, 4511, "Metric"], [4529, 4532, "Task"], [4544, 4546, "Metric"], [4566, 4568, "Method"], [4589, 4591, "Task"], [4598, 4600, "Task"], [4604, 4606, "Method"], [4631, 4634, "Task"], [4655, 4659, "Method"], [4675, 4677, "Metric"], [4682, 4684, "Metric"], [4693, 4695, "Metric"], [4759, 4761, "Metric"], [4786, 4788, "Metric"], [4790, 4792, "Method"], [4813, 4815, "Method"], [4859, 4861, "Method"], [4864, 4866, "Method"], [4884, 4887, "Method"], [4918, 4920, "Task"], [4962, 4965, "Method"], [4968, 4970, "Metric"], [5005, 5007, "Method"], [5008, 5012, "Method"], [5021, 5023, "Method"], [5029, 5033, "Task"], [5035, 5038, "Task"], [5048, 5053, "Metric"], [5065, 5067, "Task"], [5102, 5104, "Method"], [5112, 5115, "Method"], [5129, 5131, "Method"], [5150, 5152, "Method"], [5180, 5183, "Task"], [39, 40, "Method"], [442, 443, "Method"], [2702, 2703, "Method"], [2718, 2719, "Method"], [2866, 2867, "Method"], [3021, 3022, "Method"], [3257, 3258, "Method"], [3292, 3293, "Method"], [3378, 3379, "Method"], [3679, 3680, "Method"], [3731, 3732, "Method"], [4080, 4081, "Method"], [4345, 4346, "Method"], [4389, 4390, "Method"], [4552, 4553, "Method"], [4626, 4627, "Method"]], "sections": [[0, 224], [224, 1258], [1258, 1868], [1868, 2320], [2320, 2842], [2842, 2982], [2982, 3203], [3203, 3841], [3841, 4596], [4596, 4600], [4600, 4678], [4678, 4898], [4898, 5132], [5132, 5135], [5135, 5213]], "sentences": [[0, 10], [10, 37], [37, 56], [56, 71], [71, 87], [87, 107], [107, 132], [132, 157], [157, 177], [177, 196], [196, 224], [224, 227], [227, 245], [245, 258], [258, 282], [282, 307], [307, 330], [330, 358], [358, 378], [378, 401], [401, 430], [430, 455], [455, 477], [477, 490], [490, 519], [519, 528], [528, 549], [549, 567], [567, 587], [587, 613], [613, 621], [621, 630], [630, 638], [638, 666], [666, 678], [678, 709], [709, 734], [734, 736], [736, 757], [757, 774], [774, 798], [798, 831], [831, 865], [865, 893], [893, 914], [914, 917], [917, 933], [933, 943], [943, 975], [975, 979], [979, 995], [995, 1019], [1019, 1041], [1041, 1045], [1045, 1072], [1072, 1093], [1093, 1115], [1115, 1144], [1144, 1169], [1169, 1182], [1182, 1212], [1212, 1225], [1225, 1258], [1258, 1262], [1262, 1279], [1279, 1280], [1280, 1288], [1288, 1290], [1290, 1300], [1300, 1302], [1302, 1328], [1328, 1354], [1354, 1369], [1369, 1397], [1397, 1410], [1410, 1427], [1427, 1436], [1436, 1440], [1440, 1459], [1459, 1472], [1472, 1512], [1512, 1524], [1524, 1540], [1540, 1554], [1554, 1567], [1567, 1589], [1589, 1603], [1603, 1621], [1621, 1631], [1631, 1650], [1650, 1667], [1667, 1697], [1697, 1728], [1728, 1747], [1747, 1772], [1772, 1792], [1792, 1811], [1811, 1825], [1825, 1856], [1856, 1868], [1868, 1874], [1874, 1894], [1894, 1903], [1903, 1905], [1905, 1921], [1921, 1943], [1943, 1966], [1966, 1989], [1989, 2008], [2008, 2047], [2047, 2063], [2063, 2101], [2101, 2112], [2112, 2125], [2125, 2137], [2137, 2159], [2159, 2178], [2178, 2193], [2193, 2207], [2207, 2227], [2227, 2252], [2252, 2272], [2272, 2289], [2289, 2304], [2304, 2320], [2320, 2328], [2328, 2342], [2342, 2354], [2354, 2357], [2357, 2378], [2378, 2395], [2395, 2455], [2455, 2462], [2462, 2483], [2483, 2487], [2487, 2524], [2524, 2556], [2556, 2567], [2567, 2578], [2578, 2588], [2588, 2599], [2599, 2603], [2603, 2620], [2620, 2639], [2639, 2652], [2652, 2678], [2678, 2700], [2700, 2721], [2721, 2746], [2746, 2764], [2764, 2784], [2784, 2810], [2810, 2842], [2842, 2845], [2845, 2875], [2875, 2897], [2897, 2916], [2916, 2929], [2929, 2950], [2950, 2958], [2958, 2982], [2982, 2986], [2986, 3006], [3006, 3024], [3024, 3050], [3050, 3062], [3062, 3083], [3083, 3102], [3102, 3117], [3117, 3129], [3129, 3142], [3142, 3151], [3151, 3175], [3175, 3203], [3203, 3207], [3207, 3209], [3209, 3239], [3239, 3250], [3250, 3275], [3275, 3303], [3303, 3319], [3319, 3322], [3322, 3340], [3340, 3363], [3363, 3381], [3381, 3401], [3401, 3432], [3432, 3444], [3444, 3460], [3460, 3480], [3480, 3497], [3497, 3506], [3506, 3508], [3508, 3529], [3529, 3549], [3549, 3583], [3583, 3597], [3597, 3621], [3621, 3640], [3640, 3674], [3674, 3679], [3679, 3696], [3696, 3713], [3713, 3727], [3727, 3751], [3751, 3766], [3766, 3785], [3785, 3811], [3811, 3828], [3828, 3841], [3841, 3845], [3845, 3874], [3874, 3876], [3876, 3895], [3895, 3910], [3910, 3932], [3932, 3960], [3960, 3974], [3974, 3997], [3997, 4010], [4010, 4028], [4028, 4045], [4045, 4063], [4063, 4065], [4065, 4083], [4083, 4106], [4106, 4124], [4124, 4131], [4131, 4133], [4133, 4151], [4151, 4171], [4171, 4189], [4189, 4203], [4203, 4242], [4242, 4263], [4263, 4283], [4283, 4303], [4303, 4315], [4315, 4340], [4340, 4364], [4364, 4386], [4386, 4403], [4403, 4423], [4423, 4425], [4425, 4436], [4436, 4451], [4451, 4460], [4460, 4480], [4480, 4498], [4498, 4526], [4526, 4547], [4547, 4577], [4577, 4596], [4596, 4600], [4600, 4606], [4606, 4635], [4635, 4653], [4653, 4678], [4678, 4684], [4684, 4699], [4699, 4717], [4717, 4738], [4738, 4756], [4756, 4777], [4777, 4793], [4793, 4806], [4806, 4827], [4827, 4828], [4828, 4850], [4850, 4862], [4862, 4879], [4879, 4898], [4898, 4901], [4901, 4921], [4921, 4944], [4944, 4959], [4959, 4971], [4971, 4988], [4988, 5013], [5013, 5043], [5043, 5064], [5064, 5083], [5083, 5109], [5109, 5132], [5132, 5135], [5135, 5141], [5141, 5164], [5164, 5184], [5184, 5213]], "words": ["document", ":", "Compressing", "Word", "Embeddings", "via", "Deep", "Compositional", "Code", "Learning", "Natural", "language", "processing", "(", "NLP", ")", "models", "often", "require", "a", "massive", "number", "of", "parameters", "for", "word", "embeddings", ",", "resulting", "in", "a", "large", "storage", "or", "memory", "footprint", ".", "Deploying", "neural", "NLP", "models", "to", "mobile", "devices", "requires", "compressing", "the", "word", "embeddings", "without", "any", "significant", "sacrifices", "in", "performance", ".", "For", "this", "purpose", ",", "we", "propose", "to", "construct", "the", "embeddings", "with", "few", "basis", "vectors", ".", "For", "each", "word", ",", "the", "composition", "of", "basis", "vectors", "is", "determined", "by", "a", "hash", "code", ".", "To", "maximize", "the", "compression", "rate", ",", "we", "adopt", "the", "multi", "-", "codebook", "quantization", "approach", "instead", "of", "binary", "coding", "scheme", ".", "Each", "code", "is", "composed", "of", "multiple", "discrete", "numbers", ",", "such", "as", ",", "where", "the", "value", "of", "each", "component", "is", "limited", "to", "a", "fixed", "range", ".", "We", "propose", "to", "directly", "learn", "the", "discrete", "codes", "in", "an", "end", "-", "to", "-", "end", "neural", "network", "by", "applying", "the", "Gumbel", "-", "softmax", "trick", ".", "Experiments", "show", "the", "compression", "rate", "achieves", "in", "a", "sentiment", "analysis", "task", "and", "in", "machine", "translation", "tasks", "without", "performance", "loss", ".", "In", "both", "tasks", ",", "the", "proposed", "method", "can", "improve", "the", "model", "performance", "by", "slightly", "lowering", "the", "compression", "rate", ".", "Compared", "to", "other", "approaches", "such", "as", "character", "-", "level", "segmentation", ",", "the", "proposed", "method", "is", "language", "-", "independent", "and", "does", "not", "require", "modifications", "to", "the", "network", "architecture", ".", "section", ":", "Introduction", "Word", "embeddings", "play", "an", "important", "role", "in", "neural", "-", "based", "natural", "language", "processing", "(", "NLP", ")", "models", ".", "Neural", "word", "embeddings", "encapsulate", "the", "linguistic", "information", "of", "words", "in", "continuous", "vectors", ".", "However", ",", "as", "each", "word", "is", "assigned", "an", "independent", "embedding", "vector", ",", "the", "number", "of", "parameters", "in", "the", "embedding", "matrix", "can", "be", "huge", ".", "For", "example", ",", "when", "each", "embedding", "has", "500", "dimensions", ",", "the", "network", "has", "to", "hold", "100", "M", "embedding", "parameters", "to", "represent", "200", "K", "words", ".", "In", "practice", ",", "for", "a", "simple", "sentiment", "analysis", "model", ",", "the", "word", "embedding", "parameters", "account", "for", "98.8", "%", "of", "the", "total", "parameters", ".", "As", "only", "a", "small", "portion", "of", "the", "word", "embeddings", "is", "selected", "in", "the", "forward", "pass", ",", "the", "giant", "embedding", "matrix", "usually", "does", "not", "cause", "a", "speed", "issue", ".", "However", ",", "the", "massive", "number", "of", "parameters", "in", "the", "neural", "network", "results", "in", "a", "large", "storage", "or", "memory", "footprint", ".", "When", "other", "components", "of", "the", "neural", "network", "are", "also", "large", ",", "the", "model", "may", "fail", "to", "fit", "into", "GPU", "memory", "during", "training", ".", "Moreover", ",", "as", "the", "demand", "for", "low", "-", "latency", "neural", "computation", "for", "mobile", "platforms", "increases", ",", "some", "neural", "-", "based", "models", "are", "expected", "to", "run", "on", "mobile", "devices", ".", "Thus", ",", "it", "is", "becoming", "more", "important", "to", "compress", "the", "size", "of", "NLP", "models", "for", "deployment", "to", "devices", "with", "limited", "memory", "or", "storage", "capacity", ".", "In", "this", "study", ",", "we", "attempt", "to", "reduce", "the", "number", "of", "parameters", "used", "in", "word", "embeddings", "without", "hurting", "the", "model", "performance", ".", "Neural", "networks", "are", "known", "for", "the", "significant", "redundancy", "in", "the", "connections", "Denil2013PredictingPI", ".", "In", "this", "work", ",", "we", "further", "hypothesize", "that", "learning", "independent", "embeddings", "causes", "more", "redundancy", "in", "the", "embedding", "vectors", ",", "as", "the", "inter", "-", "similarity", "among", "words", "is", "ignored", ".", "Some", "words", "are", "very", "similar", "regarding", "the", "semantics", ".", "For", "example", ",", "\u201c", "dog", "\u201d", "and", "\u201c", "dogs", "\u201d", "have", "almost", "the", "same", "meaning", ",", "except", "one", "is", "plural", ".", "To", "efficiently", "represent", "these", "two", "words", ",", "it", "is", "desirable", "to", "share", "information", "between", "the", "two", "embeddings", ".", "However", ",", "a", "small", "portion", "in", "both", "vectors", "still", "has", "to", "be", "trained", "independently", "to", "capture", "the", "syntactic", "difference", ".", "Following", "the", "intuition", "of", "creating", "partially", "shared", "embeddings", ",", "instead", "of", "assigning", "each", "word", "a", "unique", "ID", ",", "we", "represent", "each", "word", "with", "a", "code", ".", "Each", "component", "is", "an", "integer", "number", "in", ".", "Ideally", ",", "similar", "words", "should", "have", "similar", "codes", ".", "For", "example", ",", "we", "may", "desire", "and", ".", "Once", "we", "have", "obtained", "such", "compact", "codes", "for", "all", "words", "in", "the", "vocabulary", ",", "we", "use", "embedding", "vectors", "to", "represent", "the", "codes", "rather", "than", "the", "unique", "words", ".", "More", "specifically", ",", "we", "create", "codebooks", ",", "each", "containing", "codeword", "vectors", ".", "The", "embedding", "of", "a", "word", "is", "computed", "by", "summing", "up", "the", "codewords", "corresponding", "to", "all", "the", "components", "in", "the", "code", "as", "where", "is", "the", "-", "th", "codeword", "in", "the", "codebook", ".", "In", "this", "way", ",", "the", "number", "of", "vectors", "in", "the", "embedding", "matrix", "will", "be", ",", "which", "is", "usually", "much", "smaller", "than", "the", "vocabulary", "size", ".", "Fig", ".", "[", "reference", "]", "gives", "an", "intuitive", "comparison", "between", "the", "compositional", "approach", "and", "the", "conventional", "approach", "(", "assigning", "unique", "IDs", ")", ".", "The", "codes", "of", "all", "the", "words", "can", "be", "stored", "in", "an", "integer", "matrix", ",", "denoted", "by", ".", "Thus", ",", "the", "storage", "footprint", "of", "the", "embedding", "layer", "now", "depends", "on", "the", "total", "size", "of", "the", "combined", "codebook", "and", "the", "code", "matrix", ".", "Although", "the", "number", "of", "embedding", "vectors", "can", "be", "greatly", "reduced", "by", "using", "such", "coding", "approach", ",", "we", "want", "to", "prevent", "any", "serious", "degradation", "in", "performance", "compared", "to", "the", "models", "using", "normal", "embeddings", ".", "In", "other", "words", ",", "given", "a", "set", "of", "baseline", "word", "embeddings", ",", "we", "wish", "to", "find", "a", "set", "of", "codes", "and", "combined", "codebook", "that", "can", "produce", "the", "embeddings", "with", "the", "same", "effectiveness", "as", ".", "A", "safe", "and", "straight", "-", "forward", "way", "is", "to", "minimize", "the", "squared", "distance", "between", "the", "baseline", "embeddings", "and", "the", "composed", "embeddings", "as", "where", "is", "the", "vocabulary", "size", ".", "The", "baseline", "embeddings", "can", "be", "a", "set", "of", "pre", "-", "trained", "vectors", "such", "as", "word2vec", "Mikolov2013DistributedRO", "or", "GloVe", "pennington2014GloVe", "embeddings", ".", "In", "Eq", ".", "[", "reference", "]", ",", "the", "baseline", "embedding", "matrix", "is", "approximated", "by", "codewords", "selected", "from", "codebooks", ".", "The", "selection", "of", "codewords", "is", "controlled", "by", "the", "code", ".", "Such", "problem", "of", "learning", "compact", "codes", "with", "multiple", "codebooks", "is", "formalized", "and", "discussed", "in", "the", "research", "field", "of", "compression", "-", "based", "source", "coding", ",", "known", "as", "product", "quantization", "Jgou2011ProductQF", "and", "additive", "quantization", "Babenko2014AdditiveQF", ",", "Martinez2016RevisitingAQ", ".", "Previous", "works", "learn", "compositional", "codes", "so", "as", "to", "enable", "an", "efficient", "similarity", "search", "of", "vectors", ".", "In", "this", "work", ",", "we", "utilize", "such", "codes", "for", "a", "different", "purpose", ",", "that", "is", ",", "constructing", "word", "embeddings", "with", "drastically", "fewer", "parameters", ".", "Due", "to", "the", "discreteness", "in", "the", "hash", "codes", ",", "it", "is", "usually", "difficult", "to", "directly", "optimize", "the", "objective", "function", "in", "Eq", ".", "[", "reference", "]", ".", "In", "this", "paper", ",", "we", "propose", "a", "simple", "and", "straight", "-", "forward", "method", "to", "learn", "the", "codes", "in", "an", "end", "-", "to", "-", "end", "neural", "network", ".", "We", "utilize", "the", "Gumbel", "-", "softmax", "trick", "Maddison2016TheCD", ",", "Jang2016CategoricalRW", "to", "find", "the", "best", "discrete", "codes", "that", "minimize", "the", "loss", ".", "Besides", "the", "simplicity", ",", "this", "approach", "also", "allows", "one", "to", "use", "any", "arbitrary", "differentiable", "loss", "function", ",", "such", "as", "cosine", "similarity", ".", "The", "contribution", "of", "this", "work", "can", "be", "summarized", "as", "follows", ":", "We", "propose", "to", "utilize", "the", "compositional", "coding", "approach", "for", "constructing", "the", "word", "embeddings", "with", "significantly", "fewer", "parameters", ".", "In", "the", "experiments", ",", "we", "show", "that", "over", "98", "%", "of", "the", "embedding", "parameters", "can", "be", "eliminated", "in", "sentiment", "analysis", "task", "without", "affecting", "performance", ".", "In", "machine", "translation", "tasks", ",", "the", "loss", "-", "free", "compression", "rate", "reaches", ".", "We", "propose", "a", "direct", "learning", "approach", "for", "the", "codes", "in", "an", "end", "-", "to", "-", "end", "neural", "network", ",", "with", "a", "Gumbel", "-", "softmax", "layer", "to", "encourage", "the", "discreteness", ".", "The", "neural", "network", "for", "learning", "codes", "will", "be", "packaged", "into", "a", "tool", ".", "With", "the", "learned", "codes", "and", "basis", "vectors", ",", "the", "computation", "graph", "for", "composing", "embeddings", "is", "fairly", "easy", "to", "implement", ",", "and", "does", "not", "require", "modifications", "to", "other", "parts", "in", "the", "neural", "network", ".", "section", ":", "Related", "Work", "Existing", "works", "for", "compressing", "neural", "networks", "include", "low", "-", "precision", "computation", "vanhoucke2011improving", ",", "hwang2014fixed", ",", "courbariaux2014low", ",", "Anwar2015FixedPO", ",", "quantization", "Chen2015CompressingNN", ",", "han2015deep", ",", "Zhou2017IncrementalNQ", ",", "network", "pruning", "LeCun1989OptimalBD", ",", "Hassibi1992SecondOD", ",", "Han2015LearningBW", ",", "Wen2016LearningSS", "and", "knowledge", "distillation", "Hinton2015DistillingTK", ".", "Network", "quantization", "such", "as", "HashedNet", "Chen2015CompressingNN", "forces", "the", "weight", "matrix", "to", "have", "few", "real", "weights", ",", "with", "a", "hash", "function", "to", "determine", "the", "weight", "assignment", ".", "To", "capture", "the", "non", "-", "uniform", "nature", "of", "the", "networks", ",", "DeepCompression", "han2015deep", "groups", "weight", "values", "into", "clusters", "based", "on", "pre", "-", "trained", "weight", "matrices", ".", "The", "weight", "assignment", "for", "each", "value", "is", "stored", "in", "the", "form", "of", "Huffman", "codes", ".", "However", ",", "as", "the", "embedding", "matrix", "is", "tremendously", "big", ",", "the", "number", "of", "hash", "codes", "a", "model", "need", "to", "maintain", "is", "still", "large", "even", "with", "Huffman", "coding", ".", "Network", "pruning", "works", "in", "a", "different", "way", "that", "makes", "a", "network", "sparse", ".", "Iterative", "pruning", "Han2015LearningBW", "prunes", "a", "weight", "value", "if", "its", "absolute", "value", "is", "smaller", "than", "a", "threshold", ".", "The", "remaining", "network", "weights", "are", "retrained", "after", "pruning", ".", "Some", "recent", "works", "See2016CompressionON", ",", "Zhang2017TowardsCA", "also", "apply", "iterative", "pruning", "to", "prune", "80", "%", "of", "the", "connections", "for", "neural", "machine", "translation", "models", ".", "In", "this", "paper", ",", "we", "compare", "the", "proposed", "method", "with", "iterative", "pruning", ".", "The", "problem", "of", "learning", "compact", "codes", "considered", "in", "this", "paper", "is", "closely", "related", "to", "learning", "to", "hash", "Weiss2008SpectralH", ",", "Kulis2009LearningTH", ",", "Liu2012SupervisedHW", ",", "which", "aims", "to", "learn", "the", "hash", "codes", "for", "vectors", "to", "facilitate", "the", "approximate", "nearest", "neighbor", "search", ".", "Initiated", "by", "product", "quantization", "Jgou2011ProductQF", ",", "subsequent", "works", "such", "as", "additive", "quantization", "Babenko2014AdditiveQF", "explore", "the", "use", "of", "multiple", "codebooks", "for", "source", "coding", ",", "resulting", "in", "compositional", "codes", ".", "We", "also", "adopt", "the", "coding", "scheme", "of", "additive", "quantization", "for", "its", "storage", "efficiency", ".", "Previous", "works", "mainly", "focus", "on", "performing", "efficient", "similarity", "search", "of", "image", "descriptors", ".", "In", "this", "work", ",", "we", "put", "more", "focus", "on", "reducing", "the", "codebook", "sizes", "and", "learning", "efficient", "codes", "to", "avoid", "performance", "loss", ".", "Joulin2016FastTextzipCT", "utilizes", "an", "improved", "version", "of", "product", "quantization", "to", "compress", "text", "classification", "models", ".", "However", ",", "to", "match", "the", "baseline", "performance", ",", "much", "longer", "hash", "codes", "are", "required", "by", "product", "quantization", ".", "This", "will", "be", "detailed", "in", "Section", "[", "reference", "]", ".", "To", "learn", "the", "codebooks", "and", "code", "assignment", ",", "additive", "quantization", "alternatively", "optimizes", "the", "codebooks", "and", "the", "discrete", "codes", ".", "The", "learning", "of", "code", "assignment", "is", "performed", "by", "Beam", "Search", "algorithm", "when", "the", "codebooks", "are", "fixed", ".", "In", "this", "work", ",", "we", "propose", "a", "straight", "-", "forward", "method", "to", "directly", "learn", "the", "code", "assignment", "and", "codebooks", "simutaneously", "in", "an", "end", "-", "to", "-", "end", "neural", "network", ".", "Some", "recent", "works", "Xia2014SupervisedHF", ",", "Liu2016DeepSH", ",", "Yang2017SupervisedLO", "in", "learning", "to", "hash", "also", "utilize", "neural", "networks", "to", "produce", "binary", "codes", "by", "applying", "binary", "constrains", "(", "e.g.", ",", "sigmoid", "function", ")", ".", "In", "this", "work", ",", "we", "encourage", "the", "discreteness", "with", "the", "Gumbel", "-", "Softmax", "trick", "for", "producing", "compositional", "codes", ".", "As", "an", "alternative", "to", "our", "approach", ",", "one", "can", "also", "reduce", "the", "number", "of", "unique", "word", "types", "by", "forcing", "a", "character", "-", "level", "segmentation", ".", "Kim2016CharacterAwareNL", "proposed", "a", "character", "-", "based", "neural", "language", "model", ",", "which", "applies", "a", "convolutional", "layer", "after", "the", "character", "embeddings", ".", "Botha2017NaturalLP", "propose", "to", "use", "char", "-", "gram", "as", "input", "features", ",", "which", "are", "further", "hashed", "to", "save", "space", ".", "Generally", ",", "using", "character", "-", "level", "inputs", "requires", "modifications", "to", "the", "model", "architecture", ".", "Moreover", ",", "some", "Asian", "languages", "such", "as", "Japanese", "and", "Chinese", "retain", "a", "large", "vocabulary", "at", "the", "character", "level", ",", "which", "makes", "the", "character", "-", "based", "approach", "difficult", "to", "be", "applied", ".", "In", "contrast", ",", "our", "approach", "does", "not", "suffer", "from", "these", "limitations", ".", "section", ":", "Advantage", "of", "Compositional", "Codes", "In", "this", "section", ",", "we", "formally", "describe", "the", "compositional", "coding", "approach", "and", "analyze", "its", "merits", "for", "compressing", "word", "embeddings", ".", "The", "coding", "approach", "follows", "the", "scheme", "in", "additive", "quantization", "Babenko2014AdditiveQF", ".", "We", "represent", "each", "word", "with", "a", "compact", "code", "that", "is", "composed", "of", "components", "such", "that", ".", "Each", "component", "is", "constrained", "to", "have", "a", "value", "in", ",", "which", "also", "indicates", "that", "bits", "are", "required", "to", "store", "each", "code", ".", "For", "convenience", ",", "is", "selected", "to", "be", "a", "number", "of", "a", "multiple", "of", ",", "so", "that", "the", "codes", "can", "be", "efficiently", "stored", ".", "If", "we", "restrict", "each", "component", "to", "values", "of", "0", "or", "1", ",", "the", "code", "for", "each", "word", "will", "be", "a", "binary", "code", ".", "In", "this", "case", ",", "the", "code", "learning", "problem", "is", "equivalent", "to", "a", "matrix", "factorization", "problem", "with", "binary", "components", ".", "Forcing", "the", "compact", "codes", "to", "be", "binary", "numbers", "can", "be", "beneficial", ",", "as", "the", "learning", "problem", "is", "usually", "easier", "to", "solve", "in", "the", "binary", "case", ",", "and", "some", "existing", "optimization", "algorithms", "in", "learning", "to", "hash", "can", "be", "reused", ".", "However", ",", "the", "compositional", "coding", "approach", "produces", "shorter", "codes", "and", "is", "thus", "more", "storage", "efficient", ".", "As", "the", "number", "of", "basis", "vectors", "is", "regardless", "of", "the", "vocabulary", "size", ",", "the", "only", "uncertain", "factor", "contributing", "to", "the", "model", "size", "is", "the", "size", "of", "the", "hash", "codes", ",", "which", "is", "proportional", "to", "the", "vocabulary", "size", ".", "Therefore", ",", "maintaining", "short", "codes", "is", "cruicial", "in", "our", "work", ".", "Suppose", "we", "wish", "the", "model", "to", "have", "a", "set", "of", "basis", "vectors", ".", "Then", "in", "the", "binary", "case", ",", "each", "code", "will", "have", "bits", ".", "For", "the", "compositional", "coding", "approach", ",", "if", "we", "can", "find", "a", "decomposition", "such", "that", ",", "then", "each", "code", "will", "have", "bits", ".", "For", "example", ",", "a", "binary", "code", "will", "have", "a", "length", "of", "256", "bits", "to", "support", "512", "basis", "vectors", ".", "In", "contrast", ",", "a", "compositional", "coding", "scheme", "will", "produce", "codes", "of", "only", "128", "bits", ".", "A", "comparison", "of", "different", "coding", "approaches", "is", "summarized", "in", "Table", "[", "reference", "]", ".", "We", "also", "report", "the", "number", "of", "basis", "vectors", "required", "to", "compute", "an", "embedding", "as", "a", "measure", "of", "computational", "cost", ".", "For", "the", "conventional", "approach", ",", "the", "number", "of", "vectors", "is", "identical", "to", "the", "vocabulary", "size", "and", "the", "computation", "is", "basically", "a", "single", "indexing", "operation", ".", "In", "the", "case", "of", "binary", "codes", ",", "the", "computation", "for", "constructing", "an", "embedding", "involves", "a", "summation", "over", "basis", "vectors", ".", "For", "the", "compositional", "approach", ",", "the", "number", "of", "vectors", "required", "to", "construct", "an", "embedding", "vector", "is", ".", "Both", "the", "binary", "and", "compositional", "approaches", "have", "significantly", "fewer", "vectors", "in", "the", "embedding", "matrix", ".", "The", "compositional", "coding", "approach", "provides", "a", "better", "balance", "with", "shorter", "codes", "and", "lower", "computational", "cost", ".", "section", ":", "Code", "Learning", "with", "Gumbel", "-", "Softmax", "Let", "be", "the", "original", "embedding", "matrix", ",", "where", "each", "embedding", "vector", "has", "dimensions", ".", "By", "using", "the", "reconstruction", "loss", "as", "the", "objective", "function", "in", "Eq", ".", "[", "reference", "]", ",", "we", "are", "actually", "finding", "an", "approximate", "matrix", "factorization", ",", "where", "is", "a", "basis", "matrix", "for", "the", "-", "th", "component", ".", "is", "a", "code", "matrix", "in", "which", "each", "row", "is", "an", "-", "dimensional", "one", "-", "hot", "vector", ".", "If", "we", "let", "be", "the", "one", "-", "hot", "vector", "corresponding", "to", "the", "code", "component", "for", "word", ",", "the", "computation", "of", "the", "word", "embeddings", "can", "be", "reformulated", "as", "Therefore", ",", "the", "problem", "of", "learning", "discrete", "codes", "can", "be", "converted", "to", "a", "problem", "of", "finding", "a", "set", "of", "optimal", "one", "-", "hot", "vectors", "and", "source", "dictionaries", "that", "minimize", "the", "reconstruction", "loss", ".", "The", "Gumbel", "-", "softmax", "reparameterization", "trick", "Maddison2016TheCD", ",", "Jang2016CategoricalRW", "is", "useful", "for", "parameterizing", "a", "discrete", "distribution", "such", "as", "the", "-", "dimensional", "one", "-", "hot", "vectors", "in", "Eq", ".", "[", "reference", "]", ".", "By", "applying", "the", "Gumbel", "-", "softmax", "trick", ",", "the", "-", "th", "elemement", "in", "is", "computed", "as", "where", "is", "a", "noise", "term", "that", "is", "sampled", "from", "the", "Gumbel", "distribution", ",", "whereas", "is", "the", "temperature", "of", "the", "softmax", ".", "In", "our", "model", ",", "the", "vector", "is", "computed", "by", "a", "simple", "neural", "network", "with", "a", "single", "hidden", "layer", "as", "In", "our", "experiments", ",", "the", "hidden", "layer", "always", "has", "a", "size", "of", ".", "We", "found", "that", "a", "fixed", "temperature", "of", "just", "works", "well", ".", "The", "Gumbel", "-", "softmax", "trick", "is", "applied", "to", "to", "obtain", ".", "Then", ",", "the", "model", "reconstructs", "the", "embedding", "with", "Eq", ".", "[", "reference", "]", "and", "computes", "the", "reconstruction", "loss", "with", "Eq", ".", "[", "reference", "]", ".", "The", "model", "architecture", "of", "the", "end", "-", "to", "-", "end", "neural", "network", "is", "illustrated", "in", "Fig", ".", "[", "reference", "]", ",", "which", "is", "effectively", "an", "auto", "-", "encoder", "with", "a", "Gumbel", "-", "softmax", "middle", "layer", ".", "The", "whole", "neural", "network", "for", "coding", "learning", "has", "five", "parameters", "(", ")", ".", "Once", "the", "coding", "learning", "model", "is", "trained", ",", "the", "code", "for", "each", "word", "can", "be", "easily", "obtained", "by", "applying", "to", "the", "one", "-", "hot", "vectors", ".", "The", "basis", "vectors", "(", "codewords", ")", "for", "composing", "the", "embeddings", "can", "be", "found", "as", "the", "row", "vectors", "in", "the", "weight", "matrix", ".", "For", "general", "NLP", "tasks", ",", "one", "can", "learn", "the", "compositional", "codes", "from", "publicly", "available", "word", "vectors", "such", "as", "GloVe", "vectors", ".", "However", ",", "for", "some", "tasks", "such", "as", "machine", "translation", ",", "the", "word", "embeddings", "are", "usually", "jointly", "learned", "with", "other", "parts", "of", "the", "neural", "network", ".", "For", "such", "tasks", ",", "one", "has", "to", "first", "train", "a", "normal", "model", "to", "obtain", "the", "baseline", "embeddings", ".", "Then", ",", "based", "on", "the", "trained", "embedding", "matrix", ",", "one", "can", "learn", "a", "set", "of", "task", "-", "specific", "codes", ".", "As", "the", "reconstructed", "embeddings", "are", "not", "identical", "to", "the", "original", "embeddings", ",", "the", "model", "parameters", "other", "than", "the", "embedding", "matrix", "have", "to", "be", "retrained", "again", ".", "The", "code", "learning", "model", "can", "not", "be", "jointly", "trained", "with", "the", "machine", "translation", "model", "as", "it", "takes", "far", "more", "iterations", "for", "the", "coding", "layer", "to", "converge", "to", "one", "-", "hot", "vectors", ".", "section", ":", "Experiments", "In", "our", "experiments", ",", "we", "focus", "on", "evaluating", "the", "maximum", "loss", "-", "free", "compression", "rate", "of", "word", "embeddings", "on", "two", "typical", "NLP", "tasks", ":", "sentiment", "analysis", "and", "machine", "translation", ".", "We", "compare", "the", "model", "performance", "and", "the", "size", "of", "embedding", "layer", "with", "the", "baseline", "model", "and", "the", "iterative", "pruning", "method", "Han2015LearningBW", ".", "Please", "note", "that", "the", "sizes", "of", "other", "parts", "in", "the", "neural", "networks", "are", "not", "included", "in", "our", "results", ".", "For", "dense", "matrices", ",", "we", "report", "the", "size", "of", "dumped", "numpy", "arrays", ".", "For", "the", "sparse", "matrices", ",", "we", "report", "the", "size", "of", "dumped", "compressed", "sparse", "column", "matrices", "(", "csc_matrix", ")", "in", "scipy", ".", "All", "float", "numbers", "take", "32", "bits", "storage", ".", "We", "enable", "the", "\u201c", "compressed", "\u201d", "option", "when", "dumping", "the", "matrices", ",", "without", "this", "option", ",", "the", "file", "size", "is", "about", "times", "bigger", ".", "subsection", ":", "Code", "Learning", "To", "learn", "efficient", "compact", "codes", "for", "each", "word", ",", "our", "proposed", "method", "requires", "a", "set", "of", "baseline", "embedding", "vectors", ".", "For", "the", "sentiment", "analysis", "task", ",", "we", "learn", "the", "codes", "based", "on", "the", "publicly", "available", "GloVe", "vectors", ".", "For", "the", "machine", "translation", "task", ",", "we", "first", "train", "a", "normal", "neural", "machine", "translation", "(", "NMT", ")", "model", "to", "obtain", "task", "-", "specific", "word", "embeddings", ".", "Then", "we", "learn", "the", "codes", "using", "the", "pre", "-", "trained", "embeddings", ".", "We", "train", "the", "end", "-", "to", "-", "end", "network", "described", "in", "Section", "[", "reference", "]", "to", "learn", "the", "codes", "automatically", ".", "In", "each", "iteration", ",", "a", "small", "batch", "of", "the", "embeddings", "is", "sampled", "uniformly", "from", "the", "baseline", "embedding", "matrix", ".", "The", "network", "parameters", "are", "optimized", "to", "minimize", "the", "reconstruction", "loss", "of", "the", "sampled", "embeddings", ".", "In", "our", "experiments", ",", "the", "batch", "size", "is", "set", "to", "128", ".", "We", "use", "Adam", "optimizer", "kingma2014adam", "with", "a", "fixed", "learning", "rate", "of", "0.0001", ".", "The", "training", "is", "run", "for", "200", "K", "iterations", ".", "Every", "1", ",", "000", "iterations", ",", "we", "examine", "the", "loss", "on", "a", "fixed", "validation", "set", "and", "save", "the", "parameters", "if", "the", "loss", "decreases", ".", "We", "evenly", "distribute", "the", "model", "training", "to", "4", "GPUs", "using", "the", "nccl", "package", ",", "so", "that", "one", "round", "of", "code", "learning", "takes", "around", "15", "minutes", "to", "complete", ".", "subsection", ":", "Sentiment", "Analysis", "Dataset", ":", "For", "sentiment", "analysis", ",", "we", "use", "a", "standard", "separation", "of", "IMDB", "movie", "review", "dataset", "Maas2011LearningWV", ",", "which", "contains", "25k", "reviews", "for", "training", "and", "25", "K", "reviews", "for", "testing", "purpose", ".", "We", "lowercase", "and", "tokenize", "all", "texts", "with", "the", "nltk", "package", ".", "We", "choose", "the", "300", "-", "dimensional", "uncased", "GloVe", "word", "vectors", "(", "trained", "on", "42B", "tokens", "of", "Common", "Crawl", "data", ")", "as", "our", "baseline", "embeddings", ".", "The", "vocabulary", "for", "the", "model", "training", "contains", "all", "words", "appears", "both", "in", "the", "IMDB", "dataset", "and", "the", "GloVe", "vocabulary", ",", "which", "results", "in", "around", "75", "K", "words", ".", "We", "truncate", "the", "texts", "of", "reviews", "to", "assure", "they", "are", "not", "longer", "than", "400", "words", ".", "Model", "architecture", ":", "Both", "the", "baseline", "model", "and", "the", "compressed", "models", "have", "the", "same", "computational", "graph", "except", "the", "embedding", "layer", ".", "The", "model", "is", "composed", "of", "a", "single", "LSTM", "layer", "with", "150", "hidden", "units", "and", "a", "softmax", "layer", "for", "predicting", "the", "binary", "label", ".", "For", "the", "baseline", "model", ",", "the", "embedding", "layer", "contains", "a", "large", "embedding", "matrix", "initialized", "by", "GloVe", "embeddings", ".", "For", "the", "compressed", "models", "based", "on", "the", "compositional", "coding", ",", "the", "embedding", "layer", "maintains", "a", "matrix", "of", "basis", "vectors", ".", "Suppose", "we", "use", "a", "coding", "scheme", ",", "the", "basis", "matrix", "will", "then", "have", "a", "shape", "of", ",", "which", "is", "initialized", "by", "the", "concatenated", "weight", "matrices", "in", "the", "code", "learning", "model", ".", "The", "embedding", "parameters", "for", "both", "models", "remain", "fixed", "during", "the", "training", ".", "For", "the", "models", "with", "network", "pruning", ",", "the", "sparse", "embedding", "matrix", "is", "finetuned", "during", "training", ".", "Training", "details", ":", "The", "models", "are", "trained", "with", "Adam", "optimizer", "for", "15", "epochs", "with", "a", "fixed", "learning", "rate", "of", ".", "At", "the", "end", "of", "each", "epoch", ",", "we", "evaluate", "the", "loss", "on", "a", "small", "validation", "set", ".", "The", "parameters", "with", "lowest", "validation", "loss", "are", "saved", ".", "Results", ":", "For", "different", "settings", "of", "the", "number", "of", "components", "and", "the", "number", "of", "codewords", ",", "we", "train", "the", "code", "learning", "network", ".", "The", "average", "reconstruction", "loss", "on", "a", "fixed", "validation", "set", "is", "summarized", "in", "the", "left", "of", "Table", "[", "reference", "]", ".", "For", "reference", ",", "we", "also", "report", "the", "total", "size", "(", "MB", ")", "of", "the", "embedding", "layer", "in", "the", "right", "table", ",", "which", "includes", "the", "sizes", "of", "the", "basis", "matrix", "and", "the", "hash", "table", ".", "We", "can", "see", "that", "increasing", "either", "or", "can", "effectively", "decrease", "the", "reconstruction", "loss", ".", "However", ",", "setting", "to", "a", "large", "number", "will", "result", "in", "longer", "hash", "codes", ",", "thus", "significantly", "increase", "the", "size", "of", "the", "embedding", "layer", ".", "Hence", ",", "it", "is", "important", "to", "choose", "correct", "numbers", "for", "and", "to", "balance", "the", "performance", "and", "model", "size", ".", "To", "see", "how", "the", "reconstructed", "loss", "translates", "to", "the", "classification", "accuracy", ",", "we", "train", "the", "sentiment", "analysis", "model", "for", "different", "settings", "of", "code", "schemes", "and", "report", "the", "results", "in", "Table", "[", "reference", "]", ".", "The", "baseline", "model", "using", "75k", "GloVe", "embeddings", "achieves", "an", "accuracy", "of", "87.18", "with", "an", "embedding", "matrix", "using", "78", "MB", "of", "storage", ".", "In", "this", "task", ",", "forcing", "a", "high", "compression", "rate", "with", "iterative", "pruning", "degrades", "the", "classification", "accuracy", ".", "We", "also", "show", "the", "results", "using", "normalized", "product", "quantization", "(", "NPQ", ")", "Joulin2016FastTextzipCT", ".", "We", "quantize", "the", "filtered", "GloVe", "embeddings", "with", "the", "codes", "provided", "by", "the", "authors", ",", "and", "train", "the", "models", "based", "on", "the", "quantized", "embeddings", ".", "To", "make", "the", "results", "comparable", ",", "we", "report", "the", "codebook", "size", "in", "numpy", "format", ".", "For", "our", "proposed", "methods", ",", "the", "maximum", "loss", "-", "free", "compression", "rate", "is", "achieved", "by", "a", "coding", "scheme", ".", "In", "this", "case", ",", "the", "total", "size", "of", "the", "embedding", "layer", "is", "1.23", "MB", ",", "which", "is", "equivalent", "to", "a", "compression", "rate", "of", "98.4", "%", ".", "We", "also", "found", "the", "classification", "accuracy", "can", "be", "substantially", "improved", "with", "a", "slightly", "lower", "compression", "rate", ".", "The", "improved", "model", "performance", "may", "be", "a", "byproduct", "of", "the", "strong", "regularization", ".", "subsection", ":", "Machine", "Translation", "Dataset", ":", "For", "machine", "translation", "tasks", ",", "we", "experiment", "on", "IWSLT", "2014", "German", "-", "to", "-", "English", "translation", "task", "cettolo2014report", "and", "ASPEC", "English", "-", "to", "-", "Japanese", "translation", "task", "NAKAZAWA16.621", ".", "The", "IWSLT14", "training", "data", "contains", "178", "K", "sentence", "pairs", ",", "which", "is", "a", "small", "dataset", "for", "machine", "translation", ".", "We", "utilize", "moses", "toolkit", "Koehn2007MosesOS", "to", "tokenize", "and", "lowercase", "both", "sides", "of", "the", "texts", ".", "Then", "we", "concatenate", "all", "five", "TED", "/", "TEDx", "development", "and", "test", "corpus", "to", "form", "a", "test", "set", "containing", "6750", "sentence", "pairs", ".", "We", "apply", "byte", "-", "pair", "encoding", "Sennrich2016NeuralMT", "to", "transform", "the", "texts", "to", "subword", "level", "so", "that", "the", "vocabulary", "has", "a", "size", "of", "20", "K", "for", "each", "language", ".", "For", "evaluation", ",", "we", "report", "tokenized", "BLEU", "using", "\u201c", "multi", "-", "bleu.perl", "\u201d", ".", "The", "ASPEC", "dataset", "contains", "300", "M", "bilingual", "pairs", "in", "the", "training", "data", "with", "the", "automatically", "estimated", "quality", "scores", "provided", "for", "each", "pair", ".", "We", "only", "use", "the", "first", "150", "M", "pairs", "for", "training", "the", "models", ".", "The", "English", "texts", "are", "tokenized", "by", "moses", "toolkit", "whereas", "the", "Japanese", "texts", "are", "tokenized", "by", "kytea", "kytea", ".", "The", "vocabulary", "size", "for", "each", "language", "is", "reduced", "to", "40", "K", "using", "byte", "-", "pair", "encoding", ".", "The", "evaluation", "is", "performed", "using", "a", "standard", "kytea", "-", "based", "post", "-", "processing", "script", "for", "this", "dataset", ".", "Model", "architecture", ":", "In", "our", "preliminary", "experiments", ",", "we", "found", "a", "coding", "works", "well", "for", "a", "vanilla", "NMT", "model", ".", "As", "it", "is", "more", "meaningful", "to", "test", "on", "a", "high", "-", "performance", "model", ",", "we", "applied", "several", "techniques", "to", "improve", "the", "performance", ".", "The", "model", "has", "a", "standard", "bi", "-", "directional", "encoder", "composed", "of", "two", "LSTM", "layers", "similar", "to", "bahdanau2014neural", ".", "The", "decoder", "contains", "two", "LSTM", "layers", ".", "Residual", "connection", "He2016DeepRL", "with", "a", "scaling", "factor", "of", "is", "applied", "to", "the", "two", "decoder", "states", "to", "compute", "the", "outputs", ".", "All", "LSTMs", "and", "embeddings", "have", "256", "hidden", "units", "in", "the", "IWSLT14", "task", "and", "1000", "hidden", "units", "in", "ASPEC", "task", ".", "The", "decoder", "states", "are", "firstly", "linearly", "transformed", "to", "600", "-", "dimensional", "vectors", "before", "computing", "the", "final", "softmax", ".", "Dropout", "with", "a", "rate", "of", "0.2", "is", "applied", "everywhere", "except", "the", "recurrent", "computation", ".", "We", "apply", "Key", "-", "Value", "Attention", "Miller2016KeyValueMN", "to", "the", "first", "decoder", ",", "where", "the", "query", "is", "the", "sum", "of", "the", "feedback", "embedding", "and", "the", "previous", "decoder", "state", "and", "the", "keys", "are", "computed", "by", "linear", "transformation", "of", "encoder", "states", ".", "Training", "details", ":", "All", "models", "are", "trained", "by", "Nesterov", "\u2019s", "accelerated", "gradient", "nesterov1983method", "with", "an", "initial", "learning", "rate", "of", "0.25", ".", "We", "evaluate", "the", "smoothed", "BLEU", "smoothed_bleu", "on", "a", "validation", "set", "composed", "of", "50", "batches", "every", "7", ",", "000", "iterations", ".", "The", "learning", "rate", "is", "reduced", "by", "a", "factor", "of", "10", "if", "no", "improvement", "is", "observed", "in", "3", "validation", "runs", ".", "The", "training", "ends", "after", "the", "learning", "rate", "is", "reduced", "three", "times", ".", "Similar", "to", "the", "code", "learning", ",", "the", "training", "is", "distributed", "to", "4", "GPUs", ",", "each", "GPU", "computes", "a", "mini", "-", "batch", "of", "16", "samples", ".", "We", "firstly", "train", "a", "baseline", "NMT", "model", "to", "obtain", "the", "task", "-", "specific", "embeddings", "for", "all", "in", "-", "vocabulary", "words", "in", "both", "languages", ".", "Then", "based", "on", "these", "baseline", "embeddings", ",", "we", "obtain", "the", "hash", "codes", "and", "basis", "vectors", "by", "training", "the", "code", "learning", "model", ".", "Finally", ",", "the", "NMT", "models", "using", "compositional", "coding", "are", "retrained", "by", "plugging", "in", "the", "reconstructed", "embeddings", ".", "Note", "that", "the", "embedding", "layer", "is", "fixed", "in", "this", "phase", ",", "other", "parameters", "are", "retrained", "from", "random", "initial", "values", ".", "Results", ":", "The", "experimental", "results", "are", "summarized", "in", "Table", "[", "reference", "]", ".", "All", "translations", "are", "decoded", "by", "the", "beam", "search", "with", "a", "beam", "size", "of", "5", ".", "The", "performance", "of", "iterative", "pruning", "varies", "between", "tasks", ".", "The", "loss", "-", "free", "compression", "rate", "reaches", "92", "%", "on", "ASPEC", "dataset", "by", "pruning", "90", "%", "of", "the", "connections", ".", "However", ",", "with", "the", "same", "pruning", "ratio", ",", "a", "modest", "performance", "loss", "is", "observed", "in", "IWSLT14", "dataset", ".", "For", "the", "models", "using", "compositional", "coding", ",", "the", "loss", "-", "free", "compression", "rate", "is", "94", "%", "for", "the", "IWSLT14", "dataset", "and", "99", "%", "for", "the", "ASPEC", "dataset", ".", "Similar", "to", "the", "sentiment", "analysis", "task", ",", "a", "significant", "performance", "improvement", "can", "be", "observed", "by", "slightly", "lowering", "the", "compression", "rate", ".", "Note", "that", "the", "sizes", "of", "NMT", "models", "are", "still", "quite", "large", "due", "to", "the", "big", "softmax", "layer", "and", "the", "recurrent", "layers", ",", "which", "are", "not", "reported", "in", "the", "table", ".", "Please", "refer", "to", "existing", "works", "such", "as", "Zhang2017TowardsCA", "for", "the", "techniques", "of", "compressing", "layers", "other", "than", "word", "embeddings", ".", "section", ":", "Qualitative", "Analysis", "subsection", ":", "Examples", "of", "Learned", "Codes", "In", "Table", "[", "reference", "]", ",", "we", "show", "some", "examples", "of", "learned", "codes", "based", "on", "the", "300", "-", "dimensional", "uncased", "GloVe", "embeddings", "used", "in", "the", "sentiment", "analysis", "task", ".", "We", "can", "see", "that", "the", "model", "learned", "to", "assign", "similar", "codes", "to", "the", "words", "with", "similar", "meanings", ".", "Such", "a", "code", "-", "sharing", "mechanism", "can", "significantly", "reduce", "the", "redundancy", "of", "the", "word", "embeddings", ",", "thus", "helping", "to", "achieve", "a", "high", "compression", "rate", ".", "subsection", ":", "Analysis", "of", "Code", "Efficiency", "Besides", "the", "performance", ",", "we", "also", "care", "about", "the", "storage", "efficiency", "of", "the", "codes", ".", "In", "the", "ideal", "situation", ",", "all", "codewords", "shall", "be", "fully", "utilized", "to", "convey", "a", "fraction", "of", "meaning", ".", "However", ",", "as", "the", "codes", "are", "automatically", "learned", ",", "it", "is", "possible", "that", "some", "codewords", "are", "abandoned", "during", "the", "training", ".", "In", "extreme", "cases", ",", "some", "\u201c", "dead", "\u201d", "codewords", "can", "be", "used", "by", "none", "of", "the", "words", ".", "To", "analyze", "the", "code", "efficiency", ",", "we", "count", "the", "number", "of", "words", "that", "contain", "a", "specific", "subcode", "in", "each", "component", ".", "Figure", "[", "reference", "]", "gives", "a", "visualization", "of", "the", "code", "balance", "for", "three", "coding", "schemes", ".", "Each", "column", "shows", "the", "counts", "of", "the", "subcodes", "of", "a", "specific", "component", ".", "In", "our", "experiments", ",", "when", "using", "a", "coding", "scheme", ",", "we", "found", "31", "%", "of", "the", "words", "have", "a", "subcode", "\u201c", "0", "\u201d", "for", "the", "first", "component", ",", "while", "the", "subcode", "\u201c", "1", "\u201d", "is", "only", "used", "by", "5", "%", "of", "the", "words", ".", "The", "assignment", "of", "codes", "is", "more", "balanced", "for", "larger", "coding", "schemes", ".", "In", "any", "coding", "scheme", ",", "even", "the", "most", "unpopular", "codeword", "is", "used", "by", "about", "1000", "words", ".", "This", "result", "indicates", "that", "the", "code", "learning", "model", "is", "capable", "of", "assigning", "codes", "efficiently", "without", "wasting", "a", "codeword", ".", "section", ":", "Conclusion", "In", "this", "work", ",", "we", "propose", "a", "novel", "method", "for", "reducing", "the", "number", "of", "parameters", "required", "in", "word", "embeddings", ".", "Instead", "of", "assigning", "each", "unique", "word", "an", "embedding", "vector", ",", "we", "compose", "the", "embedding", "vectors", "using", "a", "small", "set", "of", "basis", "vectors", ".", "The", "selection", "of", "basis", "vectors", "is", "governed", "by", "the", "hash", "code", "of", "each", "word", ".", "We", "apply", "the", "compositional", "coding", "approach", "to", "maximize", "the", "storage", "efficiency", ".", "The", "proposed", "method", "works", "by", "eliminating", "the", "redundancy", "inherent", "in", "representing", "similar", "words", "with", "independent", "embeddings", ".", "In", "our", "work", ",", "we", "propose", "a", "simple", "way", "to", "directly", "learn", "the", "discrete", "codes", "in", "a", "neural", "network", "with", "Gumbel", "-", "softmax", "trick", ".", "The", "results", "show", "that", "the", "size", "of", "the", "embedding", "layer", "was", "reduced", "by", "98", "%", "in", "IMDB", "sentiment", "analysis", "task", "and", "in", "machine", "translation", "tasks", "without", "affecting", "the", "performance", ".", "Our", "approach", "achieves", "a", "high", "loss", "-", "free", "compression", "rate", "by", "considering", "the", "semantic", "inter", "-", "similarity", "among", "different", "words", ".", "In", "qualitative", "analysis", ",", "we", "found", "the", "learned", "codes", "of", "similar", "words", "are", "very", "close", "in", "Hamming", "space", ".", "As", "our", "approach", "maintains", "a", "dense", "basis", "matrix", ",", "it", "has", "the", "potential", "to", "be", "further", "compressed", "by", "applying", "pruning", "techniques", "to", "the", "dense", "matrix", ".", "The", "advantage", "of", "compositional", "coding", "approach", "will", "be", "more", "significant", "if", "the", "size", "of", "embedding", "layer", "is", "dominated", "by", "the", "hash", "codes", ".", "bibliography", ":", "References", "appendix", ":", "Appendix", ":", "Shared", "Codes", "In", "both", "tasks", ",", "when", "we", "use", "a", "small", "code", "decomposition", ",", "we", "found", "some", "hash", "codes", "are", "assigned", "to", "multiple", "words", ".", "Table", "[", "reference", "]", "lists", "some", "samples", "of", "shared", "codes", "with", "their", "corresponding", "words", "from", "the", "sentiment", "analysis", "task", ".", "This", "phenomenon", "does", "not", "cause", "a", "problem", "in", "either", "task", ",", "as", "the", "words", "only", "have", "shared", "codes", "when", "they", "have", "almost", "the", "same", "sentiments", "or", "target", "translations", "."]}