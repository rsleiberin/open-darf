{"coref": {"Bit_per_Character__BPC_": [[137, 140]], "Hutter_Prize": [[116, 118], [2568, 2571], [3391, 3393], [4691, 4693], [2664, 2666], [2672, 2674], [2680, 2682], [3837, 3839], [4409, 4411]], "Language_Modelling": [[208, 210], [636, 638], [705, 707], [88, 90], [596, 598], [1110, 1112], [1264, 1266], [1681, 1683], [2221, 2223], [2603, 2605], [3935, 3937], [3985, 3987], [4657, 4659], [4734, 4736], [4811, 4813], [4872, 4874]], "Large_mLSTM": [[2, 4], [9, 11], [12, 13], [41, 42], [74, 75], [101, 102], [600, 602], [1736, 1738], [1750, 1752], [1753, 1754], [2160, 2163], [2164, 2165], [2167, 2168], [2216, 2217], [2252, 2253], [2269, 2270], [2384, 2385], [2429, 2430], [2497, 2498], [2512, 2513], [2542, 2543], [2738, 2739], [2792, 2793], [2815, 2816], [2827, 2828], [2884, 2885], [2902, 2903], [2931, 2932], [2952, 2953], [2992, 2993], [3010, 3011], [3081, 3082], [3094, 3095], [3106, 3108], [3131, 3132], [3171, 3174], [3299, 3302], [3402, 3403], [3501, 3502], [3524, 3525], [3540, 3541], [3724, 3725], [3752, 3753], [3775, 3777], [3785, 3786], [4405, 4407], [4526, 4529], [4550, 4551], [4618, 4620], [4661, 4662], [4705, 4706], [4737, 4738], [4748, 4749], [4784, 4785], [4817, 4818], [4847, 4848], [4866, 4867], [4884, 4885], [127, 128], [581, 583], [3255, 3256], [3443, 3444], [3797, 3798], [3884, 3885], [4495, 4496], [4671, 4672]], "Large_mLSTM__emb__WN__VD": [], "Number_of_params": [], "Text8": [[109, 110], [2566, 2567], [3574, 3576], [3576, 3577], [3867, 3868], [3894, 3895], [4700, 4701], [3793, 3794]], "Unregularised_mLSTM": [[2636, 2638], [3069, 3071], [3159, 3161], [3470, 3472]], "VD": [[3128, 3130], [3137, 3138], [3140, 3142], [3328, 3330], [3376, 3378], [3398, 3400], [4544, 4546], [4674, 4676], [3260, 3262], [4423, 4425]], "WN": [[3088, 3090], [3098, 3099], [3135, 3136], [3357, 3359], [3404, 3406], [3474, 3475], [4418, 4420]], "emb": [[3096, 3097], [3133, 3134], [3195, 3197], [3211, 3213], [3246, 3248], [3271, 3273], [3317, 3319], [3361, 3363], [4415, 4417], [3085, 3087], [3182, 3184], [3282, 3284], [4432, 4434]], "enwiki8": []}, "coref_non_salient": {"0": [[2610, 2613], [4075, 4078], [4374, 4378], [4457, 4460], [4477, 4480], [4522, 4525], [4557, 4561]], "1": [[474, 478], [2553, 2559], [3618, 3623]], "10": [[829, 831], [4643, 4646]], "11": [[4141, 4145]], "12": [[1571, 1574], [3384, 3389]], "13": [[4610, 4616], [4876, 4881]], "14": [[2265, 2266], [4090, 4091]], "15": [[354, 356], [414, 416], [544, 546], [4913, 4916]], "16": [[4298, 4301], [4396, 4399], [4702, 4705]], "17": [[16, 20], [174, 177]], "18": [[66, 69], [230, 235]], "19": [[3146, 3147], [3207, 3208]], "2": [[35, 40], [2015, 2018], [4766, 4769]], "20": [[550, 552], [995, 997], [998, 1000], [1022, 1024], [1041, 1043], [1141, 1143], [1154, 1156], [1176, 1178], [1232, 1234]], "21": [[2267, 2268], [3122, 3123], [4547, 4548], [2659, 2660], [3036, 3037], [3369, 3370], [4573, 4574]], "22": [[2789, 2790], [3462, 3464]], "23": [[3335, 3337]], "24": [[2720, 2724]], "25": [[4031, 4034], [4055, 4058], [4066, 4069], [4119, 4122], [4255, 4258]], "26": [[32, 33], [77, 78], [401, 402], [576, 577], [589, 590], [1328, 1329], [1364, 1365], [1542, 1543], [1562, 1563], [1740, 1741], [1776, 1777], [1781, 1783], [1809, 1810], [1853, 1854], [1893, 1894], [2146, 2147], [2183, 2184], [2254, 2255], [2381, 2382], [2424, 2425], [4665, 4666], [547, 548], [1276, 1277], [1284, 1285], [1377, 1378], [1393, 1394], [1512, 1513], [1566, 1567], [1582, 1583], [1612, 1613], [1645, 1646], [1836, 1837], [1926, 1927], [2397, 2398], [2501, 2502], [2550, 2551], [2795, 2796], [2833, 2834], [2887, 2888], [2905, 2906], [2923, 2924], [2955, 2956], [2998, 2999], [3015, 3016], [3694, 3695], [3745, 3746], [3765, 3766], [4605, 4606]], "27": [[2317, 2319], [2357, 2359], [2474, 2476], [2484, 2486]], "28": [[2467, 2468], [3046, 3047]], "29": [[4349, 4352]], "3": [[2307, 2308], [2327, 2328], [2426, 2427], [2431, 2432], [2761, 2763]], "30": [[283, 285], [874, 876], [1259, 1261]], "31": [[3640, 3641]], "32": [[2740, 2745], [3683, 3685], [3686, 3688]], "33": [[3432, 3434]], "34": [[3414, 3418]], "35": [[4688, 4690]], "36": [[3990, 3994], [4002, 4007], [4018, 4022], [4154, 4158], [4713, 4718]], "37": [[80, 82], [2257, 2259], [3705, 3707], [4668, 4670]], "38": [[159, 163], [4536, 4541]], "39": [[385, 388]], "4": [[1316, 1320], [1323, 1328]], "40": [[149, 152], [4531, 4534]], "41": [[3033, 3035]], "42": [[553, 554], [1001, 1002], [1226, 1227], [1244, 1245], [1294, 1295], [1564, 1565], [1742, 1744], [1779, 1780], [1568, 1569], [1598, 1599], [1675, 1676], [1685, 1686], [1791, 1792], [4598, 4599]], "43": [[3439, 3442], [3477, 3480], [3508, 3511], [3542, 3545]], "44": [[2053, 2055]], "45": [[990, 992]], "46": [[1170, 1174]], "47": [[2247, 2248]], "48": [[421, 424]], "49": [[2092, 2096]], "5": [[573, 575], [1398, 1400], [1773, 1775], [2038, 2040]], "50": [[226, 228]], "51": [[182, 185]], "52": [[1133, 1139]], "53": [[905, 908]], "54": [[130, 134], [2591, 2595], [3914, 3917], [3918, 3922], [4220, 4225], [4437, 4440], [4719, 4722]], "55": [[351, 353]], "56": [[3484, 3486]], "57": [[4563, 4568]], "58": [[4498, 4505]], "59": [[2349, 2352]], "6": [[5, 7], [21, 23], [201, 206], [2012, 2014], [2082, 2085], [2152, 2154]], "60": [[3465, 3467]], "61": [[2019, 2020], [3897, 3898]], "62": [[1884, 1888]], "63": [[2122, 2125]], "64": [[2118, 2121]], "65": [[2343, 2345]], "66": [[2546, 2548]], "67": [[3125, 3127]], "68": [[2007, 2011]], "69": [[438, 440]], "7": [[2573, 2580], [3995, 3998], [4092, 4096], [4146, 4150]], "70": [[3056, 3060]], "71": [[346, 348]], "72": [[211, 213]], "73": [[223, 224]], "74": [[2626, 2629]], "75": [[517, 519], [1026, 1028], [1333, 1335], [1949, 1951]], "76": [[217, 221]], "77": [[214, 216]], "78": [[1163, 1164], [1255, 1256], [1770, 1771], [1877, 1878], [1939, 1941]], "8": [[178, 179], [236, 237], [376, 377], [405, 406], [442, 443], [453, 454], [649, 650], [739, 740], [786, 787], [825, 826], [841, 842], [863, 864], [902, 903], [962, 963], [1290, 1292]], "9": [[26, 31], [1757, 1759], [394, 399]]}, "doc_id": "1e7a36c4d4f96b29e3edf51b6eb61f8e16217704", "method_subrelations": {"Large_mLSTM": [[[0, 11], "Large_mLSTM"]], "Large_mLSTM__emb__WN__VD": [[[0, 11], "Large_mLSTM"], [[13, 16], "emb"], [[18, 20], "WN"], [[22, 24], "VD"]], "Unregularised_mLSTM": [[[0, 19], "Unregularised_mLSTM"]]}, "n_ary_relations": [{"Material": "Hutter_Prize", "Method": "Large_mLSTM__emb__WN__VD", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.24"}, {"Material": "Hutter_Prize", "Method": "Large_mLSTM__emb__WN__VD", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "46M"}, {"Material": "Text8", "Method": "Large_mLSTM__emb__WN__VD", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.27"}, {"Material": "Text8", "Method": "Unregularised_mLSTM", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.40"}, {"Material": "Text8", "Method": "Large_mLSTM__emb__WN__VD", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "45M"}, {"Material": "Text8", "Method": "Unregularised_mLSTM", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "45M"}, {"Material": "enwiki8", "Method": "Large_mLSTM", "Metric": "Bit_per_Character__BPC_", "Task": "Language_Modelling", "score": "1.24"}, {"Material": "enwiki8", "Method": "Large_mLSTM", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "46M"}], "ner": [[2, 4, "Method"], [5, 7, "Task"], [9, 11, "Method"], [12, 13, "Method"], [16, 20, "Method"], [21, 23, "Task"], [26, 31, "Method"], [32, 33, "Method"], [35, 40, "Method"], [41, 42, "Method"], [66, 69, "Task"], [74, 75, "Method"], [77, 78, "Method"], [80, 82, "Method"], [101, 102, "Method"], [109, 110, "Material"], [116, 118, "Material"], [130, 134, "Material"], [137, 140, "Metric"], [149, 152, "Metric"], [159, 163, "Method"], [174, 177, "Method"], [178, 179, "Method"], [182, 185, "Method"], [201, 206, "Task"], [208, 210, "Task"], [211, 213, "Task"], [214, 216, "Task"], [217, 221, "Method"], [223, 224, "Method"], [226, 228, "Method"], [230, 235, "Task"], [236, 237, "Method"], [283, 285, "Method"], [346, 348, "Method"], [351, 353, "Method"], [354, 356, "Method"], [376, 377, "Method"], [385, 388, "Task"], [401, 402, "Method"], [405, 406, "Method"], [414, 416, "Method"], [421, 424, "Method"], [438, 440, "Method"], [442, 443, "Method"], [453, 454, "Method"], [474, 478, "Method"], [517, 519, "Method"], [544, 546, "Method"], [550, 552, "Method"], [553, 554, "Method"], [573, 575, "Method"], [576, 577, "Method"], [589, 590, "Method"], [600, 602, "Method"], [636, 638, "Task"], [649, 650, "Method"], [705, 707, "Task"], [739, 740, "Method"], [786, 787, "Method"], [825, 826, "Method"], [829, 831, "Method"], [841, 842, "Method"], [863, 864, "Method"], [874, 876, "Method"], [902, 903, "Method"], [905, 908, "Method"], [962, 963, "Method"], [990, 992, "Task"], [995, 997, "Method"], [998, 1000, "Method"], [1001, 1002, "Method"], [1022, 1024, "Method"], [1026, 1028, "Method"], [1041, 1043, "Method"], [1133, 1139, "Task"], [1141, 1143, "Method"], [1154, 1156, "Method"], [1163, 1164, "Method"], [1170, 1174, "Method"], [1176, 1178, "Method"], [1226, 1227, "Method"], [1232, 1234, "Method"], [1244, 1245, "Method"], [1255, 1256, "Method"], [1259, 1261, "Method"], [1290, 1292, "Method"], [1294, 1295, "Method"], [1316, 1320, "Task"], [1323, 1328, "Task"], [1328, 1329, "Method"], [1333, 1335, "Method"], [1364, 1365, "Method"], [1398, 1400, "Method"], [1542, 1543, "Method"], [1562, 1563, "Method"], [1564, 1565, "Method"], [1571, 1574, "Method"], [1736, 1738, "Method"], [1740, 1741, "Method"], [1742, 1744, "Method"], [1750, 1752, "Method"], [1753, 1754, "Method"], [1757, 1759, "Method"], [1770, 1771, "Method"], [1773, 1775, "Method"], [1776, 1777, "Method"], [1779, 1780, "Method"], [1781, 1783, "Method"], [1809, 1810, "Method"], [1853, 1854, "Method"], [1877, 1878, "Method"], [1884, 1888, "Method"], [1893, 1894, "Method"], [1939, 1941, "Method"], [1949, 1951, "Method"], [2007, 2011, "Method"], [2012, 2014, "Task"], [2015, 2018, "Method"], [2019, 2020, "Method"], [2038, 2040, "Method"], [2053, 2055, "Method"], [2082, 2085, "Task"], [2092, 2096, "Method"], [2118, 2121, "Method"], [2122, 2125, "Method"], [2146, 2147, "Method"], [2152, 2154, "Task"], [2160, 2163, "Method"], [2164, 2165, "Method"], [2167, 2168, "Method"], [2183, 2184, "Method"], [2216, 2217, "Method"], [2247, 2248, "Metric"], [2252, 2253, "Method"], [2254, 2255, "Method"], [2257, 2259, "Method"], [2265, 2266, "Task"], [2267, 2268, "Method"], [2269, 2270, "Method"], [2307, 2308, "Method"], [2317, 2319, "Metric"], [2327, 2328, "Method"], [2343, 2345, "Method"], [2349, 2352, "Method"], [2357, 2359, "Metric"], [2381, 2382, "Method"], [2384, 2385, "Method"], [2424, 2425, "Method"], [2426, 2427, "Method"], [2429, 2430, "Method"], [2431, 2432, "Method"], [2467, 2468, "Method"], [2474, 2476, "Metric"], [2484, 2486, "Metric"], [2497, 2498, "Method"], [2512, 2513, "Method"], [2542, 2543, "Method"], [2546, 2548, "Method"], [2553, 2559, "Method"], [2566, 2567, "Material"], [2568, 2571, "Material"], [2573, 2580, "Method"], [2591, 2595, "Material"], [2610, 2613, "Method"], [2626, 2629, "Material"], [2636, 2638, "Method"], [2720, 2724, "Method"], [2738, 2739, "Method"], [2740, 2745, "Method"], [2761, 2763, "Method"], [2789, 2790, "Method"], [2792, 2793, "Method"], [2815, 2816, "Method"], [2827, 2828, "Method"], [2884, 2885, "Method"], [2902, 2903, "Method"], [2931, 2932, "Method"], [2952, 2953, "Method"], [2992, 2993, "Method"], [3010, 3011, "Method"], [3033, 3035, "Method"], [3046, 3047, "Method"], [3056, 3060, "Metric"], [3069, 3071, "Method"], [3081, 3082, "Method"], [3088, 3090, "Method"], [3094, 3095, "Method"], [3096, 3097, "Method"], [3098, 3099, "Method"], [3106, 3108, "Method"], [3122, 3123, "Method"], [3125, 3127, "Method"], [3128, 3130, "Method"], [3131, 3132, "Method"], [3133, 3134, "Method"], [3135, 3136, "Method"], [3137, 3138, "Method"], [3140, 3142, "Method"], [3146, 3147, "Method"], [3159, 3161, "Method"], [3171, 3174, "Method"], [3195, 3197, "Method"], [3207, 3208, "Method"], [3211, 3213, "Method"], [3246, 3248, "Method"], [3271, 3273, "Method"], [3299, 3302, "Method"], [3317, 3319, "Method"], [3328, 3330, "Method"], [3335, 3337, "Metric"], [3357, 3359, "Method"], [3361, 3363, "Method"], [3376, 3378, "Method"], [3384, 3389, "Method"], [3391, 3393, "Material"], [3398, 3400, "Method"], [3402, 3403, "Method"], [3404, 3406, "Method"], [3414, 3418, "Task"], [3432, 3434, "Task"], [3439, 3442, "Method"], [3462, 3464, "Method"], [3465, 3467, "Method"], [3470, 3472, "Method"], [3474, 3475, "Method"], [3477, 3480, "Method"], [3484, 3486, "Metric"], [3501, 3502, "Method"], [3508, 3511, "Method"], [3524, 3525, "Method"], [3540, 3541, "Method"], [3542, 3545, "Method"], [3574, 3576, "Material"], [3576, 3577, "Material"], [3618, 3623, "Method"], [3640, 3641, "Task"], [3683, 3685, "Method"], [3686, 3688, "Method"], [3705, 3707, "Method"], [3724, 3725, "Method"], [3752, 3753, "Method"], [3775, 3777, "Method"], [3785, 3786, "Method"], [3867, 3868, "Material"], [3894, 3895, "Material"], [3897, 3898, "Method"], [3914, 3917, "Material"], [3918, 3922, "Material"], [3990, 3994, "Method"], [3995, 3998, "Method"], [4002, 4007, "Method"], [4018, 4022, "Method"], [4031, 4034, "Method"], [4055, 4058, "Method"], [4066, 4069, "Method"], [4075, 4078, "Method"], [4090, 4091, "Task"], [4092, 4096, "Method"], [4119, 4122, "Method"], [4141, 4145, "Task"], [4146, 4150, "Method"], [4154, 4158, "Method"], [4220, 4225, "Material"], [4255, 4258, "Method"], [4298, 4301, "Method"], [4349, 4352, "Method"], [4374, 4378, "Method"], [4396, 4399, "Method"], [4405, 4407, "Method"], [4415, 4417, "Method"], [4418, 4420, "Method"], [4437, 4440, "Material"], [4457, 4460, "Method"], [4477, 4480, "Method"], [4498, 4505, "Metric"], [4522, 4525, "Method"], [4526, 4529, "Method"], [4531, 4534, "Metric"], [4536, 4541, "Method"], [4544, 4546, "Method"], [4547, 4548, "Method"], [4550, 4551, "Method"], [4557, 4561, "Method"], [4563, 4568, "Method"], [4610, 4616, "Task"], [4618, 4620, "Method"], [4643, 4646, "Method"], [4661, 4662, "Method"], [4665, 4666, "Method"], [4668, 4670, "Method"], [4674, 4676, "Method"], [4688, 4690, "Method"], [4691, 4693, "Material"], [4700, 4701, "Material"], [4702, 4705, "Method"], [4705, 4706, "Method"], [4713, 4718, "Method"], [4719, 4722, "Material"], [4737, 4738, "Method"], [4748, 4749, "Method"], [4766, 4769, "Method"], [4784, 4785, "Method"], [4817, 4818, "Method"], [4847, 4848, "Method"], [4866, 4867, "Method"], [4876, 4881, "Task"], [4884, 4885, "Method"], [4913, 4916, "Method"], [88, 90, "Task"], [127, 128, "Method"], [394, 399, "Method"], [547, 548, "Method"], [581, 583, "Method"], [596, 598, "Task"], [1110, 1112, "Task"], [1264, 1266, "Task"], [1276, 1277, "Method"], [1284, 1285, "Method"], [1377, 1378, "Method"], [1393, 1394, "Method"], [1512, 1513, "Method"], [1566, 1567, "Method"], [1568, 1569, "Method"], [1582, 1583, "Method"], [1598, 1599, "Method"], [1612, 1613, "Method"], [1645, 1646, "Method"], [1675, 1676, "Method"], [1681, 1683, "Task"], [1685, 1686, "Method"], [1791, 1792, "Method"], [1836, 1837, "Method"], [1926, 1927, "Method"], [2221, 2223, "Task"], [2397, 2398, "Method"], [2501, 2502, "Method"], [2550, 2551, "Method"], [2603, 2605, "Task"], [2659, 2660, "Method"], [2664, 2666, "Material"], [2672, 2674, "Material"], [2680, 2682, "Material"], [2795, 2796, "Method"], [2833, 2834, "Method"], [2887, 2888, "Method"], [2905, 2906, "Method"], [2923, 2924, "Method"], [2955, 2956, "Method"], [2998, 2999, "Method"], [3015, 3016, "Method"], [3036, 3037, "Method"], [3085, 3087, "Method"], [3182, 3184, "Method"], [3255, 3256, "Method"], [3260, 3262, "Method"], [3282, 3284, "Method"], [3369, 3370, "Method"], [3443, 3444, "Method"], [3694, 3695, "Method"], [3745, 3746, "Method"], [3765, 3766, "Method"], [3793, 3794, "Material"], [3797, 3798, "Method"], [3837, 3839, "Material"], [3884, 3885, "Method"], [3935, 3937, "Task"], [3985, 3987, "Task"], [4409, 4411, "Material"], [4423, 4425, "Method"], [4432, 4434, "Method"], [4495, 4496, "Method"], [4573, 4574, "Method"], [4598, 4599, "Method"], [4605, 4606, "Method"], [4657, 4659, "Task"], [4671, 4672, "Method"], [4734, 4736, "Task"], [4811, 4813, "Task"], [4872, 4874, "Task"]], "sections": [[0, 171], [171, 642], [642, 993], [993, 1321], [1321, 1559], [1559, 1734], [1734, 1942], [1942, 2203], [2203, 2206], [2206, 2662], [2662, 3572], [3572, 3912], [3912, 4591], [4591, 4924], [4924, 4927]], "sentences": [[0, 7], [7, 41], [41, 70], [70, 92], [92, 119], [119, 171], [171, 174], [174, 194], [194, 217], [217, 254], [254, 281], [281, 310], [310, 332], [332, 376], [376, 410], [410, 435], [435, 472], [472, 514], [514, 541], [541, 578], [578, 600], [600, 620], [620, 642], [642, 649], [649, 663], [663, 701], [701, 732], [732, 756], [756, 774], [774, 837], [837, 872], [872, 899], [899, 945], [945, 993], [993, 997], [997, 1016], [1016, 1040], [1040, 1062], [1062, 1093], [1093, 1133], [1133, 1163], [1163, 1211], [1211, 1225], [1225, 1240], [1240, 1288], [1288, 1321], [1321, 1328], [1328, 1357], [1357, 1376], [1376, 1391], [1391, 1428], [1428, 1464], [1464, 1489], [1489, 1499], [1499, 1509], [1509, 1532], [1532, 1559], [1559, 1565], [1565, 1582], [1582, 1612], [1612, 1645], [1645, 1680], [1680, 1707], [1707, 1734], [1734, 1738], [1738, 1778], [1778, 1829], [1829, 1862], [1862, 1889], [1889, 1904], [1904, 1917], [1917, 1942], [1942, 1946], [1946, 1962], [1962, 1995], [1995, 2015], [2015, 2037], [2037, 2056], [2056, 2086], [2086, 2115], [2115, 2142], [2142, 2156], [2156, 2179], [2179, 2203], [2203, 2206], [2206, 2210], [2210, 2228], [2228, 2260], [2260, 2289], [2289, 2320], [2320, 2369], [2369, 2400], [2400, 2422], [2422, 2446], [2446, 2465], [2465, 2496], [2496, 2511], [2511, 2540], [2540, 2560], [2560, 2581], [2581, 2614], [2614, 2662], [2662, 2667], [2667, 2685], [2685, 2714], [2714, 2731], [2731, 2757], [2757, 2789], [2789, 2803], [2803, 2824], [2824, 2847], [2847, 2889], [2889, 2920], [2920, 2940], [2940, 2952], [2952, 2992], [2992, 3003], [3003, 3028], [3028, 3042], [3042, 3064], [3064, 3076], [3076, 3119], [3119, 3135], [3135, 3140], [3140, 3157], [3157, 3170], [3170, 3193], [3193, 3209], [3209, 3250], [3250, 3291], [3291, 3311], [3311, 3325], [3325, 3341], [3341, 3354], [3354, 3371], [3371, 3394], [3394, 3407], [3407, 3435], [3435, 3477], [3477, 3507], [3507, 3526], [3526, 3572], [3572, 3576], [3576, 3603], [3603, 3610], [3610, 3651], [3651, 3664], [3664, 3692], [3692, 3720], [3720, 3752], [3752, 3768], [3768, 3782], [3782, 3796], [3796, 3828], [3828, 3881], [3881, 3912], [3912, 3917], [3917, 3938], [3938, 3952], [3952, 3972], [3972, 3995], [3995, 4013], [4013, 4059], [4059, 4086], [4086, 4119], [4119, 4146], [4146, 4166], [4166, 4182], [4182, 4202], [4202, 4219], [4219, 4254], [4254, 4297], [4297, 4362], [4362, 4402], [4402, 4445], [4445, 4484], [4484, 4494], [4494, 4516], [4516, 4549], [4549, 4591], [4591, 4594], [4594, 4617], [4617, 4650], [4650, 4671], [4671, 4702], [4702, 4705], [4705, 4723], [4723, 4747], [4747, 4782], [4782, 4792], [4792, 4814], [4814, 4839], [4839, 4854], [4854, 4903], [4903, 4924], [4924, 4927]], "words": ["document", ":", "Multiplicative", "LSTM", "for", "sequence", "modelling", "We", "introduce", "multiplicative", "LSTM", "(", "mLSTM", ")", ",", "a", "recurrent", "neural", "network", "architecture", "for", "sequence", "modelling", "that", "combines", "the", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "and", "multiplicative", "recurrent", "neural", "network", "architectures", ".", "mLSTM", "is", "characterised", "by", "its", "ability", "to", "have", "different", "recurrent", "transition", "functions", "for", "each", "possible", "input", ",", "which", "we", "argue", "makes", "it", "more", "expressive", "for", "autoregressive", "density", "estimation", ".", "We", "demonstrate", "empirically", "that", "mLSTM", "outperforms", "standard", "LSTM", "and", "its", "deep", "variants", "for", "a", "range", "of", "character", "level", "language", "modelling", "tasks", ".", "In", "this", "version", "of", "the", "paper", ",", "we", "regularise", "mLSTM", "to", "achieve", "1.27", "bits", "/", "char", "on", "text8", "and", "1.24", "bits", "/", "char", "on", "Hutter", "Prize", ".", "We", "also", "apply", "a", "purely", "byte", "-", "level", "mLSTM", "on", "the", "WikiText", "-", "2", "dataset", "to", "achieve", "a", "character", "level", "entropy", "of", "1.26", "bits", "/", "char", ",", "corresponding", "to", "a", "word", "level", "perplexity", "of", "88.8", ",", "which", "is", "comparable", "to", "word", "level", "LSTMs", "regularised", "in", "similar", "ways", "on", "the", "same", "task", ".", "section", ":", "Introduction", "Recurrent", "neural", "networks", "(", "RNNs", ")", "are", "powerful", "sequence", "density", "estimators", "that", "can", "use", "long", "contexts", "to", "make", "predictions", ".", "They", "have", "achieved", "tremendous", "success", "in", "(", "conditional", ")", "sequence", "modelling", "tasks", "such", "as", "language", "modelling", ",", "machine", "translation", "and", "speech", "recognition", ".", "Generative", "models", "of", "sequences", "can", "apply", "factorization", "via", "the", "product", "rule", "to", "perform", "density", "estimation", "of", "the", "sequence", ",", "RNNs", "can", "model", "sequences", "with", "the", "above", "factorization", "by", "using", "a", "hidden", "state", "to", "summarize", "past", "inputs", ".", "The", "hidden", "state", "vector", "is", "updated", "recursively", "using", "the", "previous", "hidden", "state", "vector", "and", "the", "current", "input", "as", "where", "is", "a", "differentiable", "function", "with", "learnable", "parameters", ".", "In", "a", "vanilla", "RNN", ",", "multiplies", "its", "inputs", "by", "a", "matrix", "and", "squashes", "the", "result", "with", "a", "non", "-", "linear", "function", "such", "as", "a", "hyperbolic", "tangent", "(", ")", ".", "The", "updated", "hidden", "state", "vector", "is", "then", "used", "to", "predict", "a", "probability", "distribution", "over", "the", "next", "sequence", "element", ",", "using", "function", ".", "In", "the", "case", "where", "consists", "of", "mutually", "exclusive", "discrete", "outcomes", ",", "may", "apply", "a", "matrix", "multiplication", "followed", "by", "a", "softmax", "function", ":", "Generative", "RNNs", "can", "evaluate", "log", "-", "likelihoods", "of", "sequences", "exactly", ",", "and", "are", "differentiable", "with", "respect", "to", "these", "log", "-", "likelihoods", ".", "RNNs", "can", "be", "difficult", "to", "train", "due", "to", "the", "vanishing", "gradient", "problem", ",", "but", "advances", "such", "as", "the", "long", "short", "-", "term", "memory", "architecture", "(", "LSTM", ")", "have", "allowed", "RNNs", "to", "be", "successful", ".", "Despite", "their", "success", ",", "generative", "RNNs", "(", "as", "well", "as", "other", "conditional", "generative", "models", ")", "are", "known", "to", "have", "problems", "with", "recovering", "from", "mistakes", ".", "Each", "time", "the", "recursive", "function", "of", "the", "RNN", "is", "applied", "and", "the", "hidden", "state", "is", "updated", ",", "the", "RNN", "must", "decide", "which", "information", "from", "the", "previous", "hidden", "state", "to", "store", ",", "due", "to", "its", "limited", "capacity", ".", "If", "the", "RNN", "\u2019s", "hidden", "representation", "remembers", "the", "wrong", "information", "and", "reaches", "a", "bad", "numerical", "state", "for", "predicting", "future", "sequence", "elements", ",", "for", "instance", "as", "a", "result", "of", "an", "unexpected", "input", ",", "it", "may", "take", "many", "time", "-", "steps", "to", "recover", ".", "We", "argue", "that", "RNN", "architectures", "with", "hidden", "-", "to", "-", "hidden", "transition", "functions", "that", "are", "input", "-", "dependent", "are", "better", "suited", "to", "recover", "from", "surprising", "inputs", ".", "Our", "approach", "to", "generative", "RNNs", "combines", "LSTM", "units", "with", "multiplicative", "RNN", "(", "mRNN", ")", "factorized", "hidden", "weights", ",", "allowing", "flexible", "input", "-", "dependent", "transitions", "that", "are", "easier", "to", "control", "due", "to", "the", "gating", "units", "of", "LSTM", ".", "We", "compare", "this", "multiplicative", "LSTM", "hybrid", "architecture", "with", "other", "variants", "of", "LSTM", "on", "a", "range", "of", "character", "level", "language", "modelling", "tasks", ".", "Multiplicative", "LSTM", "is", "most", "appropriate", "when", "it", "can", "learn", "parameters", "specifically", "for", "each", "possible", "input", "at", "a", "given", "timestep", ".", "Therefore", ",", "its", "main", "application", "is", "to", "sequences", "of", "discrete", "mutually", "exclusive", "elements", ",", "such", "as", "language", "modelling", "and", "related", "problems", ".", "subsection", ":", "Input", "-", "dependent", "transition", "functions", "RNNs", "learn", "a", "mapping", "from", "previous", "hidden", "state", "and", "input", "to", "hidden", "state", ".", "Let", "denote", "the", "input", "to", "the", "next", "hidden", "state", "before", "any", "non", "-", "linear", "operation", ":", "where", "is", "the", "hidden", "-", "to", "-", "hidden", "weight", "matrix", ",", "and", "is", "the", "input", "-", "to", "-", "hidden", "weight", "matrix", ".", "For", "problems", "such", "as", "language", "modelling", ",", "is", "a", "one", "-", "hot", "vector", ",", "meaning", "that", "the", "output", "of", "is", "a", "column", "in", ",", "corresponding", "to", "the", "unit", "element", "in", ".", "The", "possible", "future", "hidden", "states", "in", "an", "RNN", "can", "be", "viewed", "as", "a", "tree", "structure", ",", "as", "shown", "in", "Figure", "[", "reference", "]", ".", "For", "an", "alphabet", "of", "inputs", "and", "a", "fixed", ",", "there", "will", "be", "possible", "transition", "functions", "between", "and", ".", "The", "relative", "magnitude", "of", "to", "will", "need", "to", "be", "large", "for", "the", "RNN", "to", "be", "able", "to", "use", "long", "range", "dependencies", ",", "and", "the", "resulting", "possible", "hidden", "state", "vectors", "will", "therefore", "be", "highly", "correlated", "across", "the", "possible", "inputs", ",", "limiting", "the", "width", "of", "the", "tree", "and", "making", "it", "harder", "for", "the", "RNN", "to", "form", "distinct", "hidden", "representations", "for", "different", "sequences", "of", "inputs", ".", "However", ",", "if", "the", "RNN", "has", "flexible", "input", "-", "dependent", "transition", "functions", ",", "the", "tree", "will", "be", "able", "to", "grow", "wider", "more", "quickly", ",", "giving", "the", "RNN", "the", "flexibility", "to", "represent", "more", "probability", "distributions", ".", "In", "a", "vanilla", "RNN", ",", "it", "is", "difficult", "to", "allow", "inputs", "to", "greatly", "affect", "the", "hidden", "state", "vector", "without", "erasing", "information", "from", "the", "past", "hidden", "state", ".", "However", ",", "an", "RNN", "with", "a", "transition", "function", "mapping", "dependent", "on", "the", "input", "would", "allow", "the", "relative", "values", "of", "to", "vary", "with", "each", "possible", "input", ",", "without", "overwriting", "the", "contribution", "from", "the", "previous", "hidden", "state", ",", "allowing", "for", "more", "long", "term", "information", "to", "be", "stored", ".", "This", "ability", "to", "adjust", "to", "new", "inputs", "quickly", "while", "limiting", "the", "overwriting", "of", "information", "should", "make", "an", "RNN", "more", "robust", "to", "mistakes", "when", "it", "encounters", "surprising", "inputs", ",", "as", "the", "hidden", "vector", "is", "less", "likely", "to", "get", "trapped", "in", "a", "bad", "numerical", "state", "for", "making", "future", "predictions", ".", "subsection", ":", "Multiplicative", "RNN", "The", "multiplicative", "RNN", "(", "mRNN", ")", "is", "an", "architecture", "designed", "specifically", "to", "allow", "flexible", "input", "-", "dependent", "transitions", ".", "Its", "formulation", "was", "inspired", "by", "the", "tensor", "RNN", ",", "an", "RNN", "architecture", "that", "allows", "for", "a", "different", "transition", "matrix", "for", "each", "possible", "input", ".", "The", "tensor", "RNN", "features", "a", "3", "-", "way", "tensor", ",", "which", "contains", "a", "separately", "learned", "transition", "matrix", "for", "each", "input", "dimension", ".", "The", "3", "-", "way", "tensor", "can", "be", "stored", "as", "an", "array", "of", "matrices", "where", "superscript", "is", "used", "to", "denote", "the", "index", "in", "the", "array", ",", "and", "is", "the", "dimensionality", "of", ".", "The", "specific", "hidden", "-", "to", "-", "hidden", "weight", "matrix", "used", "for", "a", "given", "input", "is", "then", "For", "language", "modelling", "problems", ",", "only", "one", "unit", "of", "will", "be", "on", ",", "and", "will", "be", "the", "matrix", "in", "corresponding", "to", "that", "unit", ".", "Hidden", "-", "to", "-", "hidden", "propagation", "in", "the", "tensor", "RNN", "is", "then", "given", "by", "The", "large", "number", "of", "parameters", "in", "the", "tensor", "RNN", "make", "it", "impractical", "for", "most", "problems", ".", "mRNNs", "can", "be", "thought", "of", "as", "a", "shared", "-", "parameter", "approximation", "to", "the", "tensor", "RNN", "that", "use", "a", "factorized", "hidden", "-", "to", "-", "hidden", "transition", "matrix", "in", "place", "of", "the", "normal", "RNN", "hidden", "-", "to", "-", "hidden", "matrix", ",", "with", "an", "input", "-", "dependent", "intermediate", "diagonal", "matrix", ".", "The", "input", "-", "dependent", "hidden", "-", "to", "-", "hidden", "weight", "matrix", ",", "is", "then", "An", "mRNN", "is", "thus", "equivalent", "to", "a", "tensor", "RNN", "using", "the", "above", "form", "for", ".", "For", "readability", ",", "an", "mRNN", "can", "also", "be", "described", "using", "intermediate", "state", "as", "follows", ":", "mRNNs", "have", "improved", "on", "vanilla", "RNNs", "at", "character", "level", "language", "modelling", "tasks", ",", "but", "have", "fallen", "short", "of", "the", "more", "popular", "LSTM", "architecture", ",", "for", "instance", "as", "shown", "with", "LSTM", "baselines", "from", ".", "The", "standard", "RNN", "units", "in", "an", "mRNN", "do", "not", "provide", "an", "easy", "way", "for", "information", "to", "bypass", "its", "complex", "transitions", ",", "resulting", "in", "the", "potential", "for", "difficulty", "in", "retaining", "long", "term", "information", ".", "subsection", ":", "Long", "short", "-", "term", "memory", "LSTM", "is", "a", "commonly", "used", "RNN", "architecture", "that", "uses", "a", "series", "of", "multiplicative", "gates", "to", "control", "how", "information", "flows", "in", "and", "out", "of", "internal", "states", "of", "the", "network", ".", "There", "are", "several", "slightly", "different", "variants", "of", "LSTM", ",", "and", "we", "present", "the", "variant", "used", "in", "our", "experiments", ".", "The", "LSTM", "hidden", "state", "receives", "inputs", "from", "the", "input", "layer", "and", "the", "previous", "hidden", "state", ":", "The", "LSTM", "network", "also", "has", "3", "gating", "units", "\u2013", "input", "gate", ",", "output", "gate", ",", "and", "forget", "gate", "\u2013", "that", "have", "both", "recurrent", "and", "feed", "-", "forward", "connections", ":", "where", "is", "the", "logistic", "sigmoid", "function", ".", "The", "input", "gate", "controls", "how", "much", "of", "the", "input", "to", "each", "hidden", "unit", "is", "written", "to", "the", "internal", "state", "vector", ",", "and", "the", "forget", "gate", "determines", "how", "much", "of", "the", "previous", "internal", "state", "is", "preserved", ".", "This", "combination", "of", "write", "and", "forget", "gates", "allows", "the", "network", "to", "control", "what", "information", "should", "be", "stored", "and", "overwritten", "across", "each", "time", "-", "step", ".", "The", "internal", "state", "is", "updated", "by", "The", "output", "gate", "controls", "how", "much", "of", "each", "unit", "\u2019s", "activation", "is", "preserved", ".", "It", "allows", "the", "LSTM", "cell", "to", "keep", "information", "that", "is", "not", "relevant", "to", "the", "current", "output", ",", "but", "may", "be", "relevant", "later", ".", "The", "final", "output", "of", "the", "hidden", "state", "is", "given", "by", "LSTM", "\u2019s", "ability", "to", "control", "how", "information", "is", "stored", "in", "each", "unit", "has", "proven", "generally", "useful", ".", "subsection", ":", "Comparing", "LSTM", "with", "mRNN", "The", "LSTM", "and", "mRNN", "architectures", "both", "feature", "multiplicative", "units", ",", "but", "these", "units", "serve", "different", "purposes", ".", "LSTM", "\u2019s", "gates", "are", "designed", "to", "control", "the", "flow", "of", "information", "through", "the", "network", ",", "whereas", "mRNN", "\u2019s", "gates", "are", "designed", "to", "allow", "transition", "functions", "to", "vary", "across", "inputs", ".", "LSTM", "gates", "receive", "input", "from", "both", "the", "input", "units", "and", "hidden", "units", ",", "allowing", "multiplicative", "interactions", "between", "hidden", "units", ",", "but", "also", "potentially", "limiting", "the", "extent", "of", "input", "-", "hidden", "multiplicative", "interaction", ".", "LSTM", "gates", "are", "also", "squashed", "with", "a", "sigmoid", ",", "forcing", "them", "to", "take", "values", "between", "0", "and", "1", ",", "which", "makes", "them", "easier", "to", "control", ",", "but", "less", "expressive", "than", "mRNN", "\u2019s", "linear", "gates", ".", "For", "language", "modelling", "problems", ",", "mRNN", "\u2019s", "linear", "gates", "do", "not", "need", "to", "be", "controlled", "by", "the", "network", "because", "they", "are", "explicitly", "learned", "for", "each", "input", ".", "They", "are", "also", "placed", "in", "between", "a", "product", "of", "2", "dense", "matrices", ",", "giving", "more", "flexibility", "to", "the", "possible", "values", "of", "the", "final", "product", "of", "matrices", ".", "section", ":", "Multiplicative", "LSTM", "Since", "the", "LSTM", "and", "mRNN", "architectures", "are", "complimentary", ",", "we", "propose", "the", "multiplicative", "LSTM", "(", "mLSTM", ")", ",", "a", "hybrid", "architecture", "that", "combines", "the", "factorized", "hidden", "-", "to", "-", "hidden", "transition", "of", "mRNNs", "with", "the", "gating", "framework", "from", "LSTMs", ".", "The", "mRNN", "and", "LSTM", "architectures", "can", "be", "combined", "by", "adding", "connections", "from", "the", "mRNN", "\u2019s", "intermediate", "state", "(", "which", "is", "redefined", "below", "for", "convenience", ")", "to", "each", "gating", "units", "in", "the", "LSTM", ",", "resulting", "in", "the", "following", "system", ":", "We", "set", "the", "dimensionality", "of", "and", "equal", "for", "all", "our", "experiments", ".", "We", "also", "chose", "to", "share", "across", "all", "LSTM", "unit", "types", ",", "resulting", "in", "a", "model", "with", "1.25", "times", "the", "number", "of", "recurrent", "weights", "as", "LSTM", "for", "the", "same", "number", "of", "hidden", "units", ".", "The", "goal", "of", "this", "architecture", "is", "to", "combine", "the", "flexible", "input", "-", "dependent", "transitions", "of", "mRNNs", "with", "the", "long", "time", "lag", "and", "information", "control", "of", "LSTMs", ".", "The", "gated", "units", "of", "LSTMs", "could", "make", "it", "easier", "to", "control", "(", "or", "bypass", ")", "the", "complex", "transitions", "in", "that", "result", "from", "the", "factorized", "hidden", "weight", "matrix", ".", "The", "additional", "sigmoid", "input", "and", "forget", "gates", "featured", "in", "LSTM", "units", "allow", "even", "more", "flexible", "input", "-", "dependent", "transition", "functions", "than", "in", "regular", "mRNNs", ".", "section", ":", "Related", "approaches", "Many", "recently", "proposed", "RNN", "architectures", "use", "recurrent", "depth", ",", "which", "is", "depth", "between", "recurrent", "steps", ".", "Recurrent", "depth", "allows", "more", "non", "-", "linearity", "in", "the", "combination", "of", "inputs", "and", "previous", "hidden", "states", "from", "every", "time", "step", ",", "which", "in", "turn", "allows", "for", "more", "flexible", "input", "-", "dependent", "transitions", ".", "Recurrent", "depth", "has", "been", "found", "to", "perform", "better", "than", "other", "kinds", "of", "non", "-", "recurrent", "depth", "for", "sequence", "modelling", ".", "Recurrent", "highway", "networks", "(", "RHNs", ")", "use", "a", "more", "sophisticated", "recurrent", "depth", "that", "carefully", "controls", "propagation", "through", "layers", "using", "gating", "units", ".", "The", "gating", "units", "also", "allow", "for", "a", "greater", "deal", "of", "multiplicative", "interaction", "between", "the", "inputs", "and", "hidden", "units", ".", "While", "adding", "recurrent", "depth", "could", "improve", "our", "model", ",", "we", "believe", "that", "maximizing", "the", "input", "-", "dependent", "flexibility", "of", "the", "transition", "function", "is", "more", "important", "for", "expressive", "sequence", "modelling", ".", "Recurrent", "depth", "can", "do", "this", "through", "non", "-", "linear", "layers", "combining", "hidden", "and", "input", "contributions", ",", "but", "our", "method", "can", "do", "this", "independently", "of", "non", "-", "linear", "depth", ".", "Another", "approach", ",", "multiplicative", "integration", "RNNs", "(", "MI", "-", "RNNs", ")", ",", "use", "Hadamard", "products", "instead", "of", "addition", "when", "combining", "contributions", "from", "input", "and", "hidden", "units", ".", "When", "applying", "this", "to", "LSTM", ",", "this", "architecture", "achieves", "impressive", "sequence", "modelling", "results", ".", "The", "main", "difference", "between", "multiplicative", "integration", "LSTM", "and", "mLSTM", "is", "that", "mLSTM", "applies", "the", "Hadamard", "product", "between", "the", "multiplication", "of", "two", "matrices", ".", "In", "the", "case", "of", "LSTM", ",", "this", "allows", "for", "the", "potential", "for", "greater", "expressiveness", ",", "without", "significantly", "increasing", "the", "size", "of", "the", "model", ".", "section", ":", "Experiments", "subsection", ":", "System", "Setup", "Our", "experiments", "measure", "the", "performance", "of", "mLSTM", "for", "character", "-", "level", "language", "modelling", "tasks", "of", "varying", "complexity", ".", "Our", "initial", "experiments", ",", "which", "appeared", "in", "previous", "versions", "of", "this", "work", ",", "were", "mainly", "designed", "to", "compare", "the", "convergence", "and", "final", "performance", "of", "mLSTM", "vs", "LSTM", "and", "its", "deep", "variants", ".", "Our", "follow", "up", "experiments", "explored", "training", "and", "regularisation", "of", "mLSTM", "in", "more", "detail", ",", "with", "goal", "of", "comparing", "more", "directly", "with", "the", "most", "competitive", "architectures", "in", "the", "literature", ".", "Our", "initial", "and", "follow", "up", "experiments", "used", "slightly", "different", "set", "ups", ";", "initial", "experiments", "used", "a", "variant", "of", "RMSprop", ",", ",", "with", "normalized", "updates", "in", "place", "of", "a", "learning", "rate", ".", "All", "unnormalized", "update", "directions", ",", "computed", "by", "RMSprop", ",", "were", "normalized", "to", "have", "length", ",", "where", "was", "decayed", "exponentially", "over", "training", ":", "This", "update", "rule", "is", "similar", "to", "applying", "gradient", "norm", "clipping", ",", "with", "a", "very", "high", "learning", "rate", "balanced", "out", "by", "a", "very", "low", "gradient", "norm", "threshold", ".", "The", "initial", "experiments", "also", "used", "a", "slightly", "non", "-", "standard", "version", "of", "LSTM", "(", "and", "mLSTM", ")", "with", "the", "output", "gate", "inside", "of", "the", "final", "tanh", "of", "the", "LSTM", "cell", ".", "This", "gave", "us", "slightly", "better", "results", "in", "preliminary", "experiments", "with", "very", "small", "models", ",", "but", "likely", "does", "not", "make", "much", "difference", ".", "We", "use", "LSTM", "(", "RMSprop", ")", "and", "mLSTM", "(", "RMSprop", ")", "in", "tables", "to", "distinguish", "results", "obtained", "by", "these", "initial", "set", "of", "experiments", ".", "For", "our", "follow", "up", "experiments", ",", "we", "use", "more", "standard", "methodology", "to", "be", "more", "comparable", "to", "the", "literature", ".", "We", "used", "ADAM", ",", "always", "starting", "with", "an", "initial", "learning", "rate", "of", "and", "decaying", "this", "linearly", "to", "a", "minimum", "learning", "rate", "(", "which", "was", "always", "in", "the", "range", "to", ")", ".", "The", "mLSTMs", "used", "the", "standard", "LSTM", "cell", "with", "the", "output", "gate", "outside", "the", "tanh", ".", "These", "mLSTMs", "also", "used", "scaled", "orthogonal", "initialisations", "for", "the", "hidden", "weights", ",", "an", "initial", "forget", "gate", "bias", "of", "3", ",", "and", "truncated", "backpropogation", "lengths", "from", "200", "to", "250", ".", "We", "compared", "mLSTM", "to", "previously", "reported", "regular", "LSTM", ",", "stacked", "LSTM", ",", "and", "RNN", "character", "-", "level", "language", "models", ".", "We", "run", "detailed", "experiments", "on", "the", "text8", "and", "Hutter", "Prize", "datasets", "to", "test", "medium", "scale", "character", "-", "level", "language", "modelling", ".", "We", "test", "our", "best", "model", "from", "these", "experiments", "on", "the", "WikiText", "-", "2", "dataset", "to", "measure", "performance", "on", "smaller", "scale", "character", "level", "language", "modelling", ",", "and", "to", "compare", "with", "word", "level", "models", ".", "Previous", "versions", "of", "the", "paper", "also", "report", "a", "character", "level", "result", "on", "Penn", "Treebank", "dataset", "of", "1.35", "bits", "/", "char", "with", "an", "unregularised", "mLSTM", ",", "however", "we", "do", "not", "include", "this", "experiment", "in", "this", "version", "as", "we", "have", "no", "results", "with", "our", "updated", "training", "and", "regularisation", "methodology", ".", "subsection", ":", "Hutter", "Prize", "dataset", "We", "performed", "experiments", "using", "the", "Hutter", "Prize", "dataset", ",", "originally", "used", "for", "the", "Hutter", "Prize", "compression", "benchmark", ".", "This", "dataset", "consists", "mostly", "of", "English", "language", "text", "and", "mark", "-", "up", "language", "text", ",", "but", "also", "contains", "text", "in", "other", "languages", ",", "including", "non", "-", "Latin", "languages", ".", "The", "dataset", "is", "modelled", "using", "a", "UTF", "-", "8", "encoding", ",", "and", "contains", "205", "unique", "bytes", ".", "In", "our", "initial", "experiments", ",", "we", "compared", "mLSTMs", "and", "2", "-", "layer", "stacked", "LSTMs", "for", "varying", "network", "sizes", ",", "ranging", "from", "about", "3\u201320", "million", "parameters", ".", "These", "results", "all", "used", "RMS", "prop", "with", "normalized", "updates", ",", "stopping", "after", "4", "epochs", "on", "the", "first", "95", "million", "characters", ",", "with", "test", "performance", "measured", "on", "the", "last", "5", "million", "bytes", ".", "Hyperparameters", "for", "each", "mLSTM", "and", "stacked", "LSTM", "were", "kept", "constant", "across", "all", "sizes", ".", "The", "results", ",", "shown", "in", "Figure", "[", "reference", "]", ",", "show", "that", "mLSTM", "gives", "an", "improvement", "across", "all", "network", "sizes", ".", "We", "hypothesized", "that", "mLSTM", "\u2019s", "superior", "performance", "over", "stacked", "LSTM", "was", "in", "part", "due", "to", "its", "ability", "to", "recover", "from", "surprising", "inputs", ".", "To", "test", "this", "we", "looked", "at", "each", "network", "\u2019s", "performance", "after", "viewing", "surprising", "inputs", "that", "occurred", "naturally", "in", "the", "test", "set", "by", "creating", "a", "set", "of", "the", "10", "%", "characters", "with", "the", "largest", "average", "loss", "taken", "by", "mLSTM", "and", "stacked", "LSTM", ".", "Both", "networks", "perform", "roughly", "equally", "on", "this", "set", "of", "surprising", "characters", ",", "with", "mLSTM", "and", "stacked", "LSTM", "taking", "losses", "of", "6.27", "bits", "/", "character", "and", "6.29", "bits", "/", "character", "respectively", ".", "However", ",", "stacked", "LSTM", "tended", "to", "take", "much", "larger", "losses", "than", "mLSTM", "in", "the", "timesteps", "immediately", "following", "surprising", "inputs", ".", "One", "to", "four", "time", "-", "steps", "after", "a", "surprising", "input", "occurred", ",", "mLSTM", "and", "stacked", "LSTM", "took", "average", "losses", "of", "(", "2.26", ",", "2.04", ",", "1.61", ",", "1.51", ")", "and", "(", "2.48", ",", "2.25", ",", "1.79", ",", "1.67", ")", "bits", "per", "character", "respectively", ",", "as", "shown", "in", "Figure", "[", "reference", "]", ".", "mLSTM", "\u2019s", "overall", "advantage", "over", "stacked", "LSTM", "was", "1.42", "bits", "/", "char", "to", "1.53", "bits", "/", "char", ";", "mLSTM", "\u2019s", "advantage", "over", "stacked", "LSTM", "was", "greater", "after", "a", "surprising", "input", "than", "it", "is", "in", "general", ".", "We", "also", "explore", "more", "standard", "training", "methodology", "and", "regularisation", "methods", "on", "this", "dataset", ".", "These", "experiments", "all", "used", "ADAM", ",", "and", "the", "standard", "90", "-", "5", "-", "5", "training", "validation", "test", "split", "on", "this", "dataset", ".", "We", "firstly", "consider", "a", "standard", "unregularised", "mLSTM", "trained", "with", "this", "methodology", ".", "We", "then", "experiment", "with", "an", "mLSTM", "with", "a", "linear", "embedding", "layer", "and", "weight", "normalization", "on", "recurrent", "weights", "(", "mLSTM", "+", "emb", "+", "WN", ")", ",", "which", "is", "similar", "to", "the", "mLSTM", "architecture", "used", "in", ",", "which", "was", "built", "off", "our", "initial", "work", ".", "We", "also", "consider", "regularisation", "of", "the", "later", "model", "with", "variational", "dropout", "(", "mLSTM", "+", "emb", "+", "WN", "+", "VD", ")", ".", "Variational", "dropout", "is", "a", "form", "of", "dropout", "where", "the", "dropout", "mask", "is", "shared", "across", "a", "sequence", ".", "The", "standard", "unregularised", "LSTM", "used", "1900", "hidden", "units", "and", "20", "million", "parameters", ".", "The", "weight", "normalized", "mLSTM", "used", "1900", "hidden", "units", ",", "and", "a", "linear", "embedding", "layer", "of", "400", ",", "giving", "it", "22", "million", "parameters", ".", "The", "large", "embedding", "layer", "was", "used", "because", "it", "was", "found", "to", "work", "well", "with", "dropout", ".", "Since", "this", "embedding", "layer", "is", "linear", ",", "it", "could", "potentially", "be", "removed", "during", "test", "time", "by", "multiplying", "its", "incoming", "and", "outgoing", "weight", "matrices", "to", "reduce", "the", "number", "of", "parameters", "(", "however", "we", "report", "parameter", "numbers", "with", "the", "embedding", "layer", ")", ".", "For", "the", "regularised", "weight", "normalized", "mLSTM", ",", "we", "apply", "a", "variational", "dropout", "of", "0.2", "to", "the", "hidden", "state", "and", "to", "the", "embedding", "layer", "(", "dropout", "masks", "for", "both", "the", "hidden", "state", "and", "embedding", "layer", "were", "shared", "across", "a", "sequence", ")", ".", "We", "also", "consider", "a", "larger", "version", "of", "the", "weight", "normalized", "mLSTM", "with", "2800", "hidden", "units", "and", "46", "million", "parameters", ".", "We", "increased", "the", "dropout", "in", "the", "embedding", "layer", "to", "0.5", "on", "this", "model", ".", "All", "results", "without", "variational", "dropout", "used", "early", "stopping", "on", "the", "validation", "error", "to", "reduce", "overfitting", ".", "The", "results", "for", "these", "experiments", "are", "given", "in", "table", "[", "reference", "]", ".", "Interestingly", ",", "adding", "weight", "normalization", "and", "an", "embedding", "layer", "hurt", "performance", "in", "the", "absence", "of", "regularisation", ".", "However", ",", "when", "combined", "with", "variational", "dropout", ",", "this", "model", "outperformed", "all", "previous", "static", "single", "model", "neural", "network", "results", "on", "Hutter", "Prize", ".", "We", "did", "not", "explore", "variational", "dropout", "applied", "to", "mLSTM", "without", "weight", "normalization", ".", "Earlier", "versions", "of", "this", "work", "also", "considered", "dynamic", "evaluation", "of", "mLSTMs", "on", "this", "task", ",", "however", "this", "is", "now", "in", "a", "separate", "paper", "focused", "on", "dynamic", "evaluation", ".", "We", "also", "tested", "an", "MI", "-", "LSTM", ",", "mLSTM", "\u2019s", "nearest", "neighbor", ",", "with", "a", "slightly", "larger", "size", "(", "22", "M", "parameters", ")", "and", "a", "very", "similar", "hyperparameter", "configuration", "and", "initialisation", "scheme", "(", "compared", "with", "unregularised", "mLSTM", "with", "no", "WN", ")", ".", "MI", "-", "LSTM", "achieved", "a", "relatively", "poor", "test", "set", "performance", "of", "1.53", "bits", "/", "char", ",", "as", "compared", "with", "1.40", "bits", "/", "char", "for", "mLSTM", "under", "the", "same", "settings", ".", "The", "MI", "-", "LSTM", "also", "converged", "more", "slowly", ",", "although", "eventually", "did", "require", "early", "stopping", "like", "the", "mLSTM", ".", "While", "this", "particular", "experiment", "can", "not", "conclusively", "prove", "anything", "about", "the", "relative", "utility", "of", "mLSTM", "vs.", "MI", "-", "LSTM", "on", "this", "task", ",", "it", "does", "show", "that", "the", "two", "architectures", "are", "sufficiently", "different", "to", "obtain", "very", "different", "results", "under", "the", "same", "hyper", "-", "parameter", "settings", ".", "subsection", ":", "Text8", "dataset", "Text8", "contains", "100", "million", "characters", "of", "English", "text", "taken", "from", "Wikipedia", "in", "2006", ",", "consisting", "of", "just", "the", "26", "characters", "of", "the", "English", "alphabet", "plus", "spaces", ".", "This", "dataset", "can", "be", "found", "at", ".", "This", "corpus", "has", "been", "widely", "used", "to", "benchmark", "RNN", "character", "level", "language", "models", ",", "with", "the", "first", "90", "million", "characters", "used", "for", "training", ",", "the", "next", "5", "million", "used", "for", "validation", ",", "and", "the", "final", "5", "million", "used", "for", "testing", ".", "The", "results", "of", "these", "experiments", "are", "shown", "in", "Table", "[", "reference", "]", ".", "The", "first", "set", "of", "experiments", "we", "performed", "were", "designed", "to", "be", "comparable", "to", "those", "of", ",", "who", "benchmarked", "several", "deep", "LSTMs", "against", "shallow", "LSTMs", "on", "this", "dataset", ".", "The", "shallow", "LSTM", "had", "a", "hidden", "state", "dimensionality", "of", "512", ",", "and", "the", "deep", "versions", "had", "reduced", "dimensionality", "to", "give", "them", "roughly", "the", "same", "number", "of", "parameters", ".", "Our", "experiment", "used", "an", "mLSTM", "with", "a", "hidden", "dimensionality", "of", "450", ",", "giving", "it", "slightly", "fewer", "parameters", "than", "the", "past", "work", ",", "and", "our", "own", "LSTM", "baseline", "with", "hidden", "dimensionality", "512", ".", "mLSTM", "showed", "an", "improvement", "over", "our", "baseline", "and", "the", "previously", "reported", "best", "deep", "LSTM", "variant", ".", "We", "also", "ran", "experiments", "to", "compare", "a", "large", "mLSTM", "with", "other", "reported", "experiments", ".", "We", "trained", "an", "mLSTM", "with", "hidden", "dimensionality", "of", "1900", "on", "the", "text8", "dataset", ".", "Unregularised", "mLSTM", "was", "able", "to", "fit", "the", "training", "data", "well", "and", "achieved", "a", "competitive", "performance", ";", "however", "it", "was", "outperformed", "by", "other", "architectures", "that", "are", "less", "prone", "to", "over", "-", "fitting", ".", "We", "later", "considered", "our", "best", "training", "setup", "from", "the", "Hutter", "Prize", "dataset", ",", "reusing", "the", "exact", "same", "architecture", "and", "hyper", "-", "parameters", "from", "this", "task", ",", "with", "the", "only", "difference", "being", "the", "number", "of", "input", "characters", "(", "27", "for", "text8", ")", ",", "which", "reduces", "the", "number", "of", "parameters", "to", "around", "45", "million", ".", "This", "well", "regularised", "mLSTM", "was", "able", "to", "achieve", "a", "much", "stronger", "performance", "on", "text8", ",", "tying", "RHNs", "with", "a", "recurrent", "depth", "of", "10", "for", "the", "best", "result", "on", "this", "dataset", ".", "subsection", ":", "WikiText", "-", "2", "The", "WikiText", "-", "2", "dataset", "has", "been", "a", "common", "benchmark", "for", "very", "recent", "advances", "in", "word", "-", "level", "language", "modelling", ".", "This", "dataset", "contains", "2", "million", "training", "tokens", "and", "a", "vocab", "size", "of", "33k", ".", "Documents", "are", "given", "in", "non", "-", "shuffled", "order", ",", "causing", "the", "data", "to", "contain", "more", "long", "-", "range", "dependencies", ".", "We", "use", "this", "dataset", "to", "benchmark", "how", "our", "advances", "in", "character", "-", "level", "language", "modelling", "stack", "up", "against", "word", "level", "language", "models", ".", "Character", "language", "models", "generally", "perform", "worse", "than", "word", "-", "level", "language", "models", "on", "standard", "English", "text", "benchmarks", ".", "One", "reason", "for", "this", "is", "word", "level", "language", "models", "know", "the", "test", "set", "vocabulary", "in", "advance", ",", "whereas", "character", "level", "models", "model", "a", "distribution", "over", "all", "possible", "words", ",", "including", "out", "of", "vocabulary", "words", ",", "making", "the", "task", "inherently", "more", "difficult", "from", "character", "level", "view", ".", "Furthermore", ",", "very", "rare", "words", ",", "which", "character", "level", "models", "are", "more", "equipped", "to", "handle", "than", "word", "level", "models", ",", "are", "mapped", "to", "an", "unknown", "token", ".", "From", "the", "perspective", "of", "training", ",", "character", "level", "language", "models", "must", "model", "longer", "range", "dependencies", ",", "and", "must", "learn", "a", "more", "complex", "non", "-", "linear", "fit", "to", "capture", "joint", "dependencies", "between", "characters", ".", "Character", "level", "models", "do", "have", "an", "inherent", "advantage", "of", "being", "able", "to", "capture", "subword", "language", "information", ",", "motivating", "their", "use", "on", "traditionally", "word", "-", "level", "tasks", ".", "Character", "level", "language", "models", "can", "be", "compared", "with", "word", "level", "language", "models", "by", "converting", "bits", "per", "character", "to", "perplexity", ".", "In", "this", "case", ",", "we", "model", "the", "data", "at", "the", "UTF", "-", "8", "byte", "level", ".", "The", "bits", "per", "word", "can", "be", "computed", "as", "where", "in", "this", "case", ",", "symbols", "are", "UTF", "-", "8", "bytes", ".", "2", "raised", "to", "the", "power", "of", "the", "number", "of", "bits", "/", "word", "is", "then", "the", "perplexity", ".", "The", "WikiText", "-", "2", "test", "set", "is", "245", ",", "569", "words", "long", ",", "and", "1", ",", "256", ",", "449", "bytes", "long", ",", "so", "each", "word", "is", "on", "average", "5.1165", "UTF", "-", "8", "bytes", "long", ".", "A", "character", "level", "model", "can", "also", "assign", "word", "level", "probabilities", "directly", "by", "taking", "the", "product", "of", "the", "probabilities", "of", "the", "characters", "in", "a", "word", ",", "including", "the", "probability", "of", "the", "character", "ending", "the", "word", "(", "either", "a", "space", "or", "a", "newline", ")", ".", "A", "byte", "level", "model", "is", "likely", "at", "a", "slight", "disadvantage", "compared", "with", "word", "-", "level", "because", "it", "must", "predict", "some", "information", "that", "gets", "removed", "during", "tokenization", "(", "such", "as", "spaces", "vs.", "newlines", ")", ",", "but", "the", "perplexity", "given", "by", "the", "conversion", "above", "could", "atleast", "be", "seen", "as", "an", "upper", "bound", "of", "the", "word", "level", "perplexity", "such", "a", "model", "could", "achieve", "predicting", "byte", "by", "byte", ".", "This", "is", "because", "the", "entropy", "of", "the", "file", "after", "tokenization", "(", "which", "word", "level", "models", "measure", ")", "will", "always", "be", "less", "than", "or", "equal", "to", "the", "entropy", "of", "the", "file", "before", "tokenization", "(", "which", "byte", "level", "models", "measure", ")", ".", "We", "trained", "the", "mLSTM", "configuration", "from", "the", "Hutter", "Prize", "dataset", ",", "using", "an", "embedding", "layer", ",", "weight", "normalization", ",", "and", "a", "variational", "dropout", "of", "0.5", "in", "both", "the", "hidden", "and", "embedding", "layer", ",", "to", "model", "WikiText", "-", "2", "at", "the", "byte", "level", ".", "This", "model", "contained", "46", "million", "parameters", ",", "which", "is", "larger", "than", "most", "word", "level", "models", "that", "use", "tied", "input", "and", "output", "embeddings", "to", "share", "parameters", ",", "but", "similar", "in", "size", "to", "untied", "word", "level", "models", "on", "this", "dataset", ".", "The", "results", "are", "given", "in", "table", "[", "reference", "]", ".", "Byte", "mLSTM", "achieves", "a", "byte", "-", "level", "test", "set", "cross", "entropy", "of", "bits", "/", "char", ",", "corresponding", "to", "a", "perplexity", "of", ".", "Despite", "all", "the", "disadvantages", "faced", "by", "character", "level", "models", ",", "byte", "level", "mLSTM", "achieves", "similar", "word", "level", "perplexity", "to", "previous", "word", "-", "level", "LSTM", "baselines", "that", "also", "use", "variational", "dropout", "for", "regularisation", ".", "Byte", "mLSTM", "does", "not", "perform", "as", "well", "as", "word", "-", "level", "models", "that", "use", "adaptive", "add", "-", "on", "methods", "or", "very", "recent", "advances", "in", "regularisation", "/", "hyper", "-", "parameter", "tuning", ",", "however", "it", "could", "likely", "benefit", "from", "these", "advances", "as", "well", ".", "section", ":", "Discussion", "This", "work", "combined", "the", "mRNN", "\u2019s", "factorized", "hidden", "weights", "with", "the", "LSTM", "\u2019s", "hidden", "units", "for", "generative", "modelling", "of", "discrete", "multinomial", "sequences", ".", "This", "mLSTM", "architecture", "was", "motivated", "by", "its", "ability", "to", "have", "both", "controlled", "and", "flexible", "input", "-", "dependent", "transitions", ",", "to", "allow", "for", "fast", "changes", "to", "the", "distributed", "hidden", "representation", "without", "erasing", "information", ".", "In", "a", "series", "of", "character", "-", "level", "language", "modelling", "experiments", ",", "mLSTM", "showed", "improvements", "over", "LSTM", "and", "its", "deep", "variants", ".", "mLSTM", "regularised", "with", "variational", "dropout", "performed", "favorably", "compared", "with", "baselines", "in", "the", "literature", ",", "outperforming", "all", "previous", "neural", "models", "on", "Hutter", "Prize", "and", "tying", "the", "best", "previous", "result", "on", "text8", ".", "Byte", "-", "level", "mLSTM", "was", "also", "able", "to", "perform", "competitively", "with", "word", "-", "level", "language", "models", "on", "WikiText", "-", "2", ".", "Unlike", "many", "previous", "approaches", "that", "have", "achieved", "success", "at", "character", "level", "language", "modelling", ",", "mLSTM", "does", "not", "use", "non", "-", "linear", "recurrent", "depth", ".", "All", "mLSTMs", "considered", "in", "this", "work", "only", "had", "2", "linear", "recurrent", "transition", "matrices", ",", "whereas", "comparable", "works", "such", "as", "recurrent", "highway", "networks", "use", "a", "recurrent", "depth", "of", "up", "to", "10", "to", "achieve", "best", "results", ".", "This", "makes", "mLSTM", "more", "easily", "parallelizable", "than", "these", "approaches", ".", "Additionally", ",", "our", "work", "suggests", "that", "a", "large", "depth", "is", "not", "necessary", "to", "achieve", "competitive", "results", "on", "character", "level", "language", "modelling", ".", "We", "hypothesize", "that", "mLSTM", "\u2019s", "ability", "to", "have", "very", "different", "transition", "functions", "for", "each", "possible", "input", "is", "what", "makes", "it", "successful", "at", "this", "task", ".", "While", "recurrent", "depth", "can", "accomplish", "this", "too", ",", "mLSTM", "can", "achieve", "this", "more", "efficiently", ".", "While", "these", "results", "are", "promising", ",", "it", "remains", "to", "be", "seen", "how", "mLSTM", "performs", "at", "word", "-", "level", "language", "modelling", "and", "other", "discrete", "multinomial", "generative", "modelling", "tasks", ",", "and", "whether", "mLSTM", "can", "be", "formulated", "to", "apply", "more", "broadly", "to", "tasks", "with", "continuous", "or", "non", "-", "sparse", "input", "units", ".", "We", "also", "hope", "this", "work", "will", "motivate", "further", "exploration", "in", "generative", "RNN", "architectures", "with", "flexible", "input", "-", "dependent", "transition", "functions", ".", "bibliography", ":", "References"]}