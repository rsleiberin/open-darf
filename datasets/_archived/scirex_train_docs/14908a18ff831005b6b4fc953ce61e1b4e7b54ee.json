{"coref": {"Accuracy": [[2600, 2601], [2940, 2942], [3175, 3177], [3754, 3757], [4121, 4124]], "Emotion_Classification": [[3, 5], [190, 195], [368, 371], [403, 406], [588, 591], [611, 613], [696, 698], [719, 722], [783, 787], [994, 999], [1067, 1072], [1334, 1339], [1402, 1405], [1586, 1587], [1924, 1926], [2629, 2632], [3044, 3045], [4091, 4092], [5195, 5198], [5301, 5304]], "Macro-F1": [[102, 104], [137, 139], [4001, 4003], [4135, 4141], [4146, 4148], [4230, 4234], [3803, 3805]], "SST-2_Binary_classification": [[795, 798], [799, 800], [3360, 3361], [3498, 3501], [3514, 3517], [3571, 3573]], "SemEval_2018_Task_1E-c": [[1113, 1117], [1457, 1458], [1518, 1519], [3882, 3884], [4636, 4637], [4667, 4668], [106, 107], [1118, 1119], [1380, 1381], [3652, 3653], [3678, 3679], [3688, 3689], [3913, 3914], [4011, 4012], [4017, 4018], [4522, 4523], [4713, 4714], [4763, 4764]], "Sentiment_Analysis": [], "Transformer": [[3557, 3558], [3818, 3819], [3833, 3834], [4022, 4023], [4639, 4640], [5124, 5125], [77, 78], [317, 318], [386, 387], [530, 531], [572, 573], [2138, 2139], [2214, 2215], [2236, 2237], [2299, 2300], [3368, 3369], [3442, 3443], [3684, 3685], [4066, 4067], [5209, 5210]], "Transformer__finetune_": [], "finetune": [[47, 48], [214, 216], [271, 272], [712, 713], [2395, 2396], [2545, 2546], [2584, 2585], [3934, 3935], [5185, 5186], [5203, 5204], [2371, 2372], [2640, 2641], [2932, 2933], [3062, 3063], [3162, 3163], [5140, 5141]]}, "coref_non_salient": {"0": [[1047, 1049], [5009, 5012]], "1": [[542, 545], [2311, 2313]], "10": [[2565, 2568], [2588, 2591]], "100": [[2294, 2297]], "101": [[3954, 3957]], "102": [[5034, 5036]], "11": [[2512, 2513], [2551, 2552], [2596, 2598], [3580, 3582]], "12": [[1968, 1971], [1973, 1974], [1992, 1993], [2322, 2323], [2557, 2558], [2757, 2758], [3070, 3071]], "13": [[338, 342], [5318, 5322]], "14": [[10, 12], [296, 298], [664, 666], [1845, 1847], [2364, 2366], [2386, 2388], [2509, 2511], [3164, 3166], [5310, 5312]], "15": [[4341, 4343], [4449, 4454]], "16": [[5073, 5074], [300, 301]], "17": [[5235, 5238]], "18": [[2915, 2916], [3034, 3036]], "19": [[42, 45], [267, 270], [707, 710], [815, 818], [5242, 5245]], "2": [[1324, 1326], [4332, 4334]], "20": [[2251, 2252], [5083, 5084]], "21": [[1447, 1449], [1463, 1465]], "22": [[3358, 3359]], "23": [[778, 781], [860, 862]], "24": [[1162, 1164], [1466, 1468], [4493, 4496], [4844, 4846]], "25": [[1934, 1936], [5182, 5184]], "26": [[4817, 4819]], "27": [[1938, 1941], [2122, 2124], [2397, 2399], [5085, 5087]], "28": [[2731, 2733], [2921, 2922]], "29": [[4848, 4850]], "3": [[19, 22], [1707, 1710]], "30": [[2711, 2712], [2713, 2714], [3124, 3125], [3126, 3127]], "31": [[3586, 3587], [1079, 1080], [3001, 3002], [3412, 3413], [3618, 3619], [3645, 3646], [3670, 3671], [4044, 4045], [4051, 4052], [4926, 4927], [4988, 4989]], "32": [[2659, 2662], [2736, 2739]], "33": [[2537, 2540]], "34": [[307, 308], [569, 570], [3578, 3579], [3820, 3821], [3836, 3837], [4025, 4026], [384, 385], [537, 538], [1872, 1873], [2131, 2132], [3385, 3386]], "35": [[5103, 5104]], "36": [[4157, 4161]], "37": [[23, 24], [747, 748], [1913, 1914], [3004, 3005]], "38": [[2343, 2345], [2723, 2725]], "39": [[3417, 3419]], "4": [[223, 224], [563, 565], [15, 16], [259, 260], [285, 286], [408, 409], [792, 793], [857, 858], [882, 883], [889, 890], [898, 899], [1747, 1748], [2759, 2760], [2784, 2785], [3011, 3012], [3028, 3029], [3184, 3185], [3235, 3236], [3503, 3504], [4604, 4605], [4618, 4619], [4919, 4920], [4979, 4980]], "40": [[5169, 5172]], "41": [[4209, 4211]], "42": [[1541, 1543], [3313, 3315], [3337, 3339]], "43": [[2769, 2771]], "44": [[1946, 1950]], "45": [[2349, 2350]], "46": [[2649, 2651]], "47": [[3777, 3780]], "48": [[3588, 3591]], "49": [[5239, 5241]], "5": [[2269, 2271], [4794, 4796], [4805, 4807]], "50": [[248, 251], [3813, 3816]], "51": [[1000, 1005]], "52": [[2858, 2863]], "53": [[3892, 3893]], "54": [[1159, 1161]], "55": [[3420, 3421]], "56": [[5215, 5217]], "57": [[2486, 2487]], "58": [[2329, 2331]], "59": [[2179, 2180]], "6": [[685, 687], [789, 790], [1727, 1729], [2125, 2127], [5259, 5262], [5313, 5314]], "60": [[4484, 4486]], "61": [[1397, 1401]], "62": [[4660, 4662]], "63": [[4898, 4901]], "64": [[4501, 4503]], "65": [[2670, 2676]], "66": [[1774, 1778]], "67": [[2515, 2518]], "68": [[2175, 2178]], "69": [[1563, 1565]], "7": [[3851, 3853], [3886, 3891]], "70": [[2076, 2080]], "71": [[2897, 2898]], "72": [[1983, 1986]], "73": [[839, 843]], "74": [[2346, 2347]], "75": [[1082, 1084]], "76": [[1704, 1706]], "77": [[2521, 2525]], "78": [[225, 228], [416, 419], [546, 549], [111, 114]], "79": [[584, 585]], "8": [[1589, 1591], [3115, 3117], [3146, 3149]], "80": [[2854, 2856]], "81": [[2156, 2157]], "82": [[487, 491]], "83": [[740, 742]], "84": [[3261, 3265]], "85": [[1327, 1330], [4518, 4521], [4671, 4673]], "86": [[1996, 1999]], "87": [[2393, 2394]], "88": [[2617, 2621]], "89": [[760, 763]], "9": [[4133, 4134], [4228, 4229], [3732, 3733], [3748, 3749], [4165, 4166], [4182, 4183], [4256, 4257], [4270, 4271], [4288, 4289], [4291, 4292], [4652, 4653]], "90": [[304, 306]], "91": [[1100, 1103]], "92": [[2150, 2155]], "93": [[2277, 2280]], "94": [[2198, 2200]], "95": [[960, 962]], "96": [[3623, 3626]], "97": [[1769, 1772]], "98": [[1698, 1701]], "99": [[4515, 4517]]}, "doc_id": "14908a18ff831005b6b4fc953ce61e1b4e7b54ee", "method_subrelations": {"Transformer__finetune_": [[[0, 11], "Transformer"], [[13, 21], "finetune"]]}, "n_ary_relations": [{"Material": "SST-2_Binary_classification", "Method": "Transformer__finetune_", "Metric": "Accuracy", "Task": "Sentiment_Analysis", "score": "90.9"}, {"Material": "SemEval_2018_Task_1E-c", "Method": "Transformer__finetune_", "Metric": "Macro-F1", "Task": "Emotion_Classification", "score": "56.1"}], "ner": [[3, 5, "Task"], [10, 12, "Method"], [19, 22, "Task"], [23, 24, "Task"], [42, 45, "Method"], [47, 48, "Method"], [102, 104, "Metric"], [137, 139, "Metric"], [190, 195, "Task"], [214, 216, "Method"], [223, 224, "Task"], [225, 228, "Task"], [248, 251, "Method"], [267, 270, "Method"], [271, 272, "Method"], [296, 298, "Method"], [304, 306, "Method"], [307, 308, "Method"], [338, 342, "Task"], [368, 371, "Task"], [403, 406, "Task"], [416, 419, "Task"], [487, 491, "Method"], [542, 545, "Task"], [546, 549, "Task"], [563, 565, "Task"], [569, 570, "Method"], [584, 585, "Task"], [588, 591, "Task"], [611, 613, "Task"], [664, 666, "Method"], [685, 687, "Task"], [696, 698, "Task"], [707, 710, "Method"], [712, 713, "Method"], [719, 722, "Task"], [740, 742, "Method"], [760, 763, "Method"], [778, 781, "Task"], [783, 787, "Task"], [789, 790, "Task"], [795, 798, "Material"], [799, 800, "Material"], [815, 818, "Method"], [839, 843, "Task"], [860, 862, "Task"], [960, 962, "Task"], [994, 999, "Task"], [1000, 1005, "Task"], [1047, 1049, "Method"], [1067, 1072, "Task"], [1082, 1084, "Metric"], [1100, 1103, "Method"], [1113, 1117, "Material"], [1159, 1161, "Task"], [1162, 1164, "Task"], [1324, 1326, "Metric"], [1327, 1330, "Metric"], [1334, 1339, "Task"], [1397, 1401, "Task"], [1402, 1405, "Task"], [1447, 1449, "Method"], [1457, 1458, "Material"], [1463, 1465, "Method"], [1466, 1468, "Task"], [1518, 1519, "Material"], [1541, 1543, "Method"], [1563, 1565, "Method"], [1586, 1587, "Task"], [1589, 1591, "Method"], [1698, 1701, "Task"], [1704, 1706, "Method"], [1707, 1710, "Task"], [1727, 1729, "Task"], [1769, 1772, "Task"], [1774, 1778, "Task"], [1845, 1847, "Method"], [1924, 1926, "Task"], [1934, 1936, "Task"], [1938, 1941, "Task"], [1946, 1950, "Task"], [1968, 1971, "Method"], [1973, 1974, "Method"], [1983, 1986, "Method"], [1992, 1993, "Method"], [1996, 1999, "Method"], [2076, 2080, "Method"], [2122, 2124, "Task"], [2125, 2127, "Task"], [2150, 2155, "Method"], [2156, 2157, "Method"], [2175, 2178, "Metric"], [2179, 2180, "Metric"], [2198, 2200, "Method"], [2251, 2252, "Method"], [2269, 2271, "Task"], [2277, 2280, "Method"], [2294, 2297, "Task"], [2311, 2313, "Task"], [2322, 2323, "Method"], [2329, 2331, "Task"], [2343, 2345, "Method"], [2346, 2347, "Method"], [2349, 2350, "Method"], [2364, 2366, "Method"], [2386, 2388, "Method"], [2393, 2394, "Method"], [2395, 2396, "Method"], [2397, 2399, "Task"], [2486, 2487, "Method"], [2509, 2511, "Method"], [2512, 2513, "Method"], [2515, 2518, "Method"], [2521, 2525, "Method"], [2537, 2540, "Method"], [2545, 2546, "Method"], [2551, 2552, "Method"], [2557, 2558, "Method"], [2565, 2568, "Method"], [2584, 2585, "Method"], [2588, 2591, "Method"], [2596, 2598, "Method"], [2600, 2601, "Metric"], [2617, 2621, "Method"], [2629, 2632, "Task"], [2649, 2651, "Task"], [2659, 2662, "Method"], [2670, 2676, "Task"], [2711, 2712, "Metric"], [2713, 2714, "Metric"], [2723, 2725, "Method"], [2731, 2733, "Metric"], [2736, 2739, "Method"], [2757, 2758, "Method"], [2769, 2771, "Method"], [2854, 2856, "Method"], [2858, 2863, "Method"], [2897, 2898, "Task"], [2915, 2916, "Method"], [2921, 2922, "Metric"], [2940, 2942, "Metric"], [3034, 3036, "Method"], [3044, 3045, "Task"], [3070, 3071, "Method"], [3115, 3117, "Method"], [3124, 3125, "Metric"], [3126, 3127, "Metric"], [3146, 3149, "Method"], [3164, 3166, "Method"], [3175, 3177, "Metric"], [3261, 3265, "Method"], [3313, 3315, "Method"], [3337, 3339, "Method"], [3358, 3359, "Task"], [3360, 3361, "Material"], [3417, 3419, "Method"], [3420, 3421, "Method"], [3498, 3501, "Material"], [3514, 3517, "Material"], [3557, 3558, "Method"], [3571, 3573, "Material"], [3578, 3579, "Method"], [3580, 3582, "Method"], [3586, 3587, "Method"], [3588, 3591, "Method"], [3623, 3626, "Task"], [3754, 3757, "Metric"], [3777, 3780, "Metric"], [3813, 3816, "Method"], [3818, 3819, "Method"], [3820, 3821, "Method"], [3833, 3834, "Method"], [3836, 3837, "Method"], [3851, 3853, "Method"], [3882, 3884, "Material"], [3886, 3891, "Method"], [3892, 3893, "Method"], [3934, 3935, "Method"], [3954, 3957, "Method"], [4001, 4003, "Metric"], [4022, 4023, "Method"], [4025, 4026, "Method"], [4091, 4092, "Task"], [4121, 4124, "Metric"], [4133, 4134, "Metric"], [4135, 4141, "Metric"], [4146, 4148, "Metric"], [4157, 4161, "Metric"], [4209, 4211, "Metric"], [4228, 4229, "Metric"], [4230, 4234, "Metric"], [4332, 4334, "Metric"], [4341, 4343, "Method"], [4449, 4454, "Method"], [4484, 4486, "Method"], [4493, 4496, "Task"], [4501, 4503, "Task"], [4515, 4517, "Metric"], [4518, 4521, "Metric"], [4636, 4637, "Material"], [4639, 4640, "Method"], [4660, 4662, "Method"], [4667, 4668, "Material"], [4671, 4673, "Metric"], [4794, 4796, "Task"], [4805, 4807, "Task"], [4817, 4819, "Metric"], [4844, 4846, "Task"], [4848, 4850, "Metric"], [4898, 4901, "Method"], [5009, 5012, "Method"], [5034, 5036, "Method"], [5073, 5074, "Method"], [5083, 5084, "Method"], [5085, 5087, "Task"], [5103, 5104, "Method"], [5124, 5125, "Method"], [5169, 5172, "Task"], [5182, 5184, "Task"], [5185, 5186, "Method"], [5195, 5198, "Task"], [5203, 5204, "Method"], [5215, 5217, "Task"], [5235, 5238, "Method"], [5239, 5241, "Task"], [5242, 5245, "Method"], [5259, 5262, "Task"], [5301, 5304, "Task"], [5310, 5312, "Method"], [5313, 5314, "Task"], [5318, 5322, "Task"], [15, 16, "Task"], [77, 78, "Method"], [106, 107, "Material"], [111, 114, "Task"], [259, 260, "Task"], [285, 286, "Task"], [300, 301, "Method"], [317, 318, "Method"], [384, 385, "Method"], [386, 387, "Method"], [408, 409, "Task"], [530, 531, "Method"], [537, 538, "Method"], [572, 573, "Method"], [747, 748, "Task"], [792, 793, "Task"], [857, 858, "Task"], [882, 883, "Task"], [889, 890, "Task"], [898, 899, "Task"], [1079, 1080, "Method"], [1118, 1119, "Material"], [1380, 1381, "Material"], [1747, 1748, "Task"], [1872, 1873, "Method"], [1913, 1914, "Task"], [2131, 2132, "Method"], [2138, 2139, "Method"], [2214, 2215, "Method"], [2236, 2237, "Method"], [2299, 2300, "Method"], [2371, 2372, "Method"], [2640, 2641, "Method"], [2759, 2760, "Task"], [2784, 2785, "Task"], [2932, 2933, "Method"], [3001, 3002, "Method"], [3004, 3005, "Task"], [3011, 3012, "Task"], [3028, 3029, "Task"], [3062, 3063, "Method"], [3162, 3163, "Method"], [3184, 3185, "Task"], [3235, 3236, "Task"], [3368, 3369, "Method"], [3385, 3386, "Method"], [3412, 3413, "Method"], [3442, 3443, "Method"], [3503, 3504, "Task"], [3618, 3619, "Method"], [3645, 3646, "Method"], [3652, 3653, "Material"], [3670, 3671, "Method"], [3678, 3679, "Material"], [3684, 3685, "Method"], [3688, 3689, "Material"], [3732, 3733, "Metric"], [3748, 3749, "Metric"], [3803, 3805, "Metric"], [3913, 3914, "Material"], [4011, 4012, "Material"], [4017, 4018, "Material"], [4044, 4045, "Method"], [4051, 4052, "Method"], [4066, 4067, "Method"], [4165, 4166, "Metric"], [4182, 4183, "Metric"], [4256, 4257, "Metric"], [4270, 4271, "Metric"], [4288, 4289, "Metric"], [4291, 4292, "Metric"], [4522, 4523, "Material"], [4604, 4605, "Task"], [4618, 4619, "Task"], [4652, 4653, "Metric"], [4713, 4714, "Material"], [4763, 4764, "Material"], [4919, 4920, "Task"], [4926, 4927, "Method"], [4979, 4980, "Task"], [4988, 4989, "Method"], [5140, 5141, "Method"], [5209, 5210, "Method"]], "sections": [[0, 288], [288, 737], [737, 984], [984, 1111], [1111, 1371], [1371, 1688], [1688, 1779], [1779, 1932], [1932, 2104], [2104, 2309], [2309, 2501], [2501, 2613], [2613, 2846], [2846, 3113], [3113, 3493], [3493, 3496], [3496, 3609], [3609, 3676], [3676, 3991], [3991, 4086], [4086, 4089], [4089, 4513], [4513, 4874], [4874, 5173], [5173, 5327], [5327, 5330]], "sentences": [[0, 12], [12, 36], [36, 71], [71, 108], [108, 124], [124, 182], [182, 196], [196, 236], [236, 262], [262, 288], [288, 291], [291, 299], [299, 358], [358, 377], [377, 427], [427, 464], [464, 492], [492, 522], [522, 575], [575, 602], [602, 633], [633, 661], [661, 680], [680, 703], [703, 737], [737, 740], [740, 754], [754, 782], [782, 802], [802, 811], [811, 844], [844, 863], [863, 878], [878, 891], [891, 905], [905, 914], [914, 919], [919, 927], [927, 933], [933, 939], [939, 946], [946, 951], [951, 960], [960, 965], [965, 967], [967, 979], [979, 984], [984, 991], [991, 1006], [1006, 1026], [1026, 1043], [1043, 1065], [1065, 1097], [1097, 1100], [1100, 1103], [1103, 1106], [1106, 1111], [1111, 1117], [1117, 1120], [1120, 1151], [1151, 1165], [1165, 1184], [1184, 1236], [1236, 1252], [1252, 1318], [1318, 1323], [1323, 1340], [1340, 1371], [1371, 1376], [1376, 1413], [1413, 1469], [1469, 1489], [1489, 1504], [1504, 1533], [1533, 1559], [1559, 1588], [1588, 1595], [1595, 1622], [1622, 1688], [1688, 1691], [1691, 1712], [1712, 1734], [1734, 1750], [1750, 1779], [1779, 1782], [1782, 1822], [1822, 1842], [1842, 1868], [1868, 1885], [1885, 1916], [1916, 1932], [1932, 1937], [1937, 1956], [1956, 1972], [1972, 1991], [1991, 2013], [2013, 2021], [2021, 2037], [2037, 2063], [2063, 2104], [2104, 2111], [2111, 2148], [2148, 2195], [2195, 2206], [2206, 2223], [2223, 2257], [2257, 2272], [2272, 2302], [2302, 2309], [2309, 2314], [2314, 2332], [2332, 2351], [2351, 2377], [2377, 2389], [2389, 2407], [2407, 2414], [2414, 2462], [2462, 2463], [2463, 2475], [2475, 2480], [2480, 2490], [2490, 2501], [2501, 2505], [2505, 2532], [2532, 2544], [2544, 2559], [2559, 2569], [2569, 2594], [2594, 2613], [2613, 2621], [2621, 2633], [2633, 2657], [2657, 2683], [2683, 2715], [2715, 2734], [2734, 2763], [2763, 2802], [2802, 2825], [2825, 2846], [2846, 2851], [2851, 2882], [2882, 2896], [2896, 2928], [2928, 2958], [2958, 2985], [2985, 3000], [3000, 3031], [3031, 3040], [3040, 3060], [3060, 3086], [3086, 3100], [3100, 3113], [3113, 3117], [3117, 3139], [3139, 3157], [3157, 3173], [3173, 3190], [3190, 3205], [3205, 3229], [3229, 3253], [3253, 3280], [3280, 3293], [3293, 3346], [3346, 3360], [3360, 3410], [3410, 3422], [3422, 3427], [3427, 3461], [3461, 3473], [3473, 3481], [3481, 3493], [3493, 3496], [3496, 3501], [3501, 3513], [3513, 3549], [3549, 3555], [3555, 3596], [3596, 3609], [3609, 3616], [3616, 3640], [3640, 3664], [3664, 3676], [3676, 3680], [3680, 3702], [3702, 3727], [3727, 3758], [3758, 3809], [3809, 3841], [3841, 3874], [3874, 3907], [3907, 3928], [3928, 3952], [3952, 3975], [3975, 3991], [3991, 3997], [3997, 4014], [4014, 4027], [4027, 4037], [4037, 4055], [4055, 4075], [4075, 4086], [4086, 4089], [4089, 4096], [4096, 4125], [4125, 4142], [4142, 4164], [4164, 4181], [4181, 4212], [4212, 4241], [4241, 4248], [4248, 4252], [4252, 4273], [4273, 4304], [4304, 4321], [4321, 4344], [4344, 4379], [4379, 4384], [4384, 4387], [4387, 4413], [4413, 4455], [4455, 4475], [4475, 4505], [4505, 4513], [4513, 4521], [4521, 4540], [4540, 4563], [4563, 4581], [4581, 4634], [4634, 4669], [4669, 4678], [4678, 4682], [4682, 4705], [4705, 4732], [4732, 4755], [4755, 4779], [4779, 4829], [4829, 4847], [4847, 4856], [4856, 4860], [4860, 4874], [4874, 4882], [4882, 4911], [4911, 4929], [4929, 4949], [4949, 4981], [4981, 4994], [4994, 5019], [5019, 5056], [5056, 5067], [5067, 5098], [5098, 5122], [5122, 5173], [5173, 5176], [5176, 5199], [5199, 5224], [5224, 5242], [5242, 5258], [5258, 5287], [5287, 5327], [5327, 5330]], "words": ["document", ":", "Practical", "Text", "Classification", "With", "Large", "Pre", "-", "Trained", "Language", "Models", "Multi", "-", "emotion", "sentiment", "classification", "is", "a", "natural", "language", "processing", "(", "NLP", ")", "problem", "with", "valuable", "use", "cases", "on", "real", "-", "world", "data", ".", "We", "demonstrate", "that", "large", "-", "scale", "unsupervised", "language", "modeling", "combined", "with", "finetuning", "offers", "a", "practical", "solution", "to", "this", "task", "on", "difficult", "datasets", ",", "including", "those", "with", "label", "class", "imbalance", "and", "domain", "-", "specific", "context", ".", "By", "training", "an", "attention", "-", "based", "Transformer", "network", "on", "40", "GB", "of", "text", "(", "Amazon", "reviews", ")", "and", "fine", "-", "tuning", "on", "the", "training", "set", ",", "our", "model", "achieves", "a", "0.69", "F1", "score", "on", "the", "SemEval", "Task", "1:E", "-", "c", "multidimensional", "emotion", "classification", "problem", ",", "based", "on", "the", "Plutchik", "wheel", "of", "emotions", ".", "These", "results", "are", "competitive", "with", "state", "of", "the", "art", "models", ",", "including", "strong", "F1", "scores", "on", "difficult", "(", "emotion", ")", "categories", "such", "as", "Fear", "(", "0.73", ")", ",", "Disgust", "(", "0.77", ")", "and", "Anger", "(", "0.78", ")", ",", "as", "well", "as", "competitive", "results", "on", "rare", "categories", "such", "as", "Anticipation", "(", "0.42", ")", "and", "Surprise", "(", "0.37", ")", ".", "Furthermore", ",", "we", "demonstrate", "our", "application", "on", "a", "real", "world", "text", "classification", "task", ".", "We", "create", "a", "narrowly", "collected", "text", "dataset", "of", "real", "tweets", "on", "several", "topics", ",", "and", "show", "that", "our", "finetuned", "model", "outperforms", "general", "purpose", "commercially", "available", "APIs", "for", "sentiment", "and", "multidimensional", "emotion", "classification", "on", "this", "dataset", "by", "a", "significant", "margin", ".", "We", "also", "perform", "a", "variety", "of", "additional", "studies", ",", "investigating", "properties", "of", "deep", "learning", "architectures", ",", "datasets", "and", "algorithms", "for", "achieving", "practical", "multidimensional", "sentiment", "classification", ".", "Overall", ",", "we", "find", "that", "unsupervised", "language", "modeling", "and", "finetuning", "is", "a", "simple", "framework", "for", "achieving", "high", "quality", "results", "on", "real", "-", "world", "sentiment", "classification", ".", "section", ":", "Introduction", "Recent", "work", "has", "shown", "that", "language", "models", "\u2013", "both", "RNN", "variants", "like", "the", "multiplicative", "LSTM", "(", "mLSTM", ")", ",", "as", "well", "as", "the", "attention", "-", "based", "Transformer", "network", "\u2013", "can", "be", "trained", "efficiently", "over", "very", "large", "datasets", ",", "and", "that", "the", "resulting", "models", "can", "be", "transferred", "to", "downstream", "language", "understanding", "problems", ",", "often", "matching", "or", "exceeding", "the", "previous", "state", "of", "the", "art", "approaches", "on", "academic", "datasets", ".", "However", ",", "how", "well", "do", "these", "models", "perform", "on", "practical", "text", "classification", "problems", ",", "with", "real", "world", "data", "?", "In", "this", "work", ",", "we", "train", "both", "mLSTM", "and", "Transformer", "language", "models", "on", "a", "large", "40", "GB", "text", "dataset", ",", "then", "transfer", "those", "models", "to", "two", "text", "classification", "problems", ":", "binary", "sentiment", "(", "including", "Neutral", "labels", ")", ",", "and", "multidimensional", "emotion", "classification", "based", "on", "the", "Plutchik", "wheel", "of", "emotions", ".", "We", "examine", "our", "performance", "on", "these", "tasks", ",", "both", "against", "large", "academic", "datasets", ",", "and", "on", "an", "original", "text", "dataset", "that", "we", "compiled", "from", "social", "media", "messages", "about", "several", "specific", "topics", ",", "such", "as", "video", "games", ".", "We", "demonstrate", "that", "our", "approach", "matches", "the", "state", "of", "the", "art", "on", "the", "academic", "datasets", "without", "domain", "-", "specific", "training", "and", "without", "excessive", "hyper", "-", "parameter", "tuning", ".", "Meanwhile", "on", "the", "social", "media", "dataset", ",", "our", "approach", "outperforms", "commercially", "available", "APIs", "by", "significant", "margins", ",", "even", "when", "those", "models", "are", "re", "-", "calibrated", "to", "the", "test", "set", ".", "Furthermore", ",", "we", "notice", "that", "1", ")", "the", "Transformer", "model", "generally", "out", "-", "performs", "the", "mLSTM", "model", ",", "especially", "when", "fine", "-", "tuning", "on", "multidimensional", "emotion", "classification", ",", "and", "2", ")", "fine", "-", "tuning", "the", "model", "significantly", "improves", "performance", "on", "the", "emotion", "tasks", ",", "both", "for", "the", "mLSTM", "and", "the", "Transformer", "model", ".", "We", "suggest", "that", "our", "approach", "creates", "models", "with", "good", "generalization", "to", "increasingly", "difficult", "text", "classification", "problems", ",", "and", "we", "offer", "ablation", "studies", "to", "demonstrate", "that", "effect", ".", "It", "is", "difficult", "to", "fit", "a", "single", "model", "for", "text", "classification", "across", "domains", ",", "due", "to", "unknown", "words", ",", "specialized", "context", ",", "colloquial", "language", ",", "and", "other", "differences", "between", "domains", ".", "For", "example", ",", "words", "such", "as", "war", "and", "sick", "are", "not", "necessarily", "negative", "in", "the", "context", "of", "video", "games", ",", "which", "are", "significantly", "represented", "in", "our", "dataset", ".", "By", "training", "a", "language", "model", "across", "a", "large", "text", "dataset", ",", "we", "expose", "our", "model", "to", "many", "contexts", ".", "Perhaps", "a", "small", "amount", "of", "downstream", "transfer", "is", "enough", "to", "choose", "the", "right", "context", "features", "for", "emotion", "classification", "in", "the", "appropriate", "setting", ".", "Our", "work", "shows", "that", "unsupervised", "language", "modeling", "combined", "with", "finetuning", "offers", "a", "practical", "solution", "to", "specialized", "text", "classification", "problems", ",", "including", "those", "with", "large", "category", "class", "imbalance", ",", "and", "significant", "human", "label", "disagreement", ".", "section", ":", "Background", "Supervised", "learning", "is", "difficult", "to", "apply", "to", "NLP", "problems", "because", "labels", "are", "expensive", ".", "Following", ",", "and", ",", "we", "train", "unsupervised", "text", "models", "on", "large", "amounts", "of", "unlabelled", "text", "data", ",", "and", "transfer", "the", "model", "features", "to", "small", "supervised", "text", "problems", ".", "The", "supervised", "text", "classification", "problem", "used", "for", "transfer", "is", "binary", "sentiment", "on", "the", "Stanford", "Sentiment", "Treebank", "(", "SST", ")", ".", "Some", "of", "these", "binary", "text", "examples", "are", "subtle", ".", "Prior", "works", "show", "that", "unsupervised", "language", "models", "can", "learn", "nuanced", "features", "of", "text", ",", "such", "as", "word", "ordering", "and", "double", "negation", ",", "just", "from", "the", "underlying", "task", "of", "next", "-", "word", "prediction", ".", "However", ",", "while", "this", "includes", "difficult", "examples", ",", "it", "does", "not", "necessarily", "represent", "sentiment", "on", "practical", "text", "problems", ".", "The", "source", "material", "(", "professionally", "written", "movie", "reviews", ")", "does", "not", "include", "colloquial", "language", ".", "The", "dataset", "excludes", "Neutral", "sentiment", "texts", "and", "those", "with", "weak", "directional", "sentiment", ".", "The", "dataset", "does", "not", "include", "dimensions", "of", "sentiment", "apart", "from", "Positive", "and", "Negative", ".", "TweetWatsonSadJoyFearGCLOursBinaryBinaryBinaryEncouraging", "collaboration", "among", "players", "in", "Sea", "of", "Thieves", "<", "url>", "-", "0.3020.2290.1940.150", "-", "0.80Posgot", "my", "first", "kill", "on", "Fortnite", "all", "by", "myself", "I", "\u2019", "m", "geeked", "<", "emoji", ">", "perioddddd.", "-", "0.8470.0030.6660.225", "+", "0.60NeuFar", "Cry", "5", "\u201d", "Lost", "On", "Mars", "\u201d", "Gameplay", "Walkthrough", "-", "DLC2", ":", "<", "url", ">", "via", "@YouTube", "-", "0.9090.0470.0150.873", "+", "0.00NeuNEW", "SUBMACHINE", "GUN", "IS", "INSANE", "!", "\u2014", "Fortnite", "Best", "Moments", "39", "(", "Fortnite", "Funny", "Fails", "&", "WTF", "Moments", ")", "<", "url>", "-", "0.9360.8210.1780.056", "-", "0.10Pos", "paragraph", ":", "Plutchik", "\u2019s", "Wheel", "of", "Emotions", "We", "focus", "our", "multi", "-", "dimension", "emotion", "classification", "on", "Plutchik", "\u2019s", "wheel", "of", "emotions", ".", "This", "taxonomy", ",", "in", "use", "since", "1979", ",", "aims", "to", "classify", "human", "emotions", "as", "a", "combination", "of", "four", "dualities", ":", "Joy", "-", "Sadness", ",", "Anger", "-", "Fear", ",", "Trust", "-", "Disgust", ",", "and", "Surprise", "-", "Anticipation", ".", "According", "to", "the", "basic", "emotion", "model", ",", "while", "humans", "experience", "hundreds", "of", "emotions", ",", "some", "emotions", "are", "more", "fundamental", "than", "others", ".", "The", "commercial", "general", "purpose", "emotion", "classification", "API", "that", "we", "compare", "against", ",", "IBM", "\u2019s", "Watson", ",", "offers", "classification", "scores", "for", "the", "Joy", ",", "Sadness", ",", "Fear", ",", "Disgust", "and", "Anger", "emotions", "\u2013", "all", "present", "in", "Plutchik", "\u2019s", "wheel", "(", "Fig", ".", "[", "reference", "]", ")", ".", "paragraph", ":", "SemEval", "Multidimension", "Emotion", "Dataset", "The", "SemEval", "Task", "1:E", "-", "c", "problem", "offers", "a", "training", "set", "of", "6", ",", "857", "tweets", ",", "with", "binary", "labels", "for", "the", "eight", "Plutchik", "categories", ",", "plus", "Optimism", ",", "Pessimism", ",", "and", "Love", ".", "This", "dataset", "was", "created", "through", "a", "process", "of", "text", "selection", "and", "human", "labeling", ".", "We", "show", "our", "results", "on", "this", "dataset", "and", "compare", "it", "to", "the", "current", "state", "of", "the", "art", "performance", ".", "While", "it", "is", "not", "possible", "to", "report", "rater", "agreement", "on", "these", "categories", "for", "the", "compilation", "of", "the", "dataset", ",", "the", "authors", "note", "that", "2", "out", "of", "7", "raters", "had", "to", "agree", "for", "a", "positive", "label", "to", "be", "applied", ",", "as", "requiring", "larger", "agreement", "caused", "a", "scarcity", "of", "labels", "for", "some", "categories", ".", "This", "indicates", "that", "some", "of", "the", "categories", "had", "significant", "rater", "disagreement", "between", "the", "human", "raters", ".", "The", "dataset", "also", "included", "a", "substantial", "degree", "of", "label", "class", "imbalance", ",", "with", "some", "categories", "like", "Anger", "(", "37", "%", ")", ",", "Disgust", "(", "38", "%", ")", ",", "Joy", "(", "36", "%", ")", "and", "Sadness", "(", "29", "%", ")", "represented", "often", "in", "the", "dataset", ",", "while", "others", "like", "Trust", "(", "5", "%", ")", "and", "Surprise", "(", "5", "%", ")", "present", "much", "less", "frequently", "(", "Fig", ".", "[", "reference", "]", ")", ".", "This", "class", "imbalance", "and", "human", "rater", "disagreement", "is", "not", "uncommon", "for", "real", "world", "text", "classification", "problems", ".", "SizeAngerAnticipationDisgustFearJoySadSurpriseTrustAve", "/", "NoneSemEval6", ",", "85837.214.338.018.236.229.45.35.223.0", "/", "2.9", "(", "Random", ")", "4", ",", "0217.814.75.21.721.93.44.36.68.2", "/", "52.1", "(", "Active", ")", "5", ",", "02422.010.212.35.619.76.37.16.511.2", "/", "35.6", "(", "All", ")", "13", ",", "32611.712.96.82.920.64.25.07.68.9", "/", "47.0", "paragraph", ":", "Company", "Tweet", "Dataset", "In", "addition", "to", "the", "SemEval", "tweet", "dataset", ",", "we", "wanted", "to", "see", "how", "our", "model", "would", "perform", "on", "a", "similar", "but", "domain", "-", "specific", "task", ":", "Plutchik", "emotion", "classification", "on", "tweets", "relevant", "to", "a", "particular", "company", ".", "We", "collected", "tweets", "on", "a", "variety", "of", "topics", ",", "including", ":", "Video", "game", "tweets", "Tweets", "about", "the", "company", "stock", "We", "submitted", "the", "first", "batch", "of", "4", ",", "000", "tweets", "to", "human", "raters", "on", "the", "FigureEight", "platform", ",", "with", "rules", "similar", "to", "those", "used", "by", "SemEval", ",", "which", "also", "used", "the", "FigureEight", "platform", "for", "human", "labeling", ".", "Specifically", ",", "we", "verified", "that", "raters", "passed", "our", "golden", "set", "(", "answering", "70", "%", "of", "test", "questions", "correctly", ")", ".", "We", "applied", "positive", "labels", "for", "each", "category", "where", "2", "out", "of", "5", "raters", "agreed", ".", "This", "is", "slightly", "less", "permissive", "than", "the", "2", "out", "of", "7", "raters", "used", "by", "SemEval", ",", "because", "we", "did", "not", "have", "a", "budget", "for", "7", "raters", "per", "tweet", ".", "After", "the", "first", "pass", ",", "we", "noticed", "that", "random", "sampling", "led", "to", "some", "categories", "being", "severely", "under", "-", "sampled", ",", "below", "5", "%", "of", "tweets", ".", "Thus", "we", "employed", "a", "bootstrapping", "technique", "to", "pre", "-", "classify", "tweets", "by", "category", "using", "our", "current", "model", ",", "and", "choose", "tweets", "with", "more", "likely", "emotion", "tweets", "for", "classification", ".", "See", "Active", "Learning", "section", "for", "details", ".", "We", "also", "sampled", "5", ",", "000", "tweets", "balanced", "by", "source", "category", ",", "since", "video", "game", "tweets", "have", "much", "more", "emotion", ",", "thus", "dominated", "the", "bootstrapped", "selections", ".", "Henceforth", ",", "we", "refer", "to", "the", "combined", "company", "tweets", "dataset", "consisting", "of", ":", "4", ",", "021", "random", "tweets", "5", ",", "024", "tweets", "selected", "for", "higher", "emotion", "content", "4", ",", "281", "tweets", "selected", "for", "source", "category", "balance", "DatasetJudgmentsBinaryPlutchik", "(", "3", "choices", ")(", "8", "choices", ")", "SemEval20", ",", "51477.3%61.1%Company", "(", "random", ")", "20", ",", "00580.7%67.3%Company", "(", "active", ")", "25", ",", "01779.0%52.3%Company", "(", "balanced", ")", "23", ",", "81280.0%71.0", "%", "paragraph", ":", "Finetuning", "Recent", "work", "has", "shown", "promising", "results", "using", "unsupervised", "language", "modeling", ",", "followed", "by", "transfer", "learning", "to", "natural", "language", "tasks", ",", ".", "Furthermore", ",", "these", "models", "benefit", "when", "the", "entire", "model", "is", "fine", "-", "tuned", "on", "the", "transfer", "task", ",", "as", "demonstrated", "in", ".", "Specifically", ",", "these", "methods", "have", "beaten", "the", "state", "of", "the", "art", "on", "binary", "sentiment", "classification", ".", "These", "models", "have", "also", "attained", "the", "best", "overall", "score", "on", "the", "GLUE", "Benchmark", ",", "comprised", "of", "a", "variety", "of", "text", "understanding", "tasks", ",", "including", "entailment", "and", "question", "answering", ".", "section", ":", "Methodology", "We", "use", "a", "larger", "batch", "size", "with", "shorter", "sequence", "length", ",", "specifically", "a", "global", "batch", "of", "512", "and", "sequence", "length", "64", "tokens", "(", "tokenized", "with", "a", "32", ",", "000", "BPE", "vocabulary", ",", "as", "detailed", "in", "Characters", "and", "Subword", "Units", ".", "The", "shorter", "sequence", "length", "works", "well", "because", "the", "transfer", "target", "are", "tweets", ",", "which", "are", "short", "pieces", "of", "text", ".", "We", "trained", "our", "language", "model", "on", "the", "Amazon", "Reviews", "dataset", "rather", "than", "other", "large", "datasets", "like", "BooksCorpus", ",", "because", "reviews", "are", "rich", "in", "emotional", "context", ".", "We", "also", "train", "an", "mLSTM", "network", "on", "the", "same", "dataset", ",", "based", "on", "the", "model", "from", ".", "We", "chose", "to", "compare", "these", "particular", "models", "because", "they", "work", "in", "fundamentally", "different", "ways", "and", "because", "they", "collectively", "hold", "state", "of", "the", "art", "results", "on", "many", "significant", "academic", "NLP", "benchmarks", ".", "We", "wanted", "to", "test", "these", "models", "on", "difficult", "classification", "problems", "with", "real", "-", "world", "data", ".", "paragraph", ":", "Unsupervised", "Pretraining", ".", "The", "language", "modeling", "objective", "can", "be", "summarized", "as", "a", "maximum", "likelihood", "estimation", "problem", "for", "a", "sequence", "of", "tokens", ".", "We", "treat", "our", "model", "as", "a", "function", "with", "two", "parts", ":", "an", "encoder", "and", "decoder", ".", "The", "encoder", "forms", "the", "bulk", "of", "the", "model", ",", "including", "the", "token", "embedding", "dictionary", "as", "the", "first", "module", ".", "The", "decoder", "is", "simply", "a", "softmax", "linear", "layer", "that", "projects", "the", "encoder", "output", "into", "the", "dimension", "equal", "to", "the", "vocabulary", "size", ".", "The", "objective", "to", "maximize", "is", "as", "follows", ".", "where", "is", "a", "hidden", "layer", "activation", "in", "the", "final", "layer", "of", ",", "indexed", "for", "timestep", ".", "The", "model", "is", "tasked", "with", "predicting", "the", "next", "token", "given", "all", "of", "the", "ones", "prior", "by", "outputting", "a", "probability", "distribution", "over", "the", "vocabulary", "of", "tokens", ".", "Doing", "this", "for", "each", "timestep", "produces", "each", "term", "in", "the", "sum", "of", "the", "log", "-", "likelihood", "formulation", ",", "and", "so", "maximizing", "the", "correct", "probabilities", "is", "a", "way", "to", "understand", "the", "joint", "probability", "distribution", "of", "sequences", "in", "this", "corpus", "of", "text", ".", "paragraph", ":", "Characters", "and", "Subword", "Units", ".", "While", ",", "and", "have", "shown", "state", "of", "the", "art", "results", "for", "language", "modeling", "and", "task", "transfer", "with", "character", "-", "level", "mLSTM", "models", ",", "we", "found", "that", "our", "Transformer", "model", "benefits", "from", "modeling", "language", "through", "subword", "units", ".", "Using", "a", "byte", "-", "pair", "-", "encoding", "(", "BPE", ")", "of", "various", "sized", "we", "notice", "that", "a", "32", ",", "000", "word", "-", "piece", "vocabulary", "achieves", "a", "better", "bits", "per", "character", "(", "BPC", ")", "loss", "over", "one", "epoch", "of", "the", "Amazon", "Reviews", "dataset", "than", "a", "small", "vocabulary", ".", "We", "compute", "the", "BPC", "equivalent", "over", "word", "pieces", ",", "following", ".", "For", "the", "remainder", "of", "this", "work", ",", "our", "Transformer", "models", "use", "32", ",", "000", "word", "pieces", ".", "Recent", "work", "has", "shown", "that", "it", "is", "possible", "to", "train", "a", "character", "level", "Transformer", "that", "is", "up", "to", "64", "layers", "deep", "and", "which", "beat", "state", "of", "the", "art", "BPC", "over", "large", "text", "datasets", ".", "However", "this", "requires", "intermediate", "layer", "losses", ",", "and", "other", "auxiliary", "losses", "for", "optimal", "convergence", ".", "By", "comparison", ",", "uses", "a", "bytepair", "encoding", "vocabulary", "with", "40", ",", "000", "word", "pieces", "for", "their", "state", "of", "the", "art", "results", "on", "language", "transfer", "tasks", "with", "a", "Transformer", "model", ".", "Our", "work", "closely", "follows", "their", "model", ".", "paragraph", ":", "Supervised", "Finetuning", ".", "After", "the", "pretraining", ",", "we", "initialize", "a", "new", "decoder", "to", "be", "exclusively", "trained", "on", "the", "supervised", "problem", ".", "Depending", "on", "the", "task", ",", "this", "decoder", "may", "be", "a", "single", "linear", "layer", "with", "activation", "or", "an", "MLP", ".", "We", "also", "retain", "the", "original", "decoder", "and", "continue", "to", "train", "it", "by", "using", "language", "modeling", "as", "an", "auxiliary", "loss", "when", "finetuning", "on", "the", "new", "corpus", ".", "Error", "signals", "from", "both", "decoders", "are", "backpropagated", "into", "the", "language", "model", ".", "The", "differences", "between", "the", "hyperparameters", "for", "finetuning", "and", "language", "modeling", "are", "described", "in", "Table", "[", "reference", "]", ".", "Language", "ModelingFinetuningglobal", "batch", "size512", "(", "size", "64", "on", "8", "GPUs", ")", "32sequence", "length64", "-", "kept", "short", "because", "targeting", "tweet", "applicationmax", "(", "batch", ")", "optimizerADAM\u2062lr", "(", "schedule", ")", "\u00d7210", "-", "4", "(", "cosine", "decay", "after", "linear", "warmup", "on", "2000", "iterations", ")", "\u00d7110", "-", "5", "(", "constant", "after", "1", "/", "2", "epoch", "linear", "warmup", ")", "Decoder", "moduleR\u00d7dh32000Binary", ":", "MLP", "(", "1024", "\u2192", "nc", ")", "with", "PReLU", "and", "0.3", "dropoutMulticlass", ":", "MLP", "(", "4096", "\u2192", "2048", "\u2192", "1024", "\u2192", "nc", ")", "with", "PReLU", "and", "0.3", "dropout", "#", "Epochs15LossL\u2062LM", "=", "Softmax", "Cross", "EntropySigmoid", "Binary", "Cross", "Entropy", "+", "\u22c50.02L\u2062LM", "paragraph", ":", "ELMo", "Baseline", "We", "also", "compare", "our", "language", "models", "to", "ELMo", ",", "a", "contextualized", "word", "representation", "based", "on", "a", "deep", "bidirectional", "language", "model", ",", "trained", "on", "large", "text", "corpus", ".", "We", "use", "a", "publicly", "available", "pretrained", "ELMo", "model", "from", "the", "authors", ".", "During", "finetuning", ",", "text", "is", "embedded", "with", "ELMo", "before", "being", "passed", "into", "a", "decoder", ".", "Error", "signals", "are", "backpropagated", "into", "the", "ELMo", "language", "model", ".", "Unlike", "our", "other", "models", ",", "we", "do", "not", "use", "an", "auxiliary", "language", "modeling", "loss", "during", "finetuning", ",", "as", "the", "ELMo", "language", "model", "is", "bidirectional", ".", "Finetuning", "the", "ELMo", "model", "substantially", "improves", "accuracy", "on", "our", "tasks", ",", "thus", "we", "include", "only", "finetuned", "ELMo", "results", ".", "paragraph", ":", "Multihead", "vs.", "Single", "Head", "Finetuning", "Decoders", "The", "tweet", "datasets", "are", "an", "example", "of", "a", "multilabel", "classification", "problem", ".", "We", "can", "formulate", "the", "problem", "for", "the", "finetuning", "decoder", ",", "as", "either", "a", "collection", "of", "single", "binary", "problems", "or", "multiple", "problems", "put", "together", ".", "The", "single", "binary", "problem", "formulation", "allows", "for", "a", "focus", "on", "one", "class", "and", "end", "-", "to", "-", "end", "optimization", "will", "only", "have", "one", "error", "signal", ".", "However", ",", "because", "the", "label", "classes", "are", "imbalanced", "in", "all", "categories", ",", "this", "may", "lead", "to", "a", "sparse", "gradient", "signal", "for", "the", "positive", "label", ",", "which", "may", "impact", "recall", "and", "precision", ".", "Increasing", "the", "size", "of", "to", "more", "than", "one", "linear", "layer", "leads", "to", "rapid", "overfitting", "and", "lower", "validation", "performance", ".", "The", "combined", "binary", "problems", "formulation", "(", "henceforth", "described", "as", "multihead", ")", "allows", "for", "a", "richer", "error", "signal", "that", "propagates", "more", "information", "through", "the", "encoder", "and", "sentiment", "representation", "in", ".", "In", "this", "setup", ",", "constructing", "a", "Multilayer", "network", "is", "far", "more", "useful", ",", "and", "can", "be", "thought", "of", "as", "specifically", "creating", "sentiment", "features", "to", "be", "used", "at", "the", "final", "layer", "to", "predict", "the", "presence", "of", "the", "individual", "emotions", ".", "We", "find", "that", "the", "inclusion", "of", "easier", ",", "more", "balanced", "label", "categories", "improves", "performance", "on", "harder", "ones", "in", "Table", "[", "reference", "]", ".", "However", ",", "the", "easier", "categories", "have", "slightly", "lower", "performance", "because", "the", "network", "is", "not", "being", "optimized", "for", "only", "those", "categories", ".", "paragraph", ":", "Thresholding", "Supervised", "Results", "For", "both", "the", "multihead", "MLP", "and", "the", "single", "linear", "layer", "instantiating", "of", ",", "we", "found", "that", "thresholding", "predictions", "produced", "noticeably", "better", "results", "than", "using", "a", "fixed", "threshold", "value", "such", "as", ".", "This", "makes", "sense", "since", "the", "label", "classes", "for", "most", "categories", "are", "very", "imbalanced", ".", "For", "thresholding", ",", "we", "take", "a", "dataset", "of", "tweets", "and", "split", "it", "into", "training", "(", "70", "%", ")", ",", "thresholding", "(", "10", "%", ")", "and", "validation", "(", "20", "%", ")", "sets", ".", "At", "each", "epoch", "of", "finetuning", "on", "the", "training", "set", ",", "we", "calculate", "validation", "accuracy", "and", "save", "predictions", "on", "the", "threshold", "set", "on", "the", "epoch", "for", "which", "this", "is", "maximized", ".", "To", "threshold", ",", "we", "search", "the", "discretized", "version", "of", "[", "0", ",", "1", "]", ":", "the", "linear", "space", "for", "the", "positive", "label", "threshold", "for", "each", "category", ".", "We", "denoted", "the", "threshold", "which", "gave", "the", "best", "score", "on", "the", "threshold", "set", "as", ".", "IBM", "Watson", "and", "Google", "NLP", "both", "offer", "commercial", "APIs", "for", "binary", "sentiment", "analysis", ",", "producing", "scalar", "values", "that", "correspond", "to", "a", "continuous", "[", "-", "1", ",+", "1", "]", "sentiment", "score", ".", "We", "applied", "our", "thresholding", "procedure", "to", "these", "scores", ".", "In", "the", "case", "of", "classification", "with", "neutrals", "we", "create", "two", "thresholds", "which", "we", "individually", "optimized", "jointly", "over", "as", "well", ".", "With", "the", "finetuning", "procedure", ",", "we", "found", "success", "with", "a", "decoder", ",", "whose", "two", "output", "units", "are", "probability", "estimates", "of", "the", "positive", "and", "negative", "labels", ".", "These", "units", "both", "have", "sigmoid", "activations", ",", "since", "we", "denote", "a", "neutral", "as", ".", "To", "threshold", "these", "predictions", ",", "we", "searched", "the", "cartesian", "product", "to", "determine", ".", "paragraph", ":", "Active", "Learning", "We", "hypothesized", "that", "we", "could", "achieve", "greater", "precision", "and", "recall", "on", "our", "datasets", "if", "our", "class", "label", "were", "more", "equally", "balanced", ".", "To", "this", "end", ",", "we", "employed", "an", "active", "learning", "procedure", "to", "select", "unlabeled", "tweets", "to", "be", "labeled", ".", "The", "algorithm", "consisted", "of", "first", "finetuning", "a", "language", "model", "on", "labeled", "tweets", "for", "5", "epochs", ".", "At", "peak", "validation", "accuracy", ",", "we", "obtain", "predictions", ",", "for", "Plutchik", "sentiment", "on", "the", "unlabeled", "tweets", ".", "From", "the", "labeled", "dataset", ",", "we", "calculate", "the", "negative", "class", "percentage", "for", "each", "category", ".", "Then", "we", "obtain", "category", "a", "weighting", "parameter", "so", "that", "for", "Then", ",", "we", "get", "scores", "for", "each", "unlabeled", "point", "as", "weighted", "features", ":", ".", "This", "way", ",", "positive", "predictions", "for", "sentiment", "categories", "are", "weighted", "by", "how", "much", "they", "would", "contribute", "towards", "balancing", "all", "of", "the", "class", "distributions", ".", "The", "scores", "are", "used", "as", "weights", "in", "a", "weighted", "uniform", "random", "sampler", ",", "and", "from", "this", ",", "we", "sampled", "5", ",", "000", "tweets", "to", "be", "labeled", ".", "We", "found", "that", "overall", ",", "the", "method", "produced", "tweets", "with", "more", "emotion", ".", "Not", "only", "was", "the", "positive", "class", "balance", "averaged", "across", "label", "categories", "higher", "(", "11.2", "%", "compared", "to", "8.2", "%", "for", "random", "sampling", ")", ",", "but", "the", "percentage", "of", "tweets", "which", "had", "no", "emotion", "was", "dramatically", "lower", ":", "35.6", "%", "compared", "to", "52.1", "%", "for", "random", "sampling", "(", "Table", "[", "reference", "]", ")", ".", "We", "hence", "achieved", "better", "class", "balance", "than", "the", "dataset", "prior", "to", "the", "augmentation", ".", "SST", "(", "acc", ")", "Company", "-/", "=", "/+", "Transformer", "(", "finetune", ")", "90.9%81.2%88.2", "/", "73.5", "/", "81.9mLSTM", "(", "finetune", ")", "90.4%78.2%87.0", "/", "69.3", "/", "78.38k", "mLSTM", "[]", "93.8%77.3%86.0", "/", "67.4", "/", "78.6", "[]", "93.1%", "--", "ELMo", "(", "finetuned", ")", "79.9%71.4%81.7", "/", "60.1", "/", "72.4ELMo", "+", "BiLSTM", "+", "Attn", "[", "]", "91.6%", "--", "Watson", "API84.4%56.7%42.9", "/", "54.0", "/", "73.3Google", "Sentiment", "(", "GCL", ")", "API81.3%62.5%69.6", "/", "54.0", "/", "63.8Class", "Balance50.0", "/", "50.0", "-", "22.4", "/", "46.0", "/", "31.6", "AccuracyMicro", "F1Macro", "F1", "(", "Jaccard", ")", "Transformer", "(", "ours", ")", "0.5770.6900.561", "[]", "0.5950.7090.542", "[]", "0.5820.6940.534", "AngerAnticipationDisgustFearJoySadnessSurpriseTrustAverageCompanyTransformer", "(", "MH", ")", ".684.486.441.400.634.333.269.300.443Transformer", "(", "SH", ")", ".679.491.371.400.675.286.210.279.424mLSTM", "(", "SH", ")", ".636.426.319.232.609.260.201.284.371ELMo", "(", "MH", ")", ".515.306.325.086.489.182.161.182.281Watson.358", "-", ".179.086.520.096", "-", "--", "SemevalTransformer", "(", "MH", ")", ".779.413.769.723.850.712.360.240.606Transformer", "(", "SH", ")", ".774.425.765.735.832.699.373.247.606mLSTM", "(", "SH", ")", ".668.189.691.535.763.557.103.000.438ELMo", "(", "MH", ")", ".506.215.351.172.540.348.164.239.317Watson.498", "-", ".331.149.684.359", "-", "--", "section", ":", "Results", "subsection", ":", "Binary", "Sentiment", "Tweets", "For", "binary", "sentiment", ",", "we", "compare", "our", "model", "on", "two", "tasks", ":", "the", "academic", "SST", "dataset", ",", "which", "consists", "of", "a", "balanced", "set", "of", "Positive", "and", "Negative", "labels", ",", "and", "the", "company", "tweets", "dataset", ",", "which", "consists", "of", "a", "balance", "between", "Positive", ",", "Neutral", "and", "Negative", "labels", ".", "See", "Table", "[", "reference", "]", ".", "While", "the", "Transformer", "gets", "close", "but", "does", "not", "exceed", "the", "state", "of", "the", "art", "on", "the", "SST", "dataset", ",", "it", "exceeds", "both", "the", "mLSTM", "and", "ELMo", "baseline", "as", "well", "as", "both", "Watson", "and", "Google", "Sentiment", "APIs", "on", "the", "company", "tweets", ".", "This", "is", "despite", "optimally", "calibrating", "the", "API", "results", "on", "the", "test", "set", ".", "subsection", ":", "Multi", "-", "Label", "Emotion", "Tweets", "The", "IBM", "Watson", "API", "offers", "multi", "-", "label", "emotion", "predictions", "for", "five", "categories", ":", "Anger", ",", "Disgust", ",", "Fear", ",", "Joy", "and", "Sadness", ".", "We", "compare", "our", "models", "to", "Watson", "on", "these", "categories", "for", "both", "the", "SemEval", "dataset", "and", "the", "company", "tweets", "in", "Table", "[", "reference", "]", ".", "We", "find", "that", "our", "models", "outperform", "Watson", "on", "every", "emotion", "category", ".", "paragraph", ":", "SemEval", "Tweets", "We", "submitted", "our", "finetuned", "Transformer", "model", "to", "the", "SemEval", "Task1:E", "-", "C", "challenge", ",", "as", "seen", "in", "Table", "[", "reference", "]", ".", "These", "results", "were", "computed", "by", "the", "organizers", "on", "a", "golden", "test", "set", ",", "for", "which", "we", "do", "not", "have", "access", "to", "the", "truth", "labels", ".", "Our", "model", "achieved", "the", "top", "macro", "-", "averaged", "F1", "score", "among", "all", "submission", ",", "with", "competitive", "but", "lower", "scores", "for", "the", "micro", "-", "average", "F1", "an", "the", "Jaccard", "Index", "accuracy", ".", "This", "suggests", "that", "our", "model", "out", "-", "performs", "the", "other", "top", "submission", "on", "rare", "and", "difficult", "categories", ",", "since", "macro", "-", "average", "weighs", "performance", "on", "all", "classes", "equally", ",", "and", "the", "most", "common", "categories", "of", "Joy", ",", "Anger", ",", "Disgust", "and", "Optimism", "get", "relatively", "higher", "F1", "scores", "across", "all", "models", ".", "We", "also", "compare", "the", "deep", "learning", "architectures", "of", "the", "Transformer", "and", "mLSTM", "on", "this", "dataset", "in", "Table", "[", "reference", "]", "and", "find", "that", "the", "Transformer", "outperforms", "the", "mLSTM", "across", "Plutchik", "categories", ".", "The", "winner", "of", "the", "Task1:E", "-", "c", "challenge", "trained", "a", "bidirectional", "LSTM", "with", "an", "800", ",", "000", "word", "embedding", "vocabulary", "derived", "from", "training", "word", "vectors", "on", "a", "dataset", "of", "550", "million", "tweets", ".", "Similarly", ",", "the", "second", "place", "winner", "of", "the", "SemEval", "leaderboard", "trained", "a", "word", "-", "level", "bidirectional", "LSTM", "with", "attention", ",", "as", "well", "as", "including", "non", "-", "deep", "learning", "features", "into", "their", "ensemble", ".", "Both", "submissions", "used", "training", "data", "across", "SemEval", "tasks", ",", "as", "well", "as", "additional", "training", "data", "outside", "of", "the", "training", "set", ".", "In", "comparison", ",", "we", "demonstrate", "that", "finetuning", "can", "be", "as", "effective", "on", "this", "task", ",", "despite", "training", "only", "on", "7", ",", "000", "tweets", ".", "Furthermore", ",", "out", "language", "modeling", "took", "place", "on", "the", "Amazon", "Reviews", "dataset", ",", "which", "does", "not", "contain", "emoji", ",", "hashtags", "or", "usernames", ".", "We", "would", "expect", "to", "see", "improvements", "if", "our", "unsupervised", "dataset", "contained", "emoji", ",", "for", "example", ".", "paragraph", ":", "Plutchik", "on", "Company", "Tweets", "Our", "models", "gets", "lower", "F1", "scores", "on", "the", "company", "tweets", "dataset", "than", "on", "equivalent", "SemEval", "categories", ".", "As", "with", "the", "SemEval", "challenge", "tweets", ",", "the", "Transformer", "outperformed", "the", "mLSTM", ".", "These", "results", "are", "shown", "in", "Table", "[", "reference", "]", ".", "Both", "models", "performed", "significantly", "better", "than", "the", "Watson", "API", "on", "all", "categories", "for", "which", "Watson", "supplies", "predictions", ".", "We", "could", "not", "conclusively", "determine", "whether", "the", "singlehead", "or", "the", "multihead", "Transformer", "will", "perform", "better", "on", "a", "given", "task", ".", "Thus", "we", "recommend", "trying", "both", "methods", "on", "a", "new", "dataset", ".", "section", ":", "Analysis", "paragraph", ":", "Classification", "Performance", "by", "Dataset", "Size", "We", "would", "have", "liked", "to", "label", "more", "data", "for", "the", "company", "tweets", "dataset", ",", "and", "thus", "looked", "into", "how", "much", "extra", "labeling", "contributes", "to", "finetuned", "model", "performance", "accuracy", ".", "First", ",", "let", "us", "explain", "the", "difference", "between", "micro", "and", "macro", "averaging", "of", "the", "F1", "scores", ".", "We", "can", "summarize", "the", "F1", "scores", "of", "categories", "(", "or", "any", "other", "metric", ")", "through", "macro", "and", "micro", "averaging", "to", "obtain", ".", "The", "macro", "method", "weights", "each", "class", "equally", "by", "averaging", "the", "metric", "calculated", "on", "each", "individual", "class", ".", "The", "micro", "method", "accounts", "for", "the", "class", "imbalances", "in", "each", "category", "by", "aggregating", "all", "of", "the", "true", "/", "false", "positives", "/", "negatives", "first", ",", "and", "then", "calculating", "an", "overall", "metric", ".", "In", "one", "experiment", ",", "we", "decreased", "the", "size", "of", "the", "training", "dataset", "and", "observed", "the", "resulting", "macro", "and", "micro", "averaged", "F1", "scores", "across", "all", "categories", "on", "company", "tweets", ".", "The", "results", "are", "shown", "in", "Fig", ".", "[", "reference", "]", ".", "We", "observe", "that", "the", "macro", "average", "is", "more", "sensitive", "to", "dataset", "size", "and", "falls", "more", "quickly", "than", "the", "micro", "average", ".", "The", "interpretation", "of", "this", "is", "that", "categories", "with", "worse", "class", "imbalance", "(", "which", "consequently", "influence", "macro", "more", "than", "micro", "average", ")", "benefit", "more", "from", "having", "a", "larger", "training", "dataset", "size", ".", "This", "suggests", "that", "we", "may", "obtain", "substantially", "better", "results", "with", "more", "data", "in", "the", "harder", "categories", ".", "We", "conducted", "a", "related", "experiment", "that", "focused", "on", "the", "difference", "in", "category", "performance", "when", "using", "a", "single", "head", "versus", "a", "multihead", "decoder", ".", "We", "apply", "the", "two", "architectures", "at", "different", "training", "dataset", "sizes", "for", "three", "different", "label", "categories", ":", "Anger", ",", "Anticipation", "and", "Trust", ",", "which", "we", "categorize", "as", "low", ",", "medium", "and", "high", "difficulty", ",", "respectively", ".", "As", "seen", "in", "Fig", ".", "[", "reference", "]", "it", "appears", "that", "the", "difference", "between", "the", "single", "and", "multihead", "becomes", "more", "pronounced", "for", "more", "difficult", "categories", ",", "as", "well", "as", "for", "smaller", "dataset", "sizes", ".", "We", "do", "not", "have", "enough", "data", "to", "make", "a", "firm", "conclusion", ",", "but", "this", "study", "suggests", "that", "we", "could", "get", "more", "out", "of", "the", "labeled", "data", "that", "we", "have", ",", "by", "studying", "which", "categories", "benefit", "from", "single", "head", "and", "multihead", "decoders", ".", "All", "categories", "benefit", "from", "more", "training", "data", ",", "but", "some", "categories", "benefit", "from", "from", "marginal", "labeled", "data", "than", "others", ".", "This", "suggests", "further", "and", "more", "rigorous", "study", "of", "the", "boostrapping", "methods", "we", "used", "to", "select", "tweets", "for", "our", "human", "labeling", "budget", ",", "as", "described", "in", "the", "Active", "Learning", "section", ".", "[", "t", "]", "0.263", "[", "t", "]", "0.717", "paragraph", ":", "Dataset", "Quality", "and", "Human", "Rater", "Agreement", "The", "SemEval", "dataset", "applies", "a", "positive", "label", "for", "every", "category", "where", "2", "out", "of", "7", "vetted", "raters", "agree", ".", "The", "reason", "is", "for", "the", "dataset", "to", "contain", "difficult", "and", "subtle", "examples", "of", "sentiments", ",", "not", "just", "those", "examples", "where", "everyone", "agrees", ".", "The", "raters", "also", "have", "a", "tendency", "to", "under", "-", "label", "categories", ",", "especially", "when", "presented", "multiple", "options", ".", "Following", "a", "similar", "process", ",", "we", "required", "2", "out", "of", "5", "raters", "for", "a", "positive", "label", ",", "and", "in", "the", "case", "of", "binary", "sentiment", "labels", "(", "Positive", ",", "Neutral", ",", "Negative", ")", ",", "we", "rounded", "toward", "polarized", "sentiment", "and", "away", "from", "Neutral", "labels", "in", "the", "case", "of", "a", "2", "/", "3", "split", ".", "Applying", "the", "SemEval", "-", "trained", "Transformer", "directly", "to", "our", "company", "tweets", "dataset", "gets", "reasonably", "good", "results", "(", "0.338", "macro", "average", ")", ",", "also", "validating", "that", "our", "labeling", "technique", "is", "similar", "to", "that", "of", "SemEval", ".", "Looking", "at", "rater", "agreement", "by", "dataset", "(", "Fig", ".", "[", "reference", "]", ")", ",", "we", "see", "that", "Plutchik", "category", "labels", "contain", "large", "rater", "disagreement", ",", "even", "among", "vetted", "raters", "who", "passed", "the", "golden", "set", "test", ".", "Furthermore", ",", "datasets", "with", "more", "emotions", "(", "the", "SemEval", "dataset", "and", "our", "active", "learning", "sampled", "company", "tweets", ")", "contain", "higher", "Plutchik", "disagreement", "than", "random", "company", "tweets", ".", "This", "is", "likely", "because", "raters", "tend", "to", "apply", "the", "\u201d", "No", "Emotion", "\u201d", "label", "when", "they", "are", "not", "sure", "about", "a", "category", ".", "As", "Table", "[", "reference", "]", "shows", ",", "the", "SemEval", "and", "active", "company", "tweets", "datasets", "contain", "fewer", "no", "-", "emotion", "tweets", "than", "other", "datsets", ".", "It", "would", "be", "interesting", "to", "analyze", "rater", "disagreement", "by", "category", ",", "how", "much", "this", "effects", "classifier", "convergence", ",", "whether", "getting", "7", "+", "ratings", "per", "tweet", "helps", "classifier", "convergence", ",", "and", "also", "whether", "this", "work", "could", "benefit", "from", "estimating", "rater", "quality", "via", "agreement", "with", "the", "crowd", ",", "as", "proposed", "in", ".", "However", "this", "analysis", "is", "not", "straightforward", ",", "as", "the", "truth", "data", "is", "itself", "collected", "through", "human", "labeling", ".", "Alongside", "classifier", "convergence", "by", "dataset", "size", "(", "Fig", ".", "[", "reference", "]", ")", ",", "we", "think", "that", "this", "could", "be", "an", "interesting", "area", "a", "future", "research", ".", "paragraph", ":", "Difficult", "tweets", "and", "challenging", "contexts", ".", "There", "is", "not", "sufficient", "space", "for", "a", "thorough", "analysis", ",", "but", "we", "wanted", "to", "suggest", "why", "general", "purpose", "APIs", "may", "not", "work", "well", "on", "our", "company", "tweets", "dataset", ".", "Table", "[", "reference", "]", "samples", "the", "largest", "binary", "sentiment", "disagreements", "between", "human", "raters", "and", "the", "Watson", "API", ".", "For", "simplicity", ",", "we", "restrict", "examples", "to", "video", "game", "tweets", ",", "which", "comprise", "19.1", "%", "of", "our", "test", "set", ".", "As", "we", "can", "see", ",", "all", "of", "these", "examples", "appear", "to", "ascribe", "negative", "emotion", "to", "generally", "negative", "terms", "which", ",", "in", "a", "video", "game", "context", ",", "do", "not", "indicate", "negative", "sentiment", ".", "Our", "purpose", "is", "not", "to", "castigate", "the", "Watson", "or", "the", "GCL", "APIs", ".", "Rather", ",", "we", "propose", "that", "it", "may", "not", "be", "possible", "to", "provide", "context", "-", "independent", "emotion", "classification", "scores", "that", "work", "well", "across", "text", "contexts", ".", "It", "may", "work", "better", "in", "practice", ",", "on", "some", "tasks", ",", "to", "train", "a", "large", "unsupervised", "model", "and", "to", "use", "a", "small", "amount", "of", "labeled", "data", "to", "finetune", "on", "the", "context", "present", "in", "the", "specific", "dataset", ".", "We", "would", "like", "to", "quantify", "this", "further", "in", "future", "work", ".", "Recent", "work", "shows", "that", "training", "an", "RNN", "with", "multiple", "softmax", "outputs", "leads", "to", "a", "much", "improved", "BPC", "on", "language", "modeling", ",", "especially", "for", "diverse", "datasets", "and", "models", "with", "large", "vocabularies", ".", "This", "is", "because", "the", "multiple", "softmaxes", "are", "able", "to", "capture", "a", "larger", "number", "of", "distinct", "contexts", "in", "the", "text", "than", "a", "single", "output", ".", "Perhaps", "our", "Transformer", "also", "captures", "the", "features", "relevant", "to", "a", "large", "number", "of", "distinct", "contexts", ",", "and", "the", "finetuning", "is", "able", "to", "select", "the", "most", "significant", "of", "these", "features", ",", "while", "ignoring", "those", "features", "that", "\u2013", "while", "adding", "value", "in", "general", "\u2013", "are", "not", "appropriate", "in", "a", "video", "game", "setting", ".", "section", ":", "Conclusion", "In", "this", "work", "we", "demonstrate", "that", "unsupervised", "pretraining", "and", "finetuning", "provides", "a", "flexible", "framework", "that", "is", "effective", "for", "difficult", "text", "classification", "tasks", ".", "We", "noticed", "that", "the", "finetuning", "was", "especially", "effective", "with", "the", "Transformer", "network", ",", "when", "transferring", "to", "downstream", "tasks", "with", "noisy", "labels", "and", "specialized", "context", ".", "We", "think", "that", "this", "framework", "makes", "it", "easy", "to", "customize", "a", "text", "classification", "model", "on", "niche", "tasks", ".", "Unsupervised", "language", "modeling", "can", "be", "done", "on", "general", "text", "datasets", ",", "and", "requires", "no", "labels", ".", "Meanwhile", "downstream", "task", "transfer", "works", "well", "enough", ",", "even", "on", "small", "amounts", "of", "domain", "-", "specific", "labelled", "data", ",", "to", "be", "accessible", "to", "most", "academics", "and", "small", "organization", ".", "It", "would", "be", "great", "to", "see", "this", "approach", "applied", "to", "a", "variety", "of", "practical", "text", "classification", "problems", ",", "much", "as", "and", "have", "applied", "language", "modeling", "and", "transfer", "to", "a", "variety", "of", "academic", "text", "understanding", "problems", "on", "the", "GLUE", "Benchmark", ".", "bibliography", ":", "References"]}