{"coref": {"100D_LSTMs_w__word-by-word_attention": [], "Natural_Language_Inference": [], "Parameters": [], "SNLI": [[511, 515], [516, 517], [599, 600], [792, 793], [846, 847], [2352, 2353], [2363, 2364], [2292, 2296], [2452, 2453], [3954, 3958]], "__Test_Accuracy": [[204, 205], [538, 539], [566, 567], [787, 788], [838, 839], [2691, 2692], [2844, 2845], [3948, 3949]], "__Train_Accuracy": []}, "coref_non_salient": {"0": [[119, 127], [810, 817], [1975, 1982], [738, 745]], "1": [[79, 80], [492, 494], [1957, 1958], [3663, 3664], [4011, 4013]], "10": [[652, 653], [1434, 1435], [1510, 1512], [2282, 2283]], "11": [[2274, 2278], [3802, 3806]], "12": [[1192, 1194], [1891, 1895], [2153, 2157]], "13": [[238, 241], [1130, 1133], [3851, 3854]], "14": [[625, 626], [701, 702], [784, 785], [821, 822], [856, 857], [881, 882], [942, 943], [994, 995], [1076, 1077], [1134, 1135], [1135, 1136], [1169, 1170], [1241, 1242], [1245, 1246], [1274, 1275], [1293, 1294], [1423, 1424], [1622, 1623], [1666, 1667], [1722, 1723], [2054, 2055], [2510, 2511], [2524, 2525], [2545, 2546], [2558, 2559], [2698, 2699], [2789, 2790], [2873, 2874], [2936, 2937], [2957, 2958], [3059, 3060], [3229, 3230], [3264, 3265], [3395, 3396], [956, 957], [1567, 1568], [2044, 2045], [2202, 2203], [3880, 3881]], "15": [[75, 78], [409, 412]], "16": [[1836, 1840], [1899, 1903], [2163, 2165]], "17": [[106, 112], [525, 531]], "18": [[843, 845], [3950, 3952]], "19": [[3965, 3968]], "2": [[1172, 1176], [2778, 2780], [3020, 3022]], "20": [[311, 313], [1531, 1533]], "21": [[305, 307], [308, 310]], "22": [[1186, 1188], [2529, 2531], [2665, 2667], [2959, 2961]], "23": [[661, 664], [1489, 1492]], "24": [[2981, 2983]], "25": [[3278, 3280]], "26": [[3993, 3995]], "27": [[314, 316], [976, 978], [1516, 1518]], "28": [[242, 243], [344, 345], [420, 421], [441, 442], [543, 544], [576, 577], [776, 777], [801, 802], [869, 870], [883, 884], [1141, 1142], [1581, 1582], [2224, 2225], [2798, 2799], [3209, 3210], [4088, 4089], [329, 330], [617, 618], [989, 990], [1639, 1640], [2312, 2313], [3299, 3300], [3541, 3542]], "29": [[3905, 3907]], "3": [[295, 298], [3973, 3976], [35, 38]], "30": [[299, 300], [351, 352], [971, 972]], "31": [[27, 28], [174, 175], [3898, 3899], [3911, 3912]], "32": [[1310, 1312], [2481, 2483], [2825, 2827]], "33": [[3179, 3181]], "34": [[2371, 2373]], "35": [[2517, 2519]], "36": [[4111, 4118]], "37": [[1538, 1540]], "38": [[3900, 3901]], "39": [[2010, 2014]], "4": [[96, 98], [170, 172], [389, 391], [522, 524], [555, 557], [697, 699], [1199, 1201], [4091, 4093]], "40": [[2898, 2906]], "41": [[979, 981]], "42": [[1481, 1485]], "43": [[2389, 2390]], "44": [[2820, 2821]], "45": [[2620, 2621]], "46": [[570, 572], [797, 799], [2616, 2618], [2877, 2879]], "47": [[1408, 1410]], "48": [[2630, 2632]], "49": [[392, 394]], "5": [[3986, 3988], [4020, 4023]], "50": [[1180, 1183]], "51": [[371, 373]], "52": [[50, 57]], "53": [[2017, 2019]], "54": [[982, 984]], "55": [[996, 998]], "56": [[1078, 1081]], "57": [[1525, 1527]], "58": [[156, 158]], "59": [[86, 88]], "6": [[12, 14], [1935, 1941], [2920, 2921], [3764, 3765]], "60": [[1116, 1118]], "61": [[2860, 2865]], "62": [[2212, 2213]], "63": [[8, 11]], "64": [[1504, 1506]], "65": [[2793, 2797]], "66": [[943, 946]], "67": [[1905, 1908]], "68": [[480, 486]], "69": [[1519, 1521]], "7": [[192, 195], [3872, 3875]], "70": [[438, 440]], "71": [[947, 948]], "72": [[1577, 1580]], "73": [[2384, 2385]], "74": [[2838, 2840]], "75": [[644, 650]], "76": [[21, 25]], "77": [[2219, 2223]], "78": [[2413, 2415]], "79": [[473, 475]], "8": [[892, 898], [3102, 3103], [3104, 3110], [3567, 3573], [147, 148], [774, 775], [886, 887], [1726, 1727], [1734, 1735], [1814, 1815], [1862, 1863], [2001, 2002], [2066, 2067], [2119, 2120], [2168, 2169], [2924, 2925], [3307, 3308], [3328, 3329], [3349, 3350], [3408, 3409], [3480, 3481], [3510, 3511], [3523, 3524], [3544, 3550], [3557, 3558], [3685, 3686], [3821, 3822], [3922, 3923], [4046, 4047], [4063, 4064]], "80": [[1160, 1162]], "81": [[2823, 2824]], "9": [[909, 911], [2553, 2555], [2575, 2577], [3354, 3356], [3359, 3361], [3384, 3386]]}, "doc_id": "12db83e66e50152e170d5009c425c925ad2e2c2a", "method_subrelations": {"100D_LSTMs_w__word-by-word_attention": [[[0, 36], "100D_LSTMs_w__word-by-word_attention"]]}, "n_ary_relations": [{"Material": "SNLI", "Method": "100D_LSTMs_w__word-by-word_attention", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "83.5"}, {"Material": "SNLI", "Method": "100D_LSTMs_w__word-by-word_attention", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "85.3"}, {"Material": "SNLI", "Method": "100D_LSTMs_w__word-by-word_attention", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "250k"}], "ner": [[8, 11, "Task"], [12, 14, "Task"], [21, 25, "Task"], [27, 28, "Method"], [50, 57, "Method"], [75, 78, "Method"], [79, 80, "Task"], [86, 88, "Method"], [96, 98, "Method"], [106, 112, "Method"], [119, 127, "Method"], [156, 158, "Method"], [170, 172, "Method"], [174, 175, "Method"], [192, 195, "Method"], [204, 205, "Metric"], [238, 241, "Task"], [242, 243, "Task"], [295, 298, "Task"], [299, 300, "Task"], [305, 307, "Task"], [308, 310, "Task"], [311, 313, "Task"], [314, 316, "Task"], [344, 345, "Task"], [371, 373, "Method"], [389, 391, "Method"], [392, 394, "Task"], [409, 412, "Method"], [420, 421, "Task"], [438, 440, "Method"], [441, 442, "Task"], [473, 475, "Method"], [480, 486, "Method"], [492, 494, "Task"], [511, 515, "Material"], [516, 517, "Material"], [522, 524, "Method"], [525, 531, "Method"], [538, 539, "Metric"], [543, 544, "Task"], [555, 557, "Method"], [566, 567, "Metric"], [570, 572, "Method"], [576, 577, "Task"], [599, 600, "Material"], [625, 626, "Method"], [644, 650, "Method"], [652, 653, "Task"], [661, 664, "Method"], [697, 699, "Method"], [701, 702, "Method"], [776, 777, "Task"], [784, 785, "Method"], [787, 788, "Metric"], [792, 793, "Material"], [797, 799, "Method"], [801, 802, "Task"], [810, 817, "Method"], [821, 822, "Method"], [838, 839, "Metric"], [843, 845, "Task"], [846, 847, "Material"], [856, 857, "Method"], [869, 870, "Task"], [881, 882, "Method"], [883, 884, "Task"], [892, 898, "Method"], [909, 911, "Method"], [942, 943, "Method"], [943, 946, "Method"], [947, 948, "Method"], [976, 978, "Task"], [979, 981, "Task"], [982, 984, "Task"], [994, 995, "Method"], [996, 998, "Method"], [1076, 1077, "Method"], [1078, 1081, "Method"], [1116, 1118, "Method"], [1130, 1133, "Task"], [1134, 1135, "Method"], [1135, 1136, "Method"], [1141, 1142, "Task"], [1160, 1162, "Method"], [1169, 1170, "Method"], [1172, 1176, "Method"], [1180, 1183, "Task"], [1186, 1188, "Method"], [1192, 1194, "Task"], [1199, 1201, "Method"], [1241, 1242, "Method"], [1245, 1246, "Method"], [1274, 1275, "Method"], [1293, 1294, "Method"], [1310, 1312, "Method"], [1408, 1410, "Method"], [1423, 1424, "Method"], [1434, 1435, "Task"], [1481, 1485, "Method"], [1489, 1492, "Method"], [1504, 1506, "Task"], [1510, 1512, "Task"], [1516, 1518, "Task"], [1519, 1521, "Task"], [1525, 1527, "Task"], [1531, 1533, "Task"], [1538, 1540, "Task"], [1577, 1580, "Method"], [1581, 1582, "Task"], [1622, 1623, "Method"], [1666, 1667, "Method"], [1722, 1723, "Method"], [1836, 1840, "Method"], [1891, 1895, "Task"], [1899, 1903, "Method"], [1905, 1908, "Method"], [1935, 1941, "Task"], [1957, 1958, "Task"], [1975, 1982, "Method"], [2010, 2014, "Method"], [2017, 2019, "Method"], [2054, 2055, "Method"], [2153, 2157, "Task"], [2163, 2165, "Method"], [2212, 2213, "Task"], [2219, 2223, "Method"], [2224, 2225, "Task"], [2274, 2278, "Method"], [2282, 2283, "Task"], [2352, 2353, "Material"], [2363, 2364, "Material"], [2371, 2373, "Method"], [2384, 2385, "Method"], [2389, 2390, "Task"], [2413, 2415, "Method"], [2481, 2483, "Method"], [2510, 2511, "Method"], [2517, 2519, "Task"], [2524, 2525, "Method"], [2529, 2531, "Method"], [2545, 2546, "Method"], [2553, 2555, "Method"], [2558, 2559, "Method"], [2575, 2577, "Method"], [2616, 2618, "Method"], [2620, 2621, "Method"], [2630, 2632, "Metric"], [2665, 2667, "Method"], [2691, 2692, "Metric"], [2698, 2699, "Method"], [2778, 2780, "Method"], [2789, 2790, "Method"], [2793, 2797, "Method"], [2798, 2799, "Task"], [2820, 2821, "Method"], [2823, 2824, "Method"], [2825, 2827, "Method"], [2838, 2840, "Method"], [2844, 2845, "Metric"], [2860, 2865, "Task"], [2873, 2874, "Method"], [2877, 2879, "Method"], [2898, 2906, "Method"], [2920, 2921, "Task"], [2936, 2937, "Method"], [2957, 2958, "Method"], [2959, 2961, "Method"], [2981, 2983, "Method"], [3020, 3022, "Method"], [3059, 3060, "Method"], [3102, 3103, "Method"], [3104, 3110, "Method"], [3179, 3181, "Task"], [3209, 3210, "Task"], [3229, 3230, "Method"], [3264, 3265, "Method"], [3278, 3280, "Method"], [3354, 3356, "Method"], [3359, 3361, "Method"], [3384, 3386, "Method"], [3395, 3396, "Method"], [3567, 3573, "Method"], [3663, 3664, "Task"], [3764, 3765, "Task"], [3802, 3806, "Method"], [3851, 3854, "Task"], [3872, 3875, "Method"], [3898, 3899, "Method"], [3900, 3901, "Method"], [3905, 3907, "Method"], [3911, 3912, "Method"], [3948, 3949, "Metric"], [3950, 3952, "Task"], [3965, 3968, "Method"], [3973, 3976, "Task"], [3986, 3988, "Task"], [3993, 3995, "Method"], [4011, 4013, "Task"], [4020, 4023, "Task"], [4088, 4089, "Task"], [4091, 4093, "Method"], [4111, 4118, "Task"], [35, 38, "Task"], [147, 148, "Method"], [329, 330, "Task"], [351, 352, "Task"], [617, 618, "Task"], [738, 745, "Method"], [774, 775, "Method"], [886, 887, "Method"], [956, 957, "Method"], [971, 972, "Task"], [989, 990, "Task"], [1567, 1568, "Method"], [1639, 1640, "Task"], [1726, 1727, "Method"], [1734, 1735, "Method"], [1814, 1815, "Method"], [1862, 1863, "Method"], [2001, 2002, "Method"], [2044, 2045, "Method"], [2066, 2067, "Method"], [2119, 2120, "Method"], [2168, 2169, "Method"], [2202, 2203, "Method"], [2292, 2296, "Material"], [2312, 2313, "Task"], [2452, 2453, "Material"], [2924, 2925, "Method"], [3299, 3300, "Task"], [3307, 3308, "Method"], [3328, 3329, "Method"], [3349, 3350, "Method"], [3408, 3409, "Method"], [3480, 3481, "Method"], [3510, 3511, "Method"], [3523, 3524, "Method"], [3541, 3542, "Task"], [3544, 3550, "Method"], [3557, 3558, "Method"], [3685, 3686, "Method"], [3821, 3822, "Method"], [3880, 3881, "Method"], [3922, 3923, "Method"], [3954, 3958, "Material"], [4046, 4047, "Method"], [4063, 4064, "Method"]], "sections": [[0, 14], [14, 211], [211, 848], [848, 940], [940, 1128], [1128, 1184], [1184, 1486], [1486, 1933], [1933, 2284], [2284, 2444], [2444, 3183], [3183, 3276], [3276, 3832], [3832, 4119], [4119, 4121], [4121, 4157], [4157, 4159]], "sentences": [[0, 14], [14, 17], [17, 62], [62, 89], [89, 113], [113, 139], [139, 159], [159, 183], [183, 211], [211, 214], [214, 238], [238, 289], [289, 335], [335, 432], [432, 455], [455, 476], [476, 504], [504, 510], [510, 548], [548, 578], [578, 587], [587, 591], [591, 620], [620, 654], [654, 686], [686, 694], [694, 732], [732, 766], [766, 782], [782, 807], [807, 848], [848, 851], [851, 875], [875, 903], [903, 940], [940, 943], [943, 990], [990, 1053], [1053, 1093], [1093, 1094], [1094, 1128], [1128, 1135], [1135, 1162], [1162, 1166], [1166, 1184], [1184, 1188], [1188, 1220], [1220, 1232], [1232, 1243], [1243, 1302], [1302, 1321], [1321, 1347], [1347, 1371], [1371, 1405], [1405, 1431], [1431, 1486], [1486, 1489], [1489, 1540], [1540, 1544], [1544, 1573], [1573, 1598], [1598, 1642], [1642, 1691], [1691, 1705], [1705, 1725], [1725, 1745], [1745, 1775], [1775, 1783], [1783, 1785], [1785, 1810], [1810, 1847], [1847, 1861], [1861, 1889], [1889, 1919], [1919, 1933], [1933, 1941], [1941, 1969], [1969, 2033], [2033, 2070], [2070, 2071], [2071, 2082], [2082, 2103], [2103, 2110], [2110, 2145], [2145, 2194], [2194, 2216], [2216, 2226], [2226, 2271], [2271, 2284], [2284, 2287], [2287, 2297], [2297, 2301], [2301, 2323], [2323, 2344], [2344, 2358], [2358, 2382], [2382, 2405], [2405, 2406], [2406, 2415], [2415, 2444], [2444, 2449], [2449, 2460], [2460, 2483], [2483, 2484], [2484, 2487], [2487, 2547], [2547, 2578], [2578, 2598], [2598, 2608], [2608, 2665], [2665, 2667], [2667, 2700], [2700, 2729], [2729, 2784], [2784, 2802], [2802, 2841], [2841, 2872], [2872, 2884], [2884, 2920], [2920, 2921], [2921, 2980], [2980, 3004], [3004, 3044], [3044, 3061], [3061, 3104], [3104, 3147], [3147, 3183], [3183, 3189], [3189, 3211], [3211, 3224], [3224, 3257], [3257, 3276], [3276, 3280], [3280, 3302], [3302, 3341], [3341, 3357], [3357, 3376], [3376, 3403], [3403, 3449], [3449, 3469], [3469, 3483], [3483, 3498], [3498, 3519], [3519, 3544], [3544, 3564], [3564, 3592], [3592, 3635], [3635, 3680], [3680, 3735], [3735, 3764], [3764, 3784], [3784, 3832], [3832, 3835], [3835, 3876], [3876, 3918], [3918, 3960], [3960, 4014], [4014, 4049], [4049, 4098], [4098, 4119], [4119, 4121], [4121, 4124], [4124, 4157], [4157, 4159]], "words": ["Published", "as", "a", "conference", "paper", "at", "ICLR", "2016", "REASONING", "ABOUT", "ENTAILMENT", "WITH", "NEURAL", "ATTENTION", "section", ":", "ABSTRACT", "While", "most", "approaches", "to", "automatically", "recognizing", "entailment", "relations", "have", "used", "classifiers", "employing", "hand", "engineered", "features", "derived", "from", "complex", "natural", "language", "processing", "pipelines", ",", "in", "practice", "their", "performance", "has", "been", "only", "slightly", "better", "than", "bag", "-", "of", "-", "word", "pair", "classifiers", "using", "only", "lexical", "similarity", ".", "The", "only", "attempt", "so", "far", "to", "build", "an", "end", "-", "to", "-", "end", "differentiable", "neural", "network", "for", "entailment", "failed", "to", "outperform", "such", "a", "simple", "similarity", "classifier", ".", "In", "this", "paper", ",", "we", "propose", "a", "neural", "model", "that", "reads", "two", "sentences", "to", "determine", "entailment", "using", "long", "short", "-", "term", "memory", "units", ".", "We", "extend", "this", "model", "with", "a", "word", "-", "by", "-", "word", "neural", "attention", "mechanism", "that", "encourages", "reasoning", "over", "entailments", "of", "pairs", "of", "words", "and", "phrases", ".", "Furthermore", ",", "we", "present", "a", "qualitative", "analysis", "of", "attention", "weights", "produced", "by", "this", "model", ",", "demonstrating", "such", "reasoning", "capabilities", ".", "On", "a", "large", "entailment", "dataset", "this", "model", "outperforms", "the", "previous", "best", "neural", "model", "and", "a", "classifier", "with", "engineered", "features", "by", "a", "substantial", "margin", ".", "It", "is", "the", "first", "generic", "end", "-", "to", "-", "end", "differentiable", "system", "that", "achieves", "state", "-", "of", "-", "the", "-", "art", "accuracy", "on", "a", "textual", "entailment", "dataset", ".", "section", ":", "INTRODUCTION", "The", "ability", "to", "determine", "the", "semantic", "relationship", "between", "two", "sentences", "is", "an", "integral", "part", "of", "machines", "that", "understand", "and", "reason", "with", "natural", "language", ".", "Recognizing", "textual", "entailment", "(", "RTE", ")", "is", "the", "task", "of", "determining", "whether", "two", "natural", "language", "sentences", "are", "(", "i", ")", "contradicting", "each", "other", ",", "(", "ii", ")", "not", "related", ",", "or", "whether", "(", "iii", ")", "the", "first", "sentence", "(", "called", "premise", ")", "entails", "the", "second", "sentence", "(", "called", "hypothesis", ")", ".", "This", "task", "is", "important", "since", "many", "natural", "language", "processing", "(", "NLP", ")", "problems", ",", "such", "as", "information", "extraction", ",", "relation", "extraction", ",", "text", "summarization", "or", "machine", "translation", ",", "rely", "on", "it", "explicitly", "or", "implicitly", "and", "could", "benefit", "from", "more", "accurate", "RTE", "systems", "[", "reference", "]", ".", "State", "-", "of", "-", "the", "-", "art", "systems", "for", "RTE", "so", "far", "relied", "heavily", "on", "engineered", "NLP", "pipelines", ",", "extensive", "manual", "creation", "of", "features", ",", "as", "well", "as", "various", "external", "resources", "and", "specialized", "subcomponents", "such", "as", "negation", "detection", "(", "e.g.", "[", "reference", "][", "reference", "][", "reference", "][", "reference", "]", ".", "Despite", "the", "success", "of", "neural", "networks", "for", "paraphrase", "detection", "(", "e.g.", "[", "reference", "][", "reference", "][", "reference", "]", ",", "end", "-", "to", "-", "end", "differentiable", "neural", "architectures", "failed", "to", "get", "close", "to", "acceptable", "performance", "for", "RTE", "due", "to", "the", "lack", "of", "large", "high", "-", "quality", "datasets", ".", "An", "end", "-", "to", "-", "end", "differentiable", "solution", "to", "RTE", "is", "desirable", ",", "since", "it", "avoids", "specific", "assumptions", "about", "the", "underlying", "language", ".", "In", "particular", ",", "there", "is", "no", "need", "for", "language", "features", "like", "part", "-", "of", "-", "speech", "tags", "or", "dependency", "parses", ".", "Furthermore", ",", "a", "generic", "sequence", "-", "to", "-", "sequence", "solution", "allows", "to", "extend", "the", "concept", "of", "capturing", "entailment", "across", "any", "sequential", "data", ",", "not", "only", "natural", "language", ".", "Recently", ",", "[", "reference", "]", "published", "the", "Stanford", "Natural", "Language", "Inference", "(", "SNLI", ")", "corpus", "accompanied", "by", "a", "neural", "network", "with", "long", "short", "-", "term", "memory", "units", "[", "reference", "]", ",", "which", "achieves", "an", "accuracy", "of", "77.6", "%", "for", "RTE", "on", "this", "dataset", ".", "It", "is", "the", "first", "time", "a", "generic", "neural", "model", "without", "hand", "-", "crafted", "features", "got", "close", "to", "the", "accuracy", "of", "a", "simple", "lexicalized", "classifier", "with", "engineered", "features", "for", "RTE", ".", "This", "can", "be", "explained", "by", "the", "high", "quality", "1", "arXiv:1509.06664v4", "[", "cs", ".", "CL", "]", "1", "Mar", "2016", "and", "size", "of", "SNLI", "compared", "to", "the", "two", "orders", "of", "magnitude", "smaller", "and", "partly", "synthetic", "datasets", "so", "far", "used", "to", "evaluate", "RTE", "systems", ".", "Bowman", "et", "al", ".", "'s", "LSTM", "encodes", "the", "premise", "and", "hypothesis", "as", "dense", "fixed", "-", "length", "vectors", "whose", "concatenation", "is", "subsequently", "used", "in", "a", "multi", "-", "layer", "perceptron", "(", "MLP", ")", "for", "classification", ".", "In", "contrast", ",", "we", "are", "proposing", "an", "attentive", "neural", "network", "that", "is", "capable", "of", "reasoning", "over", "entailments", "of", "pairs", "of", "words", "and", "phrases", "by", "processing", "the", "hypothesis", "conditioned", "on", "the", "premise", ".", "Our", "contributions", "are", "threefold", ":", "(", "i", ")", "We", "present", "a", "neural", "model", "based", "on", "LSTMs", "that", "reads", "two", "sentences", "in", "one", "go", "to", "determine", "entailment", ",", "as", "opposed", "to", "mapping", "each", "sentence", "independently", "into", "a", "semantic", "space", "(", "\u00a7", "2.2", ")", ",", "(", "ii", ")", "We", "extend", "this", "model", "with", "a", "neural", "word", "-", "by", "-", "word", "attention", "mechanism", "to", "encourage", "reasoning", "over", "entailments", "of", "pairs", "of", "words", "and", "phrases", "(", "\u00a7", "2.4", ")", ",", "and", "(", "iii", ")", "We", "provide", "a", "detailed", "qualitative", "analysis", "of", "neural", "attention", "for", "RTE", "(", "\u00a7", "4.1", ")", ".", "Our", "benchmark", "LSTM", "achieves", "an", "accuracy", "of", "80.9", "%", "on", "SNLI", ",", "outperforming", "a", "simple", "lexicalized", "classifier", "tailored", "to", "RTE", "by", "2.7", "percentage", "points", ".", "An", "extension", "with", "word", "-", "by", "-", "word", "neural", "attention", "surpasses", "this", "strong", "benchmark", "LSTM", "result", "by", "2.6", "percentage", "points", ",", "setting", "a", "new", "state", "-", "of", "-", "the", "-", "art", "accuracy", "of", "83.5", "%", "for", "recognizing", "entailment", "on", "SNLI", ".", "section", ":", "METHODS", "In", "this", "section", "we", "discuss", "LSTMs", "(", "\u00a7", "2.1", ")", "and", "describe", "how", "they", "can", "be", "applied", "to", "RTE", "(", "\u00a7", "2.2", ")", ".", "We", "introduce", "an", "extension", "of", "an", "LSTM", "for", "RTE", "with", "neural", "attention", "(", "\u00a7", "2.3", ")", "and", "word", "-", "by", "-", "word", "attention", "(", "\u00a7", "2.4", ")", ".", "Finally", ",", "we", "show", "how", "such", "attentive", "models", "can", "easily", "be", "used", "for", "attending", "both", "ways", ":", "over", "the", "premise", "conditioned", "on", "the", "hypothesis", "and", "over", "the", "hypothesis", "conditioned", "on", "the", "premise", "(", "\u00a7", "2.5", ")", ".", "section", ":", "LSTMS", "Recurrent", "neural", "networks", "(", "RNNs", ")", "with", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "units", "[", "reference", "]", "have", "been", "successfully", "applied", "to", "a", "wide", "range", "of", "NLP", "tasks", ",", "such", "as", "machine", "translation", ",", "constituency", "parsing", ",", "language", "modeling", "[", "reference", "]", "and", "recently", "RTE", "[", "reference", "]", ".", "LSTMs", "encompass", "memory", "cells", "that", "can", "store", "information", "for", "a", "long", "period", "of", "time", ",", "as", "well", "as", "three", "types", "of", "gates", "that", "control", "the", "flow", "of", "information", "into", "and", "out", "of", "these", "cells", ":", "input", "gates", "(", "Eq", ".", "2", ")", ",", "forget", "gates", "(", "Eq", ".", "3", ")", "and", "output", "gates", "(", "Eq", ".", "4", ")", ".", "Given", "an", "input", "vector", "x", "t", "at", "time", "step", "t", ",", "the", "previous", "output", "h", "t\u22121", "and", "cell", "state", "c", "t\u22121", ",", "an", "LSTM", "with", "hidden", "size", "k", "computes", "the", "next", "output", "h", "t", "and", "cell", "state", "c", "t", "as", "where", "k", "trained", "biases", "that", "parameterize", "the", "gates", "and", "transformations", "of", "the", "input", ",", "\u03c3", "denotes", "the", "element", "-", "wise", "application", "of", "the", "sigmoid", "function", "and", "the", "element", "-", "wise", "multiplication", "of", "two", "vectors", ".", "section", ":", "RECOGNIZING", "TEXTUAL", "ENTAILMENT", "WITH", "LSTMS", "LSTMs", "can", "readily", "be", "used", "for", "RTE", "by", "independently", "encoding", "the", "premise", "and", "hypothesis", "as", "dense", "vectors", "and", "taking", "their", "concatenation", "as", "input", "to", "an", "MLP", "classifier", "[", "reference", "]", ".", "This", "demonstrates", "that", "LSTMs", "can", "learn", "semantically", "rich", "sentence", "representations", "that", "are", "suitable", "for", "determining", "textual", "entailment", ".", "section", ":", "CONDITIONAL", "ENCODING", "In", "contrast", "to", "learning", "sentence", "representations", ",", "we", "are", "interested", "in", "neural", "models", "that", "read", "both", "sentences", "to", "determine", "entailment", ",", "thereby", "reasoning", "over", "entailments", "of", "pairs", "of", "words", "and", "phrases", ".", "Figure", "1", "shows", "the", "high", "-", "level", "structure", "of", "this", "model", ".", "The", "premise", "(", "left", ")", "is", "read", "by", "an", "LSTM", ".", "A", "second", "LSTM", "with", "different", "parameters", "is", "reading", "a", "delimiter", "and", "the", "hypothesis", "(", "right", ")", ",", "but", "its", "memory", "state", "is", "initialized", "with", "the", "last", "cell", "state", "of", "the", "previous", "LSTM", "(", "c", "5", "in", "the", "example", ")", ",", "i.e.", "it", "is", "conditioned", "on", "the", "representation", "that", "the", "first", "LSTM", "built", "for", "the", "premise", "(", "A", ")", ".", "We", "use", "word2vec", "vectors", "[", "reference", "]", "as", "word", "representations", ",", "which", "we", "do", "not", "optimize", "during", "training", ".", "Out", "-", "ofvocabulary", "words", "in", "the", "training", "set", "are", "randomly", "initialized", "by", "sampling", "values", "uniformly", "from", "(", "\u22120.05", ",", "0.05", ")", "and", "optimized", "during", "training", ".", "1", "Out", "-", "of", "-", "vocabulary", "words", "encountered", "at", "inference", "time", "on", "the", "validation", "and", "test", "corpus", "are", "set", "to", "fixed", "random", "vectors", ".", "By", "not", "tuning", "representations", "of", "words", "for", "which", "we", "have", "word2vec", "vectors", ",", "we", "ensure", "that", "at", "inference", "time", "their", "representation", "stays", "close", "to", "unseen", "similar", "words", "for", "which", "we", "have", "word2vec", "embeddings", ".", "We", "use", "a", "linear", "layer", "to", "project", "word", "vectors", "to", "the", "dimensionality", "of", "the", "hidden", "size", "of", "the", "LSTM", ",", "yielding", "input", "vectors", "x", "i", ".", "Finally", ",", "for", "classification", "we", "use", "a", "softmax", "layer", "over", "the", "output", "of", "a", "non", "-", "linear", "projection", "of", "the", "last", "output", "vector", "(", "h", "9", "in", "the", "example", ")", "into", "the", "target", "space", "of", "the", "three", "classes", "(", "ENTAILMENT", ",", "NEUTRAL", "or", "CONTRADICTION", ")", ",", "and", "train", "using", "the", "cross", "-", "entropy", "loss", ".", "section", ":", "ATTENTION", "Attentive", "neural", "networks", "have", "recently", "demonstrated", "success", "in", "a", "wide", "range", "of", "tasks", "ranging", "from", "handwriting", "synthesis", "[", "reference", "]", ",", "digit", "classification", "[", "reference", "]", ",", "machine", "translation", ",", "image", "captioning", "[", "reference", "]", ",", "speech", "recognition", "[", "reference", "]", "and", "sentence", "summarization", "[", "reference", "]", ",", "to", "geometric", "reasoning", "[", "reference", "]", ".", "The", "idea", "is", "to", "allow", "the", "model", "to", "attend", "over", "past", "output", "vectors", "(", "see", "Figure", "1", "B", ")", ",", "thereby", "mitigating", "the", "LSTM", "'s", "cell", "state", "bottleneck", ".", "More", "precisely", ",", "an", "LSTM", "with", "attention", "for", "RTE", "does", "not", "need", "to", "capture", "the", "whole", "semantics", "of", "the", "premise", "in", "its", "cell", "state", ".", "Instead", ",", "it", "is", "sufficient", "to", "output", "vectors", "while", "reading", "the", "premise", "and", "accumulating", "a", "representation", "in", "the", "cell", "state", "that", "informs", "the", "second", "LSTM", "which", "of", "the", "output", "vectors", "of", "the", "premise", "it", "needs", "to", "attend", "over", "to", "determine", "the", "RTE", "class", ".", "Let", "Y", "\u2208", "R", "k\u00d7L", "be", "a", "matrix", "consisting", "of", "output", "vectors", "[", "h", "1", "\u00b7", "\u00b7", "\u00b7", "h", "L", "]", "that", "the", "first", "LSTM", "produced", "when", "reading", "the", "L", "words", "of", "the", "premise", ",", "where", "k", "is", "a", "hyperparameter", "denoting", "the", "size", "of", "embeddings", "and", "hidden", "layers", ".", "Furthermore", ",", "let", "e", "L", "\u2208", "R", "L", "be", "a", "vector", "of", "1s", "and", "h", "N", "be", "the", "last", "output", "vector", "after", "the", "premise", "and", "hypothesis", "were", "processed", "by", "the", "two", "LSTMs", "respectively", ".", "The", "attention", "mechanism", "will", "produce", "a", "vector", "\u03b1", "of", "attention", "weights", "and", "a", "weighted", "representation", "r", "of", "the", "premise", "via", "where", "W", "y", ",", "W", "h", "\u2208", "R", "k\u00d7k", "are", "trained", "projection", "matrices", ",", "w", "\u2208", "R", "k", "is", "a", "trained", "parameter", "vector", "and", "w", "T", "denotes", "its", "transpose", ".", "Note", "that", "the", "outer", "product", "W", "h", "h", "N", "\u2297", "e", "L", "is", "repeating", "the", "linearly", "transformed", "h", "N", "as", "many", "times", "as", "there", "are", "words", "in", "the", "premise", "(", "i.e.", "L", "times", ")", ".", "Hence", ",", "the", "intermediate", "attention", "representation", "m", "i", "(", "ith", "column", "vector", "in", "M", ")", "of", "the", "ith", "word", "in", "the", "premise", "is", "obtained", "from", "a", "non", "-", "linear", "combination", "of", "the", "premise", "'s", "output", "vector", "h", "i", "(", "ith", "column", "vector", "in", "Y", ")", "and", "the", "transformed", "h", "N", ".", "The", "attention", "weight", "for", "the", "ith", "word", "in", "the", "premise", "is", "the", "result", "of", "a", "weighted", "combination", "(", "parameterized", "by", "w", ")", "of", "values", "in", "m", "i", ".", "The", "final", "sentence", "-", "pair", "representation", "is", "obtained", "from", "a", "non", "-", "linear", "combination", "of", "the", "attentionweighted", "representation", "r", "of", "the", "premise", "and", "the", "last", "output", "vector", "h", "N", "using", "where", "W", "p", ",", "W", "x", "\u2208", "R", "k\u00d7k", "are", "trained", "projection", "matrices", ".", "section", ":", "WORD", "-", "BY", "-", "WORD", "ATTENTION", "For", "determining", "whether", "one", "sentence", "entails", "another", "it", "can", "be", "a", "good", "strategy", "to", "check", "for", "entailment", "or", "contradiction", "of", "individual", "word", "-", "and", "phrase", "-", "pairs", ".", "To", "encourage", "such", "behavior", "we", "employ", "neural", "word", "-", "by", "-", "word", "attention", "similar", "to", ",", "[", "reference", "]", "and", "[", "reference", "]", ".", "The", "difference", "is", "that", "we", "do", "not", "use", "attention", "to", "generate", "words", ",", "but", "to", "obtain", "a", "sentence", "-", "pair", "encoding", "from", "fine", "-", "grained", "reasoning", "via", "soft", "-", "alignment", "of", "words", "and", "phrases", "in", "the", "premise", "and", "hypothesis", ".", "In", "our", "case", ",", "this", "amounts", "to", "attending", "over", "the", "first", "LSTM", "'s", "output", "vectors", "of", "the", "premise", "while", "the", "second", "LSTM", "processes", "the", "hypothesis", "one", "word", "at", "a", "time", ",", "thus", "generating", "attention", "weight", "-", "vectors", "\u03b1", "t", "over", "all", "output", "vectors", "of", "the", "premise", "for", "every", "word", "x", "t", "with", "t", "\u2208", "(", "L", "+", "1", ",", "N", ")", "in", "the", "hypothesis", "(", "Figure", "1", "C", ")", ".", "This", "can", "be", "modeled", "as", "follows", ":", "Note", "that", "r", "t", "is", "dependent", "on", "the", "previous", "attention", "representation", "r", "t\u22121", "to", "inform", "the", "model", "about", "what", "was", "attended", "over", "in", "the", "previous", "step", "(", "see", "Eq", ".", "11", "and", "13", ")", ".", "As", "in", "the", "previous", "section", ",", "the", "final", "sentence", "-", "pair", "representation", "is", "obtained", "from", "a", "non", "-", "linear", "combination", "of", "the", "last", "attention", "-", "weighted", "representation", "of", "the", "premise", "(", "here", "based", "on", "the", "last", "word", "of", "the", "hypothesis", ")", "r", "N", "and", "the", "last", "output", "vector", "using", "2.5", "TWO", "-", "WAY", "ATTENTION", "Inspired", "by", "bidirectional", "LSTMs", "that", "read", "a", "sequence", "and", "its", "reverse", "for", "improved", "encoding", "[", "reference", "]", ",", "we", "introduce", "two", "-", "way", "attention", "for", "RTE", ".", "The", "idea", "is", "to", "use", "the", "same", "model", "(", "i.e.", "same", "structure", "and", "weights", ")", "to", "attend", "over", "the", "premise", "conditioned", "on", "the", "hypothesis", ",", "as", "well", "as", "to", "attend", "over", "the", "hypothesis", "conditioned", "on", "the", "premise", ",", "by", "simply", "swapping", "the", "two", "sequences", ".", "This", "produces", "two", "sentence", "-", "pair", "representations", "that", "we", "concatenate", "for", "classification", ".", "section", ":", "EXPERIMENTS", "We", "conduct", "experiments", "on", "the", "Stanford", "Natural", "Language", "Inference", "corpus", "[", "reference", "]", ".", "This", "corpus", "is", "two", "orders", "of", "magnitude", "larger", "than", "other", "existing", "RTE", "corpora", "such", "as", "Sentences", "Involving", "Compositional", "Knowledge", "(", "SICK", ",", "[", "reference", "]", ".", "Furthermore", ",", "a", "large", "part", "of", "training", "examples", "in", "SICK", "were", "generated", "heuristically", "from", "other", "examples", ".", "In", "contrast", ",", "all", "sentence", "-", "pairs", "in", "SNLI", "stem", "from", "human", "annotators", ".", "The", "size", "and", "quality", "of", "SNLI", "make", "it", "a", "suitable", "resource", "for", "training", "neural", "architectures", "such", "as", "the", "ones", "proposed", "in", "this", "paper", ".", "We", "use", "ADAM", "[", "reference", "]", "for", "optimization", "with", "a", "first", "momentum", "coefficient", "of", "0.9", "and", "a", "second", "momentum", "coefficient", "of", "0.999", ".", "2", "For", "every", "model", "we", "perform", "a", "small", "grid", "search", "[", "reference", "]", ".", "Subsequently", ",", "we", "take", "the", "best", "configuration", "based", "on", "performance", "on", "the", "validation", "set", ",", "and", "evaluate", "only", "that", "configuration", "on", "the", "test", "set", ".", "section", ":", "RESULTS", "AND", "DISCUSSION", "Results", "on", "the", "SNLI", "corpus", "are", "summarized", "in", "Table", "1", ".", "The", "total", "number", "of", "model", "parameters", ",", "including", "tunable", "word", "representations", ",", "is", "denoted", "by", "|\u03b8|", "W", "+", "M", "(", "without", "word", "representations", "|\u03b8|", "M", ")", ".", "To", "ensure", "a", "comparable", "number", "of", "parameters", "to", "Bowman", "et", "al", ".", "'s", "model", "that", "encodes", "the", "premise", "and", "hypothesis", "independently", "using", "one", "LSTM", ",", "we", "also", "run", "experiments", "for", "conditional", "encoding", "where", "the", "parameters", "of", "both", "LSTMs", "are", "shared", "(", "\"", "Conditional", "encoding", ",", "shared", "\"", "with", "k", "=", "100", ")", "as", "opposed", "to", "using", "two", "independent", "LSTMs", ".", "In", "addition", ",", "we", "compare", "our", "attentive", "models", "to", "two", "benchmark", "LSTMs", "whose", "hidden", "sizes", "were", "chosen", "so", "that", "they", "have", "at", "least", "as", "many", "parameters", "as", "the", "attentive", "models", ".", "Since", "we", "are", "not", "tuning", "word", "vectors", "for", "which", "we", "have", "word2vec", "embeddings", ",", "the", "total", "number", "of", "parameters", "|\u03b8|", "W", "+", "M", "of", "our", "models", "is", "considerably", "smaller", ".", "We", "also", "compare", "our", "models", "against", "the", "benchmark", "lexicalized", "classifier", "used", "by", "Bowman", "et", "al", ".", ",", "which", "constructs", "features", "from", "the", "BLEU", "score", "between", "the", "premise", "and", "hypothesis", ",", "length", "difference", ",", "word", "overlap", ",", "uni", "-", "and", "bigrams", ",", "part", "-", "of", "-", "speech", "tags", ",", "as", "well", "as", "cross", "uni", "-", "and", "bigrams", ".", "Conditional", "Encoding", "We", "found", "that", "processing", "the", "hypothesis", "conditioned", "on", "the", "premise", "instead", "of", "encoding", "each", "sentence", "independently", "gives", "an", "improvement", "of", "3.3", "percentage", "points", "in", "accuracy", "over", "Bowman", "et", "al", ".", "'s", "LSTM", ".", "We", "argue", "this", "is", "due", "to", "information", "being", "able", "to", "flow", "from", "the", "part", "of", "the", "model", "that", "processes", "the", "premise", "to", "the", "part", "that", "processes", "the", "hypothesis", ".", "Specifically", ",", "the", "model", "does", "not", "waste", "capacity", "on", "encoding", "the", "hypothesis", "(", "in", "fact", "it", "does", "not", "need", "to", "encode", "the", "hypothesis", "at", "all", ")", ",", "but", "can", "read", "the", "hypothesis", "in", "a", "more", "focused", "way", "by", "checking", "words", "and", "phrases", "for", "contradictions", "and", "entailments", "based", "on", "the", "semantic", "representation", "of", "the", "premise", ".", "One", "interpretation", "is", "that", "the", "LSTM", "is", "approximating", "a", "finite", "-", "state", "automaton", "for", "RTE", "(", "cf", ".", "[", "reference", "]", ".", "Another", "difference", "to", "Bowman", "et", "al", ".", "'s", "model", "is", "that", "we", "are", "using", "word2vec", "instead", "of", "GloVe", "for", "word", "representations", "and", ",", "more", "importantly", ",", "do", "not", "fine", "-", "tune", "these", "word", "embeddings", ".", "The", "drop", "in", "accuracy", "from", "train", "to", "test", "set", "is", "less", "severe", "for", "our", "models", ",", "which", "suggest", "that", "fine", "-", "tuning", "word", "embeddings", "could", "be", "a", "cause", "of", "overfitting", ".", "Our", "LSTM", "outperforms", "a", "simple", "lexicalized", "classifier", "by", "2.7", "percentage", "points", ".", "To", "the", "best", "of", "our", "knowledge", ",", "this", "is", "the", "first", "instance", "of", "a", "neural", "end", "-", "to", "-", "end", "differentiable", "model", "to", "achieve", "state", "-", "ofthe", "-", "art", "performance", "on", "a", "textual", "entailment", "dataset", ".", "Attention", "By", "incorporating", "an", "attention", "mechanism", "we", "found", "a", "0.9", "percentage", "point", "improvement", "over", "a", "single", "LSTM", "with", "a", "hidden", "size", "of", "159", ",", "and", "a", "1.4", "percentage", "point", "increase", "over", "a", "benchmark", "model", "that", "uses", "two", "LSTMs", "for", "conditional", "encoding", "(", "one", "for", "the", "premise", "and", "one", "for", "the", "hypothesis", "conditioned", "on", "the", "representation", "of", "the", "premise", ")", ".", "The", "attention", "model", "produces", "output", "vectors", "summarizing", "contextual", "information", "of", "the", "premise", "that", "is", "useful", "to", "attend", "over", "later", "when", "reading", "the", "hypothesis", ".", "Therefore", ",", "when", "reading", "the", "premise", ",", "the", "model", "does", "not", "have", "to", "build", "up", "a", "semantic", "representation", "of", "the", "whole", "premise", ",", "but", "instead", "a", "representation", "that", "helps", "attending", "over", "the", "right", "output", "vectors", "when", "processing", "the", "hypothesis", ".", "In", "contrast", ",", "the", "output", "vectors", "of", "the", "premise", "are", "not", "used", "by", "the", "benchmark", "LSTMs", ".", "Thus", ",", "these", "models", "have", "to", "build", "up", "a", "representation", "of", "the", "whole", "premise", "and", "carry", "it", "over", "through", "the", "cell", "state", "to", "the", "part", "that", "processes", "the", "hypothesis", "-", "a", "bottleneck", "that", "can", "be", "overcome", "to", "some", "degree", "by", "using", "attention", ".", "Word", "-", "by", "-", "word", "Attention", "Enabling", "the", "model", "to", "attend", "over", "output", "vectors", "of", "the", "premise", "for", "every", "word", "in", "the", "hypothesis", "yields", "another", "1.2", "percentage", "point", "improvement", "compared", "to", "attending", "based", "only", "on", "the", "last", "output", "vector", "of", "the", "premise", ".", "We", "argue", "that", "this", "is", "due", "to", "the", "model", "being", "able", "to", "check", "for", "entailment", "or", "contradiction", "of", "individual", "words", "and", "phrases", "in", "the", "hypothesis", ",", "and", "demonstrate", "this", "effect", "in", "the", "qualitative", "analysis", "below", ".", "section", ":", "Two", "-", "way", "Attention", "Allowing", "the", "model", "to", "also", "attend", "over", "the", "hypothesis", "based", "on", "the", "premise", "does", "not", "seem", "to", "improve", "performance", "for", "RTE", ".", "We", "suspect", "that", "this", "is", "due", "to", "entailment", "being", "an", "asymmetric", "relation", ".", "Hence", ",", "using", "the", "same", "LSTM", "to", "encode", "the", "hypothesis", "(", "in", "one", "direction", ")", "and", "the", "premise", "(", "in", "the", "other", "direction", ")", "might", "lead", "to", "noise", "in", "the", "training", "signal", ".", "This", "could", "be", "addressed", "by", "training", "different", "LSTMs", "at", "the", "cost", "of", "doubling", "the", "number", "of", "model", "parameters", ".", "section", ":", "QUALITATIVE", "ANALYSIS", "It", "is", "instructive", "to", "analyze", "which", "output", "representations", "the", "model", "is", "attending", "over", "when", "deciding", "the", "class", "of", "an", "RTE", "example", ".", "Note", "that", "interpretations", "based", "on", "attention", "weights", "have", "to", "be", "taken", "with", "care", "since", "the", "model", "is", "not", "forced", "to", "solely", "rely", "on", "representations", "obtained", "from", "attention", "(", "see", "h", "N", "in", "Eq", ".", "10", "and", "14", ")", ".", "In", "the", "following", "we", "visualize", "and", "discuss", "the", "attention", "patterns", "of", "the", "presented", "attentive", "models", ".", "For", "each", "attentive", "model", "we", "hand", "-", "picked", "examples", "from", "ten", "randomly", "drawn", "samples", "of", "the", "validation", "set", ".", "Attention", "Figure", "2", "shows", "to", "what", "extent", "the", "attentive", "model", "focuses", "on", "contextual", "representations", "of", "the", "premise", "after", "both", "LSTMs", "processed", "the", "premise", "and", "hypothesis", "respectively", ".", "Note", "how", "the", "model", "pays", "attention", "to", "output", "vectors", "of", "words", "that", "are", "semantically", "coherent", "with", "the", "premise", "[", "reference", "]", "or", "in", "contradiction", ",", "as", "caused", "by", "a", "single", "word", "(", "\"", "blue", "\"", "vs.", "\"", "pink", "\"", ",", "2b", ")", "or", "multiple", "words", "[", "reference", "]", ".", "Interestingly", ",", "the", "model", "shows", "contextual", "understanding", "by", "not", "attending", "over", "\"", "yellow", "\"", ",", "the", "color", "(", "g", ")", "Figure", "3", ":", "Word", "-", "by", "-", "word", "attention", "visualizations", ".", "of", "the", "toy", ",", "but", "\"", "pink", "\"", ",", "the", "color", "of", "the", "coat", ".", "However", ",", "for", "more", "involved", "examples", "with", "longer", "premises", "we", "found", "that", "attention", "is", "more", "uniformly", "distributed", "(", "2d", ")", ".", "This", "suggests", "that", "conditioning", "attention", "only", "on", "the", "last", "output", "has", "limitations", "when", "multiple", "words", "need", "to", "be", "considered", "for", "deciding", "the", "RTE", "class", ".", "Word", "-", "by", "-", "word", "Attention", "Visualizations", "of", "word", "-", "by", "-", "word", "attention", "are", "depicted", "in", "Figure", "3", ".", "We", "found", "that", "word", "-", "by", "-", "word", "attention", "can", "easily", "detect", "if", "the", "hypothesis", "is", "simply", "a", "reordering", "of", "words", "in", "the", "premise", "(", "3a", ")", ".", "Furthermore", ",", "it", "is", "able", "to", "resolve", "synonyms", "(", "\"", "airplane", "\"", "and", "\"", "aircraft", "\"", ",", "3c", ")", "and", "capable", "of", "matching", "multi", "-", "word", "expressions", "to", "single", "words", "(", "\"", "garbage", "can", "\"", "to", "\"", "trashcan", "\"", ",", "3b", ")", ".", "It", "is", "also", "noteworthy", "that", "irrelevant", "parts", "of", "the", "premise", ",", "such", "as", "words", "capturing", "little", "meaning", "or", "whole", "uninformative", "relative", "clauses", ",", "are", "correctly", "neglected", "for", "determining", "entailment", "(", "\"", "which", "also", "has", "a", "rope", "leading", "out", "of", "it", "\"", ",", "3b", ")", ".", "Word", "-", "by", "-", "word", "attention", "seems", "to", "also", "work", "well", "when", "words", "in", "the", "premise", "and", "hypothesis", "are", "connected", "via", "deeper", "semantics", "or", "common", "-", "sense", "knowledge", "(", "\"", "snow", "\"", "can", "be", "found", "\"", "outside", "\"", "and", "a", "\"", "mother", "\"", "is", "an", "\"", "adult", "\"", ",", "3e", "and", "3", "g", ")", ".", "Furthermore", ",", "the", "model", "is", "able", "to", "resolve", "one", "-", "to", "-", "many", "relationships", "(", "\"", "kids", "\"", "to", "\"", "boy", "\"", "and", "\"", "girl", "\"", ",", "3d", ")", "Attention", "can", "fail", ",", "for", "example", "when", "the", "two", "sentences", "and", "their", "words", "are", "entirely", "unrelated", "(", "3f", ")", ".", "In", "such", "cases", ",", "the", "model", "seems", "to", "back", "up", "to", "attending", "over", "function", "words", ",", "and", "the", "sentence", "-", "pair", "representation", "is", "likely", "dominated", "by", "the", "last", "output", "vector", "(", "h", "N", ")", "instead", "of", "the", "attention", "-", "weighted", "representation", "(", "see", "Eq", ".", "14", ")", ".", "section", ":", "CONCLUSION", "In", "this", "paper", ",", "we", "show", "how", "the", "state", "-", "of", "-", "the", "-", "art", "in", "recognizing", "textual", "entailment", "on", "a", "large", ",", "humancurated", "and", "annotated", "corpus", ",", "can", "be", "improved", "with", "general", "end", "-", "to", "-", "end", "differentiable", "models", ".", "Our", "results", "demonstrate", "that", "LSTM", "recurrent", "neural", "networks", "that", "read", "pairs", "of", "sequences", "to", "produce", "a", "final", "representation", "from", "which", "a", "simple", "classifier", "predicts", "entailment", ",", "outperform", "both", "a", "neural", "baseline", "as", "well", "as", "a", "classifier", "with", "hand", "-", "engineered", "features", ".", "Extending", "these", "models", "with", "attention", "over", "the", "premise", "provides", "further", "improvements", "to", "the", "predictive", "abilities", "of", "the", "system", ",", "resulting", "in", "a", "new", "state", "-", "of", "-", "the", "-", "art", "accuracy", "for", "recognizing", "entailment", "on", "the", "Stanford", "Natural", "Language", "Inference", "corpus", ".", "The", "models", "presented", "here", "are", "general", "sequence", "models", ",", "requiring", "no", "appeal", "to", "Natural", "Languagespecific", "processing", "beyond", "tokenization", ",", "and", "are", "therefore", "a", "suitable", "target", "for", "transfer", "learning", "through", "pre", "-", "training", "the", "recurrent", "systems", "on", "other", "corpora", ",", "and", "conversely", ",", "applying", "the", "models", "trained", "on", "this", "corpus", "to", "other", "entailment", "tasks", ".", "Future", "work", "will", "focus", "on", "such", "transfer", "learning", "tasks", ",", "as", "well", "as", "scaling", "the", "methods", "presented", "here", "to", "larger", "units", "of", "text", "(", "e.g.", "paragraphs", "and", "entire", "documents", ")", "using", "hierarchical", "attention", "mechanisms", ".", "Additionally", ",", "it", "would", "be", "worthwhile", "exploring", "how", "other", ",", "more", "structured", "forms", "of", "attention", "(", "e.g.", "[", "reference", "]", ",", "or", "other", "forms", "of", "differentiable", "memory", "(", "e.g.", "[", "reference", "][", "reference", "]", "could", "help", "improve", "performance", "on", "RTE", "over", "the", "neural", "models", "presented", "in", "this", "paper", ".", "Furthermore", ",", "we", "aim", "to", "investigate", "the", "application", "of", "these", "generic", "models", "to", "non", "-", "natural", "language", "sequential", "entailment", "problems", ".", "section", ":", "section", ":", "ACKNOWLEDGEMENTS", "We", "thank", "Nando", "de", "Freitas", ",", "Samuel", "Bowman", ",", "Jonathan", "Berant", ",", "Danqi", "Chen", ",", "Christopher", "Manning", ",", "and", "the", "anonymous", "ICLR", "reviewers", "for", "their", "helpful", "comments", "on", "drafts", "of", "this", "paper", ".", "section", ":"]}