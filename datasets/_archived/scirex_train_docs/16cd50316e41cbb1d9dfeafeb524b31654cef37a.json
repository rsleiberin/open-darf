{"coref": {"CNN-LSTM": [[155, 161], [233, 234], [845, 846], [972, 989], [1595, 1600], [1602, 1603], [1947, 1948], [1966, 1967], [1989, 1990], [2372, 2373], [2400, 2401], [3215, 3217], [3749, 3751], [3777, 3779], [3816, 3818], [3995, 3998], [4031, 4034], [4057, 4058], [4264, 4268], [4327, 4334], [4457, 4460], [5664, 5665], [6060, 6061], [6775, 6776], [6921, 6924], [7072, 7073], [7083, 7084]], "Microsoft_2016b": [[893, 896], [4912, 4915], [4929, 4932]], "Percentage_error": [[38, 41], [63, 65], [132, 134], [494, 497], [558, 561], [584, 586], [1199, 1201], [1349, 1351], [1422, 1424], [1537, 1540], [2007, 2010], [2245, 2248], [2261, 2263], [2272, 2274], [3543, 3545], [4342, 4345], [4445, 4448], [4696, 4697], [5838, 5840], [5947, 5950], [6064, 6065], [6164, 6166], [6214, 6217], [6224, 6226], [6271, 6273], [6533, 6535], [6697, 6699], [7001, 7004], [7037, 7039]], "Speech_Recognition": [[6, 9], [9, 12], [16, 20], [215, 218], [263, 266], [285, 287], [336, 339], [670, 672], [704, 707], [1624, 1626], [3714, 3717], [6777, 6779], [6990, 6993], [7005, 7013], [7048, 7050]], "Switchboard___Hub500": [[25, 27], [73, 75], [542, 546], [1412, 1415], [1542, 1544], [4153, 4154], [4160, 4165], [4271, 4275], [5515, 5520], [5536, 5539], [5737, 5741], [6550, 6552], [7026, 7030]]}, "coref_non_salient": {"0": [[838, 841], [2074, 2076], [2328, 2330]], "1": [[3055, 3057], [4482, 4484], [4848, 4850], [6094, 6096]], "10": [[2036, 2038], [2993, 2995], [5139, 5141]], "100": [[5033, 5035]], "101": [[2617, 2620]], "102": [[4103, 4105]], "103": [[5402, 5404], [5413, 5415], [5469, 5471], [5476, 5478]], "104": [[1905, 1907], [6016, 6020]], "105": [[1686, 1688], [3946, 3949]], "106": [[3244, 3248], [7155, 7159]], "107": [[3250, 3254], [3523, 3527]], "108": [[2287, 2290], [2290, 2293]], "109": [[3875, 3879]], "11": [[227, 228], [1909, 1911]], "110": [[4133, 4138]], "111": [[3561, 3563]], "112": [[2713, 2715]], "113": [[4555, 4557]], "114": [[4485, 4487]], "115": [[5122, 5123]], "116": [[5309, 5314]], "117": [[5018, 5019]], "118": [[4051, 4055]], "119": [[5595, 5596]], "12": [[848, 851], [3235, 3238], [3337, 3340], [3355, 3358], [3611, 3614], [3688, 3691], [3710, 3713], [3727, 3731], [4076, 4079], [4449, 4452]], "120": [[5487, 5489]], "121": [[3579, 3583]], "122": [[5755, 5758]], "123": [[5987, 5991]], "124": [[3865, 3869]], "125": [[2150, 2152]], "126": [[5901, 5903]], "127": [[5783, 5785], [5821, 5823]], "128": [[229, 230], [1878, 1879]], "129": [[5491, 5495]], "13": [[3268, 3274], [3275, 3279]], "130": [[998, 1002], [5778, 5782]], "131": [[6683, 6687]], "132": [[1647, 1648]], "133": [[1984, 1987]], "134": [[1866, 1870]], "135": [[792, 793]], "136": [[3168, 3172], [3360, 3364], [4036, 4040], [4397, 4401]], "137": [[1361, 1364]], "138": [[2304, 2308]], "139": [[2154, 2158]], "14": [[1997, 1998], [6857, 6859]], "140": [[4547, 4549]], "141": [[3196, 3201]], "142": [[5888, 5890]], "143": [[2586, 2590]], "144": [[2888, 2891]], "145": [[5849, 5851]], "146": [[5407, 5409]], "147": [[771, 774]], "148": [[66, 68]], "149": [[795, 798]], "15": [[1714, 1716], [1794, 1796]], "150": [[1927, 1930]], "151": [[3064, 3066]], "152": [[4218, 4224]], "153": [[1723, 1728]], "154": [[3324, 3328], [3768, 3772]], "155": [[2083, 2085]], "156": [[1644, 1646], [1842, 1844], [6792, 6794]], "157": [[2908, 2909]], "158": [[5332, 5334]], "159": [[3784, 3787]], "16": [[128, 130], [817, 818], [963, 965]], "160": [[3904, 3907]], "161": [[2107, 2111]], "162": [[4853, 4855]], "163": [[3603, 3608]], "164": [[5760, 5762]], "165": [[3695, 3699]], "166": [[2450, 2452]], "167": [[322, 324]], "168": [[4951, 4953]], "169": [[3842, 3845]], "17": [[2232, 2234], [3440, 3442], [3577, 3578], [4320, 4322]], "170": [[3403, 3407]], "171": [[2592, 2595]], "172": [[3627, 3628]], "173": [[3462, 3465]], "174": [[4347, 4352]], "175": [[2979, 2984]], "176": [[3158, 3160]], "177": [[4702, 4703]], "178": [[4991, 4993]], "179": [[4511, 4513], [6196, 6199]], "18": [[1893, 1895], [5649, 5651], [5806, 5808], [6955, 6957]], "180": [[4630, 4633]], "181": [[1976, 1978]], "182": [[3009, 3013]], "183": [[1442, 1444]], "184": [[1979, 1982]], "185": [[1068, 1070]], "186": [[5022, 5023]], "187": [[779, 781]], "188": [[6349, 6353]], "189": [[3090, 3094]], "19": [[2725, 2730], [2783, 2786], [2893, 2895]], "190": [[3505, 3507]], "191": [[3574, 3576]], "192": [[4774, 4777]], "193": [[3176, 3179]], "194": [[5159, 5165]], "195": [[273, 278]], "196": [[5999, 6003]], "197": [[1016, 1019]], "198": [[2746, 2756]], "199": [[2989, 2992]], "2": [[2196, 2199], [2522, 2524]], "20": [[368, 371], [3397, 3400]], "200": [[3086, 3088]], "201": [[3799, 3801]], "202": [[4211, 4215]], "203": [[2488, 2492]], "204": [[2535, 2536]], "205": [[595, 596]], "206": [[3030, 3033]], "207": [[2629, 2630]], "208": [[3301, 3306]], "209": [[787, 789]], "21": [[347, 349], [4977, 4981]], "210": [[4090, 4092]], "211": [[6964, 6968]], "212": [[1242, 1244]], "213": [[6958, 6962]], "214": [[1606, 1608], [1695, 1697]], "215": [[1656, 1658]], "216": [[6490, 6492]], "217": [[3584, 3585]], "218": [[2688, 2696]], "219": [[2641, 2648]], "22": [[219, 222], [634, 641]], "220": [[178, 184]], "221": [[744, 746]], "222": [[1750, 1752]], "223": [[3148, 3150]], "224": [[3230, 3234]], "225": [[1376, 1379]], "226": [[2217, 2221]], "227": [[3, 5]], "228": [[3549, 3558]], "229": [[2985, 2988]], "23": [[5504, 5506], [5604, 5606]], "230": [[1406, 1409]], "231": [[692, 696]], "232": [[4716, 4718]], "233": [[3883, 3884]], "234": [[4227, 4229]], "235": [[4258, 4260]], "236": [[3152, 3156]], "237": [[1959, 1965]], "238": [[857, 862]], "239": [[3753, 3760]], "24": [[4365, 4367], [6126, 6129]], "240": [[4149, 4152]], "241": [[3897, 3898]], "242": [[3212, 3214]], "243": [[223, 226]], "244": [[2972, 2976]], "245": [[768, 770]], "246": [[874, 878]], "247": [[2500, 2508]], "248": [[4941, 4946]], "249": [[355, 357]], "25": [[415, 418], [5721, 5722]], "250": [[3052, 3054]], "251": [[1126, 1128]], "252": [[312, 313]], "253": [[2941, 2945]], "254": [[5025, 5030]], "255": [[1395, 1400]], "256": [[5069, 5073]], "257": [[3076, 3081]], "258": [[4600, 4602]], "259": [[1613, 1615]], "26": [[681, 683], [7075, 7079], [7097, 7101]], "260": [[4244, 4249]], "261": [[256, 259]], "262": [[1900, 1902]], "263": [[5554, 5562]], "27": [[731, 733], [1954, 1955], [2347, 2349]], "28": [[6718, 6720]], "29": [[3428, 3431], [4302, 4305]], "3": [[1418, 1420], [1531, 1533], [4175, 4177], [5541, 5544], [5575, 5577]], "30": [[3411, 3414], [3444, 3448], [3454, 3459], [3484, 3488], [3856, 3860], [4276, 4280]], "31": [[436, 438], [4183, 4189]], "32": [[6136, 6137], [6153, 6155]], "33": [[3129, 3130], [5411, 5412], [5743, 5744]], "34": [[831, 834], [6843, 6846]], "35": [[4593, 4595], [4690, 4692]], "36": [[3828, 3836], [4066, 4072]], "37": [[684, 686], [741, 743], [3700, 3702], [6909, 6912]], "38": [[605, 609], [1249, 1252], [6981, 6983]], "39": [[6258, 6260]], "4": [[1470, 1473], [2717, 2719], [3386, 3389], [5598, 5601], [6453, 6456]], "40": [[4807, 4809], [4866, 4868], [5915, 5917], [6040, 6042]], "41": [[3671, 3676], [4515, 4520]], "42": [[166, 169], [235, 237], [2045, 2047], [2065, 2068], [2282, 2284], [4826, 4828]], "43": [[5074, 5078], [5109, 5115], [5147, 5152]], "44": [[1334, 1337], [6415, 6417]], "45": [[5713, 5714], [5715, 5718]], "46": [[907, 911], [1157, 1159], [6245, 6247]], "47": [[5257, 5259], [5399, 5401]], "48": [[5088, 5089], [5459, 5460], [5473, 5475]], "49": [[3059, 3060], [4139, 4140]], "5": [[709, 711], [2092, 2094], [2250, 2252], [4360, 4362], [4475, 4477], [4891, 4893], [5080, 5082], [5356, 5359], [6010, 6012]], "50": [[6101, 6103]], "51": [[1228, 1235], [1434, 1436]], "52": [[190, 192], [865, 868], [5904, 5907]], "53": [[5852, 5853], [5868, 5869]], "54": [[5117, 5121]], "55": [[4042, 4046]], "56": [[5350, 5352]], "57": [[5384, 5387]], "58": [[6107, 6114]], "59": [[2078, 2079], [6048, 6049]], "6": [[1106, 1108], [1933, 1935], [4310, 4312]], "60": [[751, 755], [4009, 4013], [4196, 4200]], "61": [[3473, 3479], [6945, 6951]], "62": [[231, 232], [1699, 1710], [1711, 1712], [1788, 1790], [6814, 6816]], "63": [[3982, 3984], [4060, 4062]], "64": [[2626, 2628], [2878, 2880], [2913, 2915], [4493, 4495], [5381, 5383], [5624, 5626], [7086, 7088]], "65": [[6852, 6855]], "66": [[5800, 5803]], "67": [[3810, 3813], [6937, 6940]], "68": [[4109, 4114]], "69": [[1046, 1049], [4919, 4921], [4998, 5000]], "7": [[897, 898], [1038, 1040], [4916, 4917], [4934, 4935], [4947, 4948], [4995, 4996], [5097, 5098], [7130, 7131]], "70": [[3017, 3023], [6880, 6884]], "71": [[4722, 4726]], "72": [[5979, 5981]], "73": [[4285, 4287]], "74": [[5858, 5860]], "75": [[372, 373], [3123, 3126], [3417, 3418], [4145, 4147], [5512, 5514]], "76": [[5166, 5171], [5247, 5252]], "77": [[2040, 2042], [2071, 2072], [4619, 4620], [5243, 5244], [6241, 6242]], "78": [[3888, 3890], [6904, 6906]], "79": [[4291, 4292]], "8": [[341, 345], [5719, 5720]], "80": [[5681, 5689]], "81": [[5340, 5341], [5430, 5431]], "82": [[5417, 5419]], "83": [[7093, 7095]], "84": [[3541, 3542], [3972, 3973], [4340, 4341]], "85": [[5593, 5594]], "86": [[6146, 6150]], "87": [[7113, 7116]], "88": [[6380, 6382]], "89": [[170, 176], [1009, 1015], [2481, 2486]], "9": [[548, 553], [821, 826], [1178, 1183], [1327, 1331], [5527, 5532], [5585, 5590]], "90": [[5043, 5044], [5105, 5107]], "91": [[697, 698], [5362, 5363]], "92": [[5707, 5709]], "93": [[3203, 3206], [4385, 4388]], "94": [[3679, 3683]], "95": [[7167, 7171]], "96": [[5389, 5395]], "97": [[6724, 6726], [6730, 6732]], "98": [[4436, 4441]], "99": [[4238, 4242]]}, "doc_id": "16cd50316e41cbb1d9dfeafeb524b31654cef37a", "method_subrelations": {"CNN-LSTM": [[[0, 8], "CNN-LSTM"]], "Microsoft_2016b": [[[0, 15], "Microsoft_2016b"]]}, "n_ary_relations": [{"Material": "Switchboard___Hub500", "Method": "CNN-LSTM", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 6.6}, {"Material": "Switchboard___Hub500", "Method": "Microsoft_2016b", "Metric": "Percentage_error", "Task": "Speech_Recognition", "score": 5.8}], "ner": [[3, 5, "Task"], [6, 9, "Task"], [9, 12, "Task"], [16, 20, "Task"], [25, 27, "Material"], [38, 41, "Metric"], [63, 65, "Metric"], [66, 68, "Method"], [73, 75, "Material"], [128, 130, "Metric"], [132, 134, "Metric"], [155, 161, "Method"], [166, 169, "Method"], [170, 176, "Method"], [178, 184, "Method"], [190, 192, "Method"], [215, 218, "Task"], [219, 222, "Method"], [223, 226, "Method"], [227, 228, "Method"], [229, 230, "Method"], [231, 232, "Method"], [233, 234, "Method"], [235, 237, "Method"], [256, 259, "Task"], [263, 266, "Task"], [273, 278, "Material"], [285, 287, "Task"], [312, 313, "Material"], [322, 324, "Task"], [336, 339, "Task"], [341, 345, "Material"], [347, 349, "Material"], [355, 357, "Task"], [368, 371, "Material"], [372, 373, "Material"], [415, 418, "Material"], [436, 438, "Material"], [494, 497, "Metric"], [542, 546, "Material"], [548, 553, "Material"], [558, 561, "Metric"], [584, 586, "Metric"], [595, 596, "Task"], [605, 609, "Method"], [634, 641, "Method"], [670, 672, "Task"], [681, 683, "Task"], [684, 686, "Task"], [692, 696, "Method"], [697, 698, "Method"], [704, 707, "Task"], [709, 711, "Method"], [731, 733, "Method"], [741, 743, "Task"], [744, 746, "Method"], [751, 755, "Method"], [768, 770, "Method"], [771, 774, "Method"], [779, 781, "Method"], [787, 789, "Method"], [792, 793, "Metric"], [795, 798, "Metric"], [817, 818, "Metric"], [821, 826, "Material"], [831, 834, "Method"], [838, 841, "Method"], [845, 846, "Method"], [848, 851, "Method"], [857, 862, "Method"], [865, 868, "Method"], [874, 878, "Method"], [893, 896, "Method"], [897, 898, "Method"], [907, 911, "Task"], [963, 965, "Metric"], [972, 989, "Method"], [998, 1002, "Method"], [1009, 1015, "Method"], [1016, 1019, "Method"], [1038, 1040, "Method"], [1046, 1049, "Method"], [1068, 1070, "Method"], [1106, 1108, "Material"], [1126, 1128, "Task"], [1157, 1159, "Task"], [1178, 1183, "Material"], [1199, 1201, "Metric"], [1228, 1235, "Task"], [1242, 1244, "Material"], [1249, 1252, "Method"], [1327, 1331, "Material"], [1334, 1337, "Method"], [1349, 1351, "Metric"], [1361, 1364, "Task"], [1376, 1379, "Material"], [1395, 1400, "Metric"], [1406, 1409, "Material"], [1412, 1415, "Material"], [1418, 1420, "Material"], [1422, 1424, "Metric"], [1434, 1436, "Task"], [1442, 1444, "Task"], [1470, 1473, "Material"], [1531, 1533, "Material"], [1537, 1540, "Metric"], [1542, 1544, "Material"], [1595, 1600, "Method"], [1602, 1603, "Method"], [1606, 1608, "Method"], [1613, 1615, "Method"], [1624, 1626, "Task"], [1644, 1646, "Method"], [1647, 1648, "Method"], [1656, 1658, "Method"], [1686, 1688, "Method"], [1695, 1697, "Method"], [1699, 1710, "Method"], [1711, 1712, "Method"], [1714, 1716, "Method"], [1723, 1728, "Method"], [1750, 1752, "Method"], [1788, 1790, "Method"], [1794, 1796, "Method"], [1842, 1844, "Method"], [1866, 1870, "Method"], [1878, 1879, "Method"], [1893, 1895, "Method"], [1900, 1902, "Method"], [1905, 1907, "Method"], [1909, 1911, "Method"], [1927, 1930, "Metric"], [1933, 1935, "Material"], [1947, 1948, "Method"], [1954, 1955, "Method"], [1959, 1965, "Method"], [1966, 1967, "Method"], [1976, 1978, "Method"], [1979, 1982, "Method"], [1984, 1987, "Method"], [1989, 1990, "Method"], [1997, 1998, "Method"], [2007, 2010, "Metric"], [2036, 2038, "Metric"], [2040, 2042, "Metric"], [2045, 2047, "Method"], [2065, 2068, "Method"], [2071, 2072, "Metric"], [2074, 2076, "Method"], [2078, 2079, "Method"], [2083, 2085, "Method"], [2092, 2094, "Method"], [2107, 2111, "Material"], [2150, 2152, "Method"], [2154, 2158, "Method"], [2196, 2199, "Metric"], [2217, 2221, "Metric"], [2232, 2234, "Method"], [2245, 2248, "Metric"], [2250, 2252, "Method"], [2261, 2263, "Metric"], [2272, 2274, "Metric"], [2282, 2284, "Method"], [2287, 2290, "Method"], [2290, 2293, "Method"], [2304, 2308, "Method"], [2328, 2330, "Method"], [2347, 2349, "Method"], [2372, 2373, "Method"], [2400, 2401, "Method"], [2450, 2452, "Method"], [2481, 2486, "Method"], [2488, 2492, "Method"], [2500, 2508, "Metric"], [2522, 2524, "Metric"], [2535, 2536, "Method"], [2586, 2590, "Method"], [2592, 2595, "Method"], [2617, 2620, "Method"], [2626, 2628, "Method"], [2629, 2630, "Method"], [2641, 2648, "Method"], [2688, 2696, "Method"], [2713, 2715, "Task"], [2717, 2719, "Material"], [2725, 2730, "Material"], [2746, 2756, "Method"], [2783, 2786, "Material"], [2878, 2880, "Method"], [2888, 2891, "Method"], [2893, 2895, "Material"], [2908, 2909, "Metric"], [2913, 2915, "Method"], [2941, 2945, "Method"], [2972, 2976, "Method"], [2979, 2984, "Method"], [2985, 2988, "Method"], [2989, 2992, "Method"], [2993, 2995, "Metric"], [3009, 3013, "Method"], [3017, 3023, "Method"], [3030, 3033, "Method"], [3052, 3054, "Task"], [3055, 3057, "Task"], [3059, 3060, "Task"], [3064, 3066, "Method"], [3076, 3081, "Method"], [3086, 3088, "Method"], [3090, 3094, "Method"], [3123, 3126, "Material"], [3129, 3130, "Method"], [3148, 3150, "Task"], [3152, 3156, "Method"], [3158, 3160, "Method"], [3168, 3172, "Method"], [3176, 3179, "Metric"], [3196, 3201, "Method"], [3203, 3206, "Method"], [3212, 3214, "Method"], [3215, 3217, "Method"], [3230, 3234, "Method"], [3235, 3238, "Method"], [3244, 3248, "Method"], [3250, 3254, "Method"], [3268, 3274, "Method"], [3275, 3279, "Method"], [3301, 3306, "Method"], [3324, 3328, "Method"], [3337, 3340, "Method"], [3355, 3358, "Method"], [3360, 3364, "Method"], [3386, 3389, "Material"], [3397, 3400, "Material"], [3403, 3407, "Method"], [3411, 3414, "Material"], [3417, 3418, "Material"], [3428, 3431, "Material"], [3440, 3442, "Method"], [3444, 3448, "Material"], [3454, 3459, "Material"], [3462, 3465, "Metric"], [3473, 3479, "Material"], [3484, 3488, "Material"], [3505, 3507, "Method"], [3523, 3527, "Method"], [3541, 3542, "Metric"], [3543, 3545, "Metric"], [3549, 3558, "Method"], [3561, 3563, "Method"], [3574, 3576, "Method"], [3577, 3578, "Method"], [3579, 3583, "Method"], [3584, 3585, "Method"], [3603, 3608, "Material"], [3611, 3614, "Method"], [3627, 3628, "Task"], [3671, 3676, "Method"], [3679, 3683, "Method"], [3688, 3691, "Method"], [3695, 3699, "Method"], [3700, 3702, "Task"], [3710, 3713, "Method"], [3714, 3717, "Task"], [3727, 3731, "Method"], [3749, 3751, "Method"], [3753, 3760, "Method"], [3768, 3772, "Method"], [3777, 3779, "Method"], [3784, 3787, "Task"], [3799, 3801, "Method"], [3810, 3813, "Method"], [3816, 3818, "Method"], [3828, 3836, "Material"], [3842, 3845, "Method"], [3856, 3860, "Material"], [3865, 3869, "Method"], [3875, 3879, "Method"], [3883, 3884, "Method"], [3888, 3890, "Method"], [3897, 3898, "Task"], [3904, 3907, "Method"], [3946, 3949, "Method"], [3972, 3973, "Metric"], [3982, 3984, "Method"], [3995, 3998, "Method"], [4009, 4013, "Method"], [4031, 4034, "Method"], [4036, 4040, "Method"], [4042, 4046, "Task"], [4051, 4055, "Task"], [4057, 4058, "Method"], [4060, 4062, "Method"], [4066, 4072, "Material"], [4076, 4079, "Method"], [4090, 4092, "Method"], [4103, 4105, "Method"], [4109, 4114, "Method"], [4133, 4138, "Method"], [4139, 4140, "Task"], [4145, 4147, "Material"], [4149, 4152, "Material"], [4153, 4154, "Material"], [4160, 4165, "Material"], [4175, 4177, "Material"], [4183, 4189, "Material"], [4196, 4200, "Method"], [4211, 4215, "Material"], [4218, 4224, "Task"], [4227, 4229, "Method"], [4238, 4242, "Material"], [4244, 4249, "Method"], [4258, 4260, "Method"], [4264, 4268, "Method"], [4271, 4275, "Material"], [4276, 4280, "Material"], [4285, 4287, "Task"], [4291, 4292, "Task"], [4302, 4305, "Material"], [4310, 4312, "Material"], [4320, 4322, "Method"], [4327, 4334, "Method"], [4340, 4341, "Metric"], [4342, 4345, "Metric"], [4347, 4352, "Method"], [4360, 4362, "Method"], [4365, 4367, "Method"], [4385, 4388, "Method"], [4397, 4401, "Method"], [4436, 4441, "Method"], [4445, 4448, "Metric"], [4449, 4452, "Method"], [4457, 4460, "Method"], [4475, 4477, "Method"], [4482, 4484, "Task"], [4485, 4487, "Method"], [4493, 4495, "Method"], [4511, 4513, "Method"], [4515, 4520, "Method"], [4547, 4549, "Task"], [4555, 4557, "Method"], [4593, 4595, "Method"], [4600, 4602, "Method"], [4619, 4620, "Metric"], [4630, 4633, "Method"], [4690, 4692, "Method"], [4696, 4697, "Metric"], [4702, 4703, "Method"], [4716, 4718, "Method"], [4722, 4726, "Method"], [4774, 4777, "Method"], [4807, 4809, "Method"], [4826, 4828, "Method"], [4848, 4850, "Task"], [4853, 4855, "Method"], [4866, 4868, "Method"], [4891, 4893, "Method"], [4912, 4915, "Method"], [4916, 4917, "Method"], [4919, 4921, "Method"], [4929, 4932, "Method"], [4934, 4935, "Method"], [4941, 4946, "Method"], [4947, 4948, "Method"], [4951, 4953, "Task"], [4977, 4981, "Material"], [4991, 4993, "Method"], [4995, 4996, "Method"], [4998, 5000, "Method"], [5018, 5019, "Method"], [5022, 5023, "Method"], [5025, 5030, "Method"], [5033, 5035, "Task"], [5043, 5044, "Method"], [5069, 5073, "Task"], [5074, 5078, "Method"], [5080, 5082, "Method"], [5088, 5089, "Method"], [5097, 5098, "Method"], [5105, 5107, "Method"], [5109, 5115, "Method"], [5117, 5121, "Method"], [5122, 5123, "Method"], [5139, 5141, "Metric"], [5147, 5152, "Method"], [5159, 5165, "Method"], [5166, 5171, "Method"], [5243, 5244, "Metric"], [5247, 5252, "Method"], [5257, 5259, "Method"], [5309, 5314, "Method"], [5332, 5334, "Metric"], [5340, 5341, "Metric"], [5350, 5352, "Method"], [5356, 5359, "Method"], [5362, 5363, "Method"], [5381, 5383, "Method"], [5384, 5387, "Method"], [5389, 5395, "Method"], [5399, 5401, "Method"], [5402, 5404, "Task"], [5407, 5409, "Task"], [5411, 5412, "Method"], [5413, 5415, "Task"], [5417, 5419, "Task"], [5430, 5431, "Metric"], [5459, 5460, "Method"], [5469, 5471, "Task"], [5473, 5475, "Method"], [5476, 5478, "Task"], [5487, 5489, "Metric"], [5491, 5495, "Task"], [5504, 5506, "Material"], [5512, 5514, "Material"], [5515, 5520, "Material"], [5527, 5532, "Material"], [5536, 5539, "Material"], [5541, 5544, "Material"], [5554, 5562, "Material"], [5575, 5577, "Material"], [5585, 5590, "Material"], [5593, 5594, "Task"], [5595, 5596, "Task"], [5598, 5601, "Material"], [5604, 5606, "Material"], [5624, 5626, "Method"], [5649, 5651, "Method"], [5664, 5665, "Method"], [5681, 5689, "Method"], [5707, 5709, "Material"], [5713, 5714, "Material"], [5715, 5718, "Material"], [5719, 5720, "Material"], [5721, 5722, "Material"], [5737, 5741, "Material"], [5743, 5744, "Method"], [5755, 5758, "Method"], [5760, 5762, "Method"], [5778, 5782, "Method"], [5783, 5785, "Method"], [5800, 5803, "Metric"], [5806, 5808, "Method"], [5821, 5823, "Method"], [5838, 5840, "Metric"], [5849, 5851, "Metric"], [5852, 5853, "Method"], [5858, 5860, "Task"], [5868, 5869, "Method"], [5888, 5890, "Method"], [5901, 5903, "Method"], [5904, 5907, "Method"], [5915, 5917, "Method"], [5947, 5950, "Metric"], [5979, 5981, "Method"], [5987, 5991, "Method"], [5999, 6003, "Method"], [6010, 6012, "Method"], [6016, 6020, "Method"], [6040, 6042, "Method"], [6048, 6049, "Method"], [6060, 6061, "Method"], [6064, 6065, "Metric"], [6094, 6096, "Task"], [6101, 6103, "Metric"], [6107, 6114, "Method"], [6126, 6129, "Method"], [6136, 6137, "Metric"], [6146, 6150, "Metric"], [6153, 6155, "Metric"], [6164, 6166, "Metric"], [6196, 6199, "Method"], [6214, 6217, "Metric"], [6224, 6226, "Metric"], [6241, 6242, "Metric"], [6245, 6247, "Task"], [6258, 6260, "Method"], [6271, 6273, "Metric"], [6349, 6353, "Task"], [6380, 6382, "Method"], [6415, 6417, "Method"], [6453, 6456, "Material"], [6490, 6492, "Method"], [6533, 6535, "Metric"], [6550, 6552, "Material"], [6683, 6687, "Task"], [6697, 6699, "Metric"], [6718, 6720, "Metric"], [6724, 6726, "Metric"], [6730, 6732, "Metric"], [6775, 6776, "Method"], [6777, 6779, "Task"], [6792, 6794, "Method"], [6814, 6816, "Method"], [6843, 6846, "Method"], [6852, 6855, "Method"], [6857, 6859, "Method"], [6880, 6884, "Method"], [6904, 6906, "Method"], [6909, 6912, "Task"], [6921, 6924, "Method"], [6937, 6940, "Method"], [6945, 6951, "Material"], [6955, 6957, "Method"], [6958, 6962, "Method"], [6964, 6968, "Metric"], [6981, 6983, "Method"], [6990, 6993, "Task"], [7001, 7004, "Metric"], [7005, 7013, "Task"], [7026, 7030, "Material"], [7037, 7039, "Metric"], [7048, 7050, "Task"], [7072, 7073, "Method"], [7075, 7079, "Task"], [7083, 7084, "Method"], [7086, 7088, "Method"], [7093, 7095, "Method"], [7097, 7101, "Task"], [7113, 7116, "Method"], [7130, 7131, "Method"], [7155, 7159, "Method"], [7167, 7171, "Material"]], "sections": [[0, 238], [238, 1090], [1090, 1593], [1593, 1600], [1600, 1945], [1945, 2043], [2043, 2285], [2285, 2479], [2479, 3050], [3050, 3228], [3228, 3677], [3677, 4128], [4128, 4325], [4325, 4480], [4480, 4910], [4910, 4987], [4987, 5067], [5067, 5330], [5330, 5497], [5497, 5502], [5502, 5622], [5622, 5877], [5877, 6243], [6243, 6764], [6764, 6994], [6994, 7102], [7102, 7172], [7172, 7175]], "sentences": [[0, 9], [9, 31], [31, 62], [62, 109], [109, 143], [143, 148], [148, 193], [193, 205], [205, 238], [238, 241], [241, 281], [281, 358], [358, 411], [411, 439], [439, 473], [473, 491], [491, 517], [517, 554], [554, 583], [583, 599], [599, 624], [624, 642], [642, 673], [673, 687], [687, 740], [740, 775], [775, 799], [799, 842], [842, 939], [939, 940], [940, 944], [944, 955], [955, 966], [966, 990], [990, 1003], [1003, 1016], [1016, 1035], [1035, 1055], [1055, 1090], [1090, 1094], [1094, 1115], [1115, 1129], [1129, 1144], [1144, 1160], [1160, 1172], [1172, 1193], [1193, 1211], [1211, 1236], [1236, 1266], [1266, 1293], [1293, 1307], [1307, 1338], [1338, 1365], [1365, 1392], [1392, 1422], [1422, 1445], [1445, 1446], [1446, 1463], [1463, 1492], [1492, 1513], [1513, 1528], [1528, 1545], [1545, 1573], [1573, 1593], [1593, 1600], [1600, 1603], [1603, 1609], [1609, 1617], [1617, 1630], [1630, 1649], [1649, 1679], [1679, 1693], [1693, 1711], [1711, 1736], [1736, 1750], [1750, 1769], [1769, 1787], [1787, 1813], [1813, 1837], [1837, 1860], [1860, 1880], [1880, 1896], [1896, 1917], [1917, 1936], [1936, 1945], [1945, 1948], [1948, 1973], [1973, 1983], [1983, 1993], [1993, 2043], [2043, 2047], [2047, 2077], [2077, 2095], [2095, 2112], [2112, 2134], [2134, 2144], [2144, 2159], [2159, 2179], [2179, 2200], [2200, 2222], [2222, 2253], [2253, 2270], [2270, 2285], [2285, 2290], [2290, 2312], [2312, 2326], [2326, 2346], [2346, 2368], [2368, 2396], [2396, 2418], [2418, 2453], [2453, 2467], [2467, 2479], [2479, 2486], [2486, 2509], [2509, 2569], [2569, 2601], [2601, 2623], [2623, 2634], [2634, 2649], [2649, 2678], [2678, 2697], [2697, 2731], [2731, 2744], [2744, 2777], [2777, 2788], [2788, 2791], [2791, 2806], [2806, 2811], [2811, 2819], [2819, 2823], [2823, 2826], [2826, 2831], [2831, 2834], [2834, 2871], [2871, 2901], [2901, 2923], [2923, 2949], [2949, 2971], [2971, 2993], [2993, 3004], [3004, 3014], [3014, 3034], [3034, 3050], [3050, 3057], [3057, 3073], [3073, 3089], [3089, 3114], [3114, 3127], [3127, 3151], [3151, 3167], [3167, 3183], [3183, 3207], [3207, 3228], [3228, 3234], [3234, 3249], [3249, 3263], [3263, 3287], [3287, 3296], [3296, 3330], [3330, 3353], [3353, 3380], [3380, 3432], [3432, 3450], [3450, 3468], [3468, 3517], [3517, 3538], [3538, 3559], [3559, 3577], [3577, 3587], [3587, 3609], [3609, 3677], [3677, 3683], [3683, 3718], [3718, 3741], [3741, 3766], [3766, 3782], [3782, 3803], [3803, 3819], [3819, 3846], [3846, 3863], [3863, 3880], [3880, 3897], [3897, 3933], [3933, 3958], [3958, 3974], [3974, 3991], [3991, 4023], [4023, 4056], [4056, 4073], [4073, 4116], [4116, 4128], [4128, 4132], [4132, 4194], [4194, 4216], [4216, 4243], [4243, 4263], [4263, 4294], [4294, 4325], [4325, 4335], [4335, 4359], [4359, 4368], [4368, 4402], [4402, 4429], [4429, 4461], [4461, 4471], [4471, 4480], [4480, 4484], [4484, 4496], [4496, 4521], [4521, 4550], [4550, 4583], [4583, 4596], [4596, 4609], [4609, 4625], [4625, 4646], [4646, 4685], [4685, 4715], [4715, 4751], [4751, 4781], [4781, 4800], [4800, 4836], [4836, 4859], [4859, 4869], [4869, 4894], [4894, 4910], [4910, 4918], [4918, 4936], [4936, 4947], [4947, 4969], [4969, 4987], [4987, 4994], [4994, 5024], [5024, 5045], [5045, 5061], [5061, 5067], [5067, 5078], [5078, 5097], [5097, 5116], [5116, 5136], [5136, 5172], [5172, 5202], [5202, 5231], [5231, 5245], [5245, 5260], [5260, 5293], [5293, 5330], [5330, 5334], [5334, 5381], [5381, 5402], [5402, 5411], [5411, 5430], [5430, 5455], [5455, 5476], [5476, 5497], [5497, 5502], [5502, 5506], [5506, 5521], [5521, 5547], [5547, 5578], [5578, 5597], [5597, 5622], [5622, 5627], [5627, 5648], [5648, 5674], [5674, 5694], [5694, 5706], [5706, 5723], [5723, 5742], [5742, 5759], [5759, 5770], [5770, 5792], [5792, 5809], [5809, 5828], [5828, 5844], [5844, 5877], [5877, 5883], [5883, 5909], [5909, 5933], [5933, 5958], [5958, 5982], [5982, 6004], [6004, 6028], [6028, 6077], [6077, 6097], [6097, 6115], [6115, 6163], [6163, 6213], [6213, 6243], [6243, 6247], [6247, 6267], [6267, 6293], [6293, 6348], [6348, 6365], [6365, 6383], [6383, 6399], [6399, 6433], [6433, 6442], [6442, 6468], [6468, 6505], [6505, 6521], [6521, 6542], [6542, 6565], [6565, 6566], [6566, 6590], [6590, 6618], [6618, 6645], [6645, 6665], [6665, 6690], [6690, 6708], [6708, 6727], [6727, 6752], [6752, 6764], [6764, 6770], [6770, 6795], [6795, 6807], [6807, 6842], [6842, 6856], [6856, 6877], [6877, 6907], [6907, 6952], [6952, 6994], [6994, 6997], [6997, 7014], [7014, 7041], [7041, 7060], [7060, 7102], [7102, 7105], [7105, 7142], [7142, 7172], [7172, 7175]], "words": ["document", ":", "Achieving", "Human", "Parity", "in", "Conversational", "Speech", "Recognition", "Conversational", "speech", "recognition", "has", "served", "as", "a", "flagship", "speech", "recognition", "task", "since", "the", "release", "of", "the", "Switchboard", "corpus", "in", "the", "1990s", ".", "In", "this", "paper", ",", "we", "measure", "the", "human", "error", "rate", "on", "the", "widely", "used", "NIST", "2000", "test", "set", ",", "and", "find", "that", "our", "latest", "automated", "system", "has", "reached", "human", "parity", ".", "The", "error", "rate", "of", "professional", "transcribers", "is", "5.9", "%", "for", "the", "Switchboard", "portion", "of", "the", "data", ",", "in", "which", "newly", "acquainted", "pairs", "of", "people", "discuss", "an", "assigned", "topic", ",", "and", "11.3", "%", "for", "the", "CallHome", "portion", "where", "friends", "and", "family", "members", "have", "open", "-", "ended", "conversations", ".", "In", "both", "cases", ",", "our", "automated", "system", "establishes", "a", "new", "state", "of", "the", "art", ",", "and", "edges", "past", "the", "human", "benchmark", ",", "achieving", "error", "rates", "of", "5.8", "%", "and", "11.0", "%", ",", "respectively", ".", "The", "key", "to", "our", "system", "\u2019s", "performance", "is", "the", "use", "of", "various", "convolutional", "and", "LSTM", "acoustic", "model", "architectures", ",", "combined", "with", "a", "novel", "spatial", "smoothing", "method", "and", "lattice", "-", "free", "MMI", "acoustic", "training", ",", "multiple", "recurrent", "neural", "network", "language", "modeling", "approaches", ",", "and", "a", "systematic", "use", "of", "system", "combination", ".", "W.Xiong", ",", "J.Droppo", ",", "X.Huang", ",", "F.Seide", ",", "M.Seltzer", ",", "A.Stolcke", ",", "D.YuandG.Zweig", "MicrosoftResearch", "TechnicalReportMSR", "-", "TR", "-", "2016", "-", "71", "RevisedFebruary2017", "Conversational", "speech", "recognition", ",", "convolutional", "neural", "networks", ",", "recurrent", "neural", "networks", ",", "VGG", ",", "ResNet", ",", "LACE", ",", "BLSTM", ",", "spatial", "smoothing", ".", "section", ":", "Introduction", "Recent", "years", "have", "seen", "human", "performance", "levels", "reached", "or", "surpassed", "in", "tasks", "ranging", "from", "the", "games", "of", "chess", "and", "Go", "to", "simple", "speech", "recognition", "tasks", "like", "carefully", "read", "newspaper", "speech", "and", "rigidly", "constrained", "small", "-", "vocabulary", "tasks", "in", "noise", ".", "In", "the", "area", "of", "speech", "recognition", ",", "much", "of", "the", "pioneering", "early", "work", "was", "driven", "by", "a", "series", "of", "carefully", "designed", "tasks", "with", "DARPA", "-", "funded", "datasets", "publicly", "released", "by", "the", "LDC", "and", "NIST", ":", "first", "simple", "ones", "like", "the", "\u201c", "resource", "management", "\u201d", "task", "with", "a", "small", "vocabulary", "and", "carefully", "controlled", "grammar", ";", "then", "read", "speech", "recognition", "in", "the", "Wall", "Street", "Journal", "task", ";", "then", "Broadcast", "News", ";", "each", "progressively", "more", "difficult", "for", "automatic", "systems", ".", "One", "of", "last", "big", "initiatives", "in", "this", "area", "was", "in", "conversational", "telephone", "speech", "(", "CTS", ")", ",", "which", "is", "especially", "difficult", "due", "to", "the", "spontaneous", "(", "neither", "read", "nor", "planned", ")", "nature", "of", "the", "speech", ",", "its", "informality", ",", "and", "the", "self", "-", "corrections", ",", "hesitations", "and", "other", "disfluencies", "that", "are", "pervasive", ".", "The", "Switchboard", "and", "later", "Fisher", "data", "collections", "of", "the", "1990s", "and", "early", "2000s", "provide", "what", "is", "to", "date", "the", "largest", "and", "best", "studied", "of", "the", "conversational", "corpora", ".", "The", "history", "of", "work", "in", "this", "area", "includes", "key", "contributions", "by", "institutions", "such", "as", "IBM", ",", "BBN", ",", "SRI", ",", "AT", "&", "T", ",", "LIMSI", ",", "Cambridge", "University", ",", "Microsoft", "and", "numerous", "others", ".", "In", "the", "past", ",", "human", "performance", "on", "this", "task", "has", "been", "widely", "cited", "as", "being", "4", "%", ".", "However", ",", "the", "error", "rate", "estimate", "in", "is", "attributed", "to", "a", "\u201c", "personal", "communication", ",", "\u201d", "and", "the", "actual", "source", "of", "this", "number", "is", "ephemeral", ".", "To", "better", "understand", "human", "performance", ",", "we", "have", "used", "professional", "transcribers", "to", "transcribe", "the", "actual", "test", "sets", "that", "we", "are", "working", "with", ",", "specifically", "the", "CallHome", "and", "Switchboard", "portions", "of", "the", "NIST", "eval", "2000", "test", "set", ".", "We", "find", "that", "the", "human", "error", "rates", "on", "these", "two", "parts", "are", "different", "almost", "by", "a", "factor", "of", "two", ",", "so", "a", "single", "number", "is", "inappropriate", "to", "cite", ".", "The", "error", "rate", "on", "Switchboard", "is", "about", "5.9", "%", ",", "and", "for", "CallHome", "11.3", "%", ".", "We", "improve", "on", "our", "recently", "reported", "conversational", "speech", "recognition", "system", "by", "about", "0.4", "%", ",", "and", "now", "exceed", "human", "performance", "by", "a", "small", "margin", ".", "Our", "progress", "is", "a", "result", "of", "the", "careful", "engineering", "and", "optimization", "of", "convolutional", "and", "recurrent", "neural", "networks", ".", "While", "the", "basic", "structures", "have", "been", "well", "known", "for", "a", "long", "period", ",", "it", "is", "only", "recently", "that", "they", "have", "dominated", "the", "field", "as", "the", "best", "models", "for", "speech", "recognition", ".", "Surprisingly", ",", "this", "is", "the", "case", "for", "both", "acoustic", "modeling", "and", "language", "modeling", ".", "In", "comparison", "to", "the", "standard", "feed", "-", "forward", "MLPs", "or", "DNNs", "that", "first", "demonstrated", "breakthrough", "performance", "on", "conversational", "speech", "recognition", ",", "these", "acoustic", "models", "have", "the", "ability", "to", "model", "a", "large", "amount", "of", "acoustic", "context", "with", "temporal", "invariance", ",", "and", "in", "the", "case", "of", "convolutional", "models", ",", "with", "frequency", "invariance", "as", "well", ".", "In", "language", "modeling", ",", "recurrent", "models", "appear", "to", "improve", "over", "classical", "N", "-", "gram", "models", "through", "the", "use", "of", "an", "unbounded", "word", "history", ",", "as", "well", "as", "the", "generalization", "ability", "of", "continuous", "word", "representations", ".", "In", "the", "meantime", ",", "ensemble", "learning", "has", "become", "commonly", "used", "in", "several", "neural", "models", ",", "to", "improve", "robustness", "by", "reducing", "bias", "and", "variance", ".", "This", "paper", "is", "an", "expanded", "version", "of", ",", "with", "the", "following", "additional", "material", ":", "A", "comprehensive", "assessment", "of", "human", "performance", "on", "the", "NIST", "eval", "2000", "test", "set", "The", "description", "of", "a", "novel", "spatial", "regularization", "method", "which", "significantly", "boosts", "our", "LSTM", "acoustic", "model", "performance", "The", "use", "of", "LSTM", "rather", "than", "RNN", "-", "LMs", ",", "and", "the", "use", "of", "a", "letter", "-", "trigram", "input", "representation", "A", "two", "-", "level", "system", "combination", ",", "based", "on", "a", "subsystem", "of", "BLSTM", "-", "system", "variants", "that", ",", "by", "itself", ",", "surpasses", "the", "best", "previously", "reported", "results", "Expanded", "coverage", "of", "the", "Microsoft", "Cognitive", "Toolkit", "(", "CNTK", ")", "used", "to", "build", "our", "models", "An", "analysis", "of", "human", "versus", "machine", "errors", ",", "which", "indicates", "substantial", "equivalence", ",", "with", "the", "exception", "of", "the", "word", "classes", "of", "backchannel", "acknowledgments", "(", "e.g.", "\u201c", "uh", "-", "huh", "\u201d", ")", "and", "hesitations", "(", "e.g.", "\u201c", "um", "\u201d", ")", ".", "The", "remainder", "of", "this", "paper", "describes", "our", "system", "in", "detail", ".", "Section", "[", "reference", "]", "describes", "our", "measurement", "of", "human", "performance", ".", "Section", "[", "reference", "]", "describes", "the", "convolutional", "neural", "net", "(", "CNN", ")", "and", "long", "-", "short", "-", "term", "memory", "(", "LSTM", ")", "models", ".", "Section", "[", "reference", "]", "describes", "our", "implementation", "of", "i", "-", "vector", "adaptation", ".", "Section", "[", "reference", "]", "presents", "out", "lattice", "-", "free", "MMI", "training", "process", ".", "Language", "model", "rescoring", "is", "a", "significant", "part", "of", "our", "system", ",", "and", "described", "in", "Section", "[", "reference", "]", ".", "We", "describe", "the", "CNTK", "toolkit", "that", "forms", "the", "basis", "of", "our", "neural", "network", "models", "in", "Section", "[", "reference", "]", ".", "Experimental", "results", "are", "presented", "in", "Section", "[", "reference", "]", ",", "followed", "by", "an", "error", "analysis", "in", "section", "[", "reference", "]", ",", "a", "review", "of", "relevant", "prior", "work", "in", "[", "reference", "]", "and", "concluding", "remarks", ".", "section", ":", "Human", "Performance", "To", "measure", "human", "performance", ",", "we", "leveraged", "an", "existing", "pipeline", "in", "which", "Microsoft", "data", "is", "transcribed", "on", "a", "weekly", "basis", ".", "This", "pipeline", "uses", "a", "large", "commercial", "vendor", "to", "perform", "two", "-", "pass", "transcription", ".", "In", "the", "first", "pass", ",", "a", "transcriber", "works", "from", "scratch", "to", "transcribe", "the", "data", ".", "In", "the", "second", "pass", ",", "a", "second", "listener", "monitors", "the", "data", "to", "do", "error", "correction", ".", "Dozens", "of", "hours", "of", "test", "data", "are", "processed", "in", "each", "batch", ".", "One", "week", ",", "we", "added", "the", "NIST", "2000", "CTS", "evaluation", "data", "to", "the", "work", "-", "list", ",", "without", "further", "comment", ".", "The", "intention", "was", "to", "measure", "the", "error", "rate", "of", "professional", "transcribers", "going", "about", "their", "normal", "everyday", "business", ".", "Aside", "from", "the", "standard", "two", "-", "pass", "checking", "in", "place", ",", "we", "did", "not", "do", "a", "complex", "multi", "-", "party", "transcription", "and", "adjudication", "process", ".", "The", "transcribers", "were", "given", "the", "same", "audio", "segments", "as", "were", "provided", "to", "the", "speech", "recognition", "system", ",", "which", "results", "in", "short", "sentences", "or", "sentence", "fragments", "from", "a", "single", "channel", ".", "This", "makes", "the", "task", "easier", "since", "the", "speakers", "are", "more", "clearly", "separated", ",", "and", "more", "difficult", "since", "the", "two", "sides", "of", "the", "conversation", "are", "not", "interleaved", ".", "Thus", ",", "it", "is", "the", "same", "condition", "as", "reported", "for", "our", "automated", "systems", ".", "The", "resulting", "numbers", "are", "5.9", "%", "for", "the", "Switchboard", "portion", ",", "and", "11.3", "%", "for", "the", "CallHome", "portion", "of", "the", "NIST", "2000", "test", "set", ",", "using", "the", "NIST", "scoring", "protocol", ".", "These", "numbers", "should", "be", "taken", "as", "an", "indication", "of", "the", "\u201c", "error", "rate", "\u201d", "of", "a", "trained", "professional", "working", "in", "industry", "-", "standard", "speech", "transcript", "production", ".", "(", "We", "have", "submitted", "the", "human", "transcripts", "thus", "produced", "to", "the", "Linguistic", "Data", "Consortium", "for", "publication", ",", "so", "as", "to", "facilitate", "research", "by", "other", "groups", ".", ")", "Past", "work", "reports", "inter", "-", "transcriber", "error", "rates", "for", "data", "taken", "from", "the", "later", "RT03", "test", "set", "(", "which", "contains", "Switchboard", "and", "Fisher", ",", "but", "no", "CallHome", "data", ")", ".", "Error", "rates", "of", "4.1", "to", "4.5", "%", "are", "reported", "for", "extremely", "careful", "multiple", "transcriptions", ",", "and", "9.6", "%", "for", "\u201c", "quick", "transcriptions", ".", "\u201d", "While", "this", "is", "a", "different", "test", "set", ",", "the", "numbers", "are", "in", "line", "with", "our", "findings", ".", "We", "note", "that", "the", "bulk", "of", "the", "Fisher", "training", "data", ",", "and", "the", "bulk", "of", "the", "data", "overall", ",", "was", "transcribed", "with", "the", "\u201c", "quick", "transcription", "\u201d", "guidelines", ".", "Thus", ",", "the", "current", "state", "of", "the", "art", "is", "actually", "far", "exceeding", "the", "noise", "level", "in", "its", "own", "training", "data", ".", "Perhaps", "the", "most", "important", "point", "is", "the", "extreme", "variability", "between", "the", "two", "test", "subsets", ".", "The", "more", "informal", "CallHome", "data", "has", "almost", "double", "the", "human", "error", "rate", "of", "the", "Switchboard", "data", ".", "Interestingly", ",", "the", "same", "informality", ",", "multiple", "speakers", "per", "channel", ",", "and", "recording", "conditions", "that", "make", "CallHome", "hard", "for", "computers", "make", "it", "difficult", "for", "people", "as", "well", ".", "Notably", ",", "the", "performance", "of", "our", "artificial", "system", "aligns", "almost", "exactly", "with", "the", "performance", "of", "people", "on", "both", "sets", ".", "section", ":", "Convolutional", "and", "LSTM", "Neural", "Networks", "subsection", ":", "CNNs", "We", "use", "three", "CNN", "variants", ".", "The", "first", "is", "the", "VGG", "architecture", "of", ".", "Compared", "to", "the", "networks", "used", "previously", "in", "image", "recognition", ",", "this", "network", "uses", "small", "(", "3x3", ")", "filters", ",", "is", "deeper", ",", "and", "applies", "up", "to", "five", "convolutional", "layers", "before", "pooling", ".", "The", "second", "network", "is", "modeled", "on", "the", "ResNet", "architecture", ",", "which", "adds", "highway", "connections", ",", "i.e.", "a", "linear", "transform", "of", "each", "layer", "\u2019s", "input", "to", "the", "layer", "\u2019s", "output", ".", "The", "only", "difference", "is", "that", "we", "apply", "Batch", "Normalization", "before", "computing", "ReLU", "activations", ".", "The", "last", "CNN", "variant", "is", "the", "LACE", "(", "layer", "-", "wise", "context", "expansion", "with", "attention", ")", "model", ".", "LACE", "is", "a", "TDNN", "variant", "in", "which", "each", "higher", "layer", "is", "a", "weighted", "sum", "of", "nonlinear", "transformations", "of", "a", "window", "of", "lower", "layer", "frames", ".", "In", "other", "words", ",", "each", "higher", "layer", "exploits", "broader", "context", "than", "lower", "layers", ".", "Lower", "layers", "focus", "on", "extracting", "simple", "local", "patterns", "while", "higher", "layers", "extract", "complex", "patterns", "that", "cover", "broader", "contexts", ".", "Since", "not", "all", "frames", "in", "a", "window", "carry", "the", "same", "importance", ",", "an", "attention", "mask", "is", "applied", ".", "The", "LACE", "model", "differs", "from", "the", "earlier", "TDNN", "models", "e.g.", "in", "the", "use", "of", "a", "learned", "attention", "mask", "and", "ResNet", "like", "linear", "pass", "-", "through", ".", "As", "illustrated", "in", "detail", "in", "Figure", "[", "reference", "]", ",", "the", "model", "is", "composed", "of", "four", "blocks", ",", "each", "with", "the", "same", "architecture", ".", "Each", "block", "starts", "with", "a", "convolution", "layer", "with", "stride", "2", "which", "sub", "-", "samples", "the", "input", "and", "increases", "the", "number", "of", "channels", ".", "This", "layer", "is", "followed", "by", "four", "RELU", "-", "convolution", "layers", "with", "jump", "links", "similar", "to", "those", "used", "in", "ResNet", ".", "Table", "[", "reference", "]", "compares", "the", "layer", "structure", "and", "parameters", "of", "the", "three", "CNN", "architectures", ".", "We", "also", "trained", "a", "fused", "model", "by", "combining", "a", "ResNet", "model", "and", "a", "VGG", "model", "at", "the", "senone", "posterior", "level", ".", "Both", "base", "models", "are", "independently", "trained", ",", "and", "then", "the", "score", "fusion", "weight", "is", "optimized", "on", "development", "data", ".", "The", "fused", "system", "is", "our", "best", "single", "system", ".", "subsection", ":", "LSTMs", "While", "our", "best", "performing", "models", "are", "convolutional", ",", "the", "use", "of", "long", "short", "-", "term", "memory", "networks", "(", "LSTMs", ")", "is", "a", "close", "second", ".", "We", "use", "a", "bidirectional", "architecture", "without", "frame", "-", "skipping", ".", "The", "core", "model", "structure", "is", "the", "LSTM", "defined", "in", ".", "We", "found", "that", "using", "networks", "with", "more", "than", "six", "layers", "did", "not", "improve", "the", "word", "error", "rate", "on", "the", "development", "set", ",", "and", "chose", "512", "hidden", "units", ",", "per", "direction", ",", "per", "layer", ",", "as", "that", "provided", "a", "reasonable", "trade", "-", "off", "between", "training", "time", "and", "final", "model", "accuracy", ".", "subsection", ":", "Spatial", "Smoothing", "Inspired", "by", "the", "human", "auditory", "cortex", ",", "where", "neighboring", "neurons", "tend", "to", "simultaneously", "activate", ",", "we", "employ", "a", "spatial", "smoothing", "technique", "to", "improve", "the", "accuracy", "of", "our", "LSTM", "models", ".", "The", "smoothing", "is", "implemented", "as", "a", "regularization", "term", "on", "the", "activations", "between", "layers", "of", "the", "acoustic", "model", ".", "First", ",", "each", "vector", "of", "activations", "is", "re", "-", "interpreted", "as", "a", "2", "-", "dimensional", "image", ".", "For", "example", ",", "if", "there", "are", "512", "neurons", ",", "they", "are", "interpreted", "as", "the", "pixels", "of", "a", "16", "by", "32", "image", ".", "Second", ",", "this", "image", "is", "high", "-", "pass", "filtered", ".", "The", "filter", "is", "implemented", "as", "a", "circular", "convolution", "with", "a", "3", "by", "3", "kernel", ".", "The", "center", "tap", "of", "the", "kernel", "has", "a", "value", "of", ",", "and", "the", "remaining", "eight", "having", "a", "value", "of", ".", "Third", ",", "the", "energy", "of", "this", "high", "-", "pass", "filtered", "image", "is", "computed", "and", "added", "to", "the", "training", "objective", "function", ".", "We", "have", "found", "empirically", "that", "a", "suitable", "scale", "for", "this", "energy", "is", "with", "respect", "to", "the", "existing", "cross", "entropy", "objective", "function", ".", "The", "overall", "effect", "of", "this", "process", "is", "to", "make", "the", "training", "algorithm", "prefer", "models", "that", "have", "correlated", "neurons", ",", "and", "to", "improve", "the", "word", "error", "rate", "of", "the", "acoustic", "model", ".", "Table", "[", "reference", "]", "shows", "the", "benefit", "in", "error", "rate", "for", "some", "of", "our", "early", "systems", ".", "We", "observed", "error", "reductions", "of", "between", "5", "and", "10", "%", "relative", "from", "spatial", "smoothing", ".", "section", ":", "Speaker", "Adaptive", "Modeling", "Speaker", "adaptive", "modeling", "in", "our", "system", "is", "based", "on", "conditioning", "the", "network", "on", "an", "i", "-", "vector", "characterization", "of", "each", "speaker", ".", "A", "100", "-", "dimensional", "i", "-", "vector", "is", "generated", "for", "each", "conversation", "side", ".", "For", "the", "LSTM", "system", ",", "the", "conversation", "-", "side", "i", "-", "vector", "is", "appended", "to", "each", "frame", "of", "input", ".", "For", "convolutional", "networks", ",", "this", "approach", "is", "inappropriate", "because", "we", "do", "not", "expect", "to", "see", "spatially", "contiguous", "patterns", "in", "the", "input", ".", "Instead", ",", "for", "the", "CNNs", ",", "we", "add", "a", "learnable", "weight", "matrix", "to", "each", "layer", ",", "and", "add", "to", "the", "activation", "of", "the", "layer", "before", "the", "nonlinearity", ".", "Thus", ",", "in", "the", "CNN", ",", "the", "i", "-", "vector", "essentially", "serves", "as", "an", "speaker", "-", "dependent", "bias", "to", "each", "layer", ".", "Note", "that", "the", "i", "-", "vectors", "are", "estimated", "using", "MFCC", "features", ";", "by", "using", "them", "subsequently", "in", "systems", "based", "on", "log", "-", "filterbank", "features", ",", "we", "may", "benefit", "from", "a", "form", "of", "feature", "combination", ".", "Performance", "improvements", "from", "i", "-", "vectors", "are", "shown", "in", "Table", "[", "reference", "]", ".", "The", "full", "experimental", "setup", "is", "described", "in", "Section", "[", "reference", "]", ".", "section", ":", "Lattice", "-", "Free", "Sequence", "Training", "After", "standard", "cross", "-", "entropy", "training", ",", "we", "optimize", "the", "model", "parameters", "using", "the", "maximum", "mutual", "information", "(", "MMI", ")", "objective", "function", ".", "Denoting", "a", "word", "sequence", "by", "and", "its", "corresponding", "acoustic", "realization", "by", ",", "the", "training", "criterion", "is", "As", "noted", "in", ",", "the", "necessary", "gradient", "for", "use", "in", "backpropagation", "is", "a", "simple", "function", "of", "the", "posterior", "probability", "of", "a", "particular", "acoustic", "model", "state", "at", "a", "given", "time", ",", "as", "computed", "by", "summing", "over", "all", "possible", "word", "sequences", "in", "an", "unconstrained", "manner", ".", "As", "first", "done", "in", ",", "and", "more", "recently", "in", ",", "this", "can", "be", "accomplished", "with", "a", "straightforward", "alpha", "-", "beta", "computation", "over", "the", "finite", "state", "acceptor", "representing", "the", "decoding", "search", "space", ".", "In", ",", "the", "search", "space", "is", "taken", "to", "be", "an", "acceptor", "representing", "the", "composition", "for", "a", "unigram", "language", "model", "on", "words", ".", "In", ",", "a", "language", "model", "on", "phonemes", "is", "used", "instead", ".", "In", "our", "implementation", ",", "we", "use", "a", "mixed", "-", "history", "acoustic", "unit", "language", "model", ".", "In", "this", "model", ",", "the", "probability", "of", "transitioning", "into", "a", "new", "context", "-", "dependent", "phonetic", "state", "(", "senone", ")", "is", "conditioned", "on", "both", "the", "senone", "and", "phone", "history", ".", "We", "found", "this", "model", "to", "perform", "better", "than", "either", "purely", "word", "-", "based", "or", "phone", "-", "based", "models", ".", "Based", "on", "a", "set", "of", "initial", "experiments", ",", "we", "developed", "the", "following", "procedure", ":", "Perform", "a", "forced", "alignment", "of", "the", "training", "data", "to", "select", "lexical", "variants", "and", "determine", "frame", "-", "aligned", "senone", "sequences", ".", "Compress", "consecutive", "framewise", "occurrences", "of", "a", "single", "senone", "into", "a", "single", "occurrence", ".", "Estimate", "an", "unsmoothed", ",", "variable", "-", "length", "N", "-", "gram", "language", "model", "from", "this", "data", ",", "where", "the", "history", "state", "consists", "of", "the", "previous", "phone", "and", "previous", "senones", "within", "the", "current", "phone", ".", "To", "illustrate", "this", ",", "consider", "the", "sample", "senone", "sequence", "{", "s", "_", "s2.1288", ",", "s", "_", "s3.1061", ",", "s", "_", "s4.1096", "}", ",", "{", "eh", "_", "s2.527", ",", "eh", "_", "s3.128", ",", "eh", "_", "s4.66", "}", ",", "{", "t", "_", "s2.729", ",", "t", "_", "s3.572", ",", "t", "_", "s4.748}.", "When", "predicting", "the", "state", "following", "eh", "_", "s4.66", "the", "history", "consists", "of", "(", "s", ",", "eh", "_", "s2.527", ",", "eh", "_", "s3.128", ",", "eh", "_", "s4.66", ")", ",", "and", "following", "t", "_", "s2.729", ",", "the", "history", "is", "(", "eh", ",", "t", "_", "s2.729", ")", ".", "We", "construct", "the", "denominator", "graph", "from", "this", "language", "model", ",", "and", "HMM", "transition", "probabilities", "as", "determined", "by", "transition", "-", "counting", "in", "the", "senone", "sequences", "found", "in", "the", "training", "data", ".", "Our", "approach", "not", "only", "largely", "reduces", "the", "complexity", "of", "building", "up", "the", "language", "model", "but", "also", "provides", "very", "reliable", "training", "performance", ".", "We", "have", "found", "it", "convenient", "to", "do", "the", "full", "computation", ",", "without", "pruning", ",", "in", "a", "series", "of", "matrix", "-", "vector", "operations", "on", "the", "GPU", ".", "The", "underlying", "acceptor", "is", "represented", "with", "a", "sparse", "matrix", ",", "and", "we", "maintain", "a", "dense", "likelihood", "vector", "for", "each", "time", "frame", ".", "The", "alpha", "and", "beta", "recursions", "are", "implemented", "with", "CUSPARSE", "level", "-", "2", "routines", ":", "sparse", "-", "matrix", ",", "dense", "vector", "multiplies", ".", "Run", "time", "is", "about", "100", "times", "faster", "than", "real", "time", ".", "As", "in", ",", "we", "use", "cross", "-", "entropy", "regularization", ".", "In", "all", "the", "lattice", "-", "free", "MMI", "(", "LFMMI", ")", "experiments", "mentioned", "below", "we", "use", "a", "trigram", "language", "model", ".", "Most", "of", "the", "gain", "is", "usually", "obtained", "after", "processing", "24", "to", "48", "hours", "of", "data", ".", "section", ":", "LM", "Rescoring", "and", "System", "Combination", "An", "initial", "decoding", "is", "done", "with", "a", "WFST", "decoder", ",", "using", "the", "architecture", "described", "in", ".", "We", "use", "an", "N", "-", "gram", "language", "model", "trained", "and", "pruned", "with", "the", "SRILM", "toolkit", ".", "The", "first", "-", "pass", "LM", "has", "approximately", "15.9", "million", "bigrams", ",", "trigrams", ",", "and", "4grams", ",", "and", "a", "vocabulary", "of", "30", ",", "500", "words", ".", "It", "gives", "a", "perplexity", "of", "69", "on", "the", "1997", "CTS", "evaluation", "transcripts", ".", "The", "initial", "decoding", "produces", "a", "lattice", "with", "the", "pronunciation", "variants", "marked", ",", "from", "which", "500", "-", "best", "lists", "are", "generated", "for", "rescoring", "purposes", ".", "Subsequent", "N", "-", "best", "rescoring", "uses", "an", "unpruned", "LM", "comprising", "145", "million", "N", "-", "grams", ".", "All", "N", "-", "gram", "LMs", "were", "estimated", "by", "a", "maximum", "entropy", "criterion", "as", "described", "in", ".", "The", "N", "-", "best", "hypotheses", "are", "then", "rescored", "using", "a", "combination", "of", "the", "large", "N", "-", "gram", "LM", "and", "several", "neural", "net", "LMs", ".", "We", "have", "experimented", "with", "both", "RNN", "LMs", "and", "LSTM", "LMs", ",", "and", "describe", "the", "details", "in", "the", "following", "two", "sections", ".", "subsection", ":", "RNN", "-", "LM", "setup", "Our", "RNN", "-", "LMs", "are", "trained", "and", "evaluated", "using", "the", "CUED", "-", "RNNLM", "toolkit", ".", "Our", "RNN", "-", "LM", "configuration", "has", "several", "distinctive", "features", ",", "as", "described", "below", ".", "We", "trained", "both", "standard", ",", "forward", "-", "predicting", "RNN", "-", "LMs", "and", "backward", "RNN", "-", "LMs", "that", "predict", "words", "in", "reverse", "temporal", "order", ".", "The", "log", "probabilities", "from", "both", "models", "are", "added", ".", "As", "is", "customary", ",", "the", "RNN", "-", "LM", "probability", "estimates", "are", "interpolated", "at", "the", "word", "-", "level", "with", "corresponding", "N", "-", "gram", "LM", "probabilities", "(", "separately", "for", "the", "forward", "and", "backward", "models", ")", ".", "In", "addition", ",", "we", "trained", "a", "second", "RNN", "-", "LM", "for", "each", "direction", ",", "obtained", "by", "starting", "with", "different", "random", "initial", "weights", ".", "The", "two", "RNN", "-", "LMs", "and", "the", "N", "-", "gram", "LM", "for", "each", "direction", "are", "interpolated", "with", "weights", "of", "(", "0.375", ",", "0.375", ",", "0.25", ")", ".", "In", "order", "to", "make", "use", "of", "LM", "training", "data", "that", "is", "not", "fully", "matched", "to", "the", "target", "conversational", "speech", "domain", ",", "we", "start", "RNN", "-", "LM", "training", "with", "the", "union", "of", "in", "-", "domain", "(", "here", ",", "CTS", ")", "and", "out", "-", "of", "-", "domain", "(", "e.g.", ",", "Web", ")", "data", ".", "Upon", "convergence", ",", "the", "network", "undergoes", "a", "second", "training", "phase", "using", "the", "in", "-", "domain", "data", "only", ".", "Both", "training", "phases", "use", "in", "-", "domain", "validation", "data", "to", "regulate", "the", "learning", "rate", "schedule", "and", "termination", ".", "Because", "the", "size", "of", "the", "out", "-", "of", "-", "domain", "data", "is", "a", "multiple", "of", "the", "in", "-", "domain", "data", ",", "a", "standard", "training", "on", "a", "simple", "union", "of", "the", "data", "would", "not", "yield", "a", "well", "-", "matched", "model", ",", "and", "have", "poor", "perplexity", "in", "the", "target", "domain", ".", "We", "found", "best", "results", "with", "an", "RNN", "-", "LM", "configuration", "that", "had", "a", "second", ",", "non", "-", "recurrent", "hidden", "layer", ".", "This", "produced", "lower", "perplexity", "and", "word", "error", "than", "the", "standard", ",", "single", "-", "hidden", "-", "layer", "RNN", "-", "LM", "architecture", ".", "The", "overall", "network", "architecture", "thus", "had", "two", "hidden", "layers", "with", "1000", "units", "each", ",", "using", "ReLU", "nonlinearities", ".", "Training", "used", "noise", "-", "contrastive", "estimation", "(", "NCE", ")", ".", "The", "RNN", "-", "LM", "output", "vocabulary", "consists", "of", "all", "words", "occurring", "more", "than", "once", "in", "the", "in", "-", "domain", "training", "set", ".", "While", "the", "RNN", "-", "LM", "estimates", "a", "probability", "for", "unknown", "words", ",", "we", "take", "a", "different", "approach", "in", "rescoring", ":", "The", "number", "of", "out", "-", "of", "-", "set", "words", "is", "recorded", "for", "each", "hypothesis", "and", "a", "penalty", "for", "them", "is", "estimated", "for", "them", "when", "optimizing", "the", "relative", "weights", "for", "all", "model", "scores", "(", "acoustic", ",", "LM", ",", "pronunciation", ")", ",", "using", "the", "SRILM", "nbest", "-", "optimize", "tool", ".", "subsection", ":", "LSTM", "-", "LM", "setup", "After", "obtaining", "good", "results", "with", "RNN", "-", "LMs", "we", "also", "explored", "the", "LSTM", "recurrent", "network", "architecture", "for", "language", "modeling", ",", "inspired", "by", "recent", "work", "showing", "gains", "over", "RNN", "-", "LMs", "for", "conversational", "speech", "recognition", ".", "In", "addition", "to", "applying", "the", "lessons", "learned", "from", "our", "RNN", "-", "LM", "experiments", ",", "we", "explored", "additional", "alternatives", ",", "as", "described", "below", ".", "There", "are", "two", "types", "of", "input", "vectors", "our", "LSTM", "LMs", "take", ",", "word", "based", "one", "-", "hot", "vector", "input", "and", "letter", "trigram", "vector", "input", ".", "Including", "both", "forward", "and", "backward", "models", ",", "we", "trained", "four", "different", "LSTM", "LMs", "in", "total", ".", "For", "the", "word", "based", "input", ",", "we", "leveraged", "the", "approach", "from", "to", "tie", "the", "input", "embedding", "and", "output", "embedding", "together", ".", "Here", "we", "also", "used", "a", "two", "-", "phase", "training", "schedule", "to", "train", "the", "LSTM", "LMs", ".", "First", "we", "train", "the", "model", "on", "the", "combination", "of", "in", "-", "domain", "and", "out", "-", "domain", "data", "for", "four", "data", "passes", "without", "any", "learning", "rate", "adjustment", ".", "We", "then", "start", "from", "the", "resulting", "model", "and", "train", "on", "in", "-", "domain", "data", "until", "convergence", ".", "Overall", "the", "letter", "trigram", "based", "models", "perform", "a", "little", "better", "than", "the", "word", "based", "language", "model", ".", "We", "tried", "applying", "dropout", "on", "both", "types", "of", "language", "models", "but", "did", "n\u2019t", "see", "an", "improvement", ".", "Convergence", "was", "improved", "through", "a", "variation", "of", "self", "-", "stabilization", ",", "in", "which", "each", "output", "vector", "of", "non", "-", "linearities", "are", "scaled", "by", ",", "where", "a", "is", "a", "scalar", "that", "is", "learned", "for", "each", "output", ".", "This", "has", "a", "similar", "effect", "as", "the", "scale", "of", "the", "well", "-", "known", "batch", "normalization", "technique", ",", "but", "can", "be", "used", "in", "recurrent", "loops", ".", "Table", "[", "reference", "]", "shows", "the", "impact", "of", "number", "of", "layers", "on", "the", "final", "perplexities", ".", "Based", "on", "this", ",", "we", "proceeded", "with", "three", "hidden", "layers", ",", "with", "1000", "hidden", "units", "each", ".", "The", "perplexities", "of", "each", "LSTM", "-", "LM", "we", "used", "in", "the", "final", "combination", "(", "before", "interpolating", "with", "the", "N", "-", "gram", "model", ")", "can", "be", "found", "in", "Table", "[", "reference", "]", ".", "For", "the", "final", "system", ",", "we", "interpolated", "two", "LSTM", "-", "LMs", "with", "an", "N", "-", "gram", "LM", "for", "the", "forward", "-", "direction", "LM", ",", "and", "similarly", "for", "the", "backward", "-", "direction", "LM", ".", "All", "LSTMs", "use", "three", "hidden", "layers", "and", "are", "trained", "on", "in", "-", "domain", "and", "web", "data", ".", "Unlike", "for", "the", "RNN", "-", "LMs", ",", "the", "two", "models", "being", "interpolated", "differ", "not", "just", "in", "their", "random", "initialization", ",", "but", "also", "in", "the", "input", "encoding", "(", "one", "uses", "a", "triletter", "encoding", ",", "the", "other", "a", "one", "-", "hot", "word", "encoding", ")", ".", "The", "forward", "and", "backward", "LM", "log", "probability", "scores", "are", "combined", "additively", ".", "subsection", ":", "Training", "data", "The", "4", "-", "gram", "language", "model", "for", "decoding", "was", "trained", "on", "the", "available", "CTS", "transcripts", "from", "the", "DARPA", "EARS", "program", ":", "Switchboard", "(", "3", "M", "words", ")", ",", "BBN", "Switchboard", "-", "2", "transcripts", "(", "850k", ")", ",", "Fisher", "(", "21", "M", ")", ",", "English", "CallHome", "(", "200k", ")", ",", "and", "the", "University", "of", "Washington", "conversational", "Web", "corpus", "(", "191", "M", ")", ".", "A", "separate", "N", "-", "gram", "model", "was", "trained", "from", "each", "source", "and", "interpolated", "with", "weights", "optimized", "on", "RT", "-", "03", "transcripts", ".", "For", "the", "unpruned", "large", "rescoring", "4", "-", "gram", ",", "an", "additional", "LM", "component", "was", "added", ",", "trained", "on", "133", "M", "word", "of", "LDC", "Broadcast", "News", "texts", ".", "The", "N", "-", "gram", "LM", "configuration", "is", "modeled", "after", "that", "described", "in", ",", "except", "that", "maxent", "smoothing", "was", "used", ".", "The", "RNN", "and", "LSTM", "LMs", "were", "trained", "on", "Switchboard", "and", "Fisher", "transcripts", "as", "in", "-", "domain", "data", "(", "20", "M", "words", "for", "gradient", "computation", ",", "3", "M", "for", "validation", ")", ".", "To", "this", "we", "added", "62", "M", "words", "of", "UW", "Web", "data", "as", "out", "-", "of", "-", "domain", "data", ",", "for", "use", "in", "the", "two", "-", "phase", "training", "procedure", "described", "above", ".", "subsection", ":", "RNN", "-", "LM", "and", "LSTM", "-", "LM", "performance", "Table", "[", "reference", "]", "gives", "perplexity", "and", "word", "error", "performance", "for", "various", "recurrent", "neural", "net", "LM", "setups", ",", "from", "simple", "to", "more", "complex", ".", "The", "acoustic", "model", "used", "was", "the", "ResNet", "CNN", ".", "Note", "that", ",", "unlike", "the", "results", "in", "Tables", "[", "reference", "]", "and", "[", "reference", "]", ",", "the", "neural", "net", "LMs", "in", "Table", "[", "reference", "]", "are", "interpolated", "with", "the", "N", "-", "gram", "LM", ".", "As", "can", "be", "seen", ",", "each", "of", "the", "measures", "described", "earlier", "adds", "incremental", "gains", ",", "which", ",", "however", ",", "add", "up", "to", "a", "substantial", "improvement", "overall", ".", "The", "total", "gain", "relative", "to", "a", "purely", "N", "-", "gram", "based", "system", "is", "a", "20", "%", "relative", "error", "reduction", "with", "RNN", "-", "LMs", ",", "and", "23", "%", "with", "LSTM", "-", "LMs", ".", "As", "shown", "later", "(", "see", "Table", "[", "reference", "]", ")", "the", "gains", "with", "different", "acoustic", "models", "are", "similar", ".", "subsection", ":", "System", "Combination", "The", "LM", "rescoring", "is", "carried", "out", "separately", "for", "each", "acoustic", "model", ".", "The", "rescored", "N", "-", "best", "lists", "from", "each", "subsystem", "are", "then", "aligned", "into", "a", "single", "confusion", "network", "using", "the", "SRILM", "nbest", "-", "rover", "tool", ".", "However", ",", "the", "number", "of", "potential", "candidate", "systems", "is", "too", "large", "to", "allow", "an", "all", "-", "out", "combination", ",", "both", "for", "practical", "reasons", "and", "due", "to", "overfitting", "issues", ".", "Instead", ",", "we", "perform", "a", "greedy", "search", ",", "starting", "with", "the", "single", "best", "system", ",", "and", "successively", "adding", "additional", "systems", ",", "to", "find", "a", "small", "set", "of", "systems", "that", "are", "maximally", "complementary", ".", "The", "RT", "-", "02", "Switchboard", "set", "was", "used", "for", "this", "search", "procedure", ".", "We", "experimented", "with", "two", "search", "algorithms", "to", "find", "good", "subsets", "of", "systems", ".", "We", "always", "start", "with", "the", "system", "giving", "the", "best", "individual", "accuracy", "on", "the", "development", "set", ".", "In", "one", "approach", ",", "a", "greedy", "forward", "search", "then", "adds", "systems", "incrementally", "to", "the", "combination", ",", "giving", "each", "equal", "weight", ".", "If", "no", "improvement", "is", "found", "with", "any", "of", "the", "unused", "systems", ",", "we", "try", "adding", "each", "with", "successively", "lower", "relative", "weights", "of", "0.5", ",", "0.2", ",", "and", "0.1", ",", "and", "stop", "if", "none", "of", "these", "give", "an", "improvement", ".", "A", "second", "variant", "of", "the", "search", "procedure", "that", "can", "give", "lower", "error", "(", "as", "measured", "on", "the", "devset", ")", "estimates", "the", "best", "system", "weights", "for", "each", "incremental", "combination", "candidate", ".", "The", "weight", "estimation", "is", "done", "using", "an", "expectation", "-", "maximization", "algorithm", "based", "on", "aligning", "the", "reference", "words", "to", "the", "confusion", "networks", ",", "and", "maximizing", "the", "weighted", "probability", "of", "the", "correct", "word", "at", "each", "alignment", "position", ".", "To", "avoid", "overfitting", ",", "the", "weights", "for", "an", "-", "way", "combination", "are", "smoothed", "hierarchically", ",", "i.e.", ",", "interpolated", "with", "the", "weights", "from", "the", "-", "way", "system", "that", "preceded", "it", ".", "This", "tends", "to", "give", "robust", "weights", "that", "are", "biased", "toward", "the", "early", "(", "i.e.", ",", "better", ")", "subsystems", ".", "The", "final", "system", "incorporated", "a", "variety", "of", "BLSTM", "models", "with", "roughly", "similar", "performance", ",", "but", "differing", "in", "various", "metaparameters", "(", "number", "of", "senones", ",", "use", "of", "spatial", "smoothing", ",", "and", "choice", "of", "pronunciation", "dictionaries", ")", ".", "To", "further", "limit", "the", "number", "of", "free", "parameters", "to", "be", "estimated", "in", "system", "combination", ",", "we", "performed", "system", "selection", "in", "two", "stages", ".", "First", ",", "we", "selected", "the", "four", "best", "BLSTM", "systems", ".", "We", "then", "combined", "these", "with", "equal", "weights", "and", "treated", "them", "as", "a", "single", "subsystem", "in", "searching", "for", "a", "larger", "combination", "including", "other", "acoustic", "models", ".", "This", "yielded", "our", "best", "overall", "combined", "system", ",", "as", "reported", "in", "Section", "[", "reference", "]", ".", "section", ":", "Microsoft", "Cognitive", "Toolkit", "(", "CNTK", ")", "All", "neural", "networks", "in", "the", "final", "system", "were", "trained", "with", "the", "Microsoft", "Cognitive", "Toolkit", ",", "or", "CNTK", ".", "on", "a", "Linux", "-", "based", "multi", "-", "GPU", "server", "farm", ".", "CNTK", "allows", "for", "flexible", "model", "definition", ",", "while", "at", "the", "same", "time", "scaling", "very", "efficiently", "across", "multiple", "GPUs", "and", "multiple", "servers", ".", "The", "resulting", "fast", "experimental", "turnaround", "using", "the", "full", "2000", "-", "hour", "corpus", "was", "critical", "for", "our", "work", ".", "subsection", ":", "Flexible", ",", "Terse", "Model", "Definition", "In", "CNTK", ",", "a", "neural", "network", "(", "and", "the", "training", "criteria", ")", "are", "specified", "by", "its", "formula", ",", "using", "a", "custom", "functional", "language", "(", "BrainScript", ")", ",", "or", "Python", ".", "A", "graph", "-", "based", "execution", "engine", ",", "which", "provides", "automatic", "differentiation", ",", "then", "trains", "the", "model", "\u2019s", "parameters", "through", "SGD", ".", "Leveraging", "a", "stock", "library", "of", "common", "layer", "types", ",", "networks", "can", "be", "specified", "very", "tersely", ".", "Samples", "can", "be", "found", "in", ".", "subsection", ":", "Multi", "-", "Server", "Training", "using", "1", "-", "bit", "SGD", "Training", "the", "acoustic", "models", "in", "this", "paper", "on", "a", "single", "GPU", "would", "take", "many", "weeks", "or", "even", "months", ".", "CNTK", "made", "training", "times", "feasible", "by", "parallelizing", "the", "SGD", "training", "with", "our", "1", "-", "bit", "SGD", "parallelization", "technique", ".", "This", "data", "-", "parallel", "method", "distributes", "minibatches", "over", "multiple", "worker", "nodes", ",", "and", "then", "aggregates", "the", "sub", "-", "gradients", ".", "While", "the", "necessary", "communication", "time", "would", "otherwise", "be", "prohibitive", ",", "the", "1", "-", "bit", "SGD", "method", "eliminates", "the", "bottleneck", "by", "two", "techniques", ":", "1", "-", "bit", "quantization", "of", "gradients", "and", "automatic", "minibatch", "-", "size", "scaling", ".", "In", ",", "we", "showed", "that", "gradient", "values", "can", "be", "quantized", "to", "just", "a", "single", "bit", ",", "if", "one", "carries", "over", "the", "quantization", "error", "from", "one", "minibatch", "to", "the", "next", ".", "Each", "time", "a", "sub", "-", "gradient", "is", "quantized", ",", "the", "quantization", "error", "is", "computed", "and", "remembered", ",", "and", "then", "added", "to", "the", "next", "minibatch", "\u2019s", "sub", "-", "gradient", ".", "This", "reduces", "the", "required", "bandwidth", "32", "-", "fold", "with", "minimal", "loss", "in", "accuracy", ".", "Secondly", ",", "automatic", "minibatch", "-", "size", "scaling", "progressively", "decreases", "the", "frequency", "of", "model", "updates", ".", "At", "regular", "intervals", "(", "e.g.", "every", "72h", "of", "training", "data", ")", ",", "the", "trainer", "tries", "larger", "minibatch", "sizes", "on", "a", "small", "subset", "of", "data", "and", "picks", "the", "largest", "that", "maintains", "training", "loss", ".", "These", "two", "techniques", "allow", "for", "excellent", "multi", "-", "GPU", "/", "server", "scalability", ",", "and", "reduced", "the", "acoustic", "-", "model", "training", "times", "on", "2000h", "from", "months", "to", "between", "1", "and", "3", "weeks", ",", "making", "this", "work", "feasible", ".", "subsection", ":", "Computational", "performance", "Table", "[", "reference", "]", "compares", "the", "runtimes", ",", "as", "multiples", "of", "speech", "duration", ",", "of", "various", "processing", "steps", "associated", "with", "the", "different", "acoustic", "model", "architectures", "(", "figures", "for", "DNNs", "are", "given", "only", "as", "a", "reference", "point", ",", "since", "they", "are", "not", "used", "in", "our", "system", ")", ".", "Acoustic", "model", "(", "AM", ")", "training", "comprises", "the", "forward", "and", "backward", "dynamic", "programming", "passes", ",", "as", "well", "as", "parameter", "updates", ".", "AM", "evaluation", "refers", "to", "the", "forward", "computation", "only", ".", "Decoding", "includes", "AM", "evaluation", "along", "with", "hypothesis", "search", "(", "only", "the", "former", "makes", "use", "of", "the", "GPU", ")", ".", "Runtimes", "were", "measured", "on", "a", "12", "-", "core", "Intel", "Xeon", "E5", "-", "2620v3", "CPU", "clocked", "at", "2.4GHz", ",", "with", "an", "Nvidia", "Titan", "X", "GPU", ".", "We", "observe", "that", "the", "GPU", "gives", "a", "10", "to", "100", "-", "fold", "speedup", "for", "AM", "evaluation", "over", "the", "CPU", "implementation", ".", "AM", "evaluation", "is", "thus", "reduced", "to", "a", "small", "faction", "of", "overall", "decoding", "time", ",", "making", "near", "-", "realtime", "operation", "possible", ".", "section", ":", "Experiments", "and", "Results", "subsection", ":", "Speech", "corpora", "We", "train", "with", "the", "commonly", "used", "English", "CTS", "(", "Switchboard", "and", "Fisher", ")", "corpora", ".", "Evaluation", "is", "carried", "out", "on", "the", "NIST", "2000", "CTS", "test", "set", ",", "which", "comprises", "both", "Switchboard", "(", "SWB", ")", "and", "CallHome", "(", "CH", ")", "subsets", ".", "The", "waveforms", "were", "segmented", "according", "to", "the", "NIST", "partitioned", "evaluation", "map", "(", "PEM", ")", "file", ",", "with", "150ms", "of", "dithered", "silence", "padding", "added", "in", "the", "case", "of", "the", "CallHome", "conversations", ".", "The", "Switchboard", "-", "1", "portion", "of", "the", "NIST", "2002", "CTS", "test", "set", "was", "used", "for", "tuning", "and", "development", ".", "The", "acoustic", "training", "data", "is", "comprised", "by", "LDC", "corpora", "97S62", ",", "2004S13", ",", "2005S13", ",", "2004S11", "and", "2004S09", ";", "see", "for", "a", "full", "description", ".", "subsection", ":", "Acoustic", "Model", "Details", "Forty", "-", "dimensional", "log", "-", "filterbank", "features", "were", "extracted", "every", "10", "milliseconds", ",", "using", "a", "25", "-", "millisecond", "analysis", "window", ".", "The", "CNN", "models", "used", "window", "sizes", "as", "indicated", "in", "Table", "[", "reference", "]", ",", "and", "the", "LSTMs", "processed", "one", "frame", "of", "input", "at", "a", "time", ".", "The", "bulk", "of", "our", "models", "use", "three", "state", "left", "-", "to", "-", "right", "triphone", "models", "with", "9000", "tied", "states", ".", "Additionally", ",", "we", "have", "trained", "several", "models", "with", "27k", "tied", "states", ".", "The", "phonetic", "inventory", "includes", "special", "models", "for", "noise", ",", "vocalized", "-", "noise", ",", "laughter", "and", "silence", ".", "We", "use", "a", "30k", "-", "vocabulary", "derived", "from", "the", "most", "common", "words", "in", "the", "Switchboard", "and", "Fisher", "corpora", ".", "The", "decoder", "uses", "a", "statically", "compiled", "unigram", "graph", ",", "and", "dynamically", "applies", "the", "language", "model", "score", ".", "The", "unigram", "graph", "has", "about", "300k", "states", "and", "500k", "arcs", ".", "Table", "[", "reference", "]", "shows", "the", "result", "of", "i", "-", "vector", "adaptation", "and", "LFMMI", "training", "on", "several", "of", "our", "early", "systems", ".", "We", "achieve", "a", "5\u20138", "%", "relative", "improvement", "from", "i", "-", "vectors", ",", "including", "on", "CNN", "systems", ".", "The", "last", "row", "of", "Table", "[", "reference", "]", "shows", "the", "effect", "of", "LFMMI", "training", "on", "the", "different", "models", ".", "We", "see", "a", "consistent", "7\u201310", "%", "further", "relative", "reduction", "in", "error", "rate", "for", "all", "models", ".", "Considering", "the", "great", "increase", "in", "procedural", "simplicity", "of", "LFMMI", "over", "the", "previous", "practice", "of", "writing", "lattices", "and", "post", "-", "processing", "them", ",", "we", "consider", "LFMMI", "to", "be", "a", "significant", "advance", "in", "technology", ".", "subsection", ":", "Overall", "Results", "and", "Discussion", "The", "performance", "of", "all", "our", "component", "models", "is", "shown", "in", "Table", "[", "reference", "]", ",", "along", "with", "the", "BLSTM", "combination", "and", "full", "system", "combination", "results", ".", "(", "Recall", "that", "the", "four", "best", "BLSTM", "systems", "are", "combined", "with", "equal", "weights", "first", ",", "as", "described", "in", "Section", "[", "reference", "]", ".", ")", "Key", "benchmarks", "from", "the", "literature", ",", "our", "own", "best", "results", ",", "and", "the", "measured", "human", "error", "rates", "are", "compared", "in", "Table", "[", "reference", "]", ".", "All", "models", "listed", "in", "Table", "[", "reference", "]", "are", "selected", "for", "the", "combined", "systems", "for", "one", "or", "more", "of", "the", "three", "rescoring", "LMs", ".", "The", "only", "exception", "is", "the", "VGG", "+", "ResNet", "system", ",", "which", "combines", "acoustic", "senone", "posteriors", "from", "the", "VGG", "and", "ResNet", "networks", ".", "While", "this", "yields", "our", "single", "best", "acoustic", "model", ",", "only", "the", "individual", "VGG", "and", "ResNet", "models", "are", "used", "in", "the", "overall", "system", "combination", ".", "We", "also", "observe", "that", "the", "four", "model", "variants", "chosen", "for", "the", "combined", "BLSTM", "subsystem", "differ", "incrementally", "by", "one", "hyperparameter", "(", "smoothing", ",", "number", "of", "senones", ",", "dictionary", ")", ",", "and", "that", "the", "BLSTMs", "alone", "achieve", "an", "error", "that", "is", "within", "3", "%", "relative", "of", "the", "full", "system", "combination", ".", "This", "validates", "the", "rationale", "that", "choosing", "different", "hyperparameters", "is", "an", "effective", "way", "to", "obtain", "complementary", "systems", "for", "combination", "purposes", ".", "We", "also", "assessed", "the", "lower", "bound", "of", "performance", "for", "our", "lattice", "/", "N", "-", "best", "rescoring", "paradigm", ".", "The", "500", "-", "best", "lists", "from", "the", "lattices", "generated", "with", "the", "ResNet", "CNN", "system", "had", "an", "oracle", "(", "lowest", "achievable", ")", "WER", "of", "2.7", "%", "on", "the", "Switchboard", "portion", "of", "the", "NIST", "2000", "evaluation", "set", ",", "and", "an", "oracle", "WER", "of", "4.9", "%", "on", "the", "CallHome", "portion", ".", "The", "oracle", "error", "of", "the", "combined", "system", "is", "even", "lower", "(", "though", "harder", "to", "quantify", ")", "since", "(", "1", ")", "N", "-", "best", "output", "from", "all", "systems", "are", "combined", "and", "(", "2", ")", "confusion", "network", "construction", "generates", "new", "possible", "hypotheses", "not", "contained", "in", "the", "original", "N", "-", "best", "lists", ".", "With", "oracle", "error", "rates", "less", "than", "half", "the", "currently", "achieved", "actual", "error", "rates", ",", "we", "conclude", "that", "search", "errors", "are", "not", "a", "major", "limiting", "factor", "to", "even", "better", "accuracy", ".", "section", ":", "Error", "Analysis", "In", "this", "section", ",", "we", "compare", "the", "errors", "made", "by", "our", "artificial", "recognizer", "with", "those", "made", "by", "human", "transcribers", ".", "We", "find", "that", "the", "machine", "errors", "are", "substantially", "the", "same", "as", "human", "ones", ",", "with", "one", "large", "exception", ":", "confusions", "between", "backchannel", "words", "and", "hesitations", ".", "The", "distinction", "is", "that", "backchannel", "words", "like", "\u201c", "uh", "-", "huh", "\u201d", "are", "an", "acknowledgment", "of", "the", "speaker", ",", "also", "signaling", "that", "the", "speaker", "should", "keep", "talking", ",", "while", "hesitations", "like", "\u201c", "uh", "\u201d", "are", "used", "to", "indicate", "that", "the", "current", "speaker", "has", "more", "to", "say", "and", "wants", "to", "keep", "his", "or", "her", "turn", ".", "As", "turn", "-", "management", "devices", ",", "these", "two", "classes", "of", "words", "therefore", "have", "exactly", "opposite", "functions", ".", "Table", "[", "reference", "]", "shows", "the", "ten", "most", "common", "substitutions", "for", "both", "humans", "and", "the", "artificial", "system", ".", "Tables", "[", "reference", "]", "and", "[", "reference", "]", "do", "the", "same", "for", "deletions", "and", "insertions", ".", "Focusing", "on", "the", "substitutions", ",", "we", "see", "that", "by", "far", "the", "most", "common", "error", "in", "the", "ASR", "system", "is", "the", "confusion", "of", "a", "hesitation", "in", "the", "reference", "for", "a", "backchannel", "in", "the", "hypothesis", ".", "People", "do", "not", "seem", "to", "have", "this", "problem", ".", "We", "speculate", "that", "this", "is", "due", "to", "the", "nature", "of", "the", "Fisher", "training", "corpus", ",", "where", "the", "\u201c", "quick", "transcription", "\u201d", "guidelines", "were", "predominately", "used", ".", "We", "find", "that", "there", "is", "inconsistent", "treatment", "of", "backchannel", "and", "hesitation", "in", "the", "resulting", "data", ";", "the", "relatively", "poor", "performance", "of", "the", "automatic", "system", "here", "might", "simply", "be", "due", "to", "confusions", "in", "the", "training", "data", "annotations", ".", "For", "perspective", ",", "there", "are", "over", "twenty", "-", "one", "thousand", "words", "in", "each", "test", "set", ".", "Thus", "the", "errors", "due", "to", "hesitation", "/", "backchannel", "substitutions", "account", "for", "an", "error", "rate", "of", "only", "about", "0.2", "%", "absolute", ".", "The", "most", "frequent", "substitution", "for", "people", "on", "the", "Switchboard", "corpus", "was", "mistaking", "a", "hesitation", "in", "the", "reference", "for", "the", "word", "\u201c", "hmm", ".", "\u201d", "The", "scoring", "guidelines", "treat", "\u201c", "hmm", "\u201d", "as", "a", "word", "distinct", "from", "backchannels", "and", "hesitations", ",", "so", "this", "is", "not", "a", "scoring", "mistake", ".", "Examination", "of", "the", "contexts", "in", "which", "the", "error", "is", "made", "show", "that", "it", "is", "most", "often", "intended", "to", "acknowledge", "the", "other", "speaker", ",", "i.e.", "as", "a", "backchannel", ".", "For", "both", "people", "and", "our", "automated", "system", ",", "the", "insertion", "and", "deletion", "patterns", "are", "similar", ":", "short", "function", "words", "are", "by", "far", "the", "most", "frequent", "errors", ".", "In", "particular", ",", "the", "single", "most", "common", "error", "made", "by", "the", "transcribers", "was", "to", "omit", "the", "word", "\u201c", "I.", "\u201d", "While", "we", "believe", "further", "improvement", "in", "function", "and", "content", "words", "is", "possible", ",", "the", "significance", "of", "the", "remaining", "backchannel", "/", "hesitation", "confusions", "is", "unclear", ".", "Table", "[", "reference", "]", "shows", "the", "overall", "error", "rates", "broken", "down", "by", "substitutions", ",", "insertions", "and", "deletions", ".", "We", "see", "that", "the", "human", "transcribers", "have", "a", "somewhat", "lower", "substitution", "rate", ",", "and", "a", "higher", "deletion", "rate", ".", "The", "relatively", "higher", "deletion", "rate", "might", "reflect", "a", "human", "bias", "to", "avoid", "outputting", "uncertain", "information", ",", "or", "the", "productivity", "demands", "on", "a", "professional", "transcriber", ".", "In", "all", "cases", ",", "the", "number", "of", "insertions", "is", "relatively", "small", ".", "section", ":", "Relation", "to", "Prior", "Work", "Compared", "to", "earlier", "applications", "of", "CNNs", "to", "speech", "recognition", ",", "our", "networks", "are", "much", "deeper", ",", "and", "use", "linear", "bypass", "connections", "across", "convolutional", "layers", ".", "They", "are", "similar", "in", "spirit", "to", "those", "studied", "more", "recently", "by", ".", "We", "improve", "on", "these", "architectures", "with", "the", "LACE", "model", ",", "which", "iteratively", "expands", "the", "effective", "window", "size", ",", "layer", "-", "by", "-", "layer", ",", "and", "adds", "an", "attention", "mask", "to", "differentially", "weight", "distant", "context", ".", "Our", "spatial", "regularization", "technique", "is", "similar", "in", "spirit", "to", "stimulated", "deep", "neural", "networks", ".", "Whereas", "stimulated", "networks", "use", "a", "supervision", "signal", "to", "encourage", "locality", "of", "activations", "in", "the", "model", ",", "our", "technique", "is", "automatic", ".", "Our", "use", "of", "lattice", "-", "free", "MMI", "is", "distinctive", ",", "and", "extends", "previous", "work", "by", "proposing", "the", "use", "of", "a", "mixed", "triphone", "/", "phoneme", "history", "in", "the", "language", "model", ".", "On", "the", "language", "modeling", "side", ",", "we", "achieve", "a", "performance", "boost", "by", "combining", "multiple", "LSTM", "-", "LMs", "in", "both", "forward", "and", "backward", "directions", ",", "and", "by", "using", "a", "two", "-", "phase", "training", "regimen", "to", "get", "best", "results", "from", "out", "-", "of", "-", "domain", "data", ".", "For", "our", "best", "CNN", "system", ",", "LSTM", "-", "LM", "rescoring", "yields", "a", "relative", "word", "error", "reduction", "of", "23", "%", ",", "and", "a", "20", "%", "relative", "gain", "for", "the", "combined", "recognition", "system", ",", "considerably", "larger", "than", "previously", "reported", "for", "conversational", "speech", "recognition", ".", "section", ":", "Conclusions", "We", "have", "measured", "the", "human", "error", "rate", "on", "NIST", "\u2019s", "2000", "conversational", "telephone", "speech", "recognition", "task", ".", "We", "find", "that", "there", "is", "a", "great", "deal", "of", "variability", "between", "the", "Switchboard", "and", "CallHome", "subsets", ",", "with", "5.8", "%", "and", "11.0", "%", "error", "rates", "respectively", ".", "For", "the", "first", "time", ",", "we", "report", "automatic", "recognition", "performance", "on", "par", "with", "human", "performance", "on", "this", "task", ".", "Our", "system", "\u2019s", "performance", "can", "be", "attributed", "to", "the", "systematic", "use", "of", "LSTMs", "for", "both", "acoustic", "and", "language", "modeling", ",", "as", "well", "as", "CNNs", "in", "the", "acoustic", "model", ",", "and", "extensive", "combination", "of", "complementary", "system", "for", "both", "acoustic", "and", "language", "modeling", ".", "section", ":", "Acknowledgments", "We", "thank", "Arul", "Menezes", "for", "access", "to", "the", "Microsoft", "transcription", "pipeline", ";", "Chris", "Basoglu", ",", "Amit", "Agarwal", "and", "Marko", "Radmilac", "for", "their", "invaluable", "assistance", "with", "CNTK", ";", "Jinyu", "Li", "and", "Partha", "Parthasarathy", "for", "many", "helpful", "conversations", ".", "We", "also", "thank", "X.", "Chen", "from", "Cambridge", "University", "for", "valuable", "assistance", "with", "the", "CUED", "-", "RNNLM", "toolkit", ",", "and", "the", "International", "Computer", "Science", "Institute", "for", "compute", "and", "data", "resources", ".", "bibliography", ":", "References"]}