{"coref": {"Accuracy": [[113, 115], [133, 135], [363, 364], [574, 576], [685, 689], [829, 830], [844, 848], [867, 871], [895, 896], [1363, 1366], [2835, 2839], [2887, 2888], [2918, 2919], [3495, 3497], [3524, 3528], [3618, 3620], [3702, 3704], [3935, 3937], [4110, 4112], [4474, 4478], [5595, 5599]], "Adversarial_Bi-LSTM": [], "Avg_accuracy": [], "Part-Of-Speech_Tagging": [[2, 10], [227, 228], [533, 535], [675, 676], [794, 799], [928, 930], [979, 981], [2871, 2873], [3025, 3027], [3650, 3651], [3886, 3887], [4231, 4233], [4247, 4249], [4514, 4516], [5135, 5136]], "Penn_Treebank": [[87, 91], [541, 545], [834, 837], [1438, 1443], [2288, 2291], [2526, 2528], [2529, 2530], [2860, 2864], [4303, 4306], [4570, 4575], [5286, 5289]], "UD": [[93, 95], [96, 97], [2292, 2293], [2579, 2581], [2582, 2583], [2968, 2969], [3143, 3144], [3220, 3221], [3227, 3228], [3421, 3422], [3555, 3556], [4082, 4083], [4353, 4354], [4538, 4539], [4579, 4580], [550, 552]]}, "coref_non_salient": {"0": [[2256, 2258], [4317, 4318]], "1": [[4547, 4550], [4947, 4949]], "10": [[1399, 1400], [1478, 1479], [1519, 1520], [1647, 1648], [492, 493], [1347, 1348], [1429, 1430], [1457, 1458], [1491, 1492], [1524, 1525], [1583, 1584], [1618, 1619], [1714, 1715], [2701, 2702], [2751, 2752], [2881, 2882], [3014, 3015], [3875, 3876], [5221, 5222], [5383, 5384], [5475, 5476]], "100": [[2781, 2787]], "101": [[4067, 4069], [4337, 4339]], "102": [[4320, 4322]], "103": [[1181, 1183]], "104": [[2788, 2789]], "105": [[2490, 2494]], "106": [[172, 175]], "107": [[578, 584], [4560, 4564]], "108": [[209, 214]], "109": [[1187, 1190]], "11": [[352, 354], [1008, 1010], [1062, 1064]], "110": [[3095, 3099]], "111": [[2153, 2154]], "112": [[2408, 2410]], "113": [[2653, 2654]], "114": [[1084, 1086]], "115": [[1570, 1573]], "116": [[1266, 1268]], "117": [[1503, 1505]], "118": [[1238, 1241]], "119": [[1255, 1257]], "12": [[4164, 4166], [4192, 4194], [4334, 4336]], "120": [[3613, 3617]], "121": [[1376, 1381]], "122": [[1388, 1392]], "123": [[851, 852]], "124": [[3433, 3435]], "125": [[4365, 4366]], "126": [[5241, 5242]], "127": [[1416, 1421]], "128": [[5462, 5465]], "13": [[156, 158], [810, 812], [934, 936], [3481, 3483], [4147, 4149], [4520, 4522], [5613, 5615]], "14": [[187, 190], [765, 768], [1314, 1317], [1537, 1540], [5204, 5207], [5228, 5231], [5247, 5250], [5470, 5473], [5653, 5657]], "15": [[1258, 1260], [5042, 5043]], "16": [[481, 482], [800, 801], [916, 917], [1045, 1046], [1234, 1235], [1885, 1886], [2914, 2915], [3339, 3340], [3357, 3358], [3397, 3398], [3439, 3440], [3758, 3759], [4589, 4590], [4698, 4699], [53, 54], [166, 167], [180, 181], [350, 351], [375, 376], [430, 431], [671, 672], [717, 718], [740, 741], [758, 759], [954, 955], [1055, 1056], [1082, 1083], [1095, 1096], [1127, 1128], [2982, 2983], [3032, 3033], [3061, 3062], [3072, 3073], [3153, 3154], [3468, 3469], [3583, 3584], [3641, 3642], [3720, 3721], [4291, 4292], [4468, 4469], [4506, 4507], [4835, 4836], [5215, 5216]], "17": [[29, 31], [1164, 1166], [1736, 1738], [3059, 3060]], "18": [[1749, 1750], [1756, 1757], [1814, 1815], [1825, 1826]], "19": [[710, 711], [856, 857], [1696, 1697], [3449, 3450], [3492, 3493], [3956, 3957], [5648, 5649]], "2": [[2799, 2801], [3453, 3455]], "20": [[1303, 1304], [5549, 5550], [5568, 5569]], "21": [[1242, 1243], [1245, 1246]], "22": [[11, 13], [21, 23], [270, 272], [310, 312], [330, 332], [447, 449], [487, 489], [504, 506], [523, 525], [592, 595], [626, 628], [638, 640], [780, 782], [913, 915], [994, 996], [999, 1001], [1042, 1044], [1231, 1233], [1261, 1263], [1295, 1297], [1729, 1731], [1731, 1733], [1851, 1853], [1882, 1884], [2366, 2368], [2411, 2413], [2480, 2482], [2911, 2913], [3436, 3438], [4253, 4255], [4586, 4588], [4686, 4688], [4832, 4834], [4964, 4966], [5532, 5534], [5554, 5556], [2757, 2759]], "23": [[3485, 3488], [4960, 4963]], "24": [[1173, 1174], [2746, 2747], [3311, 3312], [3355, 3356]], "25": [[1574, 1575], [1649, 1650], [494, 495], [1326, 1327], [1534, 1535], [1625, 1626], [1716, 1717], [2883, 2884], [3016, 3017], [3877, 3878], [5223, 5224], [5385, 5386], [5477, 5478]], "26": [[5232, 5233], [5392, 5393], [5435, 5436]], "27": [[4344, 4346]], "28": [[50, 51], [302, 303], [372, 373], [405, 406], [1013, 1014], [1747, 1748], [4124, 4125], [4932, 4933], [5537, 5538], [5588, 5589]], "29": [[24, 25], [152, 153], [198, 199], [333, 334], [641, 642], [649, 650], [5557, 5558], [80, 81], [107, 108], [1102, 1103], [1159, 1160], [1201, 1202], [5021, 5022], [5376, 5377], [5415, 5416]], "3": [[1672, 1673], [5167, 5168], [5197, 5200]], "30": [[3478, 3480], [3905, 3907]], "31": [[1590, 1592], [2142, 2143]], "32": [[2674, 2678], [4669, 4670]], "33": [[4216, 4218]], "34": [[5238, 5239], [5318, 5319], [5400, 5401], [5437, 5438]], "35": [[4707, 4712]], "36": [[792, 793], [890, 891], [3632, 3633], [3790, 3791], [4107, 4108]], "37": [[5311, 5314], [5356, 5359]], "38": [[2803, 2806]], "39": [[5190, 5192]], "4": [[1355, 1361], [1464, 1468], [1507, 1511]], "40": [[826, 827], [1586, 1587], [3818, 3819], [3902, 3903], [5584, 5585]], "41": [[1287, 1288], [4446, 4447], [4469, 4470], [4493, 4494], [4507, 4508], [4600, 4601], [4636, 4637], [4854, 4855], [4884, 4885], [5422, 5423], [5478, 5479]], "42": [[968, 969], [4405, 4406], [4415, 4416], [4438, 4439], [4445, 4446]], "43": [[955, 956], [4142, 4143], [4168, 4169], [4199, 4200], [4207, 4208], [4211, 4212], [4277, 4278], [4286, 4287], [4297, 4298], [4399, 4400], [4404, 4405]], "44": [[32, 34], [241, 243]], "45": [[1070, 1072], [2319, 2321], [5170, 5172]], "46": [[4716, 4719], [4850, 4852]], "47": [[1682, 1684], [1810, 1812], [1964, 1966]], "48": [[5488, 5492]], "49": [[5056, 5058]], "5": [[3585, 3588], [4527, 4529], [4700, 4703], [5023, 5026], [5119, 5122], [5632, 5635]], "50": [[229, 230], [965, 966], [4222, 4223], [4498, 4499], [160, 161], [703, 704], [815, 816], [939, 940], [961, 962], [3531, 3532], [4151, 4152], [4240, 4241], [4257, 4258], [4270, 4271], [4422, 4423], [5617, 5618]], "51": [[5378, 5380]], "52": [[2604, 2607]], "53": [[1394, 1398], [1321, 1325]], "54": [[223, 224], [383, 384], [466, 467], [529, 530], [632, 633], [986, 987], [1132, 1133], [1874, 1875], [4941, 4942], [5644, 5645], [783, 784], [806, 807], [1156, 1157], [1178, 1179], [1251, 1252], [3470, 3471]], "55": [[4059, 4061]], "56": [[3602, 3604]], "57": [[2811, 2815]], "58": [[1706, 1708]], "59": [[4551, 4553]], "6": [[61, 64], [200, 203], [219, 222], [342, 345], [450, 453], [1058, 1061], [3843, 3846], [5671, 5674]], "60": [[3239, 3241], [3328, 3330]], "61": [[4654, 4656]], "62": [[5405, 5406]], "63": [[5521, 5523]], "64": [[2031, 2033]], "65": [[1692, 1693]], "66": [[3754, 3757]], "67": [[2014, 2017]], "68": [[4099, 4103]], "69": [[2852, 2854]], "7": [[296, 298], [1048, 1050], [1742, 1744], [5234, 5237]], "70": [[232, 233]], "71": [[328, 329]], "72": [[4667, 4668]], "73": [[475, 480]], "74": [[2767, 2768]], "75": [[4797, 4799]], "76": [[3571, 3575]], "77": [[2010, 2013]], "78": [[1908, 1913]], "79": [[2003, 2006]], "8": [[1383, 1387], [1432, 1433]], "80": [[4565, 4567]], "81": [[1596, 1600]], "82": [[1660, 1661], [5606, 5607], [5639, 5640]], "83": [[5505, 5508]], "84": [[2488, 2489], [498, 499], [598, 599], [1286, 1287], [1330, 1331], [1855, 1856], [2172, 2173], [2312, 2313], [2468, 2469], [2755, 2756], [2879, 2880], [2908, 2909], [2938, 2939], [2989, 2990], [3012, 3013], [3161, 3162], [3272, 3273], [3297, 3298], [3727, 3728], [3964, 3965], [4005, 4006], [4282, 4283], [4480, 4481], [4504, 4505], [4675, 4676], [4845, 4846], [5382, 5383]], "85": [[1191, 1193]], "86": [[75, 76], [1154, 1155], [5224, 5225]], "87": [[149, 150]], "88": [[4347, 4348]], "89": [[4340, 4342]], "9": [[499, 500], [599, 600], [731, 732], [878, 879], [1856, 1857], [2535, 2536], [2565, 2566], [2587, 2588], [2929, 2930], [3260, 3261], [3315, 3316], [3599, 3600], [3991, 3992], [5669, 5670]], "90": [[1272, 1275]], "91": [[3367, 3369]], "92": [[3475, 3477]], "93": [[2555, 2556]], "94": [[2761, 2764]], "95": [[1677, 1681]], "96": [[4069, 4073]], "97": [[696, 700]], "98": [[2021, 2027]], "99": [[3122, 3124]]}, "doc_id": "1751668492bac56f0ae2b6410417515ab3215945", "method_subrelations": {"Adversarial_Bi-LSTM": [[[0, 19], "Adversarial_Bi-LSTM"]]}, "n_ary_relations": [{"Material": "Penn_Treebank", "Method": "Adversarial_Bi-LSTM", "Metric": "Accuracy", "Task": "Part-Of-Speech_Tagging", "score": "97.59"}, {"Material": "UD", "Method": "Adversarial_Bi-LSTM", "Metric": "Avg_accuracy", "Task": "Part-Of-Speech_Tagging", "score": "96.73"}], "ner": [[2, 10, "Task"], [11, 13, "Method"], [21, 23, "Method"], [24, 25, "Method"], [29, 31, "Method"], [32, 34, "Method"], [50, 51, "Metric"], [61, 64, "Task"], [87, 91, "Material"], [93, 95, "Material"], [96, 97, "Material"], [113, 115, "Metric"], [133, 135, "Metric"], [149, 150, "Metric"], [152, 153, "Method"], [156, 158, "Task"], [172, 175, "Method"], [187, 190, "Task"], [198, 199, "Method"], [200, 203, "Task"], [209, 214, "Method"], [219, 222, "Task"], [223, 224, "Task"], [227, 228, "Task"], [229, 230, "Task"], [232, 233, "Task"], [241, 243, "Method"], [270, 272, "Method"], [296, 298, "Task"], [302, 303, "Metric"], [310, 312, "Method"], [328, 329, "Method"], [330, 332, "Method"], [333, 334, "Method"], [342, 345, "Task"], [352, 354, "Task"], [363, 364, "Metric"], [372, 373, "Metric"], [383, 384, "Task"], [405, 406, "Metric"], [447, 449, "Method"], [450, 453, "Task"], [466, 467, "Task"], [475, 480, "Method"], [481, 482, "Method"], [487, 489, "Method"], [504, 506, "Method"], [523, 525, "Method"], [529, 530, "Task"], [533, 535, "Task"], [541, 545, "Material"], [574, 576, "Metric"], [578, 584, "Task"], [592, 595, "Method"], [626, 628, "Method"], [632, 633, "Task"], [638, 640, "Method"], [641, 642, "Method"], [649, 650, "Method"], [675, 676, "Task"], [685, 689, "Metric"], [696, 700, "Task"], [765, 768, "Task"], [780, 782, "Method"], [794, 799, "Task"], [800, 801, "Method"], [810, 812, "Task"], [829, 830, "Metric"], [834, 837, "Material"], [844, 848, "Metric"], [851, 852, "Method"], [867, 871, "Metric"], [895, 896, "Metric"], [913, 915, "Method"], [916, 917, "Method"], [928, 930, "Task"], [934, 936, "Task"], [965, 966, "Task"], [979, 981, "Task"], [986, 987, "Task"], [994, 996, "Method"], [999, 1001, "Method"], [1008, 1010, "Task"], [1013, 1014, "Metric"], [1042, 1044, "Method"], [1045, 1046, "Method"], [1048, 1050, "Task"], [1058, 1061, "Task"], [1062, 1064, "Task"], [1070, 1072, "Method"], [1084, 1086, "Task"], [1132, 1133, "Task"], [1164, 1166, "Method"], [1173, 1174, "Method"], [1181, 1183, "Task"], [1187, 1190, "Method"], [1191, 1193, "Task"], [1231, 1233, "Method"], [1234, 1235, "Method"], [1238, 1241, "Method"], [1242, 1243, "Method"], [1245, 1246, "Method"], [1255, 1257, "Task"], [1258, 1260, "Task"], [1261, 1263, "Method"], [1266, 1268, "Method"], [1272, 1275, "Task"], [1295, 1297, "Method"], [1314, 1317, "Task"], [1355, 1361, "Method"], [1363, 1366, "Metric"], [1376, 1381, "Method"], [1383, 1387, "Method"], [1388, 1392, "Method"], [1394, 1398, "Method"], [1399, 1400, "Method"], [1416, 1421, "Task"], [1432, 1433, "Method"], [1438, 1443, "Material"], [1464, 1468, "Method"], [1478, 1479, "Method"], [1503, 1505, "Method"], [1507, 1511, "Method"], [1519, 1520, "Method"], [1537, 1540, "Task"], [1570, 1573, "Method"], [1574, 1575, "Method"], [1590, 1592, "Task"], [1596, 1600, "Task"], [1647, 1648, "Method"], [1649, 1650, "Method"], [1672, 1673, "Task"], [1677, 1681, "Metric"], [1682, 1684, "Metric"], [1692, 1693, "Task"], [1706, 1708, "Method"], [1729, 1731, "Method"], [1731, 1733, "Method"], [1736, 1738, "Method"], [1742, 1744, "Task"], [1747, 1748, "Metric"], [1749, 1750, "Method"], [1756, 1757, "Method"], [1810, 1812, "Metric"], [1814, 1815, "Method"], [1825, 1826, "Method"], [1851, 1853, "Method"], [1874, 1875, "Task"], [1882, 1884, "Method"], [1885, 1886, "Method"], [1908, 1913, "Method"], [1964, 1966, "Metric"], [2003, 2006, "Task"], [2010, 2013, "Method"], [2014, 2017, "Method"], [2021, 2027, "Metric"], [2031, 2033, "Method"], [2142, 2143, "Task"], [2153, 2154, "Method"], [2256, 2258, "Method"], [2288, 2291, "Material"], [2292, 2293, "Material"], [2319, 2321, "Method"], [2366, 2368, "Method"], [2408, 2410, "Method"], [2411, 2413, "Method"], [2480, 2482, "Method"], [2488, 2489, "Method"], [2490, 2494, "Method"], [2526, 2528, "Material"], [2529, 2530, "Material"], [2555, 2556, "Task"], [2579, 2581, "Material"], [2582, 2583, "Material"], [2604, 2607, "Method"], [2653, 2654, "Metric"], [2674, 2678, "Method"], [2746, 2747, "Method"], [2761, 2764, "Metric"], [2767, 2768, "Task"], [2781, 2787, "Method"], [2788, 2789, "Method"], [2799, 2801, "Metric"], [2803, 2806, "Metric"], [2811, 2815, "Method"], [2835, 2839, "Metric"], [2852, 2854, "Metric"], [2860, 2864, "Material"], [2871, 2873, "Task"], [2887, 2888, "Metric"], [2911, 2913, "Method"], [2914, 2915, "Method"], [2918, 2919, "Metric"], [2968, 2969, "Material"], [3025, 3027, "Task"], [3059, 3060, "Method"], [3095, 3099, "Task"], [3122, 3124, "Task"], [3143, 3144, "Material"], [3220, 3221, "Material"], [3227, 3228, "Material"], [3239, 3241, "Metric"], [3311, 3312, "Method"], [3328, 3330, "Metric"], [3339, 3340, "Method"], [3355, 3356, "Method"], [3357, 3358, "Method"], [3367, 3369, "Task"], [3397, 3398, "Method"], [3421, 3422, "Material"], [3433, 3435, "Method"], [3436, 3438, "Method"], [3439, 3440, "Method"], [3453, 3455, "Metric"], [3475, 3477, "Task"], [3478, 3480, "Method"], [3481, 3483, "Task"], [3485, 3488, "Method"], [3495, 3497, "Metric"], [3524, 3528, "Metric"], [3555, 3556, "Material"], [3571, 3575, "Metric"], [3585, 3588, "Task"], [3602, 3604, "Task"], [3613, 3617, "Method"], [3618, 3620, "Metric"], [3650, 3651, "Task"], [3702, 3704, "Metric"], [3754, 3757, "Metric"], [3758, 3759, "Method"], [3843, 3846, "Task"], [3886, 3887, "Task"], [3905, 3907, "Method"], [3935, 3937, "Metric"], [4059, 4061, "Task"], [4067, 4069, "Method"], [4069, 4073, "Metric"], [4082, 4083, "Material"], [4099, 4103, "Task"], [4110, 4112, "Metric"], [4124, 4125, "Metric"], [4147, 4149, "Task"], [4164, 4166, "Method"], [4192, 4194, "Method"], [4216, 4218, "Method"], [4222, 4223, "Task"], [4231, 4233, "Task"], [4247, 4249, "Task"], [4253, 4255, "Method"], [4303, 4306, "Material"], [4317, 4318, "Method"], [4320, 4322, "Method"], [4334, 4336, "Method"], [4337, 4339, "Method"], [4340, 4342, "Method"], [4344, 4346, "Method"], [4347, 4348, "Method"], [4353, 4354, "Material"], [4365, 4366, "Method"], [4474, 4478, "Metric"], [4498, 4499, "Task"], [4514, 4516, "Task"], [4520, 4522, "Task"], [4527, 4529, "Task"], [4538, 4539, "Material"], [4547, 4550, "Metric"], [4551, 4553, "Metric"], [4560, 4564, "Task"], [4565, 4567, "Task"], [4570, 4575, "Material"], [4579, 4580, "Material"], [4586, 4588, "Method"], [4589, 4590, "Method"], [4654, 4656, "Metric"], [4667, 4668, "Method"], [4669, 4670, "Method"], [4686, 4688, "Method"], [4698, 4699, "Method"], [4700, 4703, "Task"], [4707, 4712, "Task"], [4716, 4719, "Metric"], [4797, 4799, "Metric"], [4832, 4834, "Method"], [4850, 4852, "Metric"], [4932, 4933, "Metric"], [4941, 4942, "Task"], [4947, 4949, "Metric"], [4960, 4963, "Method"], [4964, 4966, "Method"], [5023, 5026, "Task"], [5042, 5043, "Task"], [5056, 5058, "Task"], [5119, 5122, "Task"], [5135, 5136, "Task"], [5167, 5168, "Task"], [5190, 5192, "Method"], [5197, 5200, "Task"], [5204, 5207, "Task"], [5228, 5231, "Task"], [5232, 5233, "Task"], [5234, 5237, "Task"], [5238, 5239, "Task"], [5241, 5242, "Method"], [5247, 5250, "Task"], [5286, 5289, "Material"], [5311, 5314, "Method"], [5318, 5319, "Task"], [5356, 5359, "Method"], [5378, 5380, "Metric"], [5392, 5393, "Task"], [5400, 5401, "Task"], [5405, 5406, "Method"], [5435, 5436, "Task"], [5437, 5438, "Task"], [5462, 5465, "Method"], [5470, 5473, "Task"], [5488, 5492, "Method"], [5505, 5508, "Method"], [5521, 5523, "Method"], [5532, 5534, "Method"], [5537, 5538, "Metric"], [5554, 5556, "Method"], [5557, 5558, "Method"], [5588, 5589, "Metric"], [5595, 5599, "Metric"], [5613, 5615, "Task"], [5632, 5635, "Task"], [5644, 5645, "Task"], [5653, 5657, "Task"], [5671, 5674, "Task"], [53, 54, "Method"], [75, 76, "Method"], [80, 81, "Method"], [107, 108, "Method"], [160, 161, "Task"], [166, 167, "Method"], [180, 181, "Method"], [350, 351, "Method"], [375, 376, "Method"], [430, 431, "Method"], [492, 493, "Method"], [494, 495, "Method"], [498, 499, "Method"], [499, 500, "Method"], [550, 552, "Material"], [598, 599, "Method"], [599, 600, "Method"], [671, 672, "Method"], [703, 704, "Task"], [710, 711, "Method"], [717, 718, "Method"], [731, 732, "Method"], [740, 741, "Method"], [758, 759, "Method"], [783, 784, "Task"], [792, 793, "Method"], [806, 807, "Task"], [815, 816, "Task"], [826, 827, "Method"], [856, 857, "Method"], [878, 879, "Method"], [890, 891, "Method"], [939, 940, "Task"], [954, 955, "Method"], [955, 956, "Method"], [961, 962, "Task"], [968, 969, "Method"], [1055, 1056, "Method"], [1082, 1083, "Method"], [1095, 1096, "Method"], [1102, 1103, "Method"], [1127, 1128, "Method"], [1154, 1155, "Method"], [1156, 1157, "Task"], [1159, 1160, "Method"], [1178, 1179, "Task"], [1201, 1202, "Method"], [1251, 1252, "Task"], [1286, 1287, "Method"], [1287, 1288, "Method"], [1303, 1304, "Method"], [1321, 1325, "Method"], [1326, 1327, "Method"], [1330, 1331, "Method"], [1347, 1348, "Method"], [1429, 1430, "Method"], [1457, 1458, "Method"], [1491, 1492, "Method"], [1524, 1525, "Method"], [1534, 1535, "Method"], [1583, 1584, "Method"], [1586, 1587, "Method"], [1618, 1619, "Method"], [1625, 1626, "Method"], [1660, 1661, "Method"], [1696, 1697, "Method"], [1714, 1715, "Method"], [1716, 1717, "Method"], [1855, 1856, "Method"], [1856, 1857, "Method"], [2172, 2173, "Method"], [2312, 2313, "Method"], [2468, 2469, "Method"], [2535, 2536, "Method"], [2565, 2566, "Method"], [2587, 2588, "Method"], [2701, 2702, "Method"], [2751, 2752, "Method"], [2755, 2756, "Method"], [2757, 2759, "Method"], [2879, 2880, "Method"], [2881, 2882, "Method"], [2883, 2884, "Method"], [2908, 2909, "Method"], [2929, 2930, "Method"], [2938, 2939, "Method"], [2982, 2983, "Method"], [2989, 2990, "Method"], [3012, 3013, "Method"], [3014, 3015, "Method"], [3016, 3017, "Method"], [3032, 3033, "Method"], [3061, 3062, "Method"], [3072, 3073, "Method"], [3153, 3154, "Method"], [3161, 3162, "Method"], [3260, 3261, "Method"], [3272, 3273, "Method"], [3297, 3298, "Method"], [3315, 3316, "Method"], [3449, 3450, "Method"], [3468, 3469, "Method"], [3470, 3471, "Task"], [3492, 3493, "Method"], [3531, 3532, "Task"], [3583, 3584, "Method"], [3599, 3600, "Method"], [3632, 3633, "Method"], [3641, 3642, "Method"], [3720, 3721, "Method"], [3727, 3728, "Method"], [3790, 3791, "Method"], [3818, 3819, "Method"], [3875, 3876, "Method"], [3877, 3878, "Method"], [3902, 3903, "Method"], [3956, 3957, "Method"], [3964, 3965, "Method"], [3991, 3992, "Method"], [4005, 4006, "Method"], [4107, 4108, "Method"], [4142, 4143, "Method"], [4151, 4152, "Task"], [4168, 4169, "Method"], [4199, 4200, "Method"], [4207, 4208, "Method"], [4211, 4212, "Method"], [4240, 4241, "Task"], [4257, 4258, "Task"], [4270, 4271, "Task"], [4277, 4278, "Method"], [4282, 4283, "Method"], [4286, 4287, "Method"], [4291, 4292, "Method"], [4297, 4298, "Method"], [4399, 4400, "Method"], [4404, 4405, "Method"], [4405, 4406, "Method"], [4415, 4416, "Method"], [4422, 4423, "Task"], [4438, 4439, "Method"], [4445, 4446, "Method"], [4446, 4447, "Method"], [4468, 4469, "Method"], [4469, 4470, "Method"], [4480, 4481, "Method"], [4493, 4494, "Method"], [4504, 4505, "Method"], [4506, 4507, "Method"], [4507, 4508, "Method"], [4600, 4601, "Method"], [4636, 4637, "Method"], [4675, 4676, "Method"], [4835, 4836, "Method"], [4845, 4846, "Method"], [4854, 4855, "Method"], [4884, 4885, "Method"], [5021, 5022, "Method"], [5170, 5172, "Method"], [5215, 5216, "Method"], [5221, 5222, "Method"], [5223, 5224, "Method"], [5224, 5225, "Method"], [5376, 5377, "Method"], [5382, 5383, "Method"], [5383, 5384, "Method"], [5385, 5386, "Method"], [5415, 5416, "Method"], [5422, 5423, "Method"], [5475, 5476, "Method"], [5477, 5478, "Method"], [5478, 5479, "Method"], [5549, 5550, "Method"], [5568, 5569, "Method"], [5584, 5585, "Method"], [5606, 5607, "Method"], [5617, 5618, "Task"], [5639, 5640, "Method"], [5648, 5649, "Method"], [5669, 5670, "Method"]], "sections": [[0, 204], [204, 786], [786, 790], [790, 992], [992, 1276], [1276, 1300], [1300, 1342], [1342, 1486], [1486, 1532], [1532, 1727], [1727, 1876], [1876, 2364], [2364, 2471], [2471, 2505], [2505, 2649], [2649, 2655], [2655, 2765], [2765, 2829], [2829, 2855], [2855, 2858], [2858, 2963], [2963, 3410], [3410, 3611], [3611, 3661], [3661, 3834], [3834, 4053], [4053, 4523], [4523, 4943], [4943, 5201], [5201, 5540], [5540, 5675], [5675, 5709], [5709, 5712]], "sentences": [[0, 13], [13, 19], [19, 43], [43, 65], [65, 82], [82, 141], [141, 166], [166, 176], [176, 178], [178, 191], [191, 204], [204, 207], [207, 234], [234, 261], [261, 270], [270, 293], [293, 322], [322, 346], [346, 365], [365, 385], [385, 411], [411, 424], [424, 435], [435, 454], [454, 490], [490, 516], [516, 566], [566, 584], [584, 617], [617, 649], [649, 671], [671, 690], [690, 717], [717, 734], [734, 751], [751, 769], [769, 786], [786, 790], [790, 794], [794, 817], [817, 843], [843, 867], [867, 886], [886, 906], [906, 923], [923, 941], [941, 963], [963, 992], [992, 996], [996, 1026], [1026, 1051], [1051, 1076], [1076, 1087], [1087, 1109], [1109, 1159], [1159, 1184], [1184, 1194], [1194, 1221], [1221, 1245], [1245, 1261], [1261, 1276], [1276, 1279], [1279, 1300], [1300, 1306], [1306, 1342], [1342, 1349], [1349, 1375], [1375, 1393], [1393, 1422], [1422, 1459], [1459, 1486], [1486, 1493], [1493, 1512], [1512, 1532], [1532, 1536], [1536, 1563], [1563, 1601], [1601, 1671], [1671, 1692], [1692, 1709], [1709, 1727], [1727, 1731], [1731, 1754], [1754, 1789], [1789, 1822], [1822, 1845], [1845, 1876], [1876, 1882], [1882, 1918], [1918, 1941], [1941, 1993], [1993, 2045], [2045, 2061], [2061, 2113], [2113, 2141], [2141, 2159], [2159, 2175], [2175, 2235], [2235, 2260], [2260, 2284], [2284, 2302], [2302, 2341], [2341, 2364], [2364, 2369], [2369, 2401], [2401, 2441], [2441, 2448], [2448, 2471], [2471, 2474], [2474, 2505], [2505, 2508], [2508, 2538], [2538, 2563], [2563, 2595], [2595, 2621], [2621, 2649], [2649, 2655], [2655, 2660], [2660, 2682], [2682, 2726], [2726, 2740], [2740, 2744], [2744, 2765], [2765, 2769], [2769, 2807], [2807, 2816], [2816, 2829], [2829, 2833], [2833, 2843], [2843, 2855], [2855, 2858], [2858, 2865], [2865, 2875], [2875, 2905], [2905, 2934], [2934, 2952], [2952, 2963], [2963, 2971], [2971, 2981], [2981, 3009], [3009, 3036], [3036, 3069], [3069, 3103], [3103, 3115], [3115, 3135], [3135, 3139], [3139, 3145], [3145, 3196], [3196, 3249], [3249, 3290], [3290, 3335], [3335, 3357], [3357, 3397], [3397, 3410], [3410, 3413], [3413, 3417], [3417, 3423], [3423, 3425], [3425, 3456], [3456, 3489], [3489, 3512], [3512, 3539], [3539, 3576], [3576, 3611], [3611, 3617], [3617, 3635], [3635, 3661], [3661, 3666], [3666, 3686], [3686, 3715], [3715, 3749], [3749, 3764], [3764, 3785], [3785, 3834], [3834, 3839], [3839, 3871], [3871, 3910], [3910, 3953], [3953, 3990], [3990, 4007], [4007, 4053], [4053, 4061], [4061, 4067], [4067, 4074], [4074, 4080], [4080, 4084], [4084, 4086], [4086, 4091], [4091, 4097], [4097, 4131], [4131, 4153], [4153, 4181], [4181, 4227], [4227, 4259], [4259, 4300], [4300, 4350], [4350, 4363], [4363, 4378], [4378, 4389], [4389, 4408], [4408, 4441], [4441, 4460], [4460, 4471], [4471, 4490], [4490, 4523], [4523, 4529], [4529, 4533], [4533, 4535], [4535, 4540], [4540, 4543], [4543, 4547], [4547, 4551], [4551, 4553], [4553, 4583], [4583, 4619], [4619, 4651], [4651, 4704], [4704, 4746], [4746, 4772], [4772, 4783], [4783, 4794], [4794, 4820], [4820, 4871], [4871, 4887], [4887, 4924], [4924, 4943], [4943, 4950], [4950, 4967], [4967, 4996], [4996, 5018], [5018, 5048], [5048, 5086], [5086, 5112], [5112, 5148], [5148, 5201], [5201, 5207], [5207, 5241], [5241, 5271], [5271, 5300], [5300, 5318], [5318, 5340], [5340, 5362], [5362, 5376], [5376, 5411], [5411, 5439], [5439, 5455], [5455, 5474], [5474, 5509], [5509, 5540], [5540, 5543], [5543, 5560], [5560, 5584], [5584, 5619], [5619, 5646], [5646, 5658], [5658, 5675], [5675, 5678], [5678, 5709], [5709, 5712]], "words": ["document", ":", "Robust", "Multilingual", "Part", "-", "of", "-", "Speech", "Tagging", "via", "Adversarial", "Training", "{", "michihiro.yasunaga", ",", "jungo.kasai", ",", "dragomir.radev", "}", "@yale.edu", "Adversarial", "training", "(", "AT", ")", "is", "a", "powerful", "regularization", "method", "for", "neural", "networks", ",", "aiming", "to", "achieve", "robustness", "to", "input", "perturbations", ".", "Yet", ",", "the", "specific", "effects", "of", "the", "robustness", "obtained", "from", "AT", "are", "still", "unclear", "in", "the", "context", "of", "natural", "language", "processing", ".", "In", "this", "paper", ",", "we", "propose", "and", "analyze", "a", "neural", "POS", "tagging", "model", "that", "exploits", "AT", ".", "In", "our", "experiments", "on", "the", "Penn", "Treebank", "WSJ", "corpus", "and", "the", "Universal", "Dependencies", "(", "UD", ")", "dataset", "(", "27", "languages", ")", ",", "we", "find", "that", "AT", "not", "only", "improves", "the", "overall", "tagging", "accuracy", ",", "but", "also", "1", ")", "prevents", "over", "-", "fitting", "well", "in", "low", "resource", "languages", "and", "2", ")", "boosts", "tagging", "accuracy", "for", "rare", "/", "unseen", "words", ".", "We", "also", "demonstrate", "that", "3", ")", "the", "improved", "tagging", "performance", "by", "AT", "contributes", "to", "the", "downstream", "task", "of", "dependency", "parsing", ",", "and", "that", "4", ")", "AT", "helps", "the", "model", "to", "learn", "cleaner", "word", "representations", ".", "5", ")", "The", "proposed", "AT", "model", "is", "generally", "effective", "in", "different", "sequence", "labeling", "tasks", ".", "These", "positive", "results", "motivate", "further", "use", "of", "AT", "for", "natural", "language", "tasks", ".", "section", ":", "Introduction", "Recently", ",", "neural", "network", "-", "based", "approaches", "have", "become", "popular", "in", "many", "natural", "language", "processing", "(", "NLP", ")", "tasks", "including", "tagging", ",", "parsing", ",", "and", "translation", ".", "However", ",", "it", "has", "been", "shown", "that", "neural", "networks", "tend", "to", "be", "locally", "unstable", "and", "even", "tiny", "perturbations", "to", "the", "original", "inputs", "can", "mislead", "the", "models", ".", "Such", "maliciously", "perturbed", "inputs", "are", "called", "adversarial", "examples", ".", "Adversarial", "training", "aims", "to", "improve", "the", "robustness", "of", "a", "model", "to", "input", "perturbations", "by", "training", "on", "both", "unmodified", "examples", "and", "adversarial", "examples", ".", "Previous", "work", "on", "image", "recognition", "has", "demonstrated", "the", "enhanced", "robustness", "of", "their", "models", "to", "unseen", "images", "via", "adversarial", "training", "and", "has", "provided", "theoretical", "explanations", "of", "the", "regularization", "effects", ".", "Despite", "its", "potential", "as", "a", "powerful", "regularizer", ",", "adversarial", "training", "(", "AT", ")", "has", "yet", "to", "be", "explored", "extensively", "in", "natural", "language", "tasks", ".", "Recently", ",", "miyato2017adv", "applied", "AT", "on", "text", "classification", ",", "achieving", "state", "-", "of", "-", "the", "-", "art", "accuracy", ".", "Yet", ",", "the", "specific", "effects", "of", "the", "robustness", "obtained", "from", "AT", "are", "still", "unclear", "in", "the", "context", "of", "NLP", ".", "For", "example", ",", "research", "studies", "have", "yet", "to", "answer", "questions", "such", "as", "1", ")", "how", "can", "we", "interpret", "perturbations", "or", "robustness", "on", "natural", "language", "inputs", "?", "2", ")", "how", "are", "they", "related", "to", "linguistic", "factors", "like", "vocabulary", "statistics", "?", "3", ")", "are", "the", "effects", "of", "AT", "language", "-", "dependent", "?", "Answering", "such", "questions", "is", "crucial", "to", "understand", "and", "motivate", "the", "application", "of", "adversarial", "training", "on", "natural", "language", "tasks", ".", "In", "this", "paper", ",", "spotlighting", "a", "well", "-", "studied", "core", "problem", "of", "NLP", ",", "we", "propose", "and", "carefully", "analyze", "a", "neural", "part", "-", "of", "-", "speech", "(", "POS", ")", "tagging", "model", "that", "exploits", "adversarial", "training", ".", "With", "a", "BiLSTM", "-", "CRF", "model", "as", "our", "baseline", "POS", "tagger", ",", "we", "apply", "adversarial", "training", "by", "considering", "perturbations", "to", "input", "word", "/", "character", "embeddings", ".", "In", "order", "to", "demystify", "the", "effects", "of", "adversarial", "training", "in", "the", "context", "of", "NLP", ",", "we", "conduct", "POS", "tagging", "experiments", "on", "multiple", "languages", "using", "the", "Penn", "Treebank", "WSJ", "corpus", "(", "Englsih", ")", "and", "the", "Universal", "Dependencies", "dataset", "(", "27", "languages", ")", ",", "with", "thorough", "analyses", "of", "the", "following", "points", ":", "Effects", "on", "different", "target", "languages", "Vocabulary", "statistics", "and", "tagging", "accuracy", "Influence", "on", "downstream", "tasks", "Representation", "learning", "of", "words", "In", "our", "experiments", ",", "we", "find", "that", "our", "adversarial", "training", "model", "consistently", "outperforms", "the", "baseline", "POS", "tagger", ",", "and", "even", "achieves", "state", "-", "of", "-", "the", "-", "art", "results", "on", "22", "languages", ".", "Furthermore", ",", "our", "analyses", "reveal", "the", "following", "insights", "into", "adversarial", "training", "in", "the", "context", "of", "NLP", ":", "The", "regularization", "effects", "of", "adversarial", "training", "(", "AT", ")", "are", "general", "across", "different", "languages", ".", "AT", "can", "prevent", "overfitting", "especially", "well", "when", "training", "examples", "are", "scarce", ",", "providing", "an", "effective", "tool", "to", "process", "low", "resource", "languages", ".", "AT", "can", "boost", "the", "tagging", "performance", "for", "rare", "/", "unseen", "words", "and", "increase", "the", "sentence", "-", "level", "accuracy", ".", "This", "positively", "affects", "the", "performance", "of", "down", "-", "stream", "tasks", "such", "as", "dependency", "parsing", ",", "where", "low", "sentence", "-", "level", "POS", "accuracy", "can", "be", "a", "bottleneck", ".", "AT", "helps", "the", "network", "learn", "cleaner", "word", "embeddings", ",", "showing", "stronger", "correlations", "with", "their", "POS", "tags", ".", "We", "argue", "that", "the", "effects", "of", "AT", "can", "be", "interpreted", "from", "the", "perspective", "of", "natural", "language", ".", "Finally", ",", "we", "demonstrate", "that", "the", "proposed", "AT", "model", "is", "generally", "effective", "across", "different", "sequence", "labeling", "tasks", ".", "This", "work", "therefore", "provides", "a", "strong", "motivation", "and", "basis", "for", "utilizing", "adversarial", "training", "in", "NLP", "tasks", ".", "section", ":", "Related", "Work", "subsection", ":", "POS", "Tagging", "Part", "-", "of", "-", "speech", "(", "POS", ")", "tagging", "is", "a", "fundamental", "NLP", "task", "that", "facilitates", "downstream", "tasks", "such", "as", "syntactic", "parsing", ".", "While", "current", "state", "-", "of", "-", "the", "-", "art", "POS", "taggers", "yield", "accuracy", "over", "97.5", "%", "on", "PTB", "-", "WSJ", ",", "there", "still", "remain", "issues", ".", "The", "per", "token", "accuracy", "metric", "is", "easy", "since", "taggers", "can", "easily", "assign", "correct", "POS", "tags", "to", "highly", "unambiguous", "tokens", ",", "such", "as", "punctuation", ".", "Sentence", "-", "level", "accuracy", "serves", "as", "a", "more", "realistic", "metric", "for", "POS", "taggers", "but", "it", "still", "remains", "low", ".", "Another", "problem", "with", "current", "POS", "taggers", "is", "that", "their", "accuracy", "deteriorates", "drastically", "on", "low", "resource", "languages", "and", "rare", "words", ".", "In", "this", "work", ",", "we", "demonstrate", "that", "adversarial", "training", "(", "AT", ")", "can", "mitigate", "these", "issues", ".", "It", "is", "empirically", "shown", "that", "POS", "tagging", "performance", "can", "greatly", "affect", "downstream", "tasks", "such", "as", "dependency", "parsing", ".", "In", "this", "work", ",", "we", "also", "demonstrate", "that", "the", "improvements", "obtained", "from", "our", "AT", "POS", "tagger", "actually", "contribute", "to", "dependency", "parsing", ".", "Nonetheless", ",", "parsing", "with", "gold", "POS", "tags", "still", "yields", "better", "results", ",", "bolstering", "the", "view", "that", "POS", "tagging", "is", "an", "essential", "task", "in", "NLP", "that", "needs", "further", "development", ".", "subsection", ":", "Adversarial", "Training", "The", "concept", "of", "adversarial", "training", "was", "originally", "introduced", "in", "the", "context", "of", "image", "classification", "to", "improve", "the", "robustness", "of", "a", "model", "by", "training", "on", "input", "images", "with", "malicious", "perturbations", ".", "Previous", "work", "has", "provided", "a", "theoretical", "framework", "to", "understand", "adversarial", "examples", "and", "the", "regularization", "effects", "of", "adversarial", "training", "(", "AT", ")", "in", "image", "recognition", ".", "Recently", ",", "miyato2017adv", "applied", "AT", "to", "a", "natural", "language", "task", "(", "text", "classification", ")", "by", "extending", "the", "concept", "of", "adversarial", "perturbations", "to", "word", "embeddings", ".", "Wu2017adv", "further", "explored", "the", "possibility", "of", "AT", "in", "relation", "extraction", ".", "Both", "report", "improved", "performance", "on", "their", "tasks", "via", "AT", ",", "but", "the", "specific", "effects", "of", "AT", "have", "yet", "to", "be", "analyzed", ".", "In", "our", "work", ",", "we", "aim", "to", "address", "this", "issue", "by", "providing", "detailed", "analyses", "on", "the", "effects", "of", "AT", "from", "the", "perspective", "of", "NLP", ",", "such", "as", "different", "languages", ",", "vocabulary", "statistics", ",", "word", "embedding", "distribution", ",", "and", "aim", "to", "motivate", "future", "research", "that", "exploits", "AT", "in", "NLP", "tasks", ".", "AT", "is", "related", "to", "other", "regularization", "methods", "that", "add", "noise", "to", "data", "such", "as", "dropout", "and", "its", "variant", "for", "NLP", "tasks", ",", "word", "dropout", ".", "xie17_data_noising", "discuss", "various", "data", "noising", "techniques", "for", "language", "modeling", ".", "While", "these", "methods", "produce", "random", "noise", ",", "AT", "generates", "perturbations", "that", "the", "current", "model", "is", "particularly", "vulnerable", "to", ",", "and", "thus", "is", "claimed", "to", "be", "effective", ".", "It", "should", "be", "noted", "that", "while", "related", "in", "name", ",", "adversarial", "training", "(", "AT", ")", "differs", "from", "Generative", "Adversarial", "Networks", "(", "GANs", ")", ".", "GANs", "have", "already", "been", "applied", "to", "NLP", "tasks", "such", "as", "dialogue", "generation", "and", "transfer", "learning", ".", "Adversarial", "training", "also", "differs", "from", "adversarial", "evaluation", ",", "recently", "proposed", "for", "reading", "comprehension", "tasks", ".", "section", ":", "Method", "In", "this", "section", ",", "we", "introduce", "our", "baseline", "POS", "tagging", "model", "and", "explain", "how", "we", "implement", "adversarial", "training", "on", "top", ".", "subsection", ":", "Baseline", "POS", "Tagging", "Model", "Following", "the", "recent", "top", "-", "performing", "models", "for", "sequence", "labeling", "tasks", ",", "we", "employ", "a", "Bi", "-", "directional", "LSTM", "-", "CRF", "model", "as", "our", "baseline", "(", "see", "Figure", "[", "reference", "]", "for", "an", "illustration", ")", ".", "paragraph", ":", "Character", "-", "level", "BiLSTM", ".", "Prior", "work", "has", "shown", "that", "incorporating", "character", "-", "level", "representations", "of", "words", "can", "boost", "POS", "tagging", "accuracy", "by", "capturing", "morphological", "information", "present", "in", "each", "language", ".", "Major", "neural", "character", "-", "level", "models", "include", "the", "character", "-", "level", "CNN", "and", "(", "Bi", ")", "LSTM", ".", "A", "Bi", "-", "directional", "LSTM", "(", "BiLSTM", ")", "processes", "each", "sequence", "both", "forward", "and", "backward", "to", "capture", "sequential", "information", ",", "while", "preventing", "the", "vanishing", "/", "exploding", "gradient", "problem", ".", "We", "observed", "that", "the", "character", "-", "level", "BiLSTM", "outperformed", "the", "CNN", "by", "0.1", "%", "on", "the", "PTB", "-", "WSJ", "development", "set", ",", "and", "hence", "in", "all", "of", "our", "experiments", "we", "use", "the", "character", "-", "level", "BiLSTM", ".", "Specifically", ",", "we", "generate", "a", "character", "-", "level", "representation", "for", "each", "word", "by", "feeding", "its", "character", "embeddings", "into", "the", "BiLSTM", "and", "obtaining", "the", "concatenated", "final", "states", ".", "paragraph", ":", "Word", "-", "level", "BiLSTM", ".", "Each", "word", "in", "a", "sentence", "is", "represented", "by", "concatenating", "its", "word", "embedding", "and", "its", "character", "-", "level", "representation", ".", "They", "are", "fed", "into", "another", "level", "of", "BiLSTM", "(", "word", "-", "level", "BiLSTM", ")", "to", "process", "the", "entire", "sentence", ".", "paragraph", ":", "CRF", ".", "In", "sequence", "labeling", "tasks", "it", "is", "beneficial", "to", "consider", "the", "correlations", "between", "neighboring", "labels", "and", "jointly", "decode", "the", "best", "chain", "of", "labels", "for", "a", "given", "sentence", ".", "With", "this", "motivation", ",", "we", "apply", "a", "conditional", "random", "field", "(", "CRF", ")", "on", "top", "of", "the", "word", "-", "level", "BiLSTM", "to", "perform", "POS", "tag", "inference", "with", "global", "normalization", ",", "addressing", "the", "\u201c", "label", "bias", "\u201d", "problem", ".", "Specifically", ",", "given", "an", "input", "sentence", ",", "we", "pass", "the", "output", "sequence", "of", "the", "word", "-", "level", "BiLSTM", "to", "a", "first", "-", "order", "chain", "CRF", "to", "compute", "the", "conditional", "probability", "of", "the", "target", "label", "sequence", ":", "where", "represents", "all", "of", "the", "model", "parameters", "(", "in", "the", "BiLSTMs", "and", "CRF", ")", ",", "and", "denote", "the", "input", "embeddings", "and", "the", "target", "POS", "tag", "sequence", ",", "respectively", ",", "for", "the", "given", "sentence", ".", "For", "training", ",", "we", "minimize", "the", "negative", "log", "-", "likelihood", "(", "loss", "function", ")", "with", "respect", "to", "the", "model", "parameters", ".", "Decoding", "searches", "for", "the", "POS", "tag", "sequence", "with", "the", "highest", "conditional", "probability", "using", "the", "Viterbi", "algorithm", ".", "For", "more", "detail", "about", "the", "BiLSTM", "-", "CRF", "formulation", ",", "refer", "to", "ma", "-", "hovy:2016:P16", "-", "1", ".", "subsection", ":", "Adversarial", "Training", "Adversarial", "training", "is", "a", "powerful", "regularization", "method", ",", "primarily", "explored", "in", "image", "recognition", "to", "improve", "the", "robustness", "of", "classifiers", "to", "input", "perturbations", ".", "Given", "a", "classifier", ",", "we", "first", "generate", "input", "examples", "that", "are", "very", "close", "to", "original", "inputs", "(", "so", "should", "yield", "the", "same", "labels", ")", "yet", "are", "likely", "to", "be", "misclassified", "by", "the", "current", "model", ".", "Specifically", ",", "these", "adversarial", "examples", "are", "generated", "by", "adding", "small", "perturbations", "to", "the", "inputs", "in", "the", "direction", "that", "significantly", "increases", "the", "loss", "function", "of", "the", "classifier", "(", "worst", "-", "case", "perturbations", ")", ".", "Then", ",", "the", "classifier", "is", "trained", "on", "the", "mixture", "of", "clean", "examples", "and", "adversarial", "examples", "to", "improve", "the", "stability", "to", "input", "perturbations", ".", "In", "this", "work", ",", "we", "incorporate", "adversarial", "training", "into", "our", "baseline", "POS", "tagger", ",", "aiming", "to", "achieve", "better", "regularization", "effects", "and", "to", "provide", "their", "interpretations", "in", "the", "context", "of", "NLP", ".", "paragraph", ":", "Generating", "adversarial", "examples", ".", "Adversarial", "training", "(", "AT", ")", "considers", "continuous", "perturbations", "to", "inputs", ",", "so", "we", "define", "perturbations", "at", "the", "level", "of", "dense", "word", "/", "character", "embeddings", "rather", "than", "one", "-", "hot", "vector", "representations", ",", "similarly", "to", "miyato2017adv", ".", "Specifically", ",", "given", "an", "input", "sentence", ",", "we", "consider", "the", "concatenation", "of", "all", "the", "word", "/", "character", "embeddings", "in", "the", "sentence", ":", ".", "To", "prepare", "an", "adversarial", "example", ",", "we", "aim", "to", "generate", "the", "worst", "-", "case", "perturbation", "of", "a", "small", "bounded", "norm", "that", "maximizes", "the", "loss", "function", "of", "the", "current", "model", ":", "where", "is", "the", "current", "value", "of", "the", "model", "parameters", ",", "treated", "as", "a", "constant", ",", "and", "y", "denotes", "the", "target", "labels", ".", "Since", "the", "exact", "computation", "of", "such", "\u03b7", "is", "intractable", "in", "complex", "neural", "networks", ",", "we", "employ", "the", "Fast", "Gradient", "Method", "i.e.", "first", "order", "approximation", "to", "obtain", "an", "approximate", "worst", "-", "case", "perturbation", "of", "norm", ",", "by", "a", "single", "gradient", "computation", ":", "is", "a", "hyperparameter", "to", "be", "determined", "in", "the", "development", "dataset", ".", "Note", "that", "the", "perturbation", "\u03b7", "is", "generated", "in", "the", "direction", "that", "significantly", "increases", "the", "loss", ".", "We", "find", "such", "\u03b7", "against", "the", "current", "model", "parameterized", "by", ",", "at", "each", "training", "step", ",", "and", "construct", "an", "adversarial", "example", "by", "However", ",", "if", "we", "do", "not", "restrict", "the", "norm", "of", "word", "/", "character", "embeddings", ",", "the", "model", "could", "trivially", "learn", "embeddings", "of", "large", "norms", "to", "make", "the", "perturbations", "insignificant", ".", "To", "prevent", "this", "issue", ",", "we", "normalize", "word", "/", "character", "embeddings", "so", "that", "they", "have", "mean", "0", "and", "variance", "1", "for", "every", "entry", ",", "as", "in", "miyato2017adv", ".", "The", "normalization", "is", "performed", "every", "time", "we", "feed", "input", "embeddings", "into", "the", "LSTMs", "and", "generate", "adversarial", "examples", ".", "To", "ensure", "a", "fair", "comparison", ",", "we", "also", "normalize", "input", "embeddings", "in", "our", "baseline", "model", ".", "While", "miyato2017adv", "set", "the", "norm", "of", "a", "perturbation", "(", "Eq", "[", "reference", "]", ")", "to", "be", "a", "fixed", "value", "for", "all", "input", "sentences", ",", "to", "generate", "adversarial", "examples", "for", "an", "entire", "sentence", "of", "a", "variable", "length", "and", "to", "include", "character", "embeddings", "besides", "word", "embeddings", ",", "we", "make", "the", "perturbation", "size", "adaptive", "to", "the", "dimension", "of", "the", "concatenated", "input", "embedding", ".", "We", "set", "to", "be", "(", "i.e.", ",", "proportional", "to", "D", ")", ",", "as", "the", "expected", "squared", "norm", "of", "s", "after", "the", "embedding", "normalization", "is", ".", "The", "scaling", "factor", "is", "selected", "from", "0.001", ",", "0.005", ",", "0.01", ",", "0.05", ",", "0.1", "based", "on", "the", "development", "performance", "in", "each", "treebank", ".", "We", "used", "0.01", "for", "PTB", "-", "WSJ", "and", "UD", "-", "Spanish", ",", "and", "0.05", "for", "the", "rest", ".", "Note", "that", "would", "generate", "no", "noise", "(", "identical", "to", "the", "baseline", ")", ";", "if", ",", "the", "generated", "adversarial", "perturbation", "would", "have", "a", "norm", "comparable", "to", "the", "original", "embedding", ",", "which", "could", "change", "the", "semantics", "of", "the", "input", "sentence", ".", "Hence", ",", "the", "optimal", "perturbation", "scale", "should", "lie", "in", "between", "and", "be", "small", "enough", "to", "preserve", "the", "semantics", "of", "the", "original", "input", ".", "paragraph", ":", "Adversarial", "training", ".", "At", "each", "training", "step", ",", "we", "generate", "adversarial", "examples", "against", "the", "current", "model", ",", "and", "train", "on", "the", "mixture", "of", "clean", "examples", "and", "adversarial", "examples", "to", "achieve", "robustness", "to", "input", "perturbations", ".", "To", "this", "end", ",", "we", "define", "the", "loss", "function", "for", "adversarial", "training", "as", ":", "where", ",", "represent", "the", "loss", "from", "a", "clean", "example", "and", "the", "loss", "from", "its", "adversarial", "example", ",", "respectively", ",", "and", "determines", "the", "weighting", "between", "them", ".", "We", "used", "in", "all", "our", "experiments", ".", "This", "objective", "function", "can", "be", "optimized", "with", "respect", "to", "the", "model", "parameters", "\u03b8", ",", "in", "the", "same", "manner", "as", "the", "baseline", "model", ".", "section", ":", "Experiments", "To", "fully", "analyze", "the", "effects", "of", "adversarial", "training", ",", "we", "train", "and", "evaluate", "our", "baseline", "/", "adversarial", "POS", "tagging", "models", "on", "both", "a", "standard", "English", "dataset", "and", "a", "multilingual", "dataset", ".", "subsection", ":", "Datasets", "As", "a", "standard", "English", "dataset", ",", "we", "use", "the", "Wall", "Street", "Journal", "(", "WSJ", ")", "portion", "of", "the", "Penn", "Treebank", "(", "PTB", ")", ",", "containing", "45", "different", "POS", "tags", ".", "We", "adopt", "the", "standard", "split", ":", "sections", "0", "-", "18", "for", "training", ",", "19", "-", "21", "for", "development", "and", "22", "-", "24", "for", "testing", ".", "For", "multilingual", "POS", "tagging", "experiments", ",", "to", "compare", "with", "prior", "work", ",", "we", "use", "treebanks", "from", "Universal", "Dependencies", "(", "UD", ")", "v1.2", "(", "17", "POS", ")", "with", "the", "given", "data", "splits", ".", "We", "experiment", "on", "languages", "for", "which", "pre", "-", "trained", "Polyglot", "word", "embeddings", "are", "available", ",", "resulting", "in", "27", "languages", "listed", "in", "Table", "[", "reference", "]", ".", "We", "regard", "languages", "with", "less", "than", "60k", "tokens", "of", "training", "data", "as", "low", "-", "resource", "(", "Table", "[", "reference", "]", ",", "bottom", ")", ",", "as", "in", "plank2016multilingual", ".", "subsection", ":", "Training", "&", "Evaluation", "Details", "paragraph", ":", "Model", "settings", ".", "We", "initialize", "word", "embeddings", "with", "100", "-", "dimensional", "GloVe", "for", "English", ",", "and", "with", "64", "-", "dimensional", "Polyglot", "for", "other", "languages", ".", "We", "use", "30", "-", "dimensional", "character", "embeddings", ",", "and", "set", "the", "state", "sizes", "of", "character", "/", "word", "-", "level", "BiLSTM", "to", "be", "50", ",", "200", "for", "English", ",", "50", ",", "100", "for", "low", "resource", "languages", ",", "and", "50", ",", "150", "for", "other", "languages", ".", "The", "model", "parameters", "and", "character", "embeddings", "are", "randomly", "initialized", ",", "as", "in", "ma", "-", "hovy:2016:P16", "-", "1", ".", "We", "apply", "dropout", "to", "input", "embeddings", "and", "BiLSTM", "outputs", "for", "both", "baseline", "and", "adversarial", "training", ",", "with", "dropout", "rate", "0.5", ".", "paragraph", ":", "Optimization", ".", "We", "train", "the", "model", "parameters", "and", "word", "/", "character", "embeddings", "by", "the", "mini", "-", "batch", "stochastic", "gradient", "descent", "(", "SGD", ")", "with", "batch", "size", "10", ",", "momentum", "0.9", ",", "initial", "learning", "rate", "0.01", "and", "decay", "rate", "0.05", ".", "We", "also", "use", "a", "gradient", "clipping", "of", "5.0", ".", "The", "models", "are", "trained", "with", "early", "stopping", "based", "on", "the", "development", "performance", ".", "paragraph", ":", "Evaluation", ".", "We", "evaluate", "per", "token", "tagging", "accuracy", "on", "test", "sets", ".", "We", "repeat", "the", "experiment", "three", "times", "and", "report", "the", "statistical", "significance", ".", "subsection", ":", "Results", "paragraph", ":", "PTB", "-", "WSJ", "dataset", ".", "Table", "[", "reference", "]", "shows", "the", "POS", "tagging", "results", ".", "As", "expected", ",", "our", "baseline", "(", "BiLSTM", "-", "CRF", ")", "model", "(", "accuracy", "97.54", "%", ")", "performs", "on", "par", "with", "other", "state", "-", "of", "-", "the", "-", "art", "systems", ".", "Built", "upon", "this", "baseline", ",", "our", "adversarial", "training", "(", "AT", ")", "model", "reaches", "accuracy", "97.58", "%", "thanks", "to", "its", "regularization", "power", ",", "outperforming", "recent", "POS", "taggers", "except", "wang:2015", ".", "The", "improvement", "over", "the", "baseline", "is", "statistically", "significant", ",", "with", "-", "value", "0.05", "on", "the", "-", "test", ".", "We", "provide", "additional", "analysis", "on", "this", "result", "in", "later", "sections", ".", "paragraph", ":", "Multilingual", "dataset", "(", "UD", ")", ".", "Experimental", "results", "are", "summarized", "in", "Table", "[", "reference", "]", ".", "Our", "AT", "model", "shows", "clear", "advantages", "over", "the", "baseline", "in", "all", "of", "the", "27", "languages", "(", "average", "improvement", "0.25", "%", ";", "see", "the", "two", "shaded", "columns", ")", ".", "Considering", "that", "our", "baseline", "(", "BiLSTM", "-", "CRF", ")", "is", "already", "a", "top", "performing", "model", "for", "POS", "tagging", ",", "these", "improvements", "made", "by", "AT", "are", "substantial", ".", "The", "improvements", "are", "also", "statistically", "significant", "for", "all", "the", "languages", ",", "with", "-", "value", "0.05", "on", "the", "-", "test", ",", "suggesting", "that", "the", "regularization", "by", "AT", "is", "generally", "effective", "across", "different", "languages", ".", "Moreover", ",", "our", "AT", "model", "achieves", "state", "-", "of", "-", "the", "-", "art", "on", "nearly", "all", "of", "the", "languages", ",", "except", "the", "five", "where", "plank2016multilingual", "\u2019s", "multi", "-", "task", "BiLSTM", "yielded", "better", "results", ".", "Among", "the", "five", ",", "most", "languages", "are", "morphologically", "rich", "(", ")", ".", "We", "suspect", "that", "their", "joint", "training", "of", "word", "rarity", "may", "be", "of", "particular", "help", "in", "processing", "morphologically", "complex", "words", ".", "English", "(", "WSJ", ")", "Word", "Frequency", "French", "(", "UD", ")", "Word", "Frequency", "Additionally", ",", "we", "see", "that", "our", "AT", "model", "achieves", "notably", "large", "improvements", "over", "the", "baseline", "in", "resource", "-", "poor", "languages", "(", "the", "bottom", "of", "Table", "[", "reference", "]", ")", ",", "with", "average", "improvement", "0.35", "%", ",", "as", "compared", "to", "that", "for", "resource", "-", "rich", "languages", ",", "0.20", "%", ".", "To", "further", "visualize", "the", "regularization", "effects", ",", "we", "present", "the", "learning", "curves", "for", "three", "representative", "languages", ",", "English", "(", "WSJ", ")", ",", "French", "(", "UD", "-", "fr", ")", "and", "Romanian", "(", "UD", "-", "ro", ",", "low", "-", "resource", ")", ",", "based", "on", "the", "development", "loss", "(", "see", "Figure", "[", "reference", "]", ")", ".", "For", "all", "the", "three", "languages", ",", "we", "can", "observe", "that", "the", "AT", "model", "(", "red", "solid", "line", ")", "prevents", "overfitting", "better", "than", "the", "baseline", "(", "black", "dotted", "line", ")", ",", "and", "this", "advantage", "is", "more", "significant", "in", "low", "resource", "languages", ".", "For", "example", ",", "in", "Romanian", ",", "the", "baseline", "model", "starts", "to", "increase", "development", "loss", "after", "1", ",", "000", "iterations", "even", "with", "dropout", ",", "whereas", "the", "AT", "model", "keeps", "improving", "until", "2", ",", "500", "iterations", ",", "achieving", "notably", "lower", "development", "loss", "(", "0.4", "down", ")", ".", "These", "results", "illustrate", "that", "AT", "can", "prevent", "overfitting", "especially", "well", "on", "small", "datasets", "and", "can", "augment", "the", "regularization", "power", "beyond", "dropout", ".", "AT", "can", "also", "be", "viewed", "as", "an", "effective", "means", "of", "data", "augmentation", ",", "where", "we", "generate", "and", "train", "with", "new", "examples", "the", "current", "model", "is", "particularly", "vulnerable", "to", "at", "every", "time", "step", ",", "enhancing", "the", "robustness", "of", "the", "model", ".", "AT", "can", "therefore", "be", "a", "promising", "tool", "to", "process", "low", "resource", "languages", ".", "section", ":", "Analysis", "English", "(", "WSJ", ")", "Word", "Frequency", "French", "(", "UD", ")", "Word", "Frequency", "In", "the", "previous", "sections", ",", "we", "demonstrated", "the", "regularization", "power", "of", "adversarial", "training", "(", "AT", ")", "on", "different", "languages", ",", "based", "on", "the", "overall", "POS", "tagging", "performance", "and", "learning", "curves", ".", "In", "this", "section", ",", "we", "conduct", "further", "analyses", "on", "the", "robustness", "of", "AT", "from", "NLP", "specific", "aspects", "such", "as", "word", "statistics", ",", "sequence", "modeling", ",", "downstream", "tasks", ",", "and", "word", "representation", "learning", ".", "We", "find", "that", "AT", "can", "boost", "tagging", "accuracy", "on", "rare", "words", "and", "neighbors", "of", "unseen", "words", "(", "\u00a7", "[", "reference", "]", ")", ".", "Furthermore", ",", "this", "robustness", "against", "rare", "/", "unseen", "words", "leads", "to", "better", "sentence", "-", "level", "accuracy", "and", "downstream", "dependency", "parsing", "(", "\u00a7", "[", "reference", "]", ")", ".", "We", "illustrate", "these", "findings", "using", "two", "major", "languages", ",", "English", "(", "WSJ", ")", "and", "French", "(", "UD", ")", ",", "which", "have", "substantially", "large", "training", "and", "testing", "data", "to", "discuss", "vocabulary", "statistics", "and", "sentence", "-", "level", "performance", ".", "Finally", ",", "we", "study", "the", "effects", "of", "AT", "on", "word", "representation", "learning", "(", "\u00a7", "[", "reference", "]", ")", ",", "and", "the", "applicability", "of", "AT", "to", "different", "sequential", "tasks", "(", "\u00a7", "[", "reference", "]", ")", ".", "subsection", ":", "Word", "-", "level", "Analysis", "Poor", "tagging", "accuracy", "on", "rare", "/", "unseen", "words", "is", "one", "of", "the", "bottlenecks", "in", "current", "POS", "taggers", ".", "Aiming", "to", "reveal", "the", "effects", "of", "AT", "on", "rare", "/", "unseen", "words", ",", "we", "analyze", "tagging", "performance", "at", "the", "word", "level", ",", "considering", "vocabulary", "statistics", ".", "paragraph", ":", "Word", "frequency", ".", "To", "define", "rare", "/", "unseen", "words", ",", "we", "consider", "each", "word", "\u2019s", "frequency", "of", "occurrence", "in", "the", "training", "set", ".", "We", "categorize", "all", "words", "in", "the", "test", "set", "based", "on", "this", "frequency", "and", "study", "the", "test", "tagging", "accuracy", "for", "each", "group", "(", "see", "Table", "[", "reference", "]", ")", ".", "In", "both", "languages", ",", "the", "AT", "model", "achieves", "large", "improvements", "over", "the", "baseline", "on", "rare", "words", "(", "e.g.", ",", "frequency", "1", "-", "10", "in", "training", ")", ",", "as", "opposed", "to", "more", "frequent", "words", ".", "This", "result", "again", "corroborates", "the", "data", "augmentation", "power", "of", "AT", "under", "small", "training", "examples", ".", "On", "the", "other", "hand", ",", "we", "did", "not", "observe", "meaningful", "improvements", "on", "unseen", "words", "(", "frequency", "0", "in", "training", ")", ".", "A", "possible", "explanation", "is", "that", "AT", "can", "facilitate", "the", "learning", "of", "words", "with", "at", "least", "a", "few", "occurrences", "in", "training", "(", "rare", "words", ")", ",", "but", "is", "not", "particularly", "effective", "in", "inferring", "the", "POS", "tags", "of", "words", "for", "which", "no", "training", "examples", "are", "given", "(", "unseen", "words", ")", ".", "paragraph", ":", "Neighboring", "words", ".", "One", "important", "characteristic", "of", "natural", "language", "tasks", "is", "the", "sequential", "nature", "of", "inputs", "(", "i.e.", ",", "sequence", "of", "words", ")", ",", "where", "each", "word", "influences", "the", "function", "of", "its", "neighboring", "words", ".", "Since", "our", "model", "uses", "BiLSTM", "-", "CRF", "for", "that", "reason", ",", "we", "also", "study", "the", "tagging", "performance", "on", "the", "neighbors", "of", "rare", "/", "unseen", "words", ",", "and", "analyze", "the", "effects", "of", "AT", "with", "the", "sequence", "model", "in", "mind", ".", "In", "Table", "[", "reference", "]", ",", "we", "cluster", "all", "words", "in", "the", "test", "set", "based", "on", "their", "frequency", "in", "training", "again", ",", "and", "consider", "the", "tagging", "accuracy", "on", "the", "neighbors", "(", "left", "and", "right", ")", "of", "these", "words", "in", "the", "test", "text", ".", "We", "observe", "that", "AT", "tends", "to", "achieve", "large", "improvements", "over", "the", "baseline", "on", "the", "neighbors", "of", "unseen", "words", "(", "training", "frequency", "0", ")", ",", "while", "the", "improvements", "on", "the", "neighbors", "of", "more", "frequent", "words", "remain", "moderate", ".", "Our", "AT", "model", "thus", "exhibits", "strong", "stability", "to", "uncertain", "neighbors", ",", "as", "compared", "to", "the", "baseline", ".", "We", "suspect", "that", "because", "we", "generate", "adversarial", "examples", "against", "entire", "input", "sentences", ",", "training", "with", "adversarial", "examples", "makes", "the", "model", "more", "robust", "not", "only", "to", "perturbations", "in", "each", "word", "but", "also", "to", "perturbations", "in", "its", "neighboring", "words", ",", "leading", "to", "greater", "stability", "to", "uncertain", "neighbors", ".", "subsection", ":", "Sentence", "-", "level", "&", "Downstream", "Analysis", "English", "(", "WSJ", ")", "Sentence", "-", "Stanford", "Parser", "Parsey", "McParseface", "level", "Acc", ".", "(", "w", "/", "gold", "tags", ")", "French", "(", "UD", ")", "Sentence", "-", "Parsey", "Universal", "level", "Acc", ".", "(", "w", "/", "gold", "tags", ")", "In", "the", "word", "-", "level", "analysis", ",", "we", "showed", "that", "AT", "can", "boost", "tagging", "accuracy", "on", "rare", "words", "and", "the", "neighbors", "of", "unseen", "words", ",", "enhancing", "overall", "robustness", "on", "rare", "/", "unseen", "words", ".", "In", "this", "section", ",", "we", "discuss", "the", "benefit", "of", "our", "improved", "POS", "tagger", "in", "a", "major", "downstream", "task", ",", "dependency", "parsing", ".", "Most", "of", "the", "recent", "state", "-", "of", "-", "the", "-", "art", "dependency", "parsers", "take", "predicted", "POS", "tags", "as", "input", "(", "e.g.", "chen2014fast", ",", "andor2016globally", ",", "DozatManning17", ")", ".", "dozat", "-", "qi", "-", "manning:2017:K17", "-", "3", "empirically", "show", "that", "their", "dependency", "parser", "gains", "significant", "improvements", "by", "using", "POS", "tags", "predicted", "by", "a", "Bi", "-", "LSTM", "POS", "tagger", ",", "while", "POS", "tags", "predicted", "by", "the", "UDPipe", "tagger", "do", "not", "contribute", "to", "parsing", "performance", "as", "much", ".", "This", "observation", "illustrates", "that", "POS", "tagging", "performance", "has", "a", "great", "influence", "on", "dependency", "parsing", ",", "motivating", "the", "hypothesis", "that", "the", "POS", "tagging", "improvements", "gained", "from", "our", "adversarial", "training", "help", "dependency", "parsing", ".", "To", "test", "the", "hypothesis", ",", "we", "consider", "three", "settings", "in", "dependency", "parsing", "of", "English", "and", "French", ":", "using", "POS", "tags", "predicted", "by", "the", "baseline", "model", ",", "using", "POS", "tags", "predicted", "by", "the", "AT", "model", ",", "and", "using", "gold", "POS", "tags", ".", "For", "English", "(", "PTB", "-", "WSJ", ")", ",", "we", "first", "convert", "the", "treebank", "into", "Stanford", "Dependencies", "(", "SD", ")", "using", "Stanford", "CoreNLP", "(", "ver", "3.8.0", ")", ",", "and", "then", "apply", "two", "well", "-", "known", "dependency", "parsers", ":", "Stanford", "Parser", "(", "ver", "3.5.0", ")", "and", "Parsey", "McParseface", "(", "SyntaxNet", ")", ".", "For", "French", "(", "UD", ")", ",", "we", "use", "Parsey", "Universal", "from", "SyntaxNet", ".", "The", "three", "parsers", "are", "all", "publicly", "available", "and", "pre", "-", "trained", "on", "corresponding", "treebanks", ".", "Table", "[", "reference", "]", "shows", "the", "results", "of", "the", "experiments", ".", "We", "can", "observe", "improvements", "in", "both", "languages", "by", "using", "the", "POS", "tags", "predicted", "by", "our", "AT", "POS", "tagger", ".", "As", "Manning:2011:from97to100", "points", "out", ",", "when", "predicted", "POS", "tags", "are", "used", "for", "downstream", "dependency", "parsing", ",", "a", "single", "bad", "mistake", "in", "a", "sentence", "can", "greatly", "damage", "the", "usefulness", "of", "the", "POS", "tagger", ".", "The", "robustness", "of", "our", "AT", "POS", "tagger", "against", "rare", "/", "unseen", "words", "helps", "to", "mitigate", "such", "an", "issue", ".", "This", "advantage", "can", "also", "be", "observed", "from", "the", "AT", "POS", "tagger", "\u2019s", "notably", "higher", "sentence", "-", "level", "accuracy", "than", "the", "baseline", "(", "see", "Table", "[", "reference", "]", "left", ")", ".", "Nonetheless", ",", "gold", "POS", "tags", "still", "yield", "better", "parsing", "results", "as", "compared", "to", "the", "baseline", "/", "AT", "POS", "taggers", ",", "supporting", "the", "claim", "that", "POS", "tagging", "needs", "further", "improvement", "for", "downstream", "tasks", ".", "subsection", ":", "Effects", "on", "Representation", "Learning", "English", "(", "WSJ", ")", "(", "GloVe", ")", "French", "(", "UD", ")", "(", "polyglot", ")", "English", "(", "WSJ", ")", "Perturbation", "scale", "Avg", ".", "cluster", "tightness", "Next", ",", "we", "perform", "an", "analysis", "on", "representation", "learning", "of", "words", "(", "word", "embeddings", ")", "for", "the", "English", "(", "PTB", "-", "WSJ", ")", "and", "French", "(", "UD", ")", "experiments", ".", "We", "hypothesize", "that", "adversarial", "training", "(", "AT", ")", "helps", "to", "learn", "better", "word", "embeddings", "so", "that", "the", "POS", "tag", "prediction", "of", "a", "word", "can", "not", "be", "influenced", "by", "a", "small", "perturbation", "in", "the", "input", "embedding", ".", "To", "verify", "this", "hypothesis", ",", "we", "cluster", "all", "words", "in", "the", "test", "set", "based", "on", "their", "correct", "POS", "tags", "and", "evaluate", "the", "tightness", "of", "the", "word", "vector", "distribution", "within", "each", "cluster", ".", "We", "compare", "this", "clustering", "quality", "among", "the", "three", "settings", ":", "1", ")", "beginning", "(", "initialized", "with", "GloVe", "or", "Polyglot", ")", ",", "2", ")", "after", "baseline", "training", "(", "50", "epochs", ")", ",", "and", "3", ")", "after", "adversarial", "training", "(", "50", "epochs", ")", ",", "to", "study", "the", "effects", "of", "AT", "on", "word", "representation", "learning", ".", "For", "evaluating", "the", "tightness", "of", "word", "vector", "distribution", ",", "we", "employ", "the", "cosine", "similarity", "metric", ",", "which", "is", "widely", "used", "as", "a", "measure", "of", "the", "closeness", "between", "two", "word", "vectors", "(", "e.g.", ",", "mikolov2013distributed", ";", "pennington", "-", "socher", "-", "manning:2014:EMNLP2014", ")", ".", "To", "measure", "the", "tightness", "of", "each", "cluster", ",", "we", "compute", "the", "cosine", "similarity", "for", "every", "pair", "of", "words", "within", ",", "and", "then", "take", "the", "average", ".", "We", "also", "report", "the", "average", "tightness", "across", "all", "the", "clusters", ".", "The", "evaluation", "results", "are", "summarized", "in", "Table", "[", "reference", "]", ".", "We", "report", "the", "tightness", "scores", "for", "the", "four", "major", "clusters", ":", "noun", ",", "verb", ",", "adjective", ",", "and", "adverb", "(", "from", "left", "to", "right", ")", ".", "As", "can", "be", "seen", "from", "the", "table", ",", "for", "both", "languages", ",", "adversarial", "training", "(", "AT", ")", "results", "in", "cleaner", "word", "embedding", "distributions", "than", "the", "baseline", ",", "with", "a", "higher", "cosine", "similarity", "within", "each", "POS", "cluster", ",", "and", "with", "a", "clear", "advantage", "in", "the", "average", "tightness", "across", "all", "the", "clusters", ".", "In", "other", "words", ",", "the", "learned", "word", "vectors", "show", "stronger", "correlations", "with", "their", "POS", "tags", ".", "This", "result", "confirms", "that", "training", "with", "adversarial", "examples", "can", "help", "to", "learn", "cleaner", "word", "embeddings", "so", "that", "the", "meaning", "/", "grammatical", "function", "of", "a", "word", "can", "not", "be", "altered", "by", "a", "small", "perturbation", "in", "its", "embedding", ".", "This", "analysis", "provides", "a", "means", "to", "interpret", "the", "robustness", "to", "input", "perturbations", ",", "from", "the", "perspective", "of", "NLP", ".", "paragraph", ":", "Relation", "with", "perturbation", "size", ".", "We", "also", "study", "how", "the", "size", "of", "added", "perturbations", "influences", "word", "representation", "learning", "in", "adversarial", "training", ".", "Recall", "that", "we", "set", "the", "norm", "of", "a", "perturbation", "to", "be", ",", "where", "is", "the", "dimension", "of", "the", "concatenated", "input", "embeddings", "(", "see", "\u00a7", "[", "reference", "]", ")", ".", "For", "instance", ",", "would", "produce", "no", "noise", ";", "would", "generate", "a", "perturbation", "of", "a", "norm", "equivalent", "to", "the", "original", "word", "embeddings", ".", "We", "hypothesize", "that", "AT", "facilitates", "word", "representation", "learning", "when", "is", "small", "enough", "to", "preserve", "the", "semantics", "of", "input", "words", ",", "but", "can", "hinder", "the", "learning", "when", "is", "too", "large", ".", "To", "test", "the", "hypothesis", ",", "we", "repeat", "the", "clustering", "evaluation", "for", "word", "embeddings", "trained", "with", "varied", "perturbation", "scale", ":", "0", ",", "0.001", ",", "0.01", ",", "0.05", ",", "0.1", ",", "0.5", "(", "see", "Table", "[", "reference", "]", ")", ".", "We", "observe", "that", "the", "quality", "of", "learned", "word", "embedding", "distribution", "keeps", "improving", "as", "goes", "up", "from", "0", "to", "0.1", ",", "but", "starts", "to", "drop", "around", ".", "We", "also", "find", "that", "this", "optimal", "in", "word", "embedding", "learning", "(", "i.e.", ",", "0.1", ")", "is", "larger", "than", "the", "which", "yielded", "the", "best", "tagging", "performance", "on", "development", "sets", "(", "i.e.", ",", "0.01", "or", "0.05", ")", ".", "A", "possible", "explanation", "is", "that", "while", "word", "embeddings", "can", "adapt", "to", "relatively", "large", "(", "e.g.", ",", "0.1", ")", "during", "training", ",", "as", "adversarial", "perturbations", "are", "generated", "at", "the", "embedding", "level", ",", "such", "could", "change", "the", "semantics", "of", "the", "input", "from", "the", "current", "tagging", "model", "\u2019s", "perspective", "and", "hinder", "the", "training", "of", "tagging", ".", "subsection", ":", "Other", "Sequence", "Labeling", "Tasks", "Finally", ",", "to", "further", "confirm", "the", "applicability", "of", "AT", ",", "we", "experiment", "with", "our", "BiLSTM", "-", "CRF", "AT", "model", "in", "different", "sequence", "labeling", "tasks", ":", "chunking", "and", "named", "entity", "recognition", "(", "NER", ")", ".", "Chunking", "can", "be", "performed", "as", "a", "sequence", "labeling", "task", "that", "assigns", "a", "chunking", "tag", "(", "B", "-", "NP", ",", "I", "-", "VP", ",", "etc", ".", ")", "to", "each", "word", ".", "We", "conduct", "experiments", "on", "the", "CoNLL", "2000", "shared", "task", "with", "the", "standard", "data", "split", ":", "PTB", "-", "WSJ", "Sections", "15", "-", "18", "for", "training", "and", "20", "for", "testing", ".", "We", "use", "Section", "19", "as", "the", "development", "set", "and", "employ", "the", "IOBES", "tagging", "scheme", ",", "following", "HashimotoXTS16", ".", "NER", "aims", "to", "assign", "an", "entity", "type", "to", "each", "word", ",", "such", "as", "person", ",", "location", ",", "organization", ",", "and", "misc", ".", "We", "conduct", "experiments", "on", "the", "CoNLL", "-", "2003", "(", "English", ")", "shared", "task", ",", "adopting", "the", "IOBES", "tagging", "scheme", "as", "in", ".", "The", "results", "are", "summarized", "in", "Table", "[", "reference", "]", "and", "[", "reference", "]", ".", "AT", "enhanced", "F1", "score", "from", "the", "baseline", "BiLSTM", "-", "CRF", "model", "\u2019s", "95.18", "to", "95.25", "for", "chunking", ",", "and", "from", "91.22", "to", "91.56", "for", "NER", ",", "also", "significantly", "outperforming", "ma", "-", "hovy:2016:P16", "-", "1", ".", "These", "improvements", "made", "by", "AT", "are", "bigger", "than", "that", "for", "English", "POS", "tagging", ",", "most", "likely", "due", "to", "the", "larger", "room", "for", "improvement", "in", "chunking", "and", "NER", ".", "The", "improvements", "are", "again", "statistically", "significant", ",", "with", "-", "value", "0.05", "on", "the", "-", "test", ".", "The", "experimental", "results", "suggest", "that", "the", "proposed", "adversarial", "training", "scheme", "is", "generally", "effective", "across", "different", "sequence", "labeling", "tasks", ".", "Our", "BiLSTM", "-", "CRF", "AT", "model", "did", "not", "reach", "the", "performance", "by", "HashimotoXTS16", "\u2019s", "multi", "-", "task", "model", "and", "peters2017semi", "\u2019s", "state", "-", "of", "-", "the", "-", "art", "system", "that", "incorporates", "pretrained", "language", "models", ".", "It", "would", "be", "interesting", "future", "work", "to", "combine", "the", "strengths", "of", "these", "joint", "models", "(", "e.g.", ",", "syntactic", "and", "semantic", "aids", ")", "and", "adversarial", "training", "(", "e.g.", ",", "robustness", ")", ".", "section", ":", "Conclusion", "We", "proposed", "and", "carefully", "analyzed", "a", "POS", "tagging", "model", "that", "exploits", "adversarial", "training", "(", "AT", ")", ".", "In", "our", "multilingual", "experiments", ",", "we", "find", "that", "AT", "achieves", "substantial", "improvements", "on", "all", "the", "languages", "tested", ",", "especially", "on", "low", "resource", "ones", ".", "AT", "also", "enhances", "the", "robustness", "to", "rare", "/", "unseen", "words", "and", "sentence", "-", "level", "accuracy", ",", "alleviating", "the", "major", "issues", "of", "current", "POS", "taggers", ",", "and", "contributing", "to", "the", "downstream", "task", ",", "dependency", "parsing", ".", "Furthermore", ",", "our", "analyses", "on", "different", "languages", ",", "word", "/", "neighbor", "statistics", "and", "word", "representation", "learning", "reveal", "the", "effects", "of", "AT", "from", "the", "perspective", "of", "NLP", ".", "The", "proposed", "AT", "model", "is", "applicable", "to", "general", "sequence", "labeling", "tasks", ".", "This", "work", "therefore", "provides", "a", "strong", "basis", "and", "motivation", "for", "utilizing", "AT", "in", "natural", "language", "tasks", ".", "section", ":", "Acknowledgements", "We", "would", "like", "to", "thank", "Rui", "Zhang", ",", "Jonathan", "Kummerfeld", ",", "Yutaro", "Yamada", ",", "as", "well", "as", "all", "the", "anonymous", "reviewers", "for", "their", "helpful", "feedback", "and", "suggestions", "on", "this", "work", ".", "bibliography", ":", "References"]}