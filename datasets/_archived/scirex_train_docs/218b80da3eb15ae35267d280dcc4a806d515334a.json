{"coref": {"CNN_Seq2Seq": [[16, 22], [23, 24], [424, 426], [730, 732], [2666, 2668], [2690, 2692], [2699, 2701], [3702, 3704], [3952, 3954], [42, 43], [112, 113], [184, 185], [342, 343], [800, 801], [865, 866], [939, 940], [1308, 1309], [1372, 1373], [1817, 1818], [1851, 1852], [2477, 2478], [2554, 2555], [2572, 2573], [2783, 2784], [3414, 3415], [3682, 3683], [3693, 3694], [3713, 3714], [3820, 3821], [3847, 3848], [3924, 3925], [4171, 4172], [4272, 4273], [4672, 4673], [4767, 4768], [4818, 4819]], "CNN_Seq2Seq___Fluency_Boost": [], "CNN_Seq2Seq___Fluency_Boost_and_inference": [], "F0_5": [], "Fluency_Boost": [[56, 59], [88, 91], [105, 108], [571, 575], [669, 673], [793, 796], [1159, 1162], [1205, 1209], [2350, 2353], [2361, 2365], [2646, 2650], [3839, 3842], [3917, 3920], [3943, 3946], [4114, 4117], [4135, 4138], [4151, 4154], [4178, 4181], [4200, 4203], [4212, 4216], [4584, 4588], [4783, 4786]], "Fluency_Boost_and_inference": [[49, 55], [109, 110], [401, 407], [584, 588], [797, 798], [2482, 2486], [2526, 2529], [2540, 2543], [2595, 2598], [3958, 3961], [4018, 4021], [4037, 4040], [4118, 4119], [4232, 4235], [4633, 4636], [4776, 4782], [4822, 4825]], "GLEU": [[141, 142], [3047, 3048], [3939, 3940], [3868, 3869], [3895, 3896], [4041, 4042]], "Grammatical_Error_Correction": [[8, 12], [32, 35], [36, 37], [199, 202], [203, 204], [236, 237], [843, 847], [1008, 1009], [2381, 2382], [2626, 2631], [2714, 2719], [3190, 3191], [3685, 3688], [3955, 3956], [4541, 4542], [4596, 4597], [4608, 4609], [4675, 4676], [4693, 4694], [4701, 4704], [4829, 4830], [836, 837], [858, 859], [1040, 1041], [3559, 3560], [3568, 3569], [3649, 3650], [3809, 3810], [3826, 3827], [4457, 4458], [4714, 4715], [4770, 4771]], "Unrestricted": []}, "coref_non_salient": {"0": [[956, 957], [3705, 3707]], "1": [[3523, 3525], [3533, 3536]], "10": [[4069, 4071], [4874, 4877]], "11": [[144, 147], [167, 168], [819, 822], [2994, 2995], [3015, 3018], [3156, 3159], [3518, 3519], [3541, 3542], [3871, 3874], [3888, 3891], [4035, 4036], [4127, 4129], [3050, 3051], [3144, 3145], [3814, 3815]], "12": [[392, 396], [568, 570], [981, 983], [4155, 4156]], "13": [[1108, 1110], [1437, 1439], [3496, 3498], [3859, 3860]], "14": [[152, 154], [805, 807], [3760, 3763], [3787, 3789]], "15": [[416, 419], [1001, 1004], [2969, 2972], [3343, 3346]], "16": [[3622, 3623], [3624, 3625], [3636, 3637]], "17": [[2722, 2723], [3216, 3217]], "18": [[1214, 1217], [1987, 1990], [2159, 2165]], "19": [[1624, 1626], [3292, 3293], [4594, 4595]], "2": [[1270, 1273], [1687, 1690], [2193, 2196], [2210, 2213], [2233, 2236], [2261, 2264], [3176, 3182]], "20": [[3758, 3759], [3755, 3756]], "21": [[2444, 2447], [3387, 3390], [3879, 3882]], "22": [[163, 164], [4125, 4126], [132, 133], [810, 811], [2986, 2987], [3003, 3004], [3061, 3062], [3098, 3099], [3111, 3112], [3121, 3122], [3138, 3139], [3812, 3813], [3861, 3862], [3902, 3903], [3969, 3970], [4028, 4029], [4142, 4143], [4186, 4187], [4274, 4275]], "23": [[3042, 3045], [3152, 3155], [3854, 3857], [3996, 3999], [4870, 4873]], "24": [[1737, 1741], [1756, 1760], [1774, 1778], [1826, 1830], [1881, 1885], [1935, 1939], [2037, 2041], [2097, 2104]], "25": [[4604, 4607]], "26": [[3068, 3070], [3127, 3131]], "27": [[678, 683], [764, 768], [2651, 2656], [3393, 3398], [4238, 4243], [4397, 4401]], "28": [[3774, 3778], [3795, 3800]], "29": [[3336, 3338]], "3": [[2089, 2093], [2151, 2156], [2185, 2189], [2279, 2283], [3349, 3353]], "30": [[852, 854], [1258, 1259]], "31": [[1195, 1197], [2423, 2425]], "32": [[3700, 3701], [3691, 3692]], "33": [[2831, 2833], [4354, 4356], [4371, 4373]], "34": [[3032, 3033]], "35": [[3834, 3836]], "36": [[71, 74], [549, 552], [1277, 1280], [1352, 1355], [1550, 1553], [1730, 1733], [1832, 1835], [1893, 1896], [2019, 2022], [2052, 2055], [2121, 2124], [2140, 2143], [2198, 2201], [2205, 2208], [2240, 2243], [2250, 2253], [2294, 2297]], "37": [[1055, 1056], [1754, 1755], [2494, 2495]], "38": [[1924, 1926], [1947, 1949]], "39": [[3696, 3699]], "4": [[3722, 3724], [4575, 4579], [4789, 4793]], "40": [[1673, 1675], [4809, 4811]], "41": [[3036, 3037]], "42": [[3715, 3719]], "43": [[1125, 1127], [3474, 3476], [4324, 4326]], "44": [[1050, 1052], [1058, 1060], [1768, 1770], [2489, 2491]], "45": [[3268, 3270], [3322, 3324]], "46": [[2964, 2966], [4794, 4796]], "47": [[3443, 3447]], "48": [[787, 788], [3934, 3935], [3936, 3937], [3962, 3963], [3989, 3990], [4012, 4013], [4015, 4016], [4167, 4168], [4204, 4205], [4253, 4254], [4435, 4436]], "49": [[4380, 4381]], "5": [[3034, 3035], [4475, 4476]], "50": [[952, 955]], "51": [[3339, 3341]], "52": [[3286, 3288]], "53": [[943, 945]], "54": [[3562, 3563], [3564, 3565], [3566, 3567], [3581, 3582]], "55": [[3254, 3257]], "56": [[706, 710], [3417, 3421]], "57": [[101, 103]], "58": [[3589, 3590]], "59": [[1264, 1266]], "6": [[1210, 1213], [1219, 1222]], "60": [[3027, 3029]], "61": [[3644, 3645]], "62": [[4320, 4322]], "63": [[3543, 3544]], "64": [[3617, 3621]], "65": [[233, 235], [244, 246], [325, 327], [686, 688], [1005, 1007], [3251, 3253], [4546, 4548]], "66": [[4503, 4508]], "67": [[3485, 3487]], "68": [[1242, 1246], [1246, 1250], [1496, 1500], [1679, 1683], [1744, 1748], [1808, 1812], [1982, 1986]], "69": [[2910, 2915]], "7": [[848, 851], [3768, 3773]], "70": [[2268, 2270]], "71": [[2839, 2844]], "72": [[3591, 3592]], "73": [[879, 881]], "74": [[3362, 3365]], "75": [[4684, 4686]], "76": [[3310, 3312]], "77": [[4360, 4362]], "78": [[2902, 2905]], "79": [[564, 566]], "8": [[3978, 3981], [4005, 4008], [4065, 4068], [4073, 4076]], "80": [[745, 747]], "81": [[4845, 4849]], "82": [[2906, 2907]], "83": [[2299, 2303]], "84": [[2916, 2917]], "85": [[2959, 2962]], "86": [[3478, 3483]], "87": [[990, 992]], "88": [[2893, 2898]], "89": [[4589, 4592]], "9": [[3673, 3674], [4460, 4463], [3599, 3602]], "90": [[2517, 2522]], "91": [[1254, 1257]], "92": [[660, 662]], "93": [[3572, 3573]], "94": [[2973, 2975]], "95": [[3167, 3173]], "96": [[2449, 2454]], "97": [[4363, 4364]], "98": [[3626, 3631], [3659, 3664]], "99": [[2636, 2640]]}, "doc_id": "218b80da3eb15ae35267d280dcc4a806d515334a", "method_subrelations": {"CNN_Seq2Seq___Fluency_Boost": [[[0, 11], "CNN_Seq2Seq"], [[14, 27], "Fluency_Boost"]], "CNN_Seq2Seq___Fluency_Boost_and_inference": [[[0, 11], "CNN_Seq2Seq"], [[14, 41], "Fluency_Boost_and_inference"]]}, "n_ary_relations": [{"Material": "Unrestricted", "Method": "CNN_Seq2Seq___Fluency_Boost", "Metric": "F0_5", "Task": "Grammatical_Error_Correction", "score": "61.34"}, {"Material": "Unrestricted", "Method": "CNN_Seq2Seq___Fluency_Boost", "Metric": "F0_5", "Task": "Grammatical_Error_Correction", "score": "76.88"}, {"Material": "Unrestricted", "Method": "CNN_Seq2Seq___Fluency_Boost_and_inference", "Metric": "GLEU", "Task": "Grammatical_Error_Correction", "score": "62.37"}], "ner": [[8, 12, "Task"], [16, 22, "Method"], [23, 24, "Method"], [32, 35, "Task"], [36, 37, "Task"], [49, 55, "Method"], [56, 59, "Method"], [71, 74, "Method"], [88, 91, "Method"], [101, 103, "Method"], [105, 108, "Method"], [109, 110, "Method"], [141, 142, "Metric"], [144, 147, "Material"], [152, 154, "Method"], [163, 164, "Material"], [167, 168, "Material"], [199, 202, "Task"], [203, 204, "Task"], [233, 235, "Method"], [236, 237, "Task"], [244, 246, "Method"], [325, 327, "Method"], [392, 396, "Task"], [401, 407, "Method"], [416, 419, "Task"], [424, 426, "Method"], [549, 552, "Method"], [564, 566, "Metric"], [568, 570, "Task"], [571, 575, "Method"], [584, 588, "Method"], [660, 662, "Task"], [669, 673, "Method"], [678, 683, "Method"], [686, 688, "Method"], [706, 710, "Task"], [730, 732, "Method"], [745, 747, "Method"], [764, 768, "Method"], [787, 788, "Metric"], [793, 796, "Method"], [797, 798, "Method"], [805, 807, "Method"], [819, 822, "Material"], [843, 847, "Task"], [848, 851, "Task"], [852, 854, "Task"], [879, 881, "Method"], [943, 945, "Method"], [952, 955, "Method"], [956, 957, "Method"], [981, 983, "Task"], [990, 992, "Method"], [1001, 1004, "Task"], [1005, 1007, "Method"], [1008, 1009, "Task"], [1050, 1052, "Method"], [1055, 1056, "Method"], [1058, 1060, "Method"], [1108, 1110, "Metric"], [1125, 1127, "Method"], [1159, 1162, "Method"], [1195, 1197, "Task"], [1205, 1209, "Method"], [1210, 1213, "Method"], [1214, 1217, "Method"], [1219, 1222, "Method"], [1242, 1246, "Method"], [1246, 1250, "Method"], [1254, 1257, "Task"], [1258, 1259, "Task"], [1264, 1266, "Method"], [1270, 1273, "Method"], [1277, 1280, "Method"], [1352, 1355, "Method"], [1437, 1439, "Metric"], [1496, 1500, "Method"], [1550, 1553, "Method"], [1624, 1626, "Task"], [1673, 1675, "Method"], [1679, 1683, "Method"], [1687, 1690, "Method"], [1730, 1733, "Method"], [1737, 1741, "Method"], [1744, 1748, "Method"], [1754, 1755, "Method"], [1756, 1760, "Method"], [1768, 1770, "Method"], [1774, 1778, "Method"], [1808, 1812, "Method"], [1826, 1830, "Method"], [1832, 1835, "Method"], [1881, 1885, "Method"], [1893, 1896, "Method"], [1924, 1926, "Task"], [1935, 1939, "Method"], [1947, 1949, "Task"], [1982, 1986, "Method"], [1987, 1990, "Method"], [2019, 2022, "Method"], [2037, 2041, "Method"], [2052, 2055, "Method"], [2089, 2093, "Method"], [2097, 2104, "Method"], [2121, 2124, "Method"], [2140, 2143, "Method"], [2151, 2156, "Method"], [2159, 2165, "Method"], [2185, 2189, "Method"], [2193, 2196, "Method"], [2198, 2201, "Method"], [2205, 2208, "Method"], [2210, 2213, "Method"], [2233, 2236, "Method"], [2240, 2243, "Method"], [2250, 2253, "Method"], [2261, 2264, "Method"], [2268, 2270, "Method"], [2279, 2283, "Method"], [2294, 2297, "Method"], [2299, 2303, "Method"], [2350, 2353, "Method"], [2361, 2365, "Method"], [2381, 2382, "Task"], [2423, 2425, "Task"], [2444, 2447, "Task"], [2449, 2454, "Task"], [2482, 2486, "Method"], [2489, 2491, "Method"], [2494, 2495, "Method"], [2517, 2522, "Method"], [2526, 2529, "Method"], [2540, 2543, "Method"], [2595, 2598, "Method"], [2626, 2631, "Task"], [2636, 2640, "Method"], [2646, 2650, "Method"], [2651, 2656, "Method"], [2666, 2668, "Method"], [2690, 2692, "Method"], [2699, 2701, "Method"], [2714, 2719, "Task"], [2722, 2723, "Method"], [2831, 2833, "Method"], [2839, 2844, "Task"], [2893, 2898, "Material"], [2902, 2905, "Material"], [2906, 2907, "Material"], [2910, 2915, "Material"], [2916, 2917, "Material"], [2959, 2962, "Material"], [2964, 2966, "Material"], [2969, 2972, "Task"], [2973, 2975, "Material"], [2994, 2995, "Material"], [3015, 3018, "Material"], [3027, 3029, "Metric"], [3032, 3033, "Method"], [3034, 3035, "Method"], [3036, 3037, "Method"], [3042, 3045, "Material"], [3047, 3048, "Metric"], [3068, 3070, "Material"], [3127, 3131, "Material"], [3152, 3155, "Material"], [3156, 3159, "Material"], [3167, 3173, "Method"], [3176, 3182, "Method"], [3190, 3191, "Task"], [3216, 3217, "Method"], [3251, 3253, "Method"], [3254, 3257, "Method"], [3268, 3270, "Metric"], [3286, 3288, "Metric"], [3292, 3293, "Task"], [3310, 3312, "Metric"], [3322, 3324, "Metric"], [3336, 3338, "Method"], [3339, 3341, "Task"], [3343, 3346, "Task"], [3349, 3353, "Method"], [3362, 3365, "Material"], [3387, 3390, "Task"], [3393, 3398, "Method"], [3417, 3421, "Task"], [3443, 3447, "Task"], [3474, 3476, "Method"], [3478, 3483, "Method"], [3485, 3487, "Material"], [3496, 3498, "Metric"], [3518, 3519, "Material"], [3523, 3525, "Method"], [3533, 3536, "Method"], [3541, 3542, "Material"], [3543, 3544, "Task"], [3562, 3563, "Method"], [3564, 3565, "Method"], [3566, 3567, "Method"], [3572, 3573, "Method"], [3581, 3582, "Method"], [3589, 3590, "Method"], [3591, 3592, "Method"], [3617, 3621, "Method"], [3622, 3623, "Method"], [3624, 3625, "Method"], [3626, 3631, "Method"], [3636, 3637, "Method"], [3644, 3645, "Method"], [3659, 3664, "Method"], [3673, 3674, "Method"], [3685, 3688, "Task"], [3696, 3699, "Method"], [3700, 3701, "Method"], [3702, 3704, "Method"], [3705, 3707, "Method"], [3715, 3719, "Method"], [3722, 3724, "Material"], [3758, 3759, "Method"], [3760, 3763, "Method"], [3768, 3773, "Task"], [3774, 3778, "Method"], [3787, 3789, "Method"], [3795, 3800, "Method"], [3834, 3836, "Material"], [3839, 3842, "Method"], [3854, 3857, "Material"], [3859, 3860, "Metric"], [3871, 3874, "Material"], [3879, 3882, "Task"], [3888, 3891, "Material"], [3917, 3920, "Method"], [3934, 3935, "Metric"], [3936, 3937, "Metric"], [3939, 3940, "Metric"], [3943, 3946, "Method"], [3952, 3954, "Method"], [3955, 3956, "Task"], [3958, 3961, "Method"], [3962, 3963, "Metric"], [3978, 3981, "Material"], [3989, 3990, "Metric"], [3996, 3999, "Material"], [4005, 4008, "Material"], [4012, 4013, "Metric"], [4015, 4016, "Metric"], [4018, 4021, "Method"], [4035, 4036, "Material"], [4037, 4040, "Method"], [4065, 4068, "Material"], [4069, 4071, "Material"], [4073, 4076, "Material"], [4114, 4117, "Method"], [4118, 4119, "Method"], [4125, 4126, "Material"], [4127, 4129, "Material"], [4135, 4138, "Method"], [4151, 4154, "Method"], [4155, 4156, "Task"], [4167, 4168, "Metric"], [4178, 4181, "Method"], [4200, 4203, "Method"], [4204, 4205, "Metric"], [4212, 4216, "Method"], [4232, 4235, "Method"], [4238, 4243, "Method"], [4253, 4254, "Metric"], [4320, 4322, "Method"], [4324, 4326, "Method"], [4354, 4356, "Method"], [4360, 4362, "Metric"], [4363, 4364, "Method"], [4371, 4373, "Method"], [4380, 4381, "Method"], [4397, 4401, "Method"], [4460, 4463, "Method"], [4475, 4476, "Method"], [4503, 4508, "Task"], [4541, 4542, "Task"], [4546, 4548, "Method"], [4575, 4579, "Material"], [4584, 4588, "Method"], [4589, 4592, "Task"], [4594, 4595, "Task"], [4596, 4597, "Task"], [4604, 4607, "Task"], [4608, 4609, "Task"], [4633, 4636, "Method"], [4675, 4676, "Task"], [4684, 4686, "Method"], [4693, 4694, "Task"], [4701, 4704, "Task"], [4776, 4782, "Method"], [4783, 4786, "Method"], [4789, 4793, "Material"], [4794, 4796, "Material"], [4809, 4811, "Method"], [4822, 4825, "Method"], [4829, 4830, "Task"], [4845, 4849, "Method"], [4870, 4873, "Material"], [4874, 4877, "Material"], [42, 43, "Method"], [112, 113, "Method"], [132, 133, "Material"], [184, 185, "Method"], [342, 343, "Method"], [800, 801, "Method"], [810, 811, "Material"], [836, 837, "Task"], [858, 859, "Task"], [865, 866, "Method"], [939, 940, "Method"], [1040, 1041, "Task"], [1308, 1309, "Method"], [1372, 1373, "Method"], [1817, 1818, "Method"], [1851, 1852, "Method"], [2477, 2478, "Method"], [2554, 2555, "Method"], [2572, 2573, "Method"], [2783, 2784, "Method"], [2986, 2987, "Material"], [3003, 3004, "Material"], [3050, 3051, "Material"], [3061, 3062, "Material"], [3098, 3099, "Material"], [3111, 3112, "Material"], [3121, 3122, "Material"], [3138, 3139, "Material"], [3144, 3145, "Material"], [3414, 3415, "Method"], [3559, 3560, "Task"], [3568, 3569, "Task"], [3599, 3602, "Method"], [3649, 3650, "Task"], [3682, 3683, "Method"], [3691, 3692, "Method"], [3693, 3694, "Method"], [3713, 3714, "Method"], [3755, 3756, "Method"], [3809, 3810, "Task"], [3812, 3813, "Material"], [3814, 3815, "Material"], [3820, 3821, "Method"], [3826, 3827, "Task"], [3847, 3848, "Method"], [3861, 3862, "Material"], [3868, 3869, "Metric"], [3895, 3896, "Metric"], [3902, 3903, "Material"], [3924, 3925, "Method"], [3969, 3970, "Material"], [4028, 4029, "Material"], [4041, 4042, "Metric"], [4142, 4143, "Material"], [4171, 4172, "Method"], [4186, 4187, "Material"], [4272, 4273, "Method"], [4274, 4275, "Material"], [4435, 4436, "Metric"], [4457, 4458, "Task"], [4672, 4673, "Method"], [4714, 4715, "Task"], [4767, 4768, "Method"], [4770, 4771, "Task"], [4818, 4819, "Method"]], "sections": [[0, 175], [175, 839], [839, 999], [999, 1240], [1240, 1735], [1735, 2087], [2087, 2348], [2348, 2442], [2442, 2447], [2447, 2624], [2624, 2877], [2877, 2880], [2880, 3161], [3161, 3545], [3545, 4450], [4450, 4753], [4753, 4878], [4878, 4881]], "sentences": [[0, 16], [16, 39], [39, 56], [56, 104], [104, 175], [175, 178], [178, 188], [188, 205], [205, 228], [228, 241], [241, 265], [265, 285], [285, 318], [318, 322], [322, 352], [352, 384], [384, 415], [415, 495], [495, 527], [527, 567], [567, 611], [611, 628], [628, 648], [648, 705], [705, 733], [733, 764], [764, 789], [789, 839], [839, 847], [847, 912], [912, 980], [980, 999], [999, 1004], [1004, 1021], [1021, 1034], [1034, 1048], [1048, 1057], [1057, 1100], [1100, 1135], [1135, 1148], [1148, 1155], [1155, 1198], [1198, 1240], [1240, 1246], [1246, 1297], [1297, 1325], [1325, 1327], [1327, 1366], [1366, 1393], [1393, 1409], [1409, 1432], [1432, 1455], [1455, 1476], [1476, 1501], [1501, 1536], [1536, 1543], [1543, 1587], [1587, 1634], [1634, 1676], [1676, 1691], [1691, 1735], [1735, 1741], [1741, 1771], [1771, 1807], [1807, 1842], [1842, 1864], [1864, 1866], [1866, 1880], [1880, 1906], [1906, 1927], [1927, 1943], [1943, 1977], [1977, 2009], [2009, 2034], [2034, 2087], [2087, 2093], [2093, 2125], [2125, 2144], [2144, 2165], [2165, 2172], [2172, 2228], [2228, 2265], [2265, 2276], [2276, 2348], [2348, 2359], [2359, 2383], [2383, 2404], [2404, 2412], [2412, 2442], [2442, 2447], [2447, 2454], [2454, 2487], [2487, 2505], [2505, 2530], [2530, 2567], [2567, 2587], [2587, 2610], [2610, 2624], [2624, 2631], [2631, 2657], [2657, 2711], [2711, 2722], [2722, 2745], [2745, 2823], [2823, 2877], [2877, 2880], [2880, 2885], [2885, 2928], [2928, 2939], [2939, 2963], [2963, 2984], [2984, 2992], [2992, 3003], [3003, 3022], [3022, 3053], [3053, 3087], [3087, 3101], [3101, 3132], [3132, 3161], [3161, 3165], [3165, 3193], [3193, 3229], [3229, 3239], [3239, 3241], [3241, 3248], [3248, 3266], [3266, 3291], [3291, 3315], [3315, 3327], [3327, 3342], [3342, 3376], [3376, 3386], [3386, 3405], [3405, 3442], [3442, 3473], [3473, 3506], [3506, 3545], [3545, 3549], [3549, 3580], [3580, 3589], [3589, 3596], [3596, 3622], [3622, 3624], [3624, 3638], [3638, 3644], [3644, 3646], [3646, 3676], [3676, 3689], [3689, 3707], [3707, 3709], [3709, 3729], [3729, 3753], [3753, 3774], [3774, 3801], [3801, 3817], [3817, 3839], [3839, 3850], [3850, 3875], [3875, 3885], [3885, 3906], [3906, 3917], [3917, 3957], [3957, 4010], [4010, 4031], [4031, 4057], [4057, 4072], [4072, 4107], [4107, 4145], [4145, 4161], [4161, 4196], [4196, 4226], [4226, 4284], [4284, 4342], [4342, 4394], [4394, 4450], [4450, 4454], [4454, 4490], [4490, 4515], [4515, 4536], [4536, 4543], [4543, 4568], [4568, 4629], [4629, 4656], [4656, 4687], [4687, 4734], [4734, 4753], [4753, 4756], [4756, 4783], [4783, 4843], [4843, 4878], [4878, 4881]], "words": ["document", ":", "Reaching", "Human", "-", "level", "Performance", "in", "Automatic", "Grammatical", "Error", "Correction", ":", "An", "Empirical", "Study", "Neural", "sequence", "-", "to", "-", "sequence", "(", "seq2seq", ")", "approaches", "have", "proven", "to", "be", "successful", "in", "grammatical", "error", "correction", "(", "GEC", ")", ".", "Based", "on", "the", "seq2seq", "framework", ",", "we", "propose", "a", "novel", "fluency", "boost", "learning", "and", "inference", "mechanism", ".", "Fluency", "boosting", "learning", "generates", "diverse", "error", "-", "corrected", "sentence", "pairs", "during", "training", ",", "enabling", "the", "error", "correction", "model", "to", "learn", "how", "to", "improve", "a", "sentence", "\u2019s", "fluency", "from", "more", "instances", ",", "while", "fluency", "boosting", "inference", "allows", "the", "model", "to", "correct", "a", "sentence", "incrementally", "with", "multiple", "inference", "steps", ".", "Combining", "fluency", "boost", "learning", "and", "inference", "with", "convolutional", "seq2seq", "models", ",", "our", "approach", "achieves", "the", "state", "-", "of", "-", "the", "-", "art", "performance", ":", "75.72", "(", ")", "on", "CoNLL", "-", "2014", "10", "annotation", "dataset", "and", "62.42", "(", "GLEU", ")", "on", "JFLEG", "test", "set", "respectively", ",", "becoming", "the", "first", "GEC", "system", "that", "reaches", "human", "-", "level", "performance", "(", "72.58", "for", "CoNLL", "and", "62.37", "for", "JFLEG", ")", "on", "both", "of", "the", "benchmarks", ".", "section", ":", "Introduction", "Sequence", "-", "to", "-", "sequence", "(", "seq2seq", ")", "models", "cho", "-", "EtAl:2014:EMNLP2014", ",", "DBLP", ":", "journals", "/", "corr", "/", "SutskeverVL14", "for", "grammatical", "error", "correction", "(", "GEC", ")", "have", "drawn", "growing", "attention", "yuan2016grammatical", ",", "xie2016neural", ",", "ji2017nested", ",", "schmaltz", "-", "EtAl:2017:EMNLP2017", ",", "sakaguchi2017grammatical", ",", "chollampatt2018", ",", "junczys2018approaching", "in", "recent", "years", ".", "However", ",", "most", "of", "the", "seq2seq", "models", "for", "GEC", "have", "two", "flaws", ".", "First", ",", "the", "seq2seq", "models", "are", "trained", "with", "only", "limited", "error", "-", "corrected", "sentence", "pairs", "like", "Figure", "[", "reference", "]", "(", "a", ")", ".", "Limited", "by", "the", "size", "of", "training", "data", ",", "the", "models", "with", "millions", "of", "parameters", "may", "not", "be", "well", "generalized", ".", "Thus", ",", "it", "is", "common", "that", "the", "models", "fail", "to", "correct", "a", "sentence", "perfectly", "even", "if", "the", "sentence", "is", "slightly", "different", "from", "the", "training", "instance", ",", "as", "illustrated", "by", "Figure", "[", "reference", "]", "(", "b", ")", ".", "Second", ",", "the", "seq2seq", "models", "usually", "can", "not", "perfectly", "correct", "a", "sentence", "with", "many", "grammatical", "errors", "through", "single", "-", "round", "seq2seq", "inference", ",", "as", "shown", "in", "Figure", "[", "reference", "]", "(", "b", ")", "and", "[", "reference", "]", "(", "c", ")", ",", "because", "some", "errors", "in", "a", "sentence", "may", "make", "the", "context", "strange", ",", "which", "confuses", "the", "models", "to", "correct", "other", "errors", ".", "To", "address", "the", "above", "-", "mentioned", "limitations", "in", "model", "learning", "and", "inference", ",", "we", "propose", "a", "novel", "fluency", "boost", "learning", "and", "inference", "mechanism", ",", "illustrated", "in", "Figure", "[", "reference", "]", ".", "For", "fluency", "boosting", "learning", ",", "not", "only", "is", "a", "seq2seq", "model", "trained", "with", "original", "error", "-", "corrected", "sentence", "pairs", ",", "but", "also", "it", "generates", "less", "fluent", "sentences", "(", "e.g.", ",", "from", "its", "n", "-", "best", "outputs", ")", "to", "establish", "new", "error", "-", "corrected", "sentence", "pairs", "by", "pairing", "them", "with", "their", "correct", "sentences", "during", "training", ",", "as", "long", "as", "the", "sentences", "\u2019", "fluency", "is", "below", "that", "of", "their", "correct", "sentences", ",", "as", "Figure", "[", "reference", "]", "(", "a", ")", "shows", ".", "Specifically", ",", "we", "call", "the", "generated", "error", "-", "corrected", "sentence", "pairs", "fluency", "boost", "sentence", "pairs", "because", "the", "sentence", "in", "the", "target", "side", "always", "improves", "fluency", "over", "that", "in", "the", "source", "side", ".", "The", "generated", "fluency", "boost", "sentence", "pairs", "during", "training", "will", "be", "used", "as", "additional", "training", "instances", "during", "subsequent", "training", "epochs", ",", "allowing", "the", "error", "correction", "model", "to", "see", "more", "grammatically", "incorrect", "sentences", "during", "training", "and", "accordingly", "improving", "its", "generalization", "ability", ".", "For", "model", "inference", ",", "fluency", "boost", "inference", "mechanism", "allows", "the", "model", "to", "correct", "a", "sentence", "incrementally", "with", "multi", "-", "round", "inference", "as", "long", "as", "the", "proposed", "edits", "can", "boost", "the", "sentence", "\u2019s", "fluency", ",", "as", "Figure", "[", "reference", "]", "(", "b", ")", "shows", ".", "For", "a", "sentence", "with", "multiple", "grammatical", "errors", ",", "some", "of", "the", "errors", "will", "be", "corrected", "first", ".", "The", "corrected", "parts", "will", "make", "the", "context", "clearer", ",", "which", "may", "benefit", "the", "model", "to", "correct", "the", "remaining", "errors", ".", "Moreover", ",", "based", "on", "the", "special", "characteristics", "of", "this", "task", "that", "the", "output", "prediction", "can", "be", "repeatedly", "edited", "and", "the", "basic", "fluency", "boost", "inference", "idea", ",", "we", "further", "propose", "a", "round", "-", "way", "correction", "approach", "that", "uses", "two", "seq2seq", "models", "whose", "decoding", "orders", "are", "left", "-", "to", "-", "right", "and", "right", "-", "to", "-", "left", "respectively", ".", "For", "round", "-", "way", "correction", ",", "a", "sentence", "will", "be", "corrected", "successively", "by", "the", "right", "-", "to", "-", "left", "and", "left", "-", "to", "-", "right", "seq2seq", "model", ".", "Since", "the", "left", "-", "to", "-", "right", "and", "right", "-", "to", "-", "left", "decoder", "decode", "a", "sequence", "with", "different", "contexts", ",", "they", "have", "their", "unique", "advantages", "for", "specific", "error", "types", ".", "Round", "-", "way", "correction", "can", "fully", "exploit", "their", "pros", "and", "make", "them", "complement", "each", "other", ",", "which", "results", "in", "a", "significant", "improvement", "of", "recall", ".", "Experiments", "show", "that", "combining", "fluency", "boost", "learning", "and", "inference", "with", "convolutional", "seq2seq", "models", ",", "our", "best", "GEC", "system", "achieves", "75.72", "on", "CoNLL", "-", "2014", "10", "annotation", "dataset", "and", "62.42", "on", "JFLEG", "test", "set", ",", "becoming", "the", "first", "system", "reaching", "human", "-", "level", "performance", "on", "both", "of", "the", "GEC", "benchmarks", ".", "section", ":", "Background", ":", "Neural", "grammatical", "error", "correction", "As", "neural", "machine", "translation", "(", "NMT", ")", ",", "a", "typical", "neural", "GEC", "approach", "uses", "an", "encoder", "-", "decoder", "seq2seq", "model", "DBLP", ":", "journals", "/", "corr", "/", "SutskeverVL14", ",", "cho", "-", "EtAl:2014:EMNLP2014", "with", "attention", "mechanism", "DBLP", ":", "journals", "/", "corr", "/", "BahdanauCB14", "to", "edit", "a", "raw", "sentence", "into", "the", "grammatically", "correct", "sentence", "it", "should", "be", ",", "as", "Figure", "[", "reference", "]", "(", "a", ")", "shows", ".", "Given", "a", "raw", "sentence", "and", "its", "corrected", "sentence", "in", "which", "and", "are", "the", "-", "th", "and", "-", "th", "words", "of", "sentence", "and", "respectively", ",", "the", "error", "correction", "seq2seq", "model", "learns", "a", "probabilistic", "mapping", "from", "error", "-", "corrected", "sentence", "pairs", "through", "maximum", "likelihood", "estimation", "(", "MLE", ")", ",", "which", "learns", "model", "parameters", "to", "maximize", "the", "following", "equation", ":", "where", "denotes", "the", "set", "of", "error", "-", "corrected", "sentence", "pairs", ".", "For", "model", "inference", ",", "an", "output", "sequence", "is", "selected", "through", "beam", "search", ",", "which", "maximizes", "the", "following", "equation", ":", "section", ":", "Fluency", "boost", "learning", "Conventional", "seq2seq", "models", "for", "GEC", "learn", "model", "parameters", "only", "from", "original", "error", "-", "corrected", "sentence", "pairs", ".", "However", ",", "such", "error", "-", "corrected", "sentence", "pairs", "are", "not", "sufficiently", "available", ".", "As", "a", "result", ",", "many", "neural", "GEC", "models", "are", "not", "very", "well", "generalized", ".", "Fortunately", ",", "neural", "GEC", "is", "different", "from", "NMT", ".", "For", "neural", "GEC", ",", "its", "goal", "is", "improving", "a", "sentence", "\u2019s", "fluency", "without", "changing", "its", "original", "meaning", ";", "thus", ",", "any", "sentence", "pair", "that", "satisfies", "this", "condition", "(", "we", "call", "it", "fluency", "boost", "condition", ")", "can", "be", "used", "as", "a", "training", "instance", ".", "In", "this", "work", ",", "we", "define", "as", "the", "fluency", "score", "of", "a", "sentence", ":", "where", "is", "the", "probability", "of", "given", "context", ",", "computed", "by", "a", "language", "model", ",", "and", "is", "the", "length", "of", "sentence", ".", "is", "actually", "the", "cross", "entropy", "of", "the", "sentence", ",", "whose", "range", "is", ".", "Accordingly", ",", "the", "range", "of", "is", ".", "The", "core", "idea", "of", "fluency", "boost", "learning", "is", "to", "generate", "fluency", "boost", "sentence", "pairs", "that", "satisfy", "the", "fluency", "boost", "condition", "during", "training", ",", "as", "Figure", "[", "reference", "]", "(", "a", ")", "illustrates", ",", "so", "that", "these", "pairs", "can", "further", "help", "model", "learning", ".", "In", "this", "section", ",", "we", "present", "three", "fluency", "boost", "learning", "strategies", ":", "back", "-", "boost", ",", "self", "-", "boost", ",", "and", "dual", "-", "boost", "that", "generate", "fluency", "boost", "sentence", "pairs", "in", "different", "ways", ",", "as", "illustrated", "in", "Figure", "[", "reference", "]", ".", "subsection", ":", "Back", "-", "boost", "learning", "Back", "-", "boost", "learning", "borrows", "the", "idea", "from", "back", "translation", "sennrich2016improving", "in", "NMT", ",", "referring", "to", "training", "a", "backward", "model", "(", "we", "call", "it", "error", "generation", "model", ",", "as", "opposed", "to", "error", "correction", "model", ")", "that", "is", "used", "to", "convert", "a", "fluent", "sentence", "to", "a", "less", "fluent", "sentence", "with", "errors", ".", "Since", "the", "less", "fluent", "sentences", "are", "generated", "by", "the", "error", "generation", "seq2seq", "model", "trained", "with", "error", "-", "corrected", "data", ",", "they", "usually", "do", "not", "change", "the", "original", "sentence", "\u2019s", "meaning", ";", "thus", ",", "they", "can", "be", "paired", "with", "their", "correct", "sentences", ",", "establishing", "fluency", "boost", "sentence", "pairs", "that", "can", "be", "used", "as", "training", "instances", "for", "error", "correction", "models", ",", "as", "Figure", "[", "reference", "]", "(", "a", ")", "shows", ".", "Specifically", ",", "we", "first", "train", "a", "seq2seq", "error", "generation", "model", "with", "which", "is", "identical", "to", "except", "that", "the", "source", "sentence", "and", "the", "target", "sentence", "are", "interchanged", ".", "Then", ",", "we", "use", "the", "model", "to", "predict", "-", "best", "outputs", "given", "a", "correct", "sentence", ".", "Given", "the", "fluency", "boost", "condition", ",", "we", "compare", "the", "fluency", "of", "each", "output", "(", "where", ")", "to", "that", "of", "its", "correct", "sentence", ".", "If", "an", "output", "sentence", "\u2019s", "fluency", "score", "is", "much", "lower", "than", "its", "correct", "sentence", ",", "we", "call", "it", "a", "disfluency", "candidate", "of", ".", "To", "formalize", "this", "process", ",", "we", "first", "define", "to", "denote", "the", "-", "best", "outputs", "predicted", "by", "model", "given", "the", "input", ".", "Then", ",", "disfluency", "candidates", "of", "a", "correct", "sentence", "can", "be", "derived", ":", "where", "denotes", "the", "disfluency", "candidate", "set", "for", "in", "back", "-", "boost", "learning", ".", "is", "a", "threshold", "to", "determine", "if", "is", "less", "fluent", "than", "and", "it", "should", "be", "slightly", "larger", "than", ",", "which", "helps", "filter", "out", "sentence", "pairs", "with", "unnecessary", "edits", "(", "e.g.", ",", "I", "like", "this", "book", ".", "I", "like", "the", "book", ".", ")", ".", "In", "the", "subsequent", "training", "epochs", ",", "the", "error", "correction", "model", "will", "not", "only", "learn", "from", "the", "original", "error", "-", "corrected", "sentence", "pairs", "(", ",", ")", ",", "but", "also", "learn", "from", "fluency", "boost", "sentence", "pairs", "(", ",", ")", "where", "is", "a", "sample", "of", ")", ".", "We", "summarize", "this", "process", "in", "Algorithm", "[", "reference", "]", "where", "is", "the", "set", "of", "original", "error", "-", "corrected", "sentence", "pairs", ",", "and", "can", "be", "tentatively", "considered", "identical", "to", "when", "there", "is", "no", "additional", "native", "data", "to", "help", "model", "training", "(", "see", "Section", "[", "reference", "]", ")", ".", "Note", "that", "we", "constrain", "the", "size", "of", "not", "to", "exceed", "(", "the", "7th", "line", "in", "Algorithm", "[", "reference", "]", ")", "to", "avoid", "that", "too", "many", "fluency", "boost", "pairs", "overwhelm", "the", "effects", "of", "the", "original", "error", "-", "corrected", "pairs", "on", "model", "learning", ".", "[", "t", "]", "Back", "-", "boost", "learning", "[", "1", "]", "Train", "error", "generation", "model", "with", ";", "each", "sentence", "pair", "Compute", "according", "to", "Eq", "(", "[", "reference", "]", ")", ";", "each", "training", "epoch", ";", "Derive", "a", "subset", "by", "randomly", "sampling", "elements", "from", ";", "each", "Establish", "a", "fluency", "boost", "pair", "by", "randomly", "sampling", ";", ";", "Update", "error", "correction", "model", "with", ";", "subsection", ":", "Self", "-", "boost", "learning", "In", "contrast", "to", "back", "-", "boost", "learning", "whose", "core", "idea", "is", "originally", "from", "NMT", ",", "self", "-", "boost", "learning", "is", "original", ",", "which", "is", "specially", "devised", "for", "neural", "GEC", ".", "The", "idea", "of", "self", "-", "boost", "learning", "is", "illustrated", "by", "Figure", "[", "reference", "]", "(", "b", ")", "and", "was", "already", "briefly", "introduced", "in", "Section", "[", "reference", "]", "and", "Figure", "[", "reference", "]", "(", "a", ")", ".", "Unlike", "back", "-", "boost", "learning", "in", "which", "an", "error", "generation", "seq2seq", "model", "is", "trained", "to", "generate", "disfluency", "candidates", ",", "self", "-", "boost", "learning", "allows", "the", "error", "correction", "model", "to", "generate", "the", "candidates", "by", "itself", ".", "Since", "the", "disfluency", "candidates", "generated", "by", "the", "error", "correction", "seq2seq", "model", "trained", "with", "error", "-", "corrected", "data", "rarely", "change", "the", "input", "sentence", "\u2019s", "meaning", ";", "thus", ",", "they", "can", "be", "used", "to", "establish", "fluency", "boost", "sentence", "pairs", ".", "For", "self", "-", "boost", "learning", ",", "given", "an", "error", "corrected", "pair", ",", "an", "error", "correction", "model", "first", "predicts", "-", "best", "outputs", "for", "the", "raw", "sentence", ".", "Among", "the", "-", "best", "outputs", ",", "any", "output", "that", "is", "not", "identical", "to", "can", "be", "considered", "as", "an", "error", "prediction", ".", "Instead", "of", "treating", "the", "error", "predictions", "useless", ",", "self", "-", "boost", "learning", "fully", "exploits", "them", ".", "Specifically", ",", "if", "an", "error", "prediction", "is", "much", "less", "fluent", "than", "that", "of", "its", "correct", "sentence", ",", "it", "will", "be", "added", "to", "\u2019s", "disfluency", "candidate", "set", ",", "as", "Eq", "(", "[", "reference", "]", ")", "shows", ":", "In", "contrast", "to", "back", "-", "boost", "learning", ",", "self", "-", "boost", "generates", "disfluency", "candidates", "from", "a", "different", "perspective", "\u2013", "by", "editing", "the", "raw", "sentence", "rather", "than", "the", "correct", "sentence", ".", "It", "is", "also", "noteworthy", "that", "is", "incrementally", "expanded", "because", "the", "error", "correction", "model", "is", "dynamically", "updated", ",", "as", "shown", "in", "Algorithm", "[", "reference", "]", ".", "[", "h", "]", "Self", "-", "boost", "learning", "[", "1", "]", "each", "sentence", "pair", ";", "each", "training", "epoch", "Update", "error", "correction", "model", "with", ";", "Derive", "a", "subset", "by", "randomly", "sampling", "elements", "from", ";", "each", "Update", "according", "to", "Eq", "(", "[", "reference", "]", ")", ";", "Establish", "a", "fluency", "boost", "pair", "by", "randomly", "sampling", ";", ";", "subsection", ":", "Dual", "-", "boost", "learning", "As", "introduced", "above", ",", "back", "-", "and", "self", "-", "boost", "learning", "generate", "disfluency", "candidates", "from", "different", "perspectives", "to", "create", "more", "fluency", "boost", "sentence", "pairs", "to", "benefit", "training", "the", "error", "correction", "model", ".", "Intuitively", ",", "the", "more", "diverse", "disfluency", "candidates", "generated", ",", "the", "more", "helpful", "for", "training", "an", "error", "correction", "model", ".", "Inspired", "by", "and", ",", "we", "propose", "a", "dual", "-", "boost", "learning", "strategy", ",", "combining", "both", "back", "-", "and", "self", "-", "boost", "\u2019s", "perspectives", "to", "generate", "disfluency", "candidates", ".", "As", "Figure", "[", "reference", "]", "(", "c", ")", "shows", ",", "disfluency", "candidates", "in", "dual", "-", "boost", "learning", "are", "from", "both", "the", "error", "generation", "model", "and", "the", "error", "correction", "model", ":", "Moreover", ",", "the", "error", "correction", "model", "and", "the", "error", "generation", "model", "are", "dual", "and", "both", "of", "them", "are", "dynamically", "updated", ",", "which", "improves", "each", "other", ":", "the", "disfluency", "candidates", "produced", "by", "error", "generation", "model", "can", "benefit", "training", "the", "error", "correction", "model", ",", "while", "the", "disfluency", "candidates", "created", "by", "error", "correction", "model", "can", "be", "used", "as", "training", "data", "for", "the", "error", "generation", "model", ".", "We", "summarize", "this", "learning", "approach", "in", "Algorithm", "[", "reference", "]", ".", "[", "t", "]", "Dual", "-", "boost", "learning", "[", "1", "]", "each", ";", ";", ";", "each", "training", "epoch", "Update", "error", "correction", "model", "with", ";", "Update", "error", "generation", "model", "with", ";", ";", ";", "Derive", "a", "subset", "by", "randomly", "sampling", "elements", "from", ";", "each", "Update", "according", "to", "Eq", "(", "[", "reference", "]", ")", ";", "Establish", "a", "fluency", "boost", "pair", "by", "randomly", "sampling", ";", ";", "Establish", "a", "reversed", "fluency", "boost", "pair", "by", "randomly", "sampling", ";", ";", "subsection", ":", "Fluency", "boost", "learning", "with", "large", "-", "scale", "native", "data", "Our", "proposed", "fluency", "boost", "learning", "strategies", "can", "be", "easily", "extended", "to", "utilize", "massive", "native", "text", "data", "which", "proved", "to", "be", "useful", "for", "GEC", ".", "As", "discussed", "in", "Section", "[", "reference", "]", ",", "when", "there", "is", "no", "additional", "native", "data", ",", "in", "Algorithm", "[", "reference", "]", "\u2013", "[", "reference", "]", "is", "identical", "to", ".", "In", "the", "case", "where", "additional", "native", "data", "is", "available", "to", "help", "model", "learning", ",", "becomes", ":", "where", "denotes", "the", "set", "of", "self", "-", "copied", "sentence", "pairs", "from", "native", "data", ".", "section", ":", "Fluency", "boost", "inference", "subsection", ":", "Multi", "-", "round", "error", "correction", "As", "we", "discuss", "in", "Section", "[", "reference", "]", ",", "some", "sentences", "with", "multiple", "grammatical", "errors", "usually", "can", "not", "be", "perfectly", "corrected", "through", "normal", "seq2seq", "inference", "which", "makes", "only", "single", "-", "round", "inference", ".", "Fortunately", ",", "neural", "GEC", "is", "different", "from", "NMT", ":", "its", "source", "and", "target", "language", "are", "the", "same", ".", "The", "characteristic", "allows", "us", "to", "edit", "a", "sentence", "more", "than", "once", "through", "multi", "-", "round", "model", "inference", ",", "which", "motivates", "our", "fluency", "boost", "inference", ".", "As", "Figure", "[", "reference", "]", "(", "b", ")", "shows", ",", "fluency", "boost", "inference", "allows", "a", "sentence", "to", "be", "incrementally", "edited", "through", "multi", "-", "round", "seq2seq", "inference", "as", "long", "as", "the", "sentence", "\u2019s", "fluency", "can", "be", "improved", ".", "Specifically", ",", "an", "error", "correction", "seq2seq", "model", "first", "takes", "a", "raw", "sentence", "as", "an", "input", "and", "outputs", "a", "hypothesis", ".", "Instead", "of", "regarding", "as", "the", "final", "prediction", ",", "fluency", "boost", "inference", "will", "then", "take", "as", "the", "input", "to", "generate", "the", "next", "output", ".", "The", "process", "will", "not", "terminate", "unless", "does", "not", "improve", "in", "terms", "of", "fluency", ".", "subsection", ":", "Round", "-", "way", "error", "correction", "Based", "on", "the", "idea", "of", "multi", "-", "round", "correction", ",", "we", "further", "propose", "an", "advanced", "fluency", "boost", "inference", "approach", ":", "round", "-", "way", "error", "correction", ".", "Instead", "of", "progressively", "correcting", "a", "sentence", "with", "the", "same", "seq2seq", "model", "as", "introduced", "in", "Section", "[", "reference", "]", ",", "round", "-", "way", "correction", "corrects", "a", "sentence", "through", "a", "right", "-", "to", "-", "left", "seq2seq", "model", "and", "a", "left", "-", "to", "-", "right", "seq2seq", "model", "successively", ",", "as", "shown", "in", "Figure", "[", "reference", "]", ".", "The", "motivation", "of", "round", "-", "way", "error", "correction", "is", "straightforward", ".", "Decoders", "with", "different", "decoding", "orders", "decode", "word", "sequences", "with", "different", "contexts", ",", "making", "them", "have", "their", "unique", "advantages", "for", "specific", "error", "types", ".", "For", "the", "example", "in", "Figure", "[", "reference", "]", ",", "the", "error", "of", "a", "lack", "of", "an", "article", "(", "i.e.", ",", "park", "\u2192", "the", "park", ")", "is", "more", "likely", "to", "be", "corrected", "by", "the", "right", "-", "to", "-", "left", "seq2seq", "model", "than", "the", "left", "-", "to", "-", "right", "one", ",", "because", "whether", "to", "add", "an", "article", "depends", "on", "the", "noun", "park", "that", "was", "already", "seen", "by", "the", "right", "-", "to", "-", "left", "model", "when", "it", "made", "the", "decision", ".", "In", "contrast", ",", "the", "left", "-", "to", "-", "right", "model", "might", "be", "better", "at", "dealing", "with", "subject", "-", "verb", "agreement", "errors", "(", "e.g.", ",", "come", "\u2192", "comes", "in", "Figure", "[", "reference", "]", ")", "because", "the", "keyword", "that", "decides", "the", "verb", "form", "is", "its", "subject", "She", "which", "is", "at", "the", "beginning", "of", "the", "sentence", ".", "section", ":", "Experiments", "subsection", ":", "Dataset", "and", "evaluation", "As", "previous", "studies", "ji2017nested", ",", "we", "use", "the", "public", "Lang", "-", "8", "Corpus", "mizumoto2011mining", ",", "tajiri2012tense", ",", "Cambridge", "Learner", "Corpus", "(", "CLC", ")", "nicholls2003cambridge", "and", "NUS", "Corpus", "of", "Learner", "English", "(", "NUCLE", ")", "dahlmeier2013building", "as", "our", "original", "error", "-", "corrected", "training", "data", ".", "Table", "[", "reference", "]", "shows", "the", "stats", "of", "the", "datasets", ".", "In", "addition", ",", "we", "also", "collect", "2", ",", "865", ",", "639", "non", "-", "public", "error", "-", "corrected", "sentence", "pairs", "from", "Lang", "-", "8.com", ".", "The", "native", "data", "we", "use", "for", "fluency", "boost", "learning", "is", "English", "Wikipedia", "that", "contains", "61", ",", "677", ",", "453", "sentences", ".", "We", "use", "CoNLL", "-", "2014", "shared", "task", "dataset", "ng2014conll", "and", "JFLEG", "napoles2017jfleg", "test", "set", "as", "our", "evaluation", "datasets", ".", "CoNLL", "-", "2014", "test", "set", "contains", "1", ",", "312", "sentences", ",", "while", "JFLEG", "test", "set", "has", "747", "sentences", ".", "Being", "consistent", "with", "the", "official", "evaluation", "metrics", ",", "we", "use", "MaxMatch", "(", "M", ")", "dahlmeier", "-", "ng:2012:NAACL", "-", "HLT", "for", "CoNLL", "-", "2014", "and", "use", "GLEU", "napoles2015ground", "for", "JFLEG", "evaluation", ".", "It", "is", "notable", "that", "the", "original", "annotations", "for", "CoNLL", "-", "2014", "dataset", "are", "from", "2", "human", "annotators", ",", "which", "are", "later", "enriched", "by", "that", "contains", "10", "human", "expert", "annotations", "for", "each", "test", "sentence", ".", "We", "evaluate", "systems", "\u2019", "performance", "using", "both", "annotation", "settings", "for", "the", "CoNLL", "dataset", ".", "To", "distinguish", "between", "these", "two", "annotation", "settings", ",", "we", "use", "CoNLL", "-", "2014", "to", "denote", "the", "original", "annotations", ",", "and", "CoNLL", "-", "10", "to", "denote", "the", "10", "-", "human", "annotations", ".", "As", "previous", "studies", ",", "we", "use", "CoNLL", "-", "2013", "test", "set", "and", "JFLEG", "dev", "set", "as", "our", "development", "sets", "for", "CoNLL", "-", "2014", "and", "JFLEG", "test", "set", "respectively", ".", "subsection", ":", "Experimental", "setting", "We", "use", "7", "-", "layer", "convolutional", "seq2seq", "models", "gehring2017convolutional", "as", "our", "error", "correction", "and", "error", "generation", "model", ",", "which", "have", "proven", "to", "be", "effective", "for", "GEC", "chollampatt2018", ".", "As", ",", "we", "set", "the", "dimensionality", "of", "word", "embeddings", "in", "both", "encoders", "and", "decoders", "to", "500", ",", "the", "hidden", "size", "of", "encoders", "and", "decoders", "to", "1", ",", "024", "and", "the", "convolution", "window", "width", "to", "3", ".", "The", "vocabularies", "of", "the", "source", "and", "target", "side", "are", "the", "most", "frequent", "30", "K", "BPE", "tokens", "for", "each", ".", "We", "train", "the", "seq2seq", "models", "using", "Nesterov", "Accelerated", "Gradient", "sutskever2013importance", "optimizer", "with", "a", "momentum", "value", "of", "0.99", ".", "The", "initial", "learning", "rate", "is", "set", "to", "0.25", "and", "it", "will", "be", "reduced", "by", "an", "order", "of", "magnitude", "if", "the", "validation", "perplexity", "stops", "improving", ".", "During", "training", ",", "we", "allow", "each", "batch", "to", "have", "at", "most", "3", ",", "000", "tokens", "per", "GPU", "and", "set", "dropout", "rate", "to", "0.2", ".", "We", "terminate", "the", "training", "process", "when", "the", "learning", "rate", "falls", "below", ".", "As", "and", ",", "we", "train", "4", "models", "with", "different", "random", "initializations", "for", "ensemble", "decoding", ".", "For", "fluency", "boost", "learning", ",", "we", "adopt", "dual", "-", "boost", "learning", "introduced", "in", "Section", "[", "reference", "]", "and", "use", "the", "English", "Wikipedia", "data", "as", "our", "native", "data", "(", "Section", "[", "reference", "]", ")", ".", "Disfluency", "candidates", "are", "generated", "from", "10", "-", "best", "outputs", ".", "For", "fluency", "boost", "inference", ",", "we", "use", "round", "-", "way", "correction", "approach", "introduced", "in", "Section", "[", "reference", "]", ".", "The", "architecture", "of", "the", "right", "-", "to", "-", "left", "seq2seq", "model", "in", "round", "-", "way", "correction", "is", "the", "same", "with", "the", "left", "-", "to", "-", "right", "one", "except", "that", "they", "decode", "sentences", "in", "the", "opposite", "directions", ".", "For", "single", "-", "round", "inference", ",", "we", "follow", "to", "generate", "12", "-", "best", "predictions", "and", "choose", "the", "best", "sentence", "after", "re", "-", "ranking", "with", "edit", "operation", "and", "language", "model", "scores", ".", "The", "language", "model", "is", "the", "5", "-", "gram", "language", "model", "trained", "on", "Common", "Crawl", "released", "by", ",", "which", "is", "also", "used", "for", "computing", "fluency", "score", "in", "Eq", "(", "[", "reference", "]", ")", ".", "As", "most", "of", "the", "systems", "sakaguchi2017grammatical", ",", "chollampatt2018", ",", "grundkiewicz2018near", "evaluated", "on", "JFLEG", "that", "use", "an", "additional", "spell", "checker", "to", "resolve", "spelling", "errors", ",", "we", "use", "a", "public", "spell", "checker", "to", "resolve", "spelling", "errors", "in", "JFLEG", "as", "preprocessing", ".", "subsection", ":", "Experimental", "results", "We", "compare", "our", "systems", "to", "the", "following", "well", "-", "known", "GEC", "systems", ":", "CAMB14", ",", "CAMB16", "and", "CAMB17", ":", "GEC", "systems", "felice2014grammatical", ",", "yuan2016grammatical", ",", "yannakoudakis2017neural", "developed", "by", "Cambridge", "University", ".", "For", "CAMB17", ",", "we", "report", "its", "best", "result", ".", "CUUI", "and", "VT16", ":", "the", "former", "system", "rozovskaya2014illinois", "uses", "a", "classifier", "-", "based", "approach", ",", "which", "is", "improved", "by", "the", "latter", "system", "rozovskaya2016grammatical", "through", "combining", "it", "with", "an", "SMT", "-", "based", "approach", ".", "AMU14", "and", "AMU16", ":", "SMT", "-", "based", "GEC", "systems", "junczys2014amu", ",", "junczys2016phrase", "developed", "by", "AMU", ".", "NUS14", ",", "NUS16", ",", "NUS17", "and", "NUS18", ":", "The", "first", "three", "GEC", "systems", "Susanto2014System", ",", "chollampatt2016adapting", ",", "chollampatt", "-", "ng:2017:BEA", "are", "SMT", "-", "based", "GEC", "systems", "that", "are", "combined", "with", "other", "techniques", "(", "e.g.", ",", "classifiers", ")", ".", "The", "last", "one", "chollampatt2018", "uses", "convolutional", "seq2seq", "models", "for", "grammatical", "error", "correction", ".", "Nested", "-", "RNN", "-", "seq2seq", ":", "a", "Recurrent", "Neural", "Network", "(", "RNN", ")", "seq2seq", "model", "with", "nested", "attention", "ji2017nested", ".", "Back", "-", "CNN", "-", "seq2seq", ":", "a", "convolutional", "seq2seq", "model", "xie2018noising", "trained", "with", "synthesized", "data", "augmented", "by", "back", "translation", ".", "Its", "core", "idea", "is", "somewhat", "similar", "to", "the", "idea", "introduced", "in", "Section", "[", "reference", "]", "and", "Section", "[", "reference", "]", "of", "this", "work", ".", "Adapted", "-", "transformer", ":", "a", "transformer", "vaswani2017attention", "based", "GEC", "system", "junczys2018approaching", "with", "techniques", "adapted", "from", "low", "-", "resource", "machine", "translation", ".", "SMT", "-", "NMT", "hybrid", ":", "the", "state", "-", "of", "-", "the", "-", "art", "GEC", "system", "grundkiewicz2018near", "that", "is", "based", "on", "an", "SMT", "-", "NMT", "hybrid", "approach", ".", "Table", "[", "reference", "]", "shows", "the", "results", "of", "GEC", "systems", "on", "CoNLL", "and", "JFLEG", "dataset", ".", "Our", "base", "convolutional", "seq2seq", "model", "outperforms", "most", "of", "previous", "GEC", "systems", "owing", "to", "the", "larger", "size", "of", "training", "data", "we", "use", ".", "Fluency", "boost", "learning", "further", "improves", "the", "base", "convolutional", "seq2seq", "model", ".", "It", "achieves", "61.34", "in", "CoNLL", "-", "2014", ",", "76.88", "score", "in", "CoNLL", "-", "10", "benchmarks", ",", "and", "61.41", "GLEU", "score", "on", "JFLEG", "test", "set", ".", "When", "we", "further", "add", "fluency", "boost", "inference", ",", "the", "system", "\u2019s", "performance", "on", "JFLEG", "test", "set", "is", "improved", "to", "62.42", "GLEU", "score", ",", "while", "its", "scores", "on", "CoNLL", "benchmarks", "drop", ".", "We", "look", "into", "the", "results", "in", "Table", "[", "reference", "]", ".", "Fluency", "boost", "learning", "improves", "the", "base", "convolutional", "seq2seq", "model", "in", "terms", "of", "all", "aspects", "(", "i.e.", ",", "precision", ",", "recall", ",", "and", "GLEU", ")", ",", "demonstrating", "fluency", "boost", "learning", "is", "actually", "helpful", "for", "training", "a", "seq2seq", "model", "for", "GEC", ".", "Adding", "fluency", "boost", "inference", "improves", "recall", "(", "from", "36.30", "to", "40.18", "on", "CoNLL", "-", "2014", "and", "from", "50.31", "to", "53.15", "on", "CoNLL", "-", "10", ")", "at", "the", "expense", "of", "a", "drop", "of", "precision", "(", "from", "74.12", "to", "68.45", "on", "CoNLL", "-", "2014", "and", "from", "88.56", "to", "84.71", "on", "CoNLL", "-", "10", ")", ".", "Since", "weighs", "precision", "twice", "as", "recall", ",", "adding", "fluency", "boost", "inference", "leads", "to", "a", "drop", "of", "on", "the", "CoNLL", "dataset", ".", "In", "contrast", ",", "for", "JFLEG", ",", "fluency", "boost", "inference", "improves", "GLEU", "score", "from", "61.41", "to", "62.42", ",", "demonstrating", "its", "effectiveness", "for", "improving", "sentences", "\u2019", "fluency", ".", "We", "compare", "our", "systems", "to", "human", "performance", "on", "CoNLL", "-", "10", "and", "JFLEG", "benchmarks", ".", "For", "CoNLL", "-", "10", ",", "we", "follow", "the", "evaluation", "setting", "in", "and", "to", "fairly", "compare", "systems", "\u2019", "performance", "to", "human", "\u2019s", ",", "which", "is", "marked", "with", "(", "SvH", ")", "in", "Table", "[", "reference", "]", ".", "Among", "our", "systems", ",", "the", "system", "with", "fluency", "boost", "learning", "and", "inference", "outperforms", "human", "\u2019s", "performance", "on", "both", "CoNLL", "and", "JFLEG", "dataset", ",", "while", "the", "system", "with", "only", "fluency", "boost", "learning", "achieves", "higher", "scores", "on", "CoNLL", "dataset", ".", "We", "further", "study", "the", "effectiveness", "of", "fluency", "boost", "learning", "and", "inference", "for", "different", "error", "types", ".", "Table", "[", "reference", "]", "shows", "the", "recall", "of", "base", "convolutional", "seq2seq", "model", "and", "the", "model", "trained", "with", "fluency", "boost", "learning", "for", "each", "error", "type", "in", "CoNLL", "-", "2014", "dataset", "(", "original", "annotation", "setting", ")", ".", "One", "can", "see", "that", "fluency", "boost", "learning", "improves", "recall", "for", "most", "error", "types", ",", "demonstrating", "that", "fluency", "boost", "learning", "approach", "can", "generate", "sentences", "with", "diverse", "errors", "to", "help", "training", ".", "To", "better", "understand", "the", "effectiveness", "of", "fluency", "boost", "inference", "(", "i.e.", ",", "round", "-", "way", "error", "correction", ")", ",", "we", "show", "in", "Table", "[", "reference", "]", "the", "recall", "of", "each", "error", "type", "of", "the", "left", "-", "to", "-", "right", "and", "the", "right", "-", "to", "-", "left", "seq2seq", "in", "CoNLL", "-", "2014", "dataset", "(", "original", "annotation", "setting", ")", ".", "Note", "that", "to", "clearly", "see", "pros", "and", "cons", "of", "the", "left", "-", "to", "-", "right", "and", "right", "-", "to", "-", "left", "model", ",", "here", "we", "do", "not", "re", "-", "rank", "their", "n", "-", "best", "results", "using", "edit", "operations", "and", "the", "language", "model", ";", "instead", ",", "we", "directly", "use", "their", "1", "-", "best", "generated", "sentence", "as", "their", "prediction", ".", "According", "to", "Table", "[", "reference", "]", ",", "the", "right", "-", "to", "-", "left", "model", "does", "better", "in", "the", "error", "types", "like", "ArtOrDet", ",", "while", "the", "left", "-", "to", "-", "right", "model", "is", "better", "at", "correcting", "the", "errors", "like", "SVA", ",", "which", "is", "consistent", "with", "our", "motivation", "in", "Section", "[", "reference", "]", ".", "When", "we", "use", "round", "-", "way", "correction", ",", "the", "errors", "that", "are", "not", "corrected", "by", "the", "right", "-", "to", "-", "left", "model", "are", "likely", "to", "be", "corrected", "by", "the", "left", "-", "to", "-", "right", "one", ",", "which", "is", "reflected", "by", "the", "recall", "improvement", "of", "most", "error", "types", ",", "as", "shown", "in", "Table", "[", "reference", "]", ".", "section", ":", "Related", "work", "Most", "of", "advanced", "GEC", "systems", "are", "classifier", "-", "based", "chodorow2007detection", ",", "de2008classifier", ",", "han2010using", ",", "leacock2010automated", ",", "tetreault2010using", ",", "dale2011helping", "or", "MT", "-", "based", "brockett2006correcting", ",", "dahlmeier2011correcting", ",", "dahlmeier2012beam", ",", "yoshimoto2013naist", ",", "yuan2013constrained", ",", "behera2013automated", ".", "For", "example", ",", "top", "-", "performing", "systems", "felice2014grammatical", ",", "rozovskaya2014illinois", ",", "junczys2014amu", "in", "CoNLL", "-", "2014", "shared", "task", "ng2014conll", "use", "either", "of", "the", "methods", ".", "Recently", ",", "many", "novel", "approaches", "Susanto2014System", ",", "chollampatt2016neural", ",", "chollampatt2016adapting", ",", "rozovskaya2016grammatical", ",", "junczys2016phrase", ",", "mizumoto2016discriminative", ",", "Yuan2016Candidate", ",", "Hoang2016Exploiting", ",", "yannakoudakis2017neural", "have", "been", "proposed", "for", "GEC", ".", "Among", "them", ",", "seq2seq", "models", "yuan2016grammatical", ",", "xie2016neural", ",", "ji2017nested", ",", "sakaguchi2017grammatical", ",", "schmaltz", "-", "EtAl:2017:EMNLP2017", ",", "chollampatt2018", ",", "junczys2018approaching", "have", "caught", "much", "attention", ".", "Unlike", "the", "models", "trained", "only", "with", "original", "error", "-", "corrected", "data", ",", "we", "propose", "a", "novel", "fluency", "boost", "learning", "mechanism", "for", "dynamic", "data", "augmentation", "along", "with", "training", "for", "GEC", ",", "despite", "some", "related", "studies", "that", "explore", "artificial", "error", "generation", "for", "GEC", "brockett2006correcting", ",", "foster2009generrate", ",", "rozovskaya2010training", ",", "rozovskaya2011algorithm", ",", "Rozovskaya2012The", ",", "felice", "-", "yuan:2014:SRW", ",", "xie2016neural", ",", "rei2017artificial", ",", "xie2018noising", ".", "Moreover", ",", "we", "propose", "fluency", "boost", "inference", "which", "allows", "the", "model", "to", "repeatedly", "edit", "a", "sentence", "as", "long", "as", "the", "sentence", "\u2019s", "fluency", "can", "be", "improved", ".", "To", "the", "best", "of", "our", "knowledge", ",", "it", "is", "the", "first", "to", "conduct", "multi", "-", "round", "seq2seq", "inference", "for", "GEC", ",", "while", "similar", "ideas", "have", "been", "proposed", "for", "NMT", "DXiaTWLQYL17", ".", "In", "addition", "to", "the", "studies", "on", "GEC", ",", "there", "is", "also", "much", "research", "on", "grammatical", "error", "detection", "leacock2010automated", ",", "rei", "-", "yannakoudakis:2016:P16", "-", "1", ",", "kaneko2017grammatical", "and", "GEC", "evaluation", "tetreault2010rethinking", ",", "madnani2011they", ",", "dahlmeier2012better", ",", "napoles2015ground", ",", "sakaguchi2016reassessing", ",", "napoles2016there", ",", "bryant2017automatic", ",", "asano2017reference", ",", "choshen2018inherent", ".", "We", "do", "not", "introduce", "them", "in", "detail", "because", "they", "are", "not", "much", "related", "to", "this", "work", "\u2019s", "contributions", ".", "section", ":", "Conclusion", "We", "present", "a", "state", "-", "of", "-", "the", "-", "art", "convolutional", "seq2seq", "model", "based", "GEC", "system", "that", "uses", "a", "novel", "fluency", "boost", "learning", "and", "inference", "mechanism", ".", "Fluency", "boost", "learning", "fully", "exploits", "both", "error", "-", "corrected", "data", "and", "native", "data", "by", "generating", "diverse", "error", "-", "corrected", "sentence", "pairs", "during", "training", ",", "which", "benefits", "model", "learning", "and", "improves", "the", "performance", "over", "the", "base", "seq2seq", "model", ",", "while", "fluency", "boost", "inference", "utilizes", "the", "characteristic", "of", "GEC", "to", "progressively", "improve", "a", "sentence", "\u2019s", "fluency", "through", "round", "-", "way", "correction", ".", "The", "powerful", "learning", "and", "inference", "mechanism", "enables", "our", "system", "to", "achieve", "state", "-", "of", "-", "the", "-", "art", "results", "and", "reach", "human", "-", "level", "performance", "on", "both", "CoNLL", "-", "2014", "and", "JFLEG", "benchmark", "datasets", ".", "bibliography", ":", "References"]}