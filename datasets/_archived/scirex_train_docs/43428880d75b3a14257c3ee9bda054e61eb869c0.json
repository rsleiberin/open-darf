{"coref": {"BLEU_score": [[777, 778], [796, 797], [815, 816], [4128, 4129], [4140, 4141], [4260, 4261], [4268, 4269], [4443, 4444], [4529, 4530], [4542, 4543], [4636, 4637], [4721, 4726], [5057, 5058], [5096, 5097], [5153, 5154], [5310, 5311], [5419, 5420], [5657, 5658], [5741, 5742], [6347, 6348], [6367, 6368], [6385, 6386], [7419, 7421], [7565, 7567]], "ConvS2S": [[4240, 4246], [4247, 4248], [4569, 4570]], "ConvS2S_BPE40k": [[3247, 3248], [3302, 3303], [4667, 4668]], "ConvS2S__ensemble_": [], "IWSLT2015_English-German": [[6082, 6083]], "IWSLT2015_German-English": [], "Machine_Translation": [[158, 160], [229, 231], [592, 594], [737, 739], [3157, 3160], [4165, 4167], [5756, 5758], [5775, 5777]], "WMT2014_English-French": [[124, 129], [799, 803], [2090, 2093], [3304, 3308], [3835, 3837], [4594, 4597], [4691, 4696], [4746, 4750], [4809, 4814], [6350, 6355]], "WMT2014_English-German": [[119, 123], [780, 784], [3259, 3263], [3965, 3969], [4709, 4713], [4741, 4745], [7489, 7491], [5227, 5231], [7318, 7322]], "WMT2016_English-Romanian": [[4190, 4191], [4251, 4254], [4255, 4258], [756, 757], [3168, 3169], [4132, 4133], [4169, 4170], [4714, 4715], [6336, 6337]], "ensemble": [[4560, 4561], [4573, 4574]]}, "coref_non_salient": {"0": [[12, 16], [526, 527], [851, 856], [2583, 2584], [2785, 2786], [6261, 6265], [6400, 6405]], "1": [[1203, 1204], [4212, 4214]], "10": [[806, 809], [4735, 4739], [6202, 6205]], "100": [[5973, 5976]], "101": [[3213, 3217], [4010, 4014], [4404, 4408]], "102": [[545, 547]], "103": [[1814, 1816]], "104": [[1211, 1213]], "105": [[2085, 2086]], "106": [[7013, 7015]], "107": [[2549, 2551]], "108": [[5169, 5171]], "109": [[2579, 2581]], "11": [[1542, 1544], [6232, 6234]], "110": [[4987, 4990]], "111": [[3821, 3822]], "112": [[6230, 6231]], "113": [[4006, 4008]], "114": [[3239, 3246]], "115": [[6154, 6156], [6197, 6200]], "116": [[3622, 3627]], "117": [[901, 903]], "118": [[1378, 1380]], "119": [[720, 723]], "12": [[101, 103], [701, 703], [2686, 2687], [5751, 5753], [5812, 5814], [6315, 6317]], "120": [[635, 636]], "121": [[617, 621]], "122": [[3521, 3523]], "123": [[5515, 5519]], "124": [[44, 46], [5494, 5496], [6268, 6270]], "125": [[4644, 4649]], "126": [[1323, 1324]], "127": [[3926, 3928]], "128": [[4620, 4625]], "129": [[1199, 1202]], "13": [[2586, 2588], [2814, 2816], [2924, 2927], [6418, 6420], [6423, 6426], [6465, 6467]], "130": [[3397, 3399]], "131": [[1494, 1496]], "132": [[5113, 5115]], "133": [[2828, 2829]], "134": [[5281, 5282]], "135": [[3471, 3476]], "136": [[7139, 7143]], "137": [[5242, 5244], [5567, 5569], [5685, 5687], [7332, 7334], [7346, 7348], [7569, 7571], [7594, 7596]], "138": [[4849, 4851]], "139": [[4344, 4345]], "14": [[27, 30], [421, 423], [465, 468], [488, 490], [1179, 1181], [6124, 6127], [6285, 6287]], "140": [[3778, 3780]], "141": [[1728, 1730]], "142": [[5664, 5666]], "143": [[90, 92]], "144": [[3442, 3447]], "145": [[4355, 4358]], "146": [[2467, 2469]], "147": [[4417, 4421]], "148": [[1187, 1192]], "149": [[192, 198]], "15": [[278, 280], [6668, 6670]], "150": [[3478, 3481]], "151": [[4938, 4940]], "152": [[2918, 2920]], "153": [[3490, 3494]], "154": [[161, 163]], "155": [[559, 563]], "156": [[3461, 3465]], "157": [[1246, 1250]], "158": [[1901, 1904]], "159": [[683, 685]], "16": [[327, 329], [447, 449], [472, 474], [551, 553], [672, 673], [1287, 1289], [2542, 2544], [6289, 6291], [6396, 6398], [6484, 6486], [6744, 6746], [7102, 7104]], "160": [[2246, 2248]], "17": [[4864, 4866], [5011, 5013], [5018, 5019]], "18": [[4017, 4020], [5316, 5319]], "19": [[1952, 1954], [5901, 5902]], "2": [[4791, 4793], [4798, 4800], [4830, 4832], [4843, 4845], [5158, 5160], [5803, 5804]], "20": [[4389, 4391], [4395, 4397]], "21": [[3652, 3654], [3667, 3669]], "22": [[110, 113], [788, 790]], "23": [[199, 200], [354, 355], [1030, 1031], [1306, 1307], [2559, 2560]], "24": [[2122, 2125], [2130, 2132]], "25": [[56, 57], [4305, 4306], [5279, 5280]], "26": [[6468, 6469], [7090, 7091], [7186, 7188]], "27": [[1011, 1015], [1182, 1186]], "28": [[281, 282], [585, 587]], "29": [[1328, 1330], [5201, 5203]], "3": [[220, 224], [864, 872]], "30": [[3568, 3569], [3570, 3571], [5903, 5905], [5999, 6000]], "31": [[5950, 5957], [5959, 5966]], "32": [[5933, 5935], [6206, 6208]], "33": [[1464, 1466], [1478, 1480], [1535, 1539], [1584, 1586], [1606, 1608], [1633, 1635], [1926, 1928], [2041, 2043], [5479, 5483], [6049, 6051]], "34": [[38, 41], [257, 260], [372, 378], [532, 535], [1319, 1322]], "35": [[4858, 4859], [5025, 5031]], "36": [[2848, 2850], [7145, 7147]], "37": [[283, 284], [429, 431]], "38": [[3589, 3591], [3602, 3604]], "39": [[64, 65], [3560, 3561]], "4": [[167, 169], [742, 743], [3164, 3167], [3404, 3406], [5945, 5947], [6092, 6093], [6100, 6103], [6141, 6142]], "40": [[1621, 1624], [6522, 6525]], "41": [[211, 213], [877, 879], [918, 920]], "42": [[4161, 4163], [4351, 4353], [4437, 4439], [6257, 6260]], "43": [[6001, 6003], [6009, 6011]], "44": [[141, 142], [4506, 4507], [5002, 5003], [5099, 5100]], "45": [[6749, 6750], [6790, 6793]], "46": [[4224, 4230]], "47": [[3659, 3661], [5636, 5638], [5734, 5736]], "48": [[3032, 3033], [3804, 3805], [7027, 7028]], "49": [[1531, 1534], [3810, 3812]], "5": [[1430, 1432], [1468, 1471]], "50": [[264, 266], [536, 538], [665, 669], [856, 860], [1297, 1301]], "51": [[3400, 3403]], "52": [[1219, 1221], [1807, 1809], [6311, 6312]], "53": [[3411, 3413], [3501, 3504]], "54": [[4949, 4950], [7512, 7513]], "55": [[379, 381], [6410, 6412]], "56": [[5737, 5738]], "57": [[2053, 2055], [2489, 2491], [5552, 5555]], "58": [[3482, 3483]], "59": [[2523, 2525]], "6": [[0, 5], [146, 150]], "60": [[1193, 1194], [6359, 6361]], "61": [[4318, 4323], [6375, 6378]], "62": [[6131, 6133]], "63": [[5274, 5276]], "64": [[4338, 4343]], "65": [[638, 640], [1293, 1296]], "66": [[5661, 5662]], "67": [[107, 108], [4526, 4527], [5399, 5400], [5415, 5416], [5651, 5652], [5856, 5857], [5914, 5916], [5928, 5929], [5942, 5943], [6077, 6078]], "68": [[2519, 2521], [7310, 7312], [1113, 1115], [2728, 2730], [4022, 4024], [4114, 4116], [7471, 7473]], "69": [[5888, 5890]], "7": [[4102, 4106], [4199, 4206]], "70": [[1794, 1795], [1889, 1890]], "71": [[695, 697], [2057, 2059], [2214, 2216], [2573, 2575], [2788, 2789], [4291, 4292], [4364, 4365], [4446, 4447], [5895, 5896]], "72": [[4372, 4373], [4440, 4441], [4608, 4609], [4629, 4630], [5165, 5166]], "73": [[7007, 7009]], "74": [[4520, 4523]], "75": [[3619, 3621], [4153, 4157]], "76": [[5639, 5640]], "77": [[5828, 5833]], "78": [[4492, 4494]], "79": [[2751, 2753]], "8": [[96, 98], [1143, 1144], [3585, 3586], [4217, 4220], [4285, 4286], [4452, 4453], [5334, 5337]], "80": [[3958, 3960]], "81": [[5667, 5668], [5768, 5769], [7305, 7307]], "82": [[4263, 4265]], "83": [[293, 295]], "84": [[4853, 4856]], "85": [[7615, 7619]], "86": [[7032, 7034]], "87": [[3486, 3489]], "88": [[6474, 6476], [6738, 6740], [6797, 6799]], "89": [[4867, 4869]], "9": [[240, 244], [3894, 3896]], "90": [[86, 89], [679, 682], [1790, 1793], [2167, 2170], [2868, 2871]], "91": [[3830, 3833]], "92": [[6086, 6090]], "93": [[2329, 2330]], "94": [[2274, 2279]], "95": [[1880, 1881]], "96": [[5969, 5972]], "97": [[1896, 1898], [6079, 6081]], "98": [[2255, 2257]], "99": [[1627, 1630]]}, "doc_id": "43428880d75b3a14257c3ee9bda054e61eb869c0", "method_subrelations": {"ConvS2S": [[[0, 7], "ConvS2S"]], "ConvS2S_BPE40k": [[[0, 14], "ConvS2S_BPE40k"]], "ConvS2S__ensemble_": [[[0, 7], "ConvS2S"], [[9, 17], "ensemble"]]}, "n_ary_relations": [{"Material": "IWSLT2015_English-German", "Method": "ConvS2S", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "26.73"}, {"Material": "IWSLT2015_German-English", "Method": "ConvS2S", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "32.31"}, {"Material": "WMT2014_English-French", "Method": "ConvS2S", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "40.46"}, {"Material": "WMT2014_English-French", "Method": "ConvS2S__ensemble_", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": 41.29}, {"Material": "WMT2014_English-German", "Method": "ConvS2S", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": "25.16"}, {"Material": "WMT2014_English-German", "Method": "ConvS2S__ensemble_", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": 26.36}, {"Material": "WMT2016_English-Romanian", "Method": "ConvS2S_BPE40k", "Metric": "BLEU_score", "Task": "Machine_Translation", "score": 29.88}], "ner": [[0, 5, "Method"], [12, 16, "Task"], [27, 30, "Method"], [38, 41, "Method"], [44, 46, "Method"], [56, 57, "Task"], [64, 65, "Task"], [86, 89, "Method"], [90, 92, "Method"], [96, 98, "Method"], [101, 103, "Method"], [107, 108, "Metric"], [110, 113, "Method"], [119, 123, "Material"], [124, 129, "Material"], [141, 142, "Method"], [146, 150, "Method"], [158, 160, "Task"], [161, 163, "Task"], [167, 169, "Task"], [192, 198, "Method"], [199, 200, "Method"], [211, 213, "Method"], [220, 224, "Method"], [229, 231, "Task"], [240, 244, "Method"], [257, 260, "Method"], [264, 266, "Task"], [278, 280, "Method"], [281, 282, "Method"], [283, 284, "Method"], [293, 295, "Metric"], [327, 329, "Method"], [354, 355, "Method"], [372, 378, "Method"], [379, 381, "Method"], [421, 423, "Method"], [429, 431, "Method"], [447, 449, "Method"], [465, 468, "Method"], [472, 474, "Method"], [488, 490, "Method"], [526, 527, "Task"], [532, 535, "Method"], [536, 538, "Task"], [545, 547, "Method"], [551, 553, "Method"], [559, 563, "Task"], [585, 587, "Method"], [592, 594, "Task"], [617, 621, "Method"], [635, 636, "Method"], [638, 640, "Method"], [665, 669, "Task"], [672, 673, "Method"], [679, 682, "Method"], [683, 685, "Method"], [695, 697, "Method"], [701, 703, "Method"], [720, 723, "Task"], [737, 739, "Task"], [742, 743, "Task"], [777, 778, "Metric"], [780, 784, "Material"], [788, 790, "Method"], [796, 797, "Metric"], [799, 803, "Material"], [806, 809, "Method"], [815, 816, "Metric"], [851, 856, "Task"], [856, 860, "Task"], [864, 872, "Method"], [877, 879, "Method"], [901, 903, "Method"], [918, 920, "Method"], [1011, 1015, "Method"], [1030, 1031, "Method"], [1143, 1144, "Method"], [1179, 1181, "Method"], [1182, 1186, "Method"], [1187, 1192, "Method"], [1193, 1194, "Method"], [1199, 1202, "Method"], [1203, 1204, "Method"], [1211, 1213, "Method"], [1219, 1221, "Method"], [1246, 1250, "Method"], [1287, 1289, "Method"], [1293, 1296, "Method"], [1297, 1301, "Task"], [1306, 1307, "Method"], [1319, 1322, "Method"], [1323, 1324, "Method"], [1328, 1330, "Task"], [1378, 1380, "Method"], [1430, 1432, "Method"], [1464, 1466, "Method"], [1468, 1471, "Method"], [1478, 1480, "Method"], [1494, 1496, "Method"], [1531, 1534, "Method"], [1535, 1539, "Method"], [1542, 1544, "Method"], [1584, 1586, "Method"], [1606, 1608, "Method"], [1621, 1624, "Method"], [1627, 1630, "Method"], [1633, 1635, "Method"], [1728, 1730, "Method"], [1790, 1793, "Method"], [1794, 1795, "Method"], [1807, 1809, "Method"], [1814, 1816, "Method"], [1880, 1881, "Method"], [1889, 1890, "Method"], [1896, 1898, "Task"], [1901, 1904, "Method"], [1926, 1928, "Method"], [1952, 1954, "Task"], [2041, 2043, "Method"], [2053, 2055, "Method"], [2057, 2059, "Method"], [2085, 2086, "Method"], [2090, 2093, "Material"], [2122, 2125, "Method"], [2130, 2132, "Method"], [2167, 2170, "Method"], [2214, 2216, "Method"], [2246, 2248, "Method"], [2255, 2257, "Method"], [2274, 2279, "Method"], [2329, 2330, "Task"], [2467, 2469, "Method"], [2489, 2491, "Method"], [2519, 2521, "Metric"], [2523, 2525, "Method"], [2542, 2544, "Method"], [2549, 2551, "Task"], [2559, 2560, "Method"], [2573, 2575, "Method"], [2579, 2581, "Method"], [2583, 2584, "Task"], [2586, 2588, "Method"], [2686, 2687, "Method"], [2751, 2753, "Method"], [2785, 2786, "Task"], [2788, 2789, "Method"], [2814, 2816, "Method"], [2828, 2829, "Task"], [2848, 2850, "Method"], [2868, 2871, "Method"], [2918, 2920, "Method"], [2924, 2927, "Method"], [3032, 3033, "Method"], [3157, 3160, "Task"], [3164, 3167, "Task"], [3213, 3217, "Method"], [3239, 3246, "Method"], [3247, 3248, "Method"], [3259, 3263, "Material"], [3302, 3303, "Method"], [3304, 3308, "Material"], [3397, 3399, "Task"], [3400, 3403, "Method"], [3404, 3406, "Task"], [3411, 3413, "Material"], [3442, 3447, "Material"], [3461, 3465, "Method"], [3471, 3476, "Method"], [3478, 3481, "Method"], [3482, 3483, "Method"], [3486, 3489, "Method"], [3490, 3494, "Method"], [3501, 3504, "Material"], [3521, 3523, "Metric"], [3560, 3561, "Task"], [3568, 3569, "Method"], [3570, 3571, "Method"], [3585, 3586, "Method"], [3589, 3591, "Method"], [3602, 3604, "Method"], [3619, 3621, "Method"], [3622, 3627, "Method"], [3652, 3654, "Metric"], [3659, 3661, "Metric"], [3667, 3669, "Metric"], [3778, 3780, "Method"], [3804, 3805, "Method"], [3810, 3812, "Method"], [3821, 3822, "Method"], [3830, 3833, "Method"], [3835, 3837, "Material"], [3894, 3896, "Method"], [3926, 3928, "Method"], [3958, 3960, "Task"], [3965, 3969, "Material"], [4006, 4008, "Method"], [4010, 4014, "Method"], [4017, 4020, "Task"], [4102, 4106, "Method"], [4128, 4129, "Metric"], [4140, 4141, "Metric"], [4153, 4157, "Method"], [4161, 4163, "Method"], [4165, 4167, "Task"], [4190, 4191, "Material"], [4199, 4206, "Method"], [4212, 4214, "Method"], [4217, 4220, "Method"], [4224, 4230, "Method"], [4240, 4246, "Method"], [4247, 4248, "Method"], [4251, 4254, "Material"], [4255, 4258, "Material"], [4260, 4261, "Metric"], [4263, 4265, "Method"], [4268, 4269, "Metric"], [4285, 4286, "Method"], [4291, 4292, "Method"], [4305, 4306, "Task"], [4318, 4323, "Task"], [4338, 4343, "Method"], [4344, 4345, "Method"], [4351, 4353, "Method"], [4355, 4358, "Method"], [4364, 4365, "Method"], [4372, 4373, "Method"], [4389, 4391, "Method"], [4395, 4397, "Method"], [4404, 4408, "Method"], [4417, 4421, "Method"], [4437, 4439, "Method"], [4440, 4441, "Method"], [4443, 4444, "Metric"], [4446, 4447, "Method"], [4452, 4453, "Method"], [4492, 4494, "Method"], [4506, 4507, "Method"], [4520, 4523, "Method"], [4526, 4527, "Metric"], [4529, 4530, "Metric"], [4542, 4543, "Metric"], [4560, 4561, "Method"], [4569, 4570, "Method"], [4573, 4574, "Method"], [4594, 4597, "Material"], [4608, 4609, "Method"], [4620, 4625, "Metric"], [4629, 4630, "Method"], [4636, 4637, "Metric"], [4644, 4649, "Method"], [4667, 4668, "Method"], [4691, 4696, "Material"], [4709, 4713, "Material"], [4721, 4726, "Metric"], [4735, 4739, "Method"], [4741, 4745, "Material"], [4746, 4750, "Material"], [4791, 4793, "Metric"], [4798, 4800, "Metric"], [4809, 4814, "Material"], [4830, 4832, "Metric"], [4843, 4845, "Metric"], [4849, 4851, "Method"], [4853, 4856, "Method"], [4858, 4859, "Method"], [4864, 4866, "Method"], [4867, 4869, "Metric"], [4938, 4940, "Method"], [4949, 4950, "Task"], [4987, 4990, "Method"], [5002, 5003, "Method"], [5011, 5013, "Method"], [5018, 5019, "Method"], [5025, 5031, "Method"], [5057, 5058, "Metric"], [5096, 5097, "Metric"], [5099, 5100, "Method"], [5113, 5115, "Material"], [5153, 5154, "Metric"], [5158, 5160, "Metric"], [5165, 5166, "Method"], [5169, 5171, "Method"], [5201, 5203, "Task"], [5242, 5244, "Method"], [5274, 5276, "Method"], [5279, 5280, "Task"], [5281, 5282, "Task"], [5310, 5311, "Metric"], [5316, 5319, "Task"], [5334, 5337, "Method"], [5399, 5400, "Metric"], [5415, 5416, "Metric"], [5419, 5420, "Metric"], [5479, 5483, "Method"], [5494, 5496, "Method"], [5515, 5519, "Method"], [5552, 5555, "Method"], [5567, 5569, "Method"], [5636, 5638, "Metric"], [5639, 5640, "Metric"], [5651, 5652, "Metric"], [5657, 5658, "Metric"], [5661, 5662, "Method"], [5664, 5666, "Metric"], [5667, 5668, "Task"], [5685, 5687, "Method"], [5734, 5736, "Metric"], [5737, 5738, "Task"], [5741, 5742, "Metric"], [5751, 5753, "Method"], [5768, 5769, "Task"], [5803, 5804, "Metric"], [5812, 5814, "Method"], [5828, 5833, "Task"], [5856, 5857, "Metric"], [5888, 5890, "Method"], [5895, 5896, "Method"], [5901, 5902, "Task"], [5903, 5905, "Method"], [5914, 5916, "Metric"], [5928, 5929, "Metric"], [5933, 5935, "Method"], [5942, 5943, "Metric"], [5945, 5947, "Task"], [5950, 5957, "Task"], [5959, 5966, "Task"], [5969, 5972, "Task"], [5973, 5976, "Task"], [5999, 6000, "Method"], [6001, 6003, "Method"], [6009, 6011, "Method"], [6049, 6051, "Method"], [6077, 6078, "Metric"], [6079, 6081, "Task"], [6082, 6083, "Material"], [6086, 6090, "Method"], [6092, 6093, "Task"], [6100, 6103, "Task"], [6124, 6127, "Method"], [6131, 6133, "Metric"], [6141, 6142, "Task"], [6154, 6156, "Method"], [6197, 6200, "Method"], [6202, 6205, "Method"], [6206, 6208, "Method"], [6230, 6231, "Method"], [6232, 6234, "Method"], [6257, 6260, "Method"], [6261, 6265, "Task"], [6268, 6270, "Method"], [6285, 6287, "Method"], [6289, 6291, "Method"], [6311, 6312, "Method"], [6315, 6317, "Method"], [6347, 6348, "Metric"], [6350, 6355, "Material"], [6359, 6361, "Method"], [6367, 6368, "Metric"], [6375, 6378, "Task"], [6385, 6386, "Metric"], [6396, 6398, "Method"], [6400, 6405, "Task"], [6410, 6412, "Method"], [6418, 6420, "Method"], [6423, 6426, "Method"], [6465, 6467, "Method"], [6468, 6469, "Task"], [6474, 6476, "Method"], [6484, 6486, "Method"], [6522, 6525, "Method"], [6668, 6670, "Method"], [6738, 6740, "Method"], [6744, 6746, "Method"], [6749, 6750, "Method"], [6790, 6793, "Method"], [6797, 6799, "Method"], [7007, 7009, "Method"], [7013, 7015, "Method"], [7027, 7028, "Method"], [7032, 7034, "Method"], [7090, 7091, "Task"], [7102, 7104, "Method"], [7139, 7143, "Method"], [7145, 7147, "Method"], [7186, 7188, "Task"], [7305, 7307, "Task"], [7310, 7312, "Metric"], [7332, 7334, "Method"], [7346, 7348, "Method"], [7419, 7421, "Metric"], [7489, 7491, "Material"], [7512, 7513, "Task"], [7565, 7567, "Metric"], [7569, 7571, "Method"], [7594, 7596, "Method"], [7615, 7619, "Method"], [756, 757, "Material"], [1113, 1115, "Metric"], [2728, 2730, "Metric"], [3168, 3169, "Material"], [4022, 4024, "Metric"], [4114, 4116, "Metric"], [4132, 4133, "Material"], [4169, 4170, "Material"], [4714, 4715, "Material"], [5227, 5231, "Material"], [5756, 5758, "Task"], [5775, 5777, "Task"], [6336, 6337, "Material"], [7318, 7322, "Material"], [7471, 7473, "Metric"]], "sections": [[0, 5], [5, 143], [143, 624], [624, 849], [849, 1284], [1284, 1326], [1326, 1529], [1529, 1725], [1725, 2577], [2577, 2795], [2795, 3146], [3146, 3150], [3150, 3287], [3287, 3555], [3555, 3897], [3897, 4151], [4151, 4425], [4425, 4719], [4719, 4726], [4726, 4789], [4789, 5199], [5199, 5545], [5545, 5847], [5847, 6084], [6084, 6090], [6090, 6247], [6247, 6415], [6415, 6470], [6470, 6734], [6734, 7022], [7022, 7189], [7189, 7602], [7602, 7604], [7604, 7636], [7636, 7638]], "sentences": [[0, 5], [5, 8], [8, 31], [31, 42], [42, 83], [83, 104], [104, 143], [143, 146], [146, 179], [179, 228], [228, 257], [257, 276], [276, 313], [313, 327], [327, 351], [351, 372], [372, 402], [402, 469], [469, 515], [515, 528], [528, 564], [564, 585], [585, 622], [622, 624], [624, 635], [635, 657], [657, 674], [674, 685], [685, 711], [711, 728], [728, 755], [755, 779], [779, 817], [817, 849], [849, 856], [856, 872], [872, 876], [876, 892], [892, 903], [903, 905], [905, 911], [911, 917], [917, 929], [929, 936], [936, 950], [950, 987], [987, 996], [996, 1003], [1003, 1004], [1004, 1032], [1032, 1074], [1074, 1082], [1082, 1094], [1094, 1104], [1104, 1135], [1135, 1145], [1145, 1155], [1155, 1156], [1156, 1161], [1161, 1162], [1162, 1163], [1163, 1176], [1176, 1205], [1205, 1240], [1240, 1284], [1284, 1289], [1289, 1302], [1302, 1316], [1316, 1317], [1317, 1326], [1326, 1330], [1330, 1345], [1345, 1355], [1355, 1360], [1360, 1367], [1367, 1380], [1380, 1386], [1386, 1406], [1406, 1424], [1424, 1434], [1434, 1443], [1443, 1445], [1445, 1452], [1452, 1480], [1480, 1482], [1482, 1494], [1494, 1529], [1529, 1534], [1534, 1557], [1557, 1569], [1569, 1576], [1576, 1577], [1577, 1591], [1591, 1597], [1597, 1600], [1600, 1608], [1608, 1617], [1617, 1631], [1631, 1657], [1657, 1676], [1676, 1703], [1703, 1725], [1725, 1733], [1733, 1788], [1788, 1816], [1816, 1844], [1844, 1851], [1851, 1860], [1860, 1868], [1868, 1899], [1899, 1921], [1921, 1925], [1925, 1949], [1949, 1968], [1968, 2007], [2007, 2029], [2029, 2049], [2049, 2060], [2060, 2071], [2071, 2073], [2073, 2083], [2083, 2089], [2089, 2115], [2115, 2133], [2133, 2161], [2161, 2171], [2171, 2173], [2173, 2176], [2176, 2205], [2205, 2210], [2210, 2241], [2241, 2259], [2259, 2260], [2260, 2263], [2263, 2286], [2286, 2287], [2287, 2294], [2294, 2295], [2295, 2297], [2297, 2298], [2298, 2304], [2304, 2306], [2306, 2313], [2313, 2331], [2331, 2355], [2355, 2374], [2374, 2382], [2382, 2415], [2415, 2438], [2438, 2446], [2446, 2486], [2486, 2512], [2512, 2541], [2541, 2567], [2567, 2577], [2577, 2581], [2581, 2612], [2612, 2634], [2634, 2658], [2658, 2677], [2677, 2734], [2734, 2750], [2750, 2780], [2780, 2795], [2795, 2798], [2798, 2817], [2817, 2842], [2842, 2858], [2858, 2880], [2880, 2898], [2898, 2911], [2911, 2975], [2975, 2999], [2999, 3018], [3018, 3030], [3030, 3049], [3049, 3070], [3070, 3071], [3071, 3075], [3075, 3092], [3092, 3108], [3108, 3146], [3146, 3150], [3150, 3153], [3153, 3168], [3168, 3169], [3169, 3173], [3173, 3195], [3195, 3210], [3210, 3234], [3234, 3259], [3259, 3264], [3264, 3287], [3287, 3290], [3290, 3304], [3304, 3309], [3309, 3342], [3342, 3353], [3353, 3359], [3359, 3372], [3372, 3404], [3404, 3407], [3407, 3438], [3438, 3496], [3496, 3528], [3528, 3555], [3555, 3561], [3561, 3576], [3576, 3616], [3616, 3649], [3649, 3683], [3683, 3684], [3684, 3697], [3697, 3723], [3723, 3737], [3737, 3759], [3759, 3775], [3775, 3791], [3791, 3812], [3812, 3851], [3851, 3897], [3897, 3900], [3900, 3921], [3921, 3939], [3939, 3947], [3947, 3963], [3963, 3964], [3964, 3991], [3991, 3992], [3992, 3996], [3996, 4009], [4009, 4050], [4050, 4065], [4065, 4080], [4080, 4084], [4084, 4100], [4100, 4120], [4120, 4150], [4150, 4151], [4151, 4157], [4157, 4168], [4168, 4191], [4191, 4195], [4195, 4221], [4221, 4235], [4235, 4275], [4275, 4305], [4305, 4317], [4317, 4425], [4425, 4428], [4428, 4445], [4445, 4482], [4482, 4499], [4499, 4520], [4520, 4548], [4548, 4569], [4569, 4586], [4586, 4609], [4609, 4613], [4613, 4640], [4640, 4654], [4654, 4673], [4673, 4719], [4719, 4726], [4726, 4730], [4730, 4761], [4761, 4775], [4775, 4789], [4789, 4793], [4793, 4828], [4828, 4839], [4839, 4867], [4867, 4891], [4891, 4910], [4910, 4928], [4928, 4947], [4947, 4977], [4977, 4996], [4996, 5032], [5032, 5085], [5085, 5098], [5098, 5137], [5137, 5155], [5155, 5183], [5183, 5199], [5199, 5203], [5203, 5217], [5217, 5249], [5249, 5283], [5283, 5298], [5298, 5310], [5310, 5320], [5320, 5342], [5342, 5372], [5372, 5390], [5390, 5406], [5406, 5421], [5421, 5463], [5463, 5494], [5494, 5520], [5520, 5545], [5545, 5551], [5551, 5570], [5570, 5603], [5603, 5624], [5624, 5642], [5642, 5663], [5663, 5679], [5679, 5713], [5713, 5719], [5719, 5740], [5740, 5743], [5743, 5754], [5754, 5765], [5765, 5790], [5790, 5821], [5821, 5847], [5847, 5853], [5853, 5870], [5870, 5888], [5888, 5903], [5903, 5933], [5933, 5942], [5942, 5978], [5978, 5995], [5995, 6012], [6012, 6043], [6043, 6060], [6060, 6084], [6084, 6090], [6090, 6093], [6093, 6116], [6116, 6147], [6147, 6192], [6192, 6235], [6235, 6247], [6247, 6253], [6253, 6283], [6283, 6307], [6307, 6318], [6318, 6334], [6334, 6387], [6387, 6415], [6415, 6420], [6420, 6458], [6458, 6470], [6470, 6476], [6476, 6510], [6510, 6521], [6521, 6538], [6538, 6570], [6570, 6590], [6590, 6601], [6601, 6616], [6616, 6631], [6631, 6635], [6635, 6666], [6666, 6691], [6691, 6702], [6702, 6716], [6716, 6734], [6734, 6740], [6740, 6759], [6759, 6763], [6763, 6779], [6779, 6794], [6794, 6827], [6827, 6838], [6838, 6844], [6844, 6851], [6851, 6866], [6866, 6874], [6874, 6888], [6888, 6892], [6892, 6910], [6910, 6922], [6922, 6926], [6926, 6930], [6930, 6994], [6994, 7022], [7022, 7027], [7027, 7044], [7044, 7086], [7086, 7095], [7095, 7134], [7134, 7159], [7159, 7189], [7189, 7197], [7197, 7215], [7215, 7225], [7225, 7228], [7228, 7238], [7238, 7239], [7239, 7278], [7278, 7291], [7291, 7304], [7304, 7324], [7324, 7341], [7341, 7356], [7356, 7367], [7367, 7395], [7395, 7414], [7414, 7449], [7449, 7468], [7468, 7505], [7505, 7526], [7526, 7558], [7558, 7565], [7565, 7590], [7590, 7602], [7602, 7604], [7604, 7607], [7607, 7636], [7636, 7638]], "words": ["Convolutional", "Sequence", "to", "Sequence", "Learning", "section", ":", "Abstract", "The", "prevalent", "approach", "to", "sequence", "to", "sequence", "learning", "maps", "an", "input", "sequence", "to", "a", "variable", "length", "output", "sequence", "via", "recurrent", "neural", "networks", ".", "We", "introduce", "an", "architecture", "based", "entirely", "on", "convolutional", "neural", "networks", ".", "Compared", "to", "recurrent", "models", ",", "computations", "over", "all", "elements", "can", "be", "fully", "parallelized", "during", "training", "to", "better", "exploit", "the", "GPU", "hardware", "and", "optimization", "is", "easier", "since", "the", "number", "of", "non", "-", "linearities", "is", "fixed", "and", "independent", "of", "the", "input", "length", ".", "Our", "use", "of", "gated", "linear", "units", "eases", "gradient", "propagation", "and", "we", "equip", "each", "decoder", "layer", "with", "a", "separate", "attention", "module", ".", "We", "outperform", "the", "accuracy", "of", "the", "deep", "LSTM", "setup", "of", "[", "reference", "]", "on", "both", "WMT'14", "English", "-", "German", "and", "WMT'14", "English", "-", "French", "translation", "at", "an", "order", "of", "magnitude", "faster", "speed", ",", "both", "on", "GPU", "and", "CPU", ".", "section", ":", "Introduction", "Sequence", "to", "sequence", "learning", "has", "been", "successful", "in", "many", "tasks", "such", "as", "machine", "translation", ",", "speech", "recognition", "[", "reference", "]", "and", "text", "summarization", "[", "reference", "][", "reference", "][", "reference", "]", "amongst", "others", ".", "The", "dominant", "approach", "to", "date", "encodes", "the", "input", "sequence", "with", "a", "series", "of", "bi", "-", "directional", "recurrent", "neural", "networks", "(", "RNN", ")", "and", "generates", "a", "variable", "length", "output", "with", "another", "set", "of", "decoder", "RNNs", ",", "both", "of", "which", "interface", "via", "a", "soft", "-", "attention", "mechanism", "[", "reference", "]", ".", "In", "machine", "translation", ",", "this", "architecture", "has", "been", "demonstrated", "to", "outperform", "traditional", "phrase", "-", "based", "models", "by", "large", "margins", "[", "reference", "][", "reference", "][", "reference", "][", "reference", "]", ".", "Convolutional", "neural", "networks", "are", "less", "common", "for", "sequence", "modeling", ",", "despite", "several", "advantages", "[", "reference", "][", "reference", "]", ".", "Compared", "to", "recurrent", "layers", ",", "convolutions", "create", "representations", "for", "fixed", "size", "contexts", ",", "however", ",", "the", "effective", "context", "size", "of", "the", "network", "can", "easily", "be", "made", "larger", "by", "stacking", "several", "layers", "on", "top", "of", "each", "other", ".", "This", "allows", "to", "precisely", "control", "the", "maximum", "length", "of", "dependencies", "to", "be", "modeled", ".", "Convolutional", "networks", "do", "not", "depend", "on", "the", "computations", "of", "the", "previous", "time", "step", "and", "therefore", "allow", "parallelization", "over", "every", "element", "in", "a", "sequence", ".", "This", "contrasts", "with", "RNNs", "which", "maintain", "a", "hidden", "state", "of", "the", "entire", "past", "that", "prevents", "parallel", "computation", "within", "a", "sequence", ".", "Multi", "-", "layer", "convolutional", "neural", "networks", "create", "hierarchical", "representations", "over", "the", "input", "sequence", "in", "which", "nearby", "input", "elements", "interact", "at", "lower", "layers", "while", "distant", "elements", "interact", "at", "higher", "layers", ".", "Hierarchical", "structure", "provides", "a", "shorter", "path", "to", "capture", "long", "-", "range", "dependencies", "compared", "to", "the", "chain", "structure", "modeled", "by", "recurrent", "networks", ",", "e.g.", "we", "can", "obtain", "a", "feature", "representation", "capturing", "relationships", "within", "a", "window", "of", "n", "words", "by", "applying", "only", "O", "(", "n", "k", ")", "convolutional", "operations", "for", "kernels", "of", "width", "k", ",", "compared", "to", "a", "linear", "number", "O", "(", "n", ")", "for", "recurrent", "neural", "networks", ".", "Inputs", "to", "a", "convolutional", "network", "are", "fed", "through", "a", "constant", "number", "of", "kernels", "and", "non", "-", "linearities", ",", "whereas", "recurrent", "networks", "apply", "up", "to", "n", "operations", "and", "non", "-", "linearities", "to", "the", "first", "word", "and", "only", "a", "single", "set", "of", "operations", "to", "the", "last", "word", ".", "Fixing", "the", "number", "of", "nonlinearities", "applied", "to", "the", "inputs", "also", "eases", "learning", ".", "Recent", "work", "has", "applied", "convolutional", "neural", "networks", "to", "sequence", "modeling", "such", "as", "[", "reference", "]", "who", "introduce", "recurrent", "pooling", "between", "a", "succession", "of", "convolutional", "layers", "or", "[", "reference", "]", "who", "tackle", "neural", "translation", "without", "attention", ".", "However", ",", "none", "of", "these", "approaches", "has", "been", "demonstrated", "improvements", "over", "state", "of", "the", "art", "results", "on", "large", "benchmark", "datasets", ".", "Gated", "convolutions", "have", "been", "previously", "explored", "for", "machine", "translation", "by", "[", "reference", "]", "but", "their", "evaluation", "was", "restricted", "to", "a", "small", "dataset", "and", "the", "model", "was", "used", "in", "tandem", "with", "a", "traditional", "count", "-", "based", "model", ".", "Architec", "-", "section", ":", "arXiv:1705.03122v3", "[", "cs", ".", "CL", "]", "25", "Jul", "2017", "tures", "which", "are", "partially", "convolutional", "have", "shown", "strong", "performance", "on", "larger", "tasks", "but", "their", "decoder", "is", "still", "recurrent", "[", "reference", "]", ".", "In", "this", "paper", "we", "propose", "an", "architecture", "for", "sequence", "to", "sequence", "modeling", "that", "is", "entirely", "convolutional", ".", "Our", "model", "is", "equipped", "with", "gated", "linear", "units", "and", "residual", "connections", "[", "reference", "]", ".", "We", "also", "use", "attention", "in", "every", "decoder", "layer", "and", "demonstrate", "that", "each", "attention", "layer", "only", "adds", "a", "negligible", "amount", "of", "overhead", ".", "The", "combination", "of", "these", "choices", "enables", "us", "to", "tackle", "large", "scale", "problems", "(", "\u00a7", "3", ")", ".", "We", "evaluate", "our", "approach", "on", "several", "large", "datasets", "for", "machine", "translation", "as", "well", "as", "summarization", "and", "compare", "to", "the", "current", "best", "architectures", "reported", "in", "the", "literature", ".", "On", "WMT'16", "English", "-", "Romanian", "translation", "we", "achieve", "a", "new", "state", "of", "the", "art", ",", "outperforming", "the", "previous", "best", "result", "by", "1.9", "BLEU", ".", "On", "WMT'14", "English", "-", "German", "we", "outperform", "the", "strong", "LSTM", "setup", "of", "[", "reference", "]", "by", "0.5", "BLEU", "and", "on", "WMT'14", "English", "-", "French", "we", "outperform", "the", "likelihood", "trained", "system", "of", "[", "reference", "]", "by", "1.6", "BLEU", ".", "Furthermore", ",", "our", "model", "can", "translate", "unseen", "sentences", "at", "an", "order", "of", "magnitude", "faster", "speed", "than", "[", "reference", "]", "on", "GPU", "and", "CPU", "hardware", "(", "\u00a7", "4", ",", "\u00a7", "5", ")", ".", "section", ":", "Recurrent", "Sequence", "to", "Sequence", "Learning", "Sequence", "to", "sequence", "modeling", "has", "been", "synonymous", "with", "recurrent", "neural", "network", "based", "encoder", "-", "decoder", "architectures", "[", "reference", "]", ".", "The", "encoder", "RNN", "processes", "an", "input", "sequence", "x", "=", "(", "x", "1", ",", ".", ".", ".", ",", "x", "m", ")", "of", "m", "elements", "and", "returns", "state", "representations", "z", "=", "(", "z", "1", ".", ".", ".", ".", ",", "z", "m", ")", ".", "The", "decoder", "RNN", "takes", "z", "and", "generates", "the", "output", "sequence", "y", "=", "(", "y", "1", ",", ".", ".", ".", ",", "y", "n", ")", "left", "to", "right", ",", "one", "element", "at", "a", "time", ".", "To", "generate", "output", "y", "i", "+", "1", ",", "the", "decoder", "computes", "a", "new", "hidden", "state", "h", "i", "+", "1", "based", "on", "the", "previous", "state", "h", "i", ",", "an", "embedding", "g", "i", "of", "the", "previous", "target", "language", "word", "y", "i", ",", "as", "well", "as", "a", "conditional", "input", "c", "i", "derived", "from", "the", "encoder", "output", "z.", "Based", "on", "this", "generic", "formulation", ",", "various", "encoder", "-", "decoder", "architectures", "have", "been", "proposed", ",", "which", "differ", "mainly", "in", "the", "conditional", "input", "and", "the", "type", "of", "RNN", ".", "Models", "without", "attention", "consider", "only", "the", "final", "encoder", "state", "z", "m", "by", "setting", "c", "i", "=", "z", "m", "for", "all", "i", ",", "or", "simply", "initialize", "the", "first", "decoder", "state", "with", "z", "m", ",", "in", "which", "case", "c", "i", "is", "not", "used", ".", "Architectures", "with", "attention", "[", "reference", "]", "compute", "c", "i", "as", "a", "weighted", "sum", "of", "(", "z", "1", ".", ".", ".", ".", ",", "z", "m", ")", "at", "each", "time", "step", ".", "The", "weights", "of", "the", "sum", "are", "referred", "to", "as", "attention", "scores", "and", "allow", "the", "network", "to", "focus", "on", "different", "parts", "of", "the", "input", "sequence", "as", "it", "generates", "the", "output", "sequences", ".", "Attention", "scores", "are", "computed", "by", "essentially", "comparing", "each", "encoder", "state", "z", "j", "to", "a", "combination", "of", "the", "previous", "decoder", "state", "h", "i", "and", "the", "last", "prediction", "y", "i", ";", "the", "result", "is", "normalized", "to", "be", "a", "distribution", "over", "input", "elements", ".", "Popular", "choices", "for", "recurrent", "networks", "in", "encoder", "-", "decoder", "models", "are", "long", "short", "term", "memory", "networks", "(", "LSTM", ";", "[", "reference", "]", "and", "gated", "recurrent", "units", "(", "GRU", ";", "[", "reference", "]", ".", "Both", "extend", "Elman", "RNNs", "[", "reference", "]", ")", "with", "a", "gating", "mechanism", "that", "allows", "the", "memorization", "of", "information", "from", "previous", "time", "steps", "in", "order", "to", "model", "long", "-", "term", "dependencies", ".", "Most", "recent", "approaches", "also", "rely", "on", "bi", "-", "directional", "encoders", "to", "build", "representations", "of", "both", "past", "and", "future", "contexts", "[", "reference", "][", "reference", "]", ".", "Models", "with", "many", "layers", "often", "rely", "on", "shortcut", "or", "residual", "connections", "[", "reference", "][", "reference", "][", "reference", "]", ".", "section", ":", "A", "Convolutional", "Architecture", "Next", "we", "introduce", "a", "fully", "convolutional", "architecture", "for", "sequence", "to", "sequence", "modeling", ".", "Instead", "of", "relying", "on", "RNNs", "to", "compute", "intermediate", "encoder", "states", "z", "and", "decoder", "states", "h", "we", "use", "convolutional", "neural", "networks", "(", "CNN", ")", ".", "section", ":", "Position", "Embeddings", "First", ",", "we", "embed", "input", "elements", "x", "=", "(", "x", "1", ",", ".", ".", ".", ",", "x", "m", ")", "in", "distributional", "space", "as", "w", "=", "(", "w", "1", ",", ".", ".", ".", ",", "w", "m", ")", ",", "where", "w", "j", "\u2208", "R", "f", "is", "a", "column", "in", "an", "embedding", "matrix", "D", "\u2208", "R", "V", "\u00d7f", ".", "We", "also", "equip", "our", "model", "with", "a", "sense", "of", "order", "by", "embedding", "the", "absolute", "position", "of", "input", "elements", "p", "=", "(", "p", "1", ",", ".", ".", ".", ",", "p", "m", ")", "where", "p", "j", "\u2208", "R", "f", ".", "Both", "are", "combined", "to", "obtain", "input", "element", "representations", "e", "=", "(", "w", "1", "+", "p", "1", ",", ".", ".", ".", ",", "w", "m", "+", "p", "m", ")", ".", "We", "proceed", "similarly", "for", "output", "elements", "that", "were", "already", "generated", "by", "the", "decoder", "network", "to", "yield", "output", "element", "representations", "that", "are", "being", "fed", "back", "into", "the", "decoder", "network", "g", "=", "(", "g", "1", ",", ".", ".", ".", ",", "g", "n", ")", ".", "Position", "embeddings", "are", "useful", "in", "our", "architecture", "since", "they", "give", "our", "model", "a", "sense", "of", "which", "portion", "of", "the", "sequence", "in", "the", "input", "or", "output", "it", "is", "currently", "dealing", "with", "(", "\u00a7", "5.4", ")", ".", "section", ":", "Convolutional", "Block", "Structure", "Both", "encoder", "and", "decoder", "networks", "share", "a", "simple", "block", "structure", "that", "computes", "intermediate", "states", "based", "on", "a", "fixed", "number", "of", "input", "elements", ".", "We", "denote", "the", "output", "of", "the", "lth", "block", "as", "h", "l", "=", "(", "h", "l", "1", ",", ".", ".", ".", ",", "h", "l", "n", ")", "for", "the", "decoder", "network", ",", "and", "z", "l", "=", "(", "z", "l", "1", ",", ".", ".", ".", ",", "z", "l", "m", ")", "for", "the", "encoder", "network", ";", "we", "refer", "to", "blocks", "and", "layers", "interchangeably", ".", "Each", "block", "contains", "a", "one", "dimensional", "convolution", "followed", "by", "a", "non", "-", "linearity", ".", "For", "a", "decoder", "network", "with", "a", "single", "block", "and", "kernel", "width", "k", ",", "each", "resulting", "state", "h", "1", "i", "contains", "information", "over", "k", "input", "elements", ".", "Stacking", "several", "blocks", "on", "top", "of", "each", "other", "increases", "the", "number", "of", "input", "elements", "represented", "in", "a", "state", ".", "For", "instance", ",", "stacking", "6", "blocks", "with", "k", "=", "5", "results", "in", "an", "input", "field", "of", "25", "elements", ",", "i.e.", "each", "output", "depends", "on", "25", "inputs", ".", "Non", "-", "linearities", "allow", "the", "networks", "to", "exploit", "the", "full", "input", "field", ",", "or", "to", "focus", "on", "fewer", "elements", "if", "needed", ".", "section", ":", "Each", "convolution", "kernel", "is", "parameterized", "as", "and", "takes", "as", "input", "X", "\u2208", "R", "k\u00d7d", "which", "is", "a", "concatenation", "of", "k", "input", "elements", "embedded", "in", "d", "dimensions", "and", "maps", "them", "to", "a", "single", "output", "element", "Y", "\u2208", "R", "2d", "that", "has", "twice", "the", "dimensionality", "of", "the", "input", "elements", ";", "subsequent", "layers", "operate", "over", "the", "k", "output", "elements", "of", "the", "previous", "layer", ".", "We", "choose", "gated", "linear", "units", "(", "GLU", ";", "[", "reference", "]", "as", "non", "-", "linearity", "which", "implement", "a", "simple", "gating", "mechanism", "over", "the", "output", "of", "the", "convolu", "-", "where", "A", ",", "B", "\u2208", "R", "d", "are", "the", "inputs", "to", "the", "non", "-", "linearity", ",", "\u2297", "is", "the", "point", "-", "wise", "multiplication", "and", "the", "output", "v", "(", "is", "half", "the", "size", "of", "Y", ".", "The", "gates", "\u03c3", "(", "B", ")", "control", "which", "inputs", "A", "of", "the", "current", "context", "are", "relevant", ".", "A", "similar", "nonlinearity", "has", "been", "introduced", "in", "[", "reference", "]", "who", "apply", "tanh", "to", "A", "but", "[", "reference", "]", "shows", "that", "GLUs", "perform", "better", "in", "the", "context", "of", "language", "modelling", ".", "To", "enable", "deep", "convolutional", "networks", ",", "we", "add", "residual", "connections", "from", "the", "input", "of", "each", "convolution", "to", "the", "output", "of", "the", "block", "[", "reference", "]", ".", "For", "encoder", "networks", "we", "ensure", "that", "the", "output", "of", "the", "convolutional", "layers", "matches", "the", "input", "length", "by", "padding", "the", "input", "at", "each", "layer", ".", "However", ",", "for", "decoder", "networks", "we", "have", "to", "take", "care", "that", "no", "future", "information", "is", "available", "to", "the", "decoder", "[", "reference", "]", ".", "Specifically", ",", "we", "pad", "the", "input", "by", "k", "\u2212", "1", "elements", "on", "both", "the", "left", "and", "right", "side", "by", "zero", "vectors", ",", "and", "then", "remove", "k", "elements", "from", "the", "end", "of", "the", "convolution", "output", ".", "We", "also", "add", "linear", "mappings", "to", "project", "between", "the", "embedding", "size", "f", "and", "the", "convolution", "outputs", "that", "are", "of", "size", "2d", ".", "We", "apply", "such", "a", "transform", "to", "w", "when", "feeding", "embeddings", "to", "the", "encoder", "network", ",", "to", "the", "encoder", "output", "z", "We", "introduce", "a", "separate", "attention", "mechanism", "for", "each", "decoder", "layer", ".", "To", "compute", "the", "attention", ",", "we", "combine", "the", "current", "decoder", "state", "h", "l", "i", "with", "an", "embedding", "of", "the", "previous", "Figure", "1", ".", "Illustration", "of", "batching", "during", "training", ".", "The", "English", "source", "sentence", "is", "encoded", "(", "top", ")", "and", "we", "compute", "all", "attention", "values", "for", "the", "four", "German", "target", "words", "(", "center", ")", "simultaneously", ".", "Our", "attentions", "are", "just", "dot", "products", "between", "decoder", "context", "representations", "(", "bottom", "left", ")", "and", "encoder", "representations", ".", "We", "add", "the", "conditional", "inputs", "computed", "by", "the", "attention", "(", "center", "right", ")", "to", "the", "decoder", "states", "which", "then", "predict", "the", "target", "words", "(", "bottom", "right", ")", ".", "The", "sigmoid", "and", "multiplicative", "boxes", "illustrate", "Gated", "Linear", "Units", ".", "target", "element", "g", "i", ":", "For", "decoder", "layer", "l", "the", "attention", "a", "l", "ij", "of", "state", "i", "and", "source", "element", "j", "is", "computed", "as", "a", "dot", "-", "product", "between", "the", "decoder", "state", "summary", "d", "The", "conditional", "input", "c", "l", "i", "to", "the", "current", "decoder", "layer", "is", "a", "weighted", "sum", "of", "the", "encoder", "outputs", "as", "well", "as", "the", "input", "element", "embeddings", "e", "j", "(", "Figure", "1", ",", "center", "right", ")", ":", "This", "is", "slightly", "different", "to", "recurrent", "approaches", "which", "compute", "both", "the", "attention", "and", "the", "weighted", "sum", "over", "z", "u", "j", "only", ".", "We", "found", "adding", "e", "j", "to", "be", "beneficial", "and", "it", "resembles", "key", "-", "value", "memory", "networks", "where", "the", "keys", "are", "the", "z", "u", "j", "and", "the", "values", "are", "the", "z", "u", "j", "+", "e", "j", "[", "reference", "]", ".", "Encoder", "outputs", "z", "u", "j", "represent", "potentially", "large", "input", "contexts", "and", "e", "j", "provides", "point", "information", "about", "a", "specific", "input", "element", "that", "is", "useful", "when", "making", "a", "prediction", ".", "Once", "c", "l", "i", "has", "been", "computed", ",", "it", "is", "simply", "added", "to", "the", "output", "of", "the", "corresponding", "decoder", "layer", "h", "l", "i", ".", "This", "can", "be", "seen", "as", "attention", "with", "multiple", "'", "hops", "'", "[", "reference", "]", "compared", "to", "single", "step", "attention", "[", "reference", "][", "reference", "][", "reference", "]", ".", "In", "particular", ",", "the", "attention", "of", "the", "first", "layer", "determines", "a", "useful", "source", "context", "which", "is", "then", "fed", "to", "the", "second", "layer", "that", "takes", "this", "information", "into", "account", "when", "computing", "attention", "etc", ".", "The", "decoder", "also", "has", "immediate", "access", "to", "the", "attention", "history", "of", "the", "k", "\u2212", "1", "previous", "time", "steps", "because", "the", "conditional", "inputs", "c", "which", "are", "input", "to", "h", "l", "i", ".", "This", "makes", "it", "easier", "for", "the", "model", "to", "take", "into", "account", "which", "previous", "inputs", "have", "been", "attended", "to", "already", "compared", "to", "recurrent", "nets", "where", "this", "information", "is", "in", "the", "recurrent", "state", "and", "needs", "to", "survive", "several", "non", "-", "linearities", ".", "Overall", ",", "our", "attention", "mechanism", "considers", "which", "words", "we", "previously", "attended", "to", "[", "reference", "]", "and", "performs", "multiple", "attention", "'", "hops", "'", "per", "time", "step", ".", "In", "Appendix", "\u00a7", "C", ",", "we", "plot", "attention", "scores", "for", "a", "deep", "decoder", "and", "show", "that", "at", "different", "layers", ",", "different", "portions", "of", "the", "source", "are", "attended", "to", ".", "Our", "convolutional", "architecture", "also", "allows", "to", "batch", "the", "attention", "computation", "across", "all", "elements", "of", "a", "sequence", "compared", "to", "RNNs", "(", "Figure", "1", ",", "middle", ")", ".", "We", "batch", "the", "computations", "of", "each", "decoder", "layer", "individually", ".", "section", ":", "Normalization", "Strategy", "We", "stabilize", "learning", "through", "careful", "weight", "initialization", "(", "\u00a7", "3.5", ")", "and", "by", "scaling", "parts", "of", "the", "network", "to", "ensure", "that", "the", "variance", "throughout", "the", "network", "does", "not", "change", "dramatically", ".", "In", "particular", ",", "we", "scale", "the", "output", "of", "residual", "blocks", "as", "well", "as", "the", "attention", "to", "preserve", "the", "variance", "of", "activations", ".", "We", "multiply", "the", "sum", "of", "the", "input", "and", "output", "of", "a", "residual", "block", "by", "\u221a", "0.5", "to", "halve", "the", "variance", "of", "the", "sum", ".", "This", "assumes", "that", "both", "summands", "have", "the", "same", "variance", "which", "is", "not", "always", "true", "but", "effective", "in", "practice", ".", "The", "conditional", "input", "c", "l", "i", "generated", "by", "the", "attention", "is", "a", "weighted", "sum", "of", "m", "vectors", "(", "2", ")", "and", "we", "counteract", "a", "change", "in", "variance", "through", "scaling", "by", "m", "1", "/", "m", ";", "we", "multiply", "by", "m", "to", "scale", "up", "the", "inputs", "to", "their", "original", "size", ",", "assuming", "the", "attention", "scores", "are", "uniformly", "distributed", ".", "This", "is", "generally", "not", "the", "case", "but", "we", "found", "it", "to", "work", "well", "in", "practice", ".", "For", "convolutional", "decoders", "with", "multiple", "attention", ",", "we", "scale", "the", "gradients", "for", "the", "encoder", "layers", "by", "the", "number", "of", "attention", "mechanisms", "we", "use", ";", "we", "exclude", "source", "word", "embeddings", ".", "We", "found", "this", "to", "stabilize", "learning", "since", "the", "encoder", "received", "too", "much", "gradient", "otherwise", ".", "section", ":", "Initialization", "Normalizing", "activations", "when", "adding", "the", "output", "of", "different", "layers", ",", "e.g.", "residual", "connections", ",", "requires", "careful", "weight", "initialization", ".", "The", "motivation", "for", "our", "initialization", "is", "the", "same", "as", "for", "the", "normalization", ":", "maintain", "the", "variance", "of", "activations", "throughout", "the", "forward", "and", "backward", "passes", ".", "All", "embeddings", "are", "initialized", "from", "a", "normal", "distribution", "with", "mean", "0", "and", "standard", "deviation", "0.1", ".", "For", "layers", "whose", "output", "is", "not", "directly", "fed", "to", "a", "gated", "linear", "unit", ",", "we", "initialize", "weights", "from", "N", "(", "0", ",", "1", "/", "n", "l", ")", "where", "n", "l", "is", "the", "number", "of", "input", "connections", "to", "each", "neuron", ".", "This", "ensures", "that", "the", "variance", "of", "a", "normally", "distributed", "input", "is", "retained", ".", "For", "layers", "which", "are", "followed", "by", "a", "GLU", "activation", ",", "we", "propose", "a", "weight", "initialization", "scheme", "by", "adapting", "the", "derivations", "in", "[", "reference", "][", "reference", "][", "reference", "]", ".", "If", "the", "GLU", "inputs", "are", "distributed", "with", "mean", "0", "and", "have", "sufficiently", "small", "variance", ",", "then", "we", "can", "approximate", "the", "output", "variance", "with", "1", "/", "4", "of", "the", "input", "variance", "(", "Appendix", "A.1", ")", ".", "Hence", ",", "we", "initialize", "the", "weights", "so", "that", "the", "input", "to", "the", "GLU", "activations", "have", "4", "times", "the", "variance", "of", "the", "layer", "input", ".", "This", "is", "achieved", "by", "drawing", "their", "initial", "values", "from", "N", "(", "0", ",", "4", "/", "n", "l", ")", ".", "Biases", "are", "uniformly", "set", "to", "zero", "when", "the", "network", "is", "constructed", ".", "We", "apply", "dropout", "to", "the", "input", "of", "some", "layers", "so", "that", "inputs", "are", "retained", "with", "a", "probability", "of", "p.", "This", "can", "be", "seen", "as", "multiplication", "with", "a", "Bernoulli", "random", "variable", "taking", "value", "1", "/", "p", "with", "probability", "p", "and", "0", "otherwise", "[", "reference", "]", ".", "The", "application", "of", "dropout", "will", "then", "cause", "the", "variance", "to", "be", "scaled", "by", "1", "/", "p", ".", "We", "aim", "to", "restore", "the", "incoming", "variance", "by", "initializing", "the", "respective", "layers", "with", "larger", "weights", ".", "Specifically", ",", "we", "use", "N", "(", "0", ",", "4p", "/", "n", "l", ")", "for", "layers", "whose", "output", "is", "subject", "to", "a", "GLU", "and", "N", "(", "0", ",", "p", "/", "n", "l", ")", "otherwise", "(", "Appendix", "A.3", ")", ".", "section", ":", "Experimental", "Setup", "section", ":", "Datasets", "We", "consider", "three", "major", "WMT", "translation", "tasks", "as", "well", "as", "a", "text", "summarization", "task", ".", "WMT'16", "English", "-", "Romanian", ".", "We", "use", "the", "same", "data", "and", "pre", "-", "processing", "as", "[", "reference", "]", "but", "remove", "sentences", "with", "more", "than", "175", "words", ".", "This", "results", "in", "2.8", "M", "sentence", "pairs", "for", "training", "and", "we", "evaluate", "on", "newstest2016", ".", "We", "experiment", "with", "word", "-", "based", "models", "using", "a", "source", "vocabulary", "of", "200", "K", "types", "and", "a", "target", "vocabulary", "of", "80", "K", "types", ".", "We", "also", "consider", "a", "joint", "source", "and", "target", "byte", "-", "pair", "encoding", "(", "BPE", ")", "with", "40", "K", "types", "[", "reference", "][", "reference", "]", ".", "WMT'14", "English", "-", "German", ".", "We", "use", "the", "same", "setup", "as", "[", "reference", "]", "which", "comprises", "4.5", "M", "sentence", "pairs", "for", "training", "and", "we", "test", "on", "newstest2014", ".", "section", ":", "3", "As", "vocabulary", "we", "use", "40", "K", "sub", "-", "word", "types", "based", "on", "BPE", ".", "WMT'14", "English", "-", "French", ".", "We", "use", "the", "full", "training", "set", "of", "36", "M", "sentence", "pairs", ",", "and", "remove", "sentences", "longer", "than", "175", "words", "as", "well", "as", "pairs", "with", "a", "source", "/", "target", "length", "ratio", "exceeding", "1.5", ".", "This", "results", "in", "35.5", "M", "sentence", "-", "pairs", "for", "training", ".", "Results", "are", "reported", "on", "newstest2014", ".", "We", "use", "a", "source", "and", "target", "vocabulary", "with", "40", "K", "BPE", "types", ".", "In", "all", "setups", "a", "small", "subset", "of", "the", "training", "data", "serves", "as", "validation", "set", "(", "about", "0.5", "-", "1", "%", "for", "each", "dataset", ")", "for", "early", "stopping", "and", "learning", "rate", "annealing", ".", "Abstractive", "summarization", ".", "We", "train", "on", "the", "Gigaword", "corpus", "[", "reference", "]", "and", "pre", "-", "process", "it", "identically", "to", "[", "reference", "]", "resulting", "in", "3.8", "M", "training", "examples", "and", "190", "K", "for", "validation", ".", "We", "evaluate", "on", "the", "DUC", "-", "2004", "test", "data", "comprising", "500", "article", "-", "title", "pairs", "[", "reference", "]", "and", "report", "three", "variants", "of", "recall", "-", "based", "ROUGE", "[", "reference", "]", ",", "namely", ",", "ROUGE", "-", "1", "(", "unigrams", ")", ",", "ROUGE", "-", "2", "(", "bigrams", ")", ",", "and", "ROUGE", "-", "L", "(", "longest", "-", "common", "substring", ")", ".", "We", "also", "evaluate", "on", "a", "Gigaword", "test", "set", "of", "2000", "pairs", "which", "is", "identical", "to", "the", "one", "used", "by", "[", "reference", "]", "and", "we", "report", "F1", "ROUGE", "similar", "to", "prior", "work", ".", "Similar", "to", "[", "reference", "]", "we", "use", "a", "source", "and", "target", "vocabulary", "of", "30", "K", "words", "and", "require", "outputs", "to", "be", "at", "least", "14", "words", "long", ".", "section", ":", "Model", "Parameters", "and", "Optimization", "We", "use", "512", "hidden", "units", "for", "both", "encoders", "and", "decoders", ",", "unless", "otherwise", "stated", ".", "All", "embeddings", ",", "including", "the", "output", "produced", "by", "the", "decoder", "before", "the", "final", "linear", "layer", ",", "have", "dimensionality", "512", ";", "we", "use", "the", "same", "dimensionalities", "for", "linear", "layers", "mapping", "between", "the", "hidden", "and", "embedding", "sizes", "(", "\u00a7", "3.2", ")", ".", "We", "train", "our", "convolutional", "models", "with", "Nesterov", "'s", "accelerated", "gradient", "method", "[", "reference", "]", ")", "using", "a", "momentum", "value", "of", "0.99", "and", "renormalize", "gradients", "if", "their", "norm", "exceeds", "0.1", "[", "reference", "]", ".", "We", "use", "a", "learning", "rate", "of", "0.25", "and", "once", "the", "validation", "perplexity", "stops", "improving", ",", "we", "reduce", "the", "learning", "rate", "by", "an", "order", "of", "magnitude", "after", "each", "epoch", "until", "it", "falls", "below", "10", "\u22124", ".", "Unless", "otherwise", "stated", ",", "we", "use", "mini", "-", "batches", "of", "64", "sentences", ".", "We", "restrict", "the", "maximum", "number", "of", "words", "in", "a", "mini", "-", "batch", "to", "make", "sure", "that", "batches", "with", "long", "sentences", "backtranslations", "/", "en", "-", "ro", ".", "3", "http:", "//", "nlp.stanford.edu", "/", "projects", "/", "nmt", "still", "fit", "in", "GPU", "memory", ".", "If", "the", "threshold", "is", "exceeded", ",", "we", "simply", "split", "the", "batch", "until", "the", "threshold", "is", "met", "and", "process", "the", "parts", "separatedly", ".", "Gradients", "are", "normalized", "by", "the", "number", "of", "non", "-", "padding", "tokens", "per", "mini", "-", "batch", ".", "We", "also", "use", "weight", "normalization", "for", "all", "layers", "except", "for", "lookup", "tables", "[", "reference", "]", ".", "Besides", "dropout", "on", "the", "embeddings", "and", "the", "decoder", "output", ",", "we", "also", "apply", "dropout", "to", "the", "input", "of", "the", "convolutional", "blocks", "[", "reference", "]", ".", "All", "models", "are", "implemented", "in", "Torch", "[", "reference", "]", "and", "trained", "on", "a", "single", "Nvidia", "M40", "GPU", "except", "for", "WMT'14", "EnglishFrench", "for", "which", "we", "use", "a", "multi", "-", "GPU", "setup", "on", "a", "single", "machine", ".", "We", "train", "on", "up", "to", "eight", "GPUs", "synchronously", "by", "maintaining", "copies", "of", "the", "model", "on", "each", "card", "and", "split", "the", "batch", "so", "that", "each", "worker", "computes", "1", "/", "8", "-", "th", "of", "the", "gradients", ";", "at", "the", "end", "we", "sum", "the", "gradients", "via", "Nvidia", "NCCL", ".", "section", ":", "Evaluation", "We", "report", "average", "results", "over", "three", "runs", "of", "each", "model", ",", "where", "each", "differs", "only", "in", "the", "initial", "random", "seed", ".", "Translations", "are", "generated", "by", "a", "beam", "search", "and", "we", "normalize", "log", "-", "likelihood", "scores", "by", "sentence", "length", ".", "We", "use", "a", "beam", "of", "width", "5", ".", "We", "divide", "the", "log", "-", "likelihoods", "of", "the", "final", "hypothesis", "in", "beam", "search", "by", "their", "length", "|y|.", "For", "WMT'14", "English", "-", "German", "we", "tune", "a", "length", "normalization", "constant", "on", "a", "separate", "development", "set", "(", "newstest2015", ")", "and", "we", "normalize", "log", "-", "likelihoods", "by", "|y|", "\u03b1", "[", "reference", "]", ".", "On", "other", "datasets", "we", "did", "not", "find", "any", "benefit", "with", "length", "normalization", ".", "For", "word", "-", "based", "models", ",", "we", "perform", "unknown", "word", "replacement", "based", "on", "attention", "scores", "after", "generation", "[", "reference", "]", ".", "Unknown", "words", "are", "replaced", "by", "looking", "up", "the", "source", "word", "with", "the", "maximum", "attention", "score", "in", "a", "precomputed", "dictionary", ".", "If", "the", "dictionary", "contains", "no", "translation", ",", "then", "we", "simply", "copy", "the", "source", "word", ".", "Dictionaries", "were", "extracted", "from", "the", "word", "aligned", "training", "data", "that", "we", "obtained", "with", "fast", "align", "[", "reference", "]", ".", "Each", "source", "word", "is", "mapped", "to", "the", "target", "word", "it", "is", "most", "frequently", "aligned", "to", ".", "In", "our", "multi", "-", "step", "attention", "(", "\u00a7", "3.3", ")", "we", "simply", "average", "the", "attention", "scores", "over", "all", "layers", ".", "Finally", ",", "we", "compute", "case", "-", "sensitive", "tokenized", "BLEU", ",", "except", "for", "WMT'16", "English", "-", "Romanian", "where", "we", "use", "detokenized", "BLEU", "to", "be", "comparable", "with", "[", "reference", "]", "5", ".", "Results", "section", ":", "Recurrent", "vs.", "Convolutional", "Models", "We", "first", "evaluate", "our", "convolutional", "model", "on", "three", "translation", "tasks", ".", "On", "WMT'16", "English", "-", "Romanian", "translation", "we", "compare", "to", "[", "reference", "]", "which", "is", "the", "winning", "entry", "on", "this", "language", "pair", "at", "WMT'16", "[", "reference", "]", ".", "Their", "model", "implements", "the", "attention", "-", "based", "sequence", "to", "sequence", "architecture", "of", "[", "reference", "]", "and", "uses", "GRU", "cells", "both", "in", "the", "encoder", "and", "decoder", ".", "We", "test", "both", "word", "-", "based", "and", "BPE", "vocabularies", "(", "\u00a7", "4", ")", ".", "Table", "1", "shows", "that", "our", "fully", "convolutional", "sequence", "to", "sequence", "model", "(", "ConvS2S", ")", "outperforms", "the", "WMT'16", "winning", "entry", "for", "English", "-", "Romanian", "by", "1.9", "BLEU", "with", "a", "BPE", "encoding", "and", "by", "1.3", "BLEU", "with", "a", "word", "factored", "vocabulary", ".", "This", "instance", "of", "our", "architecture", "has", "20", "layes", "in", "the", "encoder", "and", "20", "layers", "in", "the", "decoder", ",", "both", "using", "kernels", "of", "width", "3", "and", "hidden", "size", "512", "throughout", ".", "Training", "took", "between", "6", "and", "7.5", "days", "on", "a", "single", "GPU", ".", "On", "WMT'14", "English", "to", "German", "translation", "we", "compare", "to", "the", "following", "prior", "work", ":", "[", "reference", "]", "is", "based", "on", "a", "four", "layer", "LSTM", "attention", "model", ",", "ByteNet", "[", "reference", "]", ")", "propose", "a", "convolutional", "model", "based", "on", "characters", "without", "attention", ",", "with", "30", "layers", "in", "the", "encoder", "and", "30", "layers", "in", "the", "decoder", ",", "GNMT", "[", "reference", "]", "represents", "the", "state", "of", "the", "art", "on", "this", "dataset", "and", "they", "use", "eight", "encoder", "LSTMs", "as", "well", "as", "eight", "decoder", "LSTMs", ",", "we", "quote", "their", "result", "for", "a", "word", "-", "based", "model", ",", "such", "as", "ours", ",", "as", "well", "as", "a", "word", "-", "piece", "model", "[", "reference", "]", ".", "section", ":", "5", "The", "results", "(", "Table", "1", ")", "show", "that", "our", "convolutional", "model", "outpeforms", "GNMT", "by", "0.5", "BLEU", ".", "Our", "encoder", "has", "15", "layers", "and", "the", "decoder", "has", "15", "layers", ",", "both", "with", "512", "hidden", "units", "in", "the", "first", "ten", "layers", "and", "768", "units", "in", "the", "subsequent", "three", "layers", ",", "all", "using", "kernel", "width", "3", ".", "The", "final", "two", "layers", "have", "2048", "units", "which", "are", "just", "linear", "mappings", "with", "a", "single", "input", ".", "We", "trained", "this", "model", "on", "a", "single", "GPU", "over", "a", "period", "of", "18.5", "days", "with", "a", "batch", "size", "of", "48", ".", "LSTM", "sparse", "mixtures", "have", "shown", "strong", "accuracy", "at", "26.03", "BLEU", "for", "a", "single", "run", "[", "reference", "]", ")", "which", "compares", "to", "25.39", "BLEU", "for", "our", "best", "run", ".", "This", "mixture", "sums", "the", "output", "of", "four", "experts", ",", "not", "unlike", "an", "ensemble", "which", "sums", "the", "output", "of", "multiple", "networks", ".", "ConvS2S", "also", "benefits", "from", "ensembling", "(", "\u00a7", "5.2", ")", ",", "therefore", "mixtures", "are", "a", "promising", "direction", ".", "Finally", ",", "we", "train", "on", "the", "much", "larger", "WMT'14", "EnglishFrench", "task", "where", "we", "compare", "to", "the", "state", "of", "the", "art", "result", "of", "GNMT", "[", "reference", "]", ".", "Our", "model", "is", "trained", "with", "a", "simple", "token", "-", "level", "likelihood", "objective", "and", "we", "improve", "over", "GNMT", "in", "the", "same", "setting", "by", "1.6", "BLEU", "on", "average", ".", "We", "also", "outperform", "their", "reinforcement", "(", "RL", ")", "models", "by", "0.5", "[", "reference", "]", "We", "did", "not", "use", "the", "exact", "same", "vocabulary", "size", "because", "word", "pieces", "and", "BPE", "estimate", "the", "vocabulary", "differently", ".", "The", "translations", "produced", "by", "our", "models", "often", "match", "the", "length", "of", "the", "references", ",", "particularly", "for", "the", "large", "WMT'14", "English", "-", "French", "task", ",", "or", "are", "very", "close", "for", "small", "to", "medium", "data", "sets", "such", "as", "WMT'14", "English", "-", "German", "or", "WMT'16", "English", "-", "Romanian", ".", "section", ":", "WMT'16", "English", "-", "Romanian", "BLEU", "section", ":", "Ensemble", "Results", "Next", ",", "we", "ensemble", "eight", "likelihood", "-", "trained", "models", "for", "both", "WMT'14", "English", "-", "German", "and", "WMT'14", "English", "-", "French", "and", "compare", "to", "previous", "work", "which", "also", "reported", "ensemble", "results", ".", "For", "the", "former", ",", "we", "also", "show", "the", "result", "when", "ensembling", "10", "models", ".", "Table", "2", "shows", "that", "we", "outperform", "the", "best", "current", "ensembles", "on", "both", "datasets", ".", "section", ":", "Generation", "Speed", "Next", ",", "we", "evaluate", "the", "inference", "speed", "of", "our", "architecture", "on", "the", "development", "set", "of", "the", "WMT'14", "English", "-", "French", "task", "which", "is", "the", "concatenation", "of", "newstest2012", "and", "newstest2013", ";", "it", "comprises", "6003", "sentences", ".", "We", "measure", "generation", "speed", "both", "on", "GPU", "and", "CPU", "hardware", ".", "Specifically", ",", "we", "measure", "GPU", "speed", "on", "three", "generations", "of", "Nvidia", "cards", ":", "a", "GTX", "-", "1080ti", ",", "an", "M40", "as", "well", "as", "an", "older", "K40", "card", ".", "CPU", "timings", "are", "measured", "on", "one", "host", "with", "48", "hyperthreaded", "cores", "(", "Intel", "Xeon", "E5", "-", "2680", "@", "2.50GHz", ")", "with", "40", "workers", ".", "In", "all", "settings", ",", "we", "batch", "up", "to", "128", "sentences", ",", "composing", "batches", "with", "sentences", "of", "equal", "length", ".", "Note", "that", "the", "majority", "of", "batches", "is", "smaller", "because", "of", "the", "small", "size", "of", "the", "development", "set", ".", "We", "experiment", "with", "beams", "of", "size", "5", "as", "well", "as", "greedy", "search", ",", "i.e", "beam", "of", "size", "1", ".", "To", "make", "generation", "fast", ",", "we", "do", "not", "recompute", "convolution", "states", "that", "have", "not", "changed", "compared", "to", "the", "previous", "time", "step", "but", "rather", "copy", "(", "shift", ")", "these", "activations", ".", "We", "compare", "to", "results", "reported", "in", "[", "reference", "]", "use", "Nvidia", "K80", "GPUs", "which", "are", "essentially", "two", "K40s", ".", "We", "did", "not", "have", "such", "a", "GPU", "available", "and", "therefore", "run", "experiments", "on", "an", "older", "K40", "card", "which", "is", "inferior", "to", "a", "K80", ",", "in", "addition", "to", "the", "newer", "M40", "and", "GTX", "-", "1080ti", "cards", ".", "The", "results", "(", "Table", "3", ")", "show", "that", "our", "model", "can", "generate", "translations", "on", "a", "K40", "GPU", "at", "9.3", "times", "the", "speed", "and", "2.25", "higher", "BLEU", ";", "on", "an", "M40", "the", "speed", "-", "up", "is", "up", "to", "13.7", "times", "and", "on", "a", "GTX", "-", "1080ti", "card", "the", "speed", "is", "21.3", "times", "faster", ".", "A", "larger", "beam", "of", "size", "5", "decreases", "speed", "but", "gives", "better", "BLEU", ".", "On", "CPU", ",", "our", "model", "is", "up", "to", "9.3", "times", "faster", ",", "however", ",", "the", "GNMT", "CPU", "results", "were", "obtained", "with", "an", "88", "core", "machine", "whereas", "our", "results", "were", "obtained", "with", "just", "over", "half", "the", "number", "of", "cores", ".", "On", "a", "per", "CPU", "core", "basis", ",", "our", "model", "is", "17", "times", "faster", "at", "a", "better", "BLEU", ".", "Finally", ",", "our", "CPU", "speed", "is", "2.7", "times", "higher", "than", "GNMT", "on", "a", "custom", "TPU", "chip", "which", "shows", "that", "high", "speed", "can", "be", "achieved", "on", "commodity", "hardware", ".", "We", "do", "no", "report", "TPU", "figures", "as", "we", "do", "not", "have", "access", "to", "this", "hardware", ".", "section", ":", "Position", "Embeddings", "In", "the", "following", "sections", ",", "we", "analyze", "the", "design", "choices", "in", "our", "architecture", ".", "The", "remaining", "results", "in", "this", "paper", "are", "based", "on", "the", "WMT'14", "English", "-", "German", "task", "with", "13", "encoder", "layers", "at", "kernel", "size", "3", "and", "5", "decoder", "layers", "at", "kernel", "size", "5", ".", "We", "use", "a", "target", "vocabulary", "of", "160", "K", "words", "as", "well", "as", "vocabulary", "selection", "[", "reference", "][", "reference", "]", "to", "decrease", "the", "size", "of", "the", "output", "layer", "which", "speeds", "up", "training", "and", "testing", ".", "The", "average", "vocabulary", "size", "for", "each", "training", "batch", "is", "about", "20", "K", "target", "words", ".", "All", "figures", "are", "averaged", "over", "three", "runs", "(", "\u00a7", "4", ")", "and", "BLEU", "is", "reported", "on", "newstest2014", "before", "unknown", "word", "replacement", ".", "We", "start", "with", "an", "experiment", "that", "removes", "the", "position", "em", "-", "beddings", "from", "the", "encoder", "and", "decoder", "(", "\u00a7", "3.1", ")", ".", "These", "embeddings", "allow", "our", "model", "to", "identify", "which", "portion", "of", "the", "source", "and", "target", "sequence", "it", "is", "dealing", "with", "but", "also", "impose", "a", "restriction", "on", "the", "maximum", "sentence", "length", ".", "Table", "4", "shows", "that", "position", "embeddings", "are", "helpful", "but", "that", "our", "model", "still", "performs", "well", "without", "them", ".", "Removing", "the", "source", "position", "embeddings", "results", "in", "a", "larger", "accuracy", "decrease", "than", "target", "position", "embeddings", ".", "However", ",", "removing", "both", "source", "and", "target", "positions", "decreases", "accuracy", "only", "by", "0.5", "BLEU", ".", "We", "had", "assumed", "that", "the", "model", "would", "not", "be", "able", "to", "calibrate", "the", "length", "of", "the", "output", "sequences", "very", "well", "without", "explicit", "position", "information", ",", "however", ",", "the", "output", "lengths", "of", "models", "without", "position", "embeddings", "closely", "matches", "models", "with", "position", "information", ".", "This", "indicates", "that", "the", "models", "can", "learn", "relative", "position", "information", "within", "the", "contexts", "visible", "to", "the", "encoder", "and", "decoder", "networks", "which", "can", "observe", "up", "to", "27", "and", "25", "words", "respectively", ".", "Recurrent", "models", "typically", "do", "not", "use", "explicit", "position", "embeddings", "since", "they", "can", "learn", "where", "they", "are", "in", "the", "sequence", "through", "the", "recurrent", "hidden", "state", "computation", ".", "In", "our", "setting", ",", "the", "use", "of", "position", "embeddings", "requires", "only", "a", "simple", "addition", "to", "the", "input", "word", "embeddings", "which", "is", "a", "negligible", "overhead", ".", "section", ":", "Multi", "-", "step", "Attention", "The", "multiple", "attention", "mechanism", "(", "\u00a7", "3.3", ")", "computes", "a", "separate", "source", "context", "vector", "for", "each", "decoder", "layer", ".", "The", "computation", "also", "takes", "into", "account", "contexts", "computed", "for", "preceding", "decoder", "layers", "of", "the", "current", "time", "step", "as", "well", "as", "previous", "time", "steps", "that", "are", "within", "the", "receptive", "field", "of", "the", "decoder", ".", "How", "does", "multiple", "attention", "compare", "to", "attention", "in", "fewer", "layers", "or", "even", "only", "in", "a", "single", "layer", "as", "is", "usual", "?", "Table", "5", "shows", "that", "attention", "in", "all", "decoder", "layers", "achieves", "the", "best", "validation", "perplexity", "(", "PPL", ")", ".", "Furthermore", ",", "removing", "more", "and", "more", "attention", "layers", "decreases", "accuracy", ",", "both", "in", "terms", "of", "BLEU", "as", "well", "as", "PPL", ".", "The", "computational", "overhead", "for", "attention", "is", "very", "small", "compared", "to", "the", "rest", "of", "the", "network", ".", "Training", "with", "attention", "in", "all", "five", "decoder", "layers", "processes", "3624", "target", "words", "per", "second", "on", "average", "on", "a", "single", "GPU", ",", "compared", "to", "3772", "words", "per", "second", "for", "attention", "in", "a", "single", "layer", ".", "This", "is", "only", "Table", "5", ".", "Multi", "-", "step", "attention", "in", "all", "five", "decoder", "layers", "or", "fewer", "layers", "in", "terms", "of", "validation", "perplexity", "(", "PPL", ")", "and", "test", "BLEU", ".", "a", "4", "%", "slow", "down", "when", "adding", "4", "attention", "modules", ".", "Most", "neural", "machine", "translation", "systems", "only", "use", "a", "single", "module", ".", "This", "demonstrates", "that", "attention", "is", "not", "the", "bottleneck", "in", "neural", "machine", "translation", ",", "even", "though", "it", "is", "quadratic", "in", "the", "sequence", "length", "(", "cf", ".", "[", "reference", "]", ".", "Part", "of", "the", "reason", "for", "the", "low", "impact", "on", "speed", "is", "that", "we", "batch", "the", "computation", "of", "an", "attention", "module", "over", "all", "target", "words", ",", "similar", "to", "[", "reference", "]", ".", "However", ",", "for", "RNNs", "batching", "of", "the", "attention", "may", "be", "less", "effective", "because", "of", "the", "dependence", "on", "the", "previous", "time", "step", ".", "section", ":", "Kernel", "size", "and", "Depth", "Figure", "2", "shows", "accuracy", "when", "we", "change", "the", "number", "of", "layers", "in", "the", "encoder", "or", "decoder", ".", "The", "kernel", "width", "for", "layers", "in", "the", "encoder", "is", "3", "and", "for", "the", "decoder", "it", "is", "5", ".", "Deeper", "architectures", "are", "particularly", "beneficial", "for", "the", "encoder", "but", "less", "so", "for", "the", "decoder", ".", "Decoder", "setups", "with", "two", "layers", "already", "perform", "well", "whereas", "for", "the", "encoder", "accuracy", "keeps", "increasing", "steadily", "with", "more", "layers", "until", "up", "to", "9", "layers", "when", "accuracy", "starts", "to", "plateau", ".", "RNN", "MLE", "[", "reference", "]", "24", "Table", "6", ".", "Accuracy", "on", "two", "summarization", "tasks", "in", "terms", "of", "Rouge", "-", "1", "(", "RG", "-", "1", ")", ",", "Rouge", "-", "2", "(", "RG", "-", "2", ")", ",", "and", "Rouge", "-", "L", "(", "RG", "-", "L", ")", ".", "Aside", "from", "increasing", "the", "depth", "of", "the", "networks", ",", "we", "can", "also", "change", "the", "kernel", "width", ".", "Table", "7", "shows", "that", "encoders", "with", "narrow", "kernels", "and", "many", "layers", "perform", "better", "than", "wider", "kernels", ".", "These", "networks", "can", "also", "be", "faster", "since", "the", "amount", "of", "work", "to", "compute", "a", "kernel", "operating", "over", "3", "input", "elements", "is", "less", "than", "half", "compared", "to", "kernels", "over", "7", "elements", ".", "We", "see", "a", "similar", "picture", "for", "decoder", "networks", "with", "large", "kernel", "sizes", "(", "Table", "8", ")", ".", "[", "reference", "]", "shows", "that", "context", "sizes", "of", "20", "words", "are", "often", "sufficient", "to", "achieve", "very", "good", "accuracy", "on", "language", "modeling", "for", "English", ".", "section", ":", "Kernel", "width", "Encoder", "layers", "section", ":", "Summarization", "Finally", ",", "we", "evaluate", "our", "model", "on", "abstractive", "sentence", "summarization", "which", "takes", "a", "long", "sentence", "as", "input", "and", "outputs", "a", "shortened", "version", ".", "The", "current", "best", "models", "on", "this", "task", "are", "recurrent", "neural", "networks", "which", "either", "optimize", "the", "evaluation", "metric", "[", "reference", "]", "or", "address", "specific", "problems", "of", "summarization", "such", "as", "avoiding", "repeated", "generations", "[", "reference", "]", ".", "We", "use", "standard", "likelhood", "training", "for", "our", "model", "and", "a", "simple", "model", "with", "six", "layers", "in", "the", "encoder", "and", "decoder", "each", ",", "hidden", "size", "256", ",", "batch", "size", "128", ",", "and", "we", "trained", "on", "a", "single", "GPU", "in", "one", "night", ".", "Table", "6", "shows", "that", "our", "likelhood", "trained", "model", "outperforms", "the", "likelihood", "trained", "model", "(", "RNN", "MLE", ")", "of", "[", "reference", "]", "and", "is", "not", "far", "behind", "the", "best", "models", "on", "this", "task", "which", "benefit", "from", "task", "-", "specific", "optimization", "and", "model", "structure", ".", "We", "expect", "our", "model", "to", "benefit", "from", "these", "improvements", "as", "well", ".", "section", ":", "Conclusion", "and", "Future", "Work", "We", "introduce", "the", "first", "fully", "convolutional", "model", "for", "sequence", "to", "sequence", "learning", "that", "outperforms", "strong", "recurrent", "models", "on", "very", "large", "benchmark", "datasets", "at", "an", "order", "of", "magnitude", "faster", "speed", ".", "Compared", "to", "recurrent", "networks", ",", "our", "convolutional", "approach", "allows", "to", "discover", "compositional", "structure", "in", "the", "sequences", "more", "easily", "since", "representations", "are", "built", "hierarchically", ".", "Our", "model", "relies", "on", "gating", "and", "performs", "multiple", "attention", "steps", ".", "We", "achieve", "a", "new", "state", "of", "the", "art", "on", "several", "public", "translation", "benchmark", "data", "sets", ".", "On", "the", "WMT'16", "EnglishRomanian", "task", "we", "outperform", "the", "previous", "best", "result", "by", "1.9", "BLEU", ",", "on", "WMT'14", "English", "-", "French", "translation", "we", "improve", "over", "the", "LSTM", "model", "of", "[", "reference", "]", "by", "1.6", "BLEU", "in", "a", "comparable", "setting", ",", "and", "on", "WMT'14", "EnglishGerman", "translation", "we", "ouperform", "the", "same", "model", "by", "0.5", "BLEU", ".", "In", "future", "work", ",", "we", "would", "like", "to", "apply", "convolutional", "architectures", "to", "other", "sequence", "to", "sequence", "learning", "problems", "which", "may", "benefit", "from", "learning", "hierarchical", "representations", "as", "well", ".", "section", ":", "A.", "Weight", "Initialization", "We", "derive", "a", "weight", "initialization", "scheme", "tailored", "to", "the", "GLU", "activation", "function", "similar", "to", "[", "reference", "]", ";", "[", "reference", "]", "by", "focusing", "on", "the", "variance", "of", "activations", "within", "the", "network", "for", "both", "forward", "and", "backward", "passes", ".", "We", "also", "detail", "how", "we", "modify", "the", "weight", "initialization", "for", "dropout", ".", "section", ":", "A.1", ".", "Forward", "Pass", "Assuming", "that", "the", "inputs", "x", "l", "of", "a", "convolutional", "layer", "l", "and", "its", "weights", "W", "l", "are", "independent", "and", "identically", "distributed", "(", "i.i.d", ".", ")", ",", "the", "variance", "of", "its", "output", ",", "computed", "as", "where", "n", "l", "is", "the", "number", "inputs", "to", "the", "layer", ".", "For", "onedimensional", "convolutional", "layers", "with", "kernel", "width", "k", "and", "input", "dimension", "c", ",", "this", "is", "kc", ".", "We", "adopt", "the", "notation", "in", "[", "reference", "]", ",", "i.e.", "y", "l", ",", "w", "l", "and", "x", "l", "represent", "the", "random", "variables", "in", "y", "l", ",", "W", "l", "and", "x", "l", ".", "With", "w", "l", "and", "x", "l", "independent", "from", "each", "other", "and", "normally", "distributed", "with", "zero", "mean", ",", "this", "amounts", "to", "x", "l", "is", "the", "result", "of", "the", "GLU", "activation", "function", "y", "A", "lower", "bound", "is", "given", "by", "(", "1", "/", "4", ")", "V", "ar", "[", "y", "With", "x", "\u223c", "N", "(", "0", ",", "std", "(", "x", ")", ")", ",", "this", "yields", "With", "(", "7", ")", "We", "initialize", "the", "embedding", "matrices", "in", "our", "network", "with", "small", "variances", "(", "around", "0.01", ")", ",", "which", "allows", "us", "to", "dismiss", "the", "quadratic", "term", "and", "approximate", "the", "GLU", "output", "variance", "with", "If", "L", "network", "layers", "of", "equal", "size", "and", "with", "GLU", "activations", "are", "combined", ",", "the", "variance", "of", "the", "final", "output", "y", "L", "is", "given", "by", "Following", "[", "reference", "]", ",", "we", "aim", "to", "satisfy", "the", "condition", "so", "that", "the", "activations", "in", "a", "network", "are", "neither", "exponentially", "magnified", "nor", "reduced", ".", "This", "is", "achieved", "by", "initializing", "W", "l", "from", "N", "(", "0", ",", "4", "/", "n", "l", ")", ".", "section", ":", "A.2", ".", "Backward", "Pass", "The", "gradient", "of", "a", "convolutional", "layer", "is", "computed", "via", "backpropagation", "as", "\u2206x", "l", "=", "\u0174", "l", "y", "l", ".", "Considering", "separate", "gradients", "\u2206y", "a", "l", "and", "\u2206y", "b", "l", "for", "GLU", ",", "the", "gradient", "of", "x", "is", "given", "by", "W", "corresponds", "to", "W", "with", "re", "-", "arranged", "weights", "to", "enable", "back", "-", "propagation", ".", "Analogously", "to", "the", "forward", "pass", ",", "\u2206x", "l", ",", "w", "l", "and", "\u2206y", "l", "represent", "the", "random", "variables", "for", "the", "values", "in", "\u2206x", "l", ",", "\u0174", "l", "and", "\u2206y", "l", ",", "respectively", ".", "Note", "that", "W", "and\u0174", "contain", "the", "same", "values", ",", "i.e.\u0175", "=", "w.", "Similar", "to", "(", "3", ")", ",", "the", "variance", "of", "\u2206x", "l", "is", "Here", ",", "n", "l", "is", "the", "number", "of", "inputs", "to", "layer", "l", "+", "1", ".", "The", "gradients", "for", "the", "GLU", "inputs", "are", ":", "The", "approximation", "for", "the", "forward", "pass", "can", "be", "used", "for", "V", "ar", "[", "\u2206y", "V", "ar", "[", "\u2206y", "We", "observe", "relatively", "small", "gradients", "in", "our", "network", ",", "typically", "around", "0.001", "at", "the", "start", "of", "training", ".", "Therefore", ",", "we", "approximate", "by", "discarding", "the", "quadratic", "terms", "above", ",", "i.e.", "V", "ar", "[", "\u2206y", "V", "ar", "[", "\u2206y", "As", "for", "the", "forward", "pass", ",", "the", "above", "result", "can", "be", "generalized", "to", "backpropagation", "through", "many", "successive", "layers", ",", "resulting", "in", "For", "arbitrarily", "large", "variances", "of", "network", "inputs", "and", "activations", ",", "our", "approximations", "are", "invalid", ";", "in", "that", "case", ",", "the", "initial", "values", "for", "W", "a", "l", "and", "W", "b", "l", "would", "have", "to", "be", "balanced", "for", "the", "input", "distribution", "to", "be", "retained", ".", "Alternatively", ",", "methods", "that", "explicitly", "control", "the", "variance", "in", "the", "network", ",", "e.g.", "batch", "normalization", "[", "reference", "]", "or", "layer", "normalization", "[", "reference", "]", "could", "be", "employed", ".", "section", ":", "A.3", ".", "Dropout", "Dropout", "retains", "activations", "in", "a", "neural", "network", "with", "a", "probability", "p", "and", "sets", "them", "to", "zero", "otherwise", "[", "reference", "]", ".", "It", "is", "common", "practice", "to", "scale", "the", "retained", "activations", "by", "1", "/", "p", "during", "training", "so", "that", "the", "weights", "of", "the", "network", "do", "not", "have", "to", "be", "modified", "at", "test", "time", "when", "p", "is", "set", "to", "1", ".", "In", "this", "case", ",", "dropout", "amounts", "to", "multiplying", "activations", "Assuming", "that", "a", "the", "input", "of", "a", "convolutional", "layer", "has", "been", "subject", "to", "dropout", "with", "a", "retain", "probability", "p", ",", "the", "variations", "of", "the", "forward", "and", "backward", "activations", "from", "\u00a7", "A.1", "and", "\u00a7", "A.2", "can", "now", "be", "approximated", "with", "This", "amounts", "to", "a", "modified", "initialization", "of", "W", "l", "from", "a", "normal", "distribution", "with", "zero", "mean", "and", "a", "standard", "deviation", "of", "4p", "/", "n", ".", "For", "layers", "without", "a", "succeeding", "GLU", "activation", "function", ",", "we", "initialize", "weights", "from", "N", "(", "0", ",", "p", "/", "n", ")", "to", "calibrate", "for", "any", "immediately", "preceding", "dropout", "application", ".", "section", ":", "B.", "Upper", "Bound", "on", "Squared", "Sigmoid", "The", "sigmoid", "function", "\u03c3", "(", "x", ")", "can", "be", "expressed", "as", "a", "hyperbolic", "tangent", "by", "using", "the", "identity", "tanh", "(", "x", ")", "=", "2", "\u03c3", "(", "2x", ")", "\u2212", "1", ".", "The", "derivative", "of", "tanh", "is", "tanh", "(", "x", ")", "=", "1", "\u2212", "tanh", "2", "(", "x", ")", ",", "and", "with", "tanh", "(", "x", ")", "\u2208", "[", "0", ",", "1", "]", ",", "x", "\u2265", "0", "it", "holds", "that", "tanh", "(", "x", ")", "\u2264", "1", ",", "x", "\u2265", "0", "(", "34", ")", "We", "can", "express", "this", "relation", "with", "\u03c3", "(", "x", ")", "as", "follows", ":", "Both", "terms", "of", "this", "inequality", "have", "rotational", "symmetry", "w.r.t", "0", ",", "and", "thus", "C.", "Attention", "Visualization", "Figure", "3", "shows", "attention", "scores", "for", "a", "generated", "sentence", "from", "the", "WMT'14", "English", "-", "German", "task", ".", "The", "model", "used", "for", "this", "plot", "has", "8", "decoder", "layers", "and", "a", "80", "K", "BPE", "vocabulary", ".", "The", "attention", "passes", "in", "different", "decoder", "layers", "capture", "different", "portions", "of", "the", "source", "sentence", ".", "Layer", "1", ",", "3", "and", "6", "exhibit", "a", "linear", "alignment", ".", "The", "first", "layer", "shows", "the", "clearest", "alignment", ",", "although", "it", "is", "slightly", "off", "and", "frequently", "attends", "to", "the", "corresponding", "source", "word", "of", "the", "previously", "generated", "target", "word", ".", "Layer", "2", "and", "8", "lack", "a", "clear", "structure", "and", "are", "presumably", "collecting", "information", "about", "the", "whole", "source", "sentence", ".", "The", "fourth", "layer", "shows", "high", "alignment", "scores", "on", "nouns", "such", "as", "\"", "festival", "\"", ",", "\"", "way", "\"", "and", "\"", "work", "\"", "for", "both", "the", "generated", "target", "nouns", "as", "well", "as", "their", "preceding", "words", ".", "Note", "that", "in", "German", ",", "those", "preceding", "words", "depend", "on", "gender", "and", "object", "relationship", "of", "the", "respective", "noun", ".", "Finally", ",", "the", "attention", "scores", "in", "layer", "5", "and", "7", "focus", "on", "\"", "built", "\"", ",", "which", "is", "reordered", "in", "the", "German", "translation", "and", "is", "moved", "from", "the", "beginning", "to", "the", "very", "end", "of", "the", "sentence", ".", "One", "interpretation", "for", "this", "is", "that", "as", "generation", "progresses", ",", "the", "model", "repeatedly", "tries", "to", "perform", "the", "re", "-", "ordering", ".", "\"", "aufgebaut", "\"", "can", "be", "generated", "after", "a", "noun", "or", "pronoun", "only", ",", "which", "is", "reflected", "in", "the", "higher", "scores", "at", "positions", "2", ",", "5", ",", "8", ",", "11", "and", "13", ".", "Layer", "7", "Layer", "8", "Figure", "3", ".", "Attention", "scores", "for", "different", "decoder", "layers", "for", "a", "sentence", "translated", "from", "English", "(", "y", "-", "axis", ")", "to", "German", "(", "x", "-", "axis", ")", ".", "This", "model", "uses", "8", "decoder", "layers", "and", "a", "80k", "BPE", "vocabulary", ".", "section", ":", "section", ":", "Acknowledgements", "We", "thank", "Benjamin", "Graham", "for", "providing", "a", "fast", "1", "-", "D", "convolution", ",", "and", "Ronan", "Collobert", "as", "well", "as", "Yann", "LeCun", "for", "helpful", "discussions", "related", "to", "this", "work", ".", "section", ":"]}