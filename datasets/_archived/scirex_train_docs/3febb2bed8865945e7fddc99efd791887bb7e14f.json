{"coref": {"ACL-ARC": [], "Accuracy": [[2903, 2904], [2922, 2923], [3466, 3468], [3931, 3933], [4725, 4726], [4760, 4764], [4784, 4785], [5425, 5427], [5448, 5449], [5659, 5660], [5750, 5751], [5768, 5769], [5796, 5797], [6964, 6966]], "Avg_F1": [[3144, 3146], [6580, 6582]], "BCN": [[307, 309], [1952, 1954], [2879, 2883], [3237, 3242], [3425, 3428], [3429, 3430], [3459, 3461], [4156, 4160], [4285, 4289], [5864, 5869], [5873, 5878], [6310, 6312], [6921, 6925], [6952, 6954], [6956, 6961]], "BCN_ELMo": [[2654, 2658]], "BiDAF": [[5879, 5880]], "BiDAF___Self_Attention___ELMo__ensemble_": [[1794, 1795], [2173, 2174], [2222, 2223], [3570, 3571], [5145, 5146]], "BiDAF___Self_Attention___ELMo__single_model_": [[3765, 3766], [3922, 3923], [4037, 4038], [5400, 5401], [5418, 5419], [5541, 5542]], "BiLSTM-Attention": [[3102, 3106]], "BiLSTM-Attention___ELMo": [], "BiLSTM-CRF": [[71, 73], [376, 377], [637, 638], [845, 846], [850, 851], [947, 948], [1095, 1096], [1181, 1182], [1307, 1308], [1506, 1507], [1557, 1558], [1636, 1637], [1660, 1662], [1804, 1805], [1823, 1824], [1834, 1835], [1877, 1878], [1977, 1978], [2079, 2080], [2096, 2097], [2282, 2284], [2336, 2337], [2438, 2439], [2453, 2454], [2484, 2485], [2499, 2500], [2689, 2690], [2858, 2859], [2983, 2988], [3244, 3246], [3274, 3279], [3606, 3607], [4130, 4131], [4179, 4180], [4303, 4304], [4345, 4346], [4435, 4436], [4469, 4470], [4593, 4596], [4604, 4605], [4622, 4623], [4625, 4627], [4642, 4643], [4742, 4744], [4772, 4773], [4781, 4782], [4804, 4805], [5083, 5085], [5274, 5275], [5303, 5304], [5332, 5333], [5423, 5424], [6201, 6202], [6383, 6384], [6400, 6401], [6605, 6606], [6682, 6684], [6744, 6746], [6864, 6870], [6906, 6907], [7011, 7012], [7047, 7048], [7058, 7059]], "BiLSTM-CRF_ELMo": [[2974, 2975], [3767, 3768], [4049, 4050], [5547, 5548]], "Citation_Intent_Classification": [[3356, 3362], [5444, 5446], [6914, 6916]], "CoNLL_2003__English_": [[3183, 3184], [6820, 6825]], "CoNLL_2012": [[3134, 3139], [5380, 5382]], "Coreference_Resolution": [[2083, 2085], [3059, 3063], [3886, 3887], [5087, 5088], [6482, 6484]], "ELMo": [[66, 70], [171, 175], [314, 321], [334, 342], [355, 357], [525, 527], [567, 569], [598, 599], [609, 613], [620, 621], [652, 653], [835, 837], [839, 844], [885, 889], [902, 906], [1073, 1078], [1127, 1129], [1154, 1160], [1207, 1210], [1225, 1226], [1239, 1242], [1343, 1346], [1381, 1384], [1622, 1623], [1623, 1624], [1670, 1671], [1686, 1687], [1880, 1883], [1896, 1897], [1964, 1965], [1988, 1991], [2014, 2015], [2055, 2057], [2112, 2114], [2151, 2152], [2165, 2169], [2522, 2523], [2540, 2541], [2676, 2679], [2706, 2707], [2785, 2786], [2897, 2898], [3008, 3009], [3086, 3096], [3141, 3142], [3456, 3457], [3504, 3506], [3548, 3550], [3622, 3623], [3882, 3883], [3972, 3973], [4000, 4001], [4028, 4029], [4094, 4095], [4146, 4147], [4851, 4852], [4863, 4864], [4915, 4916], [4919, 4920], [4953, 4957], [4966, 4967], [4979, 4980], [4999, 5000], [5029, 5031], [5153, 5154], [5554, 5557], [5679, 5681], [5716, 5718], [5762, 5763], [6003, 6006], [6037, 6041], [6425, 6428], [6485, 6488], [6490, 6497], [6512, 6513], [6548, 6550], [6599, 6600], [6805, 6806], [7038, 7039]], "ELMo_Ensemble": [], "EM": [[1122, 1125], [2236, 2239], [2243, 2245], [3814, 3816], [4907, 4909], [6435, 6438]], "ESIM": [[1051, 1060], [2850, 2853], [2900, 2902], [4561, 4566], [4567, 4571], [5559, 5562], [5585, 5589], [5765, 5767], [6182, 6185], [6518, 6521], [6655, 6658], [6813, 6817]], "ESIM___ELMo": [], "ESIM___ELMo_Ensemble": [], "F1": [[2758, 2759], [3173, 3174], [3282, 3283], [3945, 3947], [5042, 5043], [6110, 6111], [6350, 6352], [6421, 6423], [6440, 6441], [6469, 6470], [6596, 6597], [6860, 6862], [6879, 6880]], "He_et_al___2017": [], "Lee_et_al___2017": [], "Named_Entity_Recognition__NER_": [[3175, 3178], [6625, 6628]], "Natural_Language_Inference": [[2828, 2832]], "OntoNotes": [[3043, 3046], [3129, 3132]], "Parameters": [], "Question_Answering": [[112, 114], [558, 560], [711, 714], [2612, 2614], [5803, 5805]], "SNLI": [[2063, 2064], [2833, 2837], [4664, 4668]], "SQuAD1_1": [], "SQuAD2_0": [[2620, 2621], [6071, 6073]], "SST-5_Fine-grained_classification": [], "Self_Attention": [[2073, 2077], [2670, 2674], [2869, 2872], [5740, 5742], [5897, 5902], [5910, 5912], [6105, 6106]], "Semantic_Role_Labeling": [[719, 723], [2937, 2940], [6139, 6142], [6454, 6460]], "Sentiment_Analysis": [[118, 120], [561, 563], [3353, 3355]], "_He_et_al___2017____ELMo": [], "_Lee_et_al___2017__ELMo": [], "__Test_Accuracy": [[2712, 2715], [3018, 3022]], "__Train_Accuracy": [[4894, 4897]], "ensemble": [[5791, 5793], [6472, 6476]], "single_model": [[1734, 1736], [1827, 1829], [3308, 3310], [3627, 3629], [3854, 3856], [4902, 4904], [5078, 5080], [5806, 5808], [6144, 6146], [6630, 6632]]}, "coref_non_salient": {"0": [[2269, 2275], [2406, 2412]], "1": [[483, 489], [4368, 4371], [4636, 4638], [4840, 4842]], "10": [[4106, 4110], [5497, 5500]], "100": [[4531, 4535]], "101": [[6774, 6776]], "102": [[5849, 5852], [6672, 6674]], "103": [[5624, 5627]], "104": [[6095, 6096]], "105": [[6272, 6275]], "106": [[4425, 4426]], "107": [[5640, 5642]], "108": [[4008, 4012]], "109": [[3702, 3704]], "11": [[865, 868], [1227, 1228]], "110": [[3667, 3671]], "111": [[6534, 6535]], "112": [[108, 110], [660, 662]], "113": [[6250, 6252]], "114": [[3490, 3492]], "115": [[1081, 1084]], "116": [[2240, 2242]], "117": [[1106, 1110]], "118": [[6158, 6164]], "119": [[1783, 1784]], "12": [[4792, 4794], [5408, 5410]], "120": [[3257, 3258]], "121": [[3261, 3262]], "122": [[4757, 4759]], "123": [[3763, 3764], [4039, 4040], [5280, 5282]], "124": [[1388, 1389]], "125": [[692, 693]], "126": [[4728, 4731]], "127": [[1003, 1008]], "128": [[1300, 1304]], "129": [[690, 691]], "13": [[3252, 3256], [6713, 6715]], "130": [[4417, 4422]], "131": [[1536, 1538], [6188, 6191], [6676, 6678]], "132": [[160, 163]], "133": [[6478, 6479]], "134": [[4631, 4635]], "135": [[4745, 4750]], "136": [[4336, 4337]], "137": [[5271, 5273]], "138": [[1042, 1043]], "139": [[142, 144], [1667, 1669]], "14": [[3990, 3993], [4006, 4007], [4090, 4091]], "140": [[6660, 6662]], "141": [[1152, 1153]], "142": [[1607, 1610]], "143": [[2370, 2373]], "144": [[1148, 1151]], "145": [[729, 732]], "146": [[5837, 5841]], "147": [[4081, 4085]], "148": [[1851, 1854]], "149": [[6267, 6269]], "15": [[1631, 1634], [1840, 1842]], "150": [[1696, 1698]], "151": [[5225, 5227]], "152": [[4718, 4719]], "153": [[5596, 5597]], "154": [[1420, 1422], [2202, 2204], [5610, 5612], [6215, 6217], [6329, 6332]], "155": [[1699, 1701]], "156": [[2323, 2325]], "157": [[1036, 1038]], "158": [[2208, 2211]], "159": [[918, 920]], "16": [[5615, 5616], [6949, 6950]], "160": [[3179, 3183]], "161": [[2935, 2936]], "162": [[3067, 3069]], "163": [[3189, 3192]], "164": [[3248, 3251]], "165": [[686, 689], [754, 756]], "166": [[407, 410]], "167": [[715, 718]], "168": [[2977, 2980]], "169": [[1958, 1961]], "17": [[4656, 4658], [4679, 4681]], "170": [[2088, 2090]], "171": [[3963, 3965]], "172": [[2252, 2257]], "173": [[2565, 2567]], "174": [[3393, 3395]], "175": [[4787, 4789]], "176": [[2224, 2228]], "177": [[1386, 1387]], "178": [[2626, 2632]], "179": [[2309, 2315]], "18": [[1220, 1222], [1873, 1875], [1967, 1969], [2040, 2042]], "180": [[2945, 2948]], "181": [[2303, 2307]], "182": [[1354, 1357]], "183": [[941, 943]], "184": [[1643, 1646]], "185": [[1613, 1619]], "186": [[134, 138]], "187": [[4052, 4054]], "188": [[2480, 2482]], "189": [[852, 853]], "19": [[1933, 1937], [6651, 6653]], "190": [[2387, 2391]], "191": [[4455, 4457]], "192": [[1402, 1405]], "193": [[633, 635]], "194": [[1955, 1956]], "195": [[991, 993]], "196": [[4573, 4576]], "197": [[1807, 1809]], "198": [[2941, 2944]], "199": [[2581, 2584]], "2": [[706, 708], [1330, 1333]], "20": [[1993, 1995], [2020, 2022]], "200": [[3541, 3545]], "201": [[1492, 1495]], "202": [[2276, 2277]], "203": [[1796, 1799]], "204": [[509, 512]], "205": [[3115, 3119]], "206": [[5576, 5581]], "207": [[1262, 1266]], "208": [[2615, 2619]], "209": [[2328, 2330]], "21": [[682, 684], [1183, 1185]], "210": [[459, 464]], "211": [[3313, 3319]], "212": [[5089, 5090]], "213": [[890, 893]], "214": [[436, 438]], "215": [[178, 182]], "216": [[1164, 1166]], "217": [[2183, 2184]], "218": [[347, 352]], "219": [[1044, 1048]], "22": [[736, 740], [1424, 1428], [1944, 1948]], "220": [[2318, 2320]], "221": [[926, 928]], "222": [[1412, 1414]], "223": [[6231, 6235]], "224": [[830, 834]], "225": [[5138, 5144]], "23": [[5288, 5291], [5383, 5385], [5405, 5407]], "24": [[3220, 3225], [3302, 3303], [3321, 3322], [6897, 6898]], "25": [[4113, 4115], [4148, 4149], [5025, 5027]], "26": [[4860, 4862], [4869, 4871]], "27": [[4277, 4279], [4443, 4445]], "28": [[622, 623], [2797, 2798], [3454, 3455], [4845, 4846], [4853, 4854]], "29": [[2229, 2232], [5359, 5361], [5543, 5544]], "3": [[4140, 4143], [4404, 4406], [4696, 4699], [4705, 4707], [4831, 4834]], "30": [[2393, 2398], [2463, 2464], [5314, 5315], [5391, 5392]], "31": [[1572, 1573], [2216, 2217]], "32": [[2, 6], [12, 16], [236, 240], [5212, 5216]], "33": [[5890, 5892], [5921, 5923], [5934, 5936], [5959, 5961], [5981, 5983]], "34": [[605, 607], [3689, 3691], [3786, 3788]], "35": [[4363, 4366], [4838, 4839]], "36": [[5602, 5605], [5947, 5949], [6320, 6322], [6735, 6737]], "37": [[1446, 1448], [1541, 1543], [6704, 6706]], "38": [[6302, 6303]], "39": [[115, 117], [555, 557], [2804, 2808], [5551, 5553]], "4": [[1450, 1452], [1510, 1514]], "40": [[811, 813], [1140, 1142], [2246, 2248], [2468, 2470], [2502, 2504], [3519, 3521], [4824, 4826], [4855, 4857]], "41": [[2854, 2855], [5563, 5564]], "42": [[4509, 4510], [4543, 4544], [4702, 4703], [4723, 4724], [4778, 4779]], "43": [[2695, 2696], [5284, 5285], [5905, 5906], [5975, 5976]], "44": [[280, 283], [1235, 1237], [3233, 3235]], "45": [[5650, 5652], [6297, 6299], [6765, 6767], [7021, 7023]], "46": [[1781, 1783], [6538, 6540]], "47": [[5633, 5635], [5893, 5895], [5924, 5926], [6668, 6670]], "48": [[2684, 2686], [2887, 2889]], "49": [[586, 589], [2554, 2557], [2729, 2732]], "5": [[1398, 1400], [5854, 5856]], "50": [[5989, 5990], [6294, 6295]], "51": [[353, 354], [600, 602], [899, 900], [2802, 2803], [3432, 3433], [6927, 6928], [6981, 6982]], "52": [[6645, 6649]], "53": [[6762, 6763], [7018, 7019]], "54": [[5692, 5695]], "55": [[1750, 1752], [5488, 5489]], "56": [[6941, 6943]], "57": [[5456, 5459]], "58": [[384, 386], [1856, 1858]], "59": [[1713, 1715], [1766, 1768], [1786, 1788], [2146, 2148], [3683, 3685], [3811, 3813], [3825, 3827], [4815, 4817], [5175, 5177], [6524, 6526]], "6": [[422, 424], [3109, 3111]], "60": [[3015, 3016], [6151, 6152], [6429, 6430]], "61": [[3263, 3264], [6642, 6643]], "62": [[6245, 6247], [6701, 6703]], "63": [[4328, 4334]], "64": [[6784, 6786]], "65": [[3097, 3098], [6498, 6499]], "66": [[6793, 6797]], "67": [[2647, 2648], [5816, 5817]], "68": [[1497, 1500], [6229, 6230]], "69": [[3364, 3367], [4670, 4672]], "7": [[3186, 3187], [4673, 4676]], "70": [[2691, 2694]], "71": [[2874, 2877]], "72": [[979, 981], [1812, 1814], [2530, 2532], [4166, 4168], [5159, 5161]], "73": [[5222, 5224]], "74": [[81, 84], [323, 326]], "75": [[1929, 1931], [2364, 2367]], "76": [[625, 627], [6237, 6239]], "77": [[3368, 3373]], "78": [[5844, 5847]], "79": [[4357, 4362], [4393, 4396]], "8": [[2455, 2458], [5305, 5308]], "80": [[6723, 6725]], "81": [[1161, 1163], [1229, 1231]], "82": [[1024, 1026], [6222, 6224]], "83": [[5502, 5505]], "84": [[6986, 6989]], "85": [[3598, 3600], [3706, 3708]], "86": [[1391, 1396], [1916, 1921], [5263, 5267]], "87": [[6720, 6721]], "88": [[4713, 4717]], "89": [[6936, 6938]], "9": [[271, 274], [547, 553]], "90": [[4751, 4756]], "91": [[6946, 6948]], "92": [[1456, 1458], [1487, 1489]], "93": [[799, 801], [1267, 1269]], "94": [[4650, 4652]], "95": [[6049, 6052]], "96": [[964, 967], [3515, 3518]], "97": [[5629, 5632]], "98": [[6225, 6227]], "99": [[6167, 6169]]}, "doc_id": "3febb2bed8865945e7fddc99efd791887bb7e14f", "method_subrelations": {"BCN_ELMo": [[[0, 3], "BCN"], [[4, 8], "ELMo"]], "BiDAF___Self_Attention___ELMo__ensemble_": [[[0, 5], "BiDAF"], [[8, 22], "Self_Attention"], [[25, 29], "ELMo"], [[31, 39], "ensemble"]], "BiDAF___Self_Attention___ELMo__single_model_": [[[0, 5], "BiDAF"], [[8, 22], "Self_Attention"], [[25, 29], "ELMo"], [[31, 43], "single_model"]], "BiLSTM-Attention___ELMo": [[[0, 16], "BiLSTM-Attention"], [[19, 23], "ELMo"]], "BiLSTM-CRF_ELMo": [[[0, 10], "BiLSTM-CRF"], [[11, 15], "ELMo"]], "ESIM___ELMo": [[[0, 4], "ESIM"], [[7, 11], "ELMo"]], "ESIM___ELMo_Ensemble": [[[0, 4], "ESIM"], [[7, 20], "ELMo_Ensemble"]], "_He_et_al___2017____ELMo": [[[1, 16], "He_et_al___2017"], [[20, 24], "ELMo"]], "_Lee_et_al___2017__ELMo": [[[1, 17], "Lee_et_al___2017"], [[19, 23], "ELMo"]]}, "n_ary_relations": [{"Material": "ACL-ARC", "Method": "BiLSTM-Attention___ELMo", "Metric": "F1", "Task": "Citation_Intent_Classification", "score": "54.6"}, {"Material": "CoNLL_2003__English_", "Method": "BiLSTM-CRF_ELMo", "Metric": "F1", "Task": "Named_Entity_Recognition__NER_", "score": "92.22"}, {"Material": "CoNLL_2012", "Method": "_Lee_et_al___2017__ELMo", "Metric": "Avg_F1", "Task": "Coreference_Resolution", "score": "70.4"}, {"Material": "OntoNotes", "Method": "_He_et_al___2017____ELMo", "Metric": "F1", "Task": "Semantic_Role_Labeling", "score": "84.6"}, {"Material": "SNLI", "Method": "ESIM___ELMo", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "88.7"}, {"Material": "SNLI", "Method": "ESIM___ELMo_Ensemble", "Metric": "__Test_Accuracy", "Task": "Natural_Language_Inference", "score": "89.3"}, {"Material": "SNLI", "Method": "ESIM___ELMo", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "91.6"}, {"Material": "SNLI", "Method": "ESIM___ELMo_Ensemble", "Metric": "__Train_Accuracy", "Task": "Natural_Language_Inference", "score": "92.1"}, {"Material": "SNLI", "Method": "ESIM___ELMo", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "8.0m"}, {"Material": "SNLI", "Method": "ESIM___ELMo_Ensemble", "Metric": "Parameters", "Task": "Natural_Language_Inference", "score": "40m"}, {"Material": "SQuAD1_1", "Method": "BiDAF___Self_Attention___ELMo__ensemble_", "Metric": "EM", "Task": "Question_Answering", "score": "81.003"}, {"Material": "SQuAD1_1", "Method": "BiDAF___Self_Attention___ELMo__single_model_", "Metric": "EM", "Task": "Question_Answering", "score": "78.580"}, {"Material": "SQuAD1_1", "Method": "BiDAF___Self_Attention___ELMo__ensemble_", "Metric": "F1", "Task": "Question_Answering", "score": "87.432"}, {"Material": "SQuAD1_1", "Method": "BiDAF___Self_Attention___ELMo__single_model_", "Metric": "F1", "Task": "Question_Answering", "score": "85.833"}, {"Material": "SQuAD2_0", "Method": "BiDAF___Self_Attention___ELMo__single_model_", "Metric": "EM", "Task": "Question_Answering", "score": "63.372"}, {"Material": "SQuAD2_0", "Method": "BiDAF___Self_Attention___ELMo__single_model_", "Metric": "F1", "Task": "Question_Answering", "score": "66.251"}, {"Material": "SST-5_Fine-grained_classification", "Method": "BCN_ELMo", "Metric": "Accuracy", "Task": "Sentiment_Analysis", "score": "54.7"}], "ner": [[2, 6, "Method"], [12, 16, "Method"], [66, 70, "Method"], [71, 73, "Method"], [81, 84, "Material"], [108, 110, "Task"], [112, 114, "Task"], [115, 117, "Task"], [118, 120, "Task"], [134, 138, "Method"], [142, 144, "Method"], [160, 163, "Method"], [171, 175, "Method"], [178, 182, "Task"], [236, 240, "Method"], [271, 274, "Task"], [280, 283, "Method"], [307, 309, "Method"], [314, 321, "Method"], [323, 326, "Material"], [334, 342, "Method"], [347, 352, "Task"], [353, 354, "Method"], [355, 357, "Method"], [376, 377, "Method"], [384, 386, "Method"], [407, 410, "Method"], [422, 424, "Method"], [436, 438, "Method"], [459, 464, "Task"], [483, 489, "Task"], [509, 512, "Task"], [525, 527, "Method"], [547, 553, "Task"], [555, 557, "Task"], [558, 560, "Task"], [561, 563, "Task"], [567, 569, "Method"], [586, 589, "Metric"], [598, 599, "Method"], [600, 602, "Method"], [605, 607, "Method"], [609, 613, "Method"], [620, 621, "Method"], [622, 623, "Method"], [625, 627, "Method"], [633, 635, "Method"], [637, 638, "Method"], [652, 653, "Method"], [660, 662, "Task"], [682, 684, "Material"], [686, 689, "Method"], [690, 691, "Material"], [692, 693, "Material"], [706, 708, "Method"], [711, 714, "Task"], [715, 718, "Task"], [719, 723, "Task"], [729, 732, "Task"], [736, 740, "Method"], [754, 756, "Method"], [799, 801, "Method"], [811, 813, "Task"], [830, 834, "Task"], [835, 837, "Method"], [839, 844, "Method"], [845, 846, "Method"], [850, 851, "Method"], [852, 853, "Method"], [865, 868, "Task"], [885, 889, "Method"], [890, 893, "Method"], [899, 900, "Method"], [902, 906, "Method"], [918, 920, "Method"], [926, 928, "Material"], [941, 943, "Material"], [947, 948, "Method"], [964, 967, "Method"], [979, 981, "Task"], [991, 993, "Method"], [1003, 1008, "Task"], [1024, 1026, "Method"], [1036, 1038, "Task"], [1042, 1043, "Method"], [1044, 1048, "Method"], [1051, 1060, "Method"], [1073, 1078, "Method"], [1081, 1084, "Task"], [1095, 1096, "Method"], [1106, 1110, "Task"], [1122, 1125, "Metric"], [1127, 1129, "Method"], [1140, 1142, "Task"], [1148, 1151, "Method"], [1152, 1153, "Method"], [1154, 1160, "Method"], [1161, 1163, "Method"], [1164, 1166, "Method"], [1181, 1182, "Method"], [1183, 1185, "Material"], [1207, 1210, "Method"], [1220, 1222, "Method"], [1225, 1226, "Method"], [1227, 1228, "Task"], [1229, 1231, "Method"], [1235, 1237, "Method"], [1239, 1242, "Method"], [1262, 1266, "Method"], [1267, 1269, "Method"], [1300, 1304, "Task"], [1307, 1308, "Method"], [1330, 1333, "Method"], [1343, 1346, "Method"], [1354, 1357, "Method"], [1381, 1384, "Method"], [1386, 1387, "Method"], [1388, 1389, "Method"], [1391, 1396, "Method"], [1398, 1400, "Method"], [1402, 1405, "Method"], [1412, 1414, "Method"], [1420, 1422, "Method"], [1424, 1428, "Method"], [1446, 1448, "Method"], [1450, 1452, "Method"], [1456, 1458, "Method"], [1487, 1489, "Method"], [1492, 1495, "Method"], [1497, 1500, "Method"], [1506, 1507, "Method"], [1510, 1514, "Method"], [1536, 1538, "Method"], [1541, 1543, "Method"], [1557, 1558, "Method"], [1572, 1573, "Task"], [1607, 1610, "Task"], [1613, 1619, "Method"], [1622, 1623, "Method"], [1623, 1624, "Method"], [1631, 1634, "Method"], [1636, 1637, "Method"], [1643, 1646, "Method"], [1660, 1662, "Method"], [1667, 1669, "Method"], [1670, 1671, "Method"], [1686, 1687, "Method"], [1696, 1698, "Method"], [1699, 1701, "Material"], [1713, 1715, "Method"], [1734, 1736, "Method"], [1750, 1752, "Task"], [1766, 1768, "Method"], [1781, 1783, "Method"], [1783, 1784, "Method"], [1786, 1788, "Method"], [1794, 1795, "Method"], [1796, 1799, "Task"], [1804, 1805, "Method"], [1807, 1809, "Method"], [1812, 1814, "Task"], [1823, 1824, "Method"], [1827, 1829, "Method"], [1834, 1835, "Method"], [1840, 1842, "Method"], [1851, 1854, "Method"], [1856, 1858, "Method"], [1873, 1875, "Method"], [1877, 1878, "Method"], [1880, 1883, "Method"], [1896, 1897, "Method"], [1916, 1921, "Method"], [1929, 1931, "Method"], [1933, 1937, "Method"], [1944, 1948, "Method"], [1952, 1954, "Method"], [1955, 1956, "Method"], [1958, 1961, "Method"], [1964, 1965, "Method"], [1967, 1969, "Method"], [1977, 1978, "Method"], [1988, 1991, "Method"], [1993, 1995, "Method"], [2014, 2015, "Method"], [2020, 2022, "Method"], [2040, 2042, "Method"], [2055, 2057, "Method"], [2063, 2064, "Material"], [2073, 2077, "Method"], [2079, 2080, "Method"], [2083, 2085, "Task"], [2088, 2090, "Method"], [2096, 2097, "Method"], [2112, 2114, "Method"], [2146, 2148, "Method"], [2151, 2152, "Method"], [2165, 2169, "Method"], [2173, 2174, "Method"], [2183, 2184, "Material"], [2202, 2204, "Method"], [2208, 2211, "Task"], [2216, 2217, "Task"], [2222, 2223, "Method"], [2224, 2228, "Method"], [2229, 2232, "Task"], [2236, 2239, "Metric"], [2240, 2242, "Metric"], [2243, 2245, "Metric"], [2246, 2248, "Task"], [2252, 2257, "Method"], [2269, 2275, "Method"], [2276, 2277, "Method"], [2282, 2284, "Method"], [2303, 2307, "Method"], [2309, 2315, "Method"], [2318, 2320, "Method"], [2323, 2325, "Method"], [2328, 2330, "Method"], [2336, 2337, "Method"], [2364, 2367, "Method"], [2370, 2373, "Method"], [2387, 2391, "Material"], [2393, 2398, "Metric"], [2406, 2412, "Method"], [2438, 2439, "Method"], [2453, 2454, "Method"], [2455, 2458, "Material"], [2463, 2464, "Metric"], [2468, 2470, "Task"], [2480, 2482, "Task"], [2484, 2485, "Method"], [2499, 2500, "Method"], [2502, 2504, "Task"], [2522, 2523, "Method"], [2530, 2532, "Task"], [2540, 2541, "Method"], [2554, 2557, "Metric"], [2565, 2567, "Method"], [2581, 2584, "Task"], [2612, 2614, "Task"], [2615, 2619, "Material"], [2620, 2621, "Material"], [2626, 2632, "Material"], [2647, 2648, "Method"], [2654, 2658, "Method"], [2670, 2674, "Method"], [2676, 2679, "Method"], [2684, 2686, "Method"], [2689, 2690, "Method"], [2691, 2694, "Method"], [2695, 2696, "Method"], [2706, 2707, "Method"], [2712, 2715, "Metric"], [2729, 2732, "Metric"], [2758, 2759, "Metric"], [2785, 2786, "Method"], [2797, 2798, "Method"], [2802, 2803, "Method"], [2804, 2808, "Task"], [2828, 2832, "Task"], [2833, 2837, "Material"], [2850, 2853, "Method"], [2854, 2855, "Method"], [2858, 2859, "Method"], [2869, 2872, "Method"], [2874, 2877, "Method"], [2879, 2883, "Method"], [2887, 2889, "Method"], [2897, 2898, "Method"], [2900, 2902, "Method"], [2903, 2904, "Metric"], [2922, 2923, "Metric"], [2935, 2936, "Method"], [2937, 2940, "Task"], [2941, 2944, "Method"], [2945, 2948, "Method"], [2974, 2975, "Method"], [2977, 2980, "Task"], [2983, 2988, "Method"], [3008, 3009, "Method"], [3015, 3016, "Method"], [3018, 3022, "Metric"], [3043, 3046, "Material"], [3059, 3063, "Task"], [3067, 3069, "Task"], [3086, 3096, "Method"], [3097, 3098, "Method"], [3102, 3106, "Method"], [3109, 3111, "Method"], [3115, 3119, "Method"], [3129, 3132, "Material"], [3134, 3139, "Material"], [3141, 3142, "Method"], [3144, 3146, "Metric"], [3173, 3174, "Metric"], [3175, 3178, "Task"], [3179, 3183, "Task"], [3183, 3184, "Material"], [3186, 3187, "Material"], [3189, 3192, "Material"], [3220, 3225, "Method"], [3233, 3235, "Method"], [3237, 3242, "Method"], [3244, 3246, "Method"], [3248, 3251, "Method"], [3252, 3256, "Method"], [3257, 3258, "Method"], [3261, 3262, "Method"], [3263, 3264, "Method"], [3274, 3279, "Method"], [3282, 3283, "Metric"], [3302, 3303, "Method"], [3308, 3310, "Method"], [3313, 3319, "Method"], [3321, 3322, "Method"], [3353, 3355, "Task"], [3356, 3362, "Task"], [3364, 3367, "Material"], [3368, 3373, "Material"], [3393, 3395, "Material"], [3425, 3428, "Method"], [3429, 3430, "Method"], [3432, 3433, "Method"], [3454, 3455, "Method"], [3456, 3457, "Method"], [3459, 3461, "Method"], [3466, 3468, "Metric"], [3490, 3492, "Task"], [3504, 3506, "Method"], [3515, 3518, "Method"], [3519, 3521, "Task"], [3541, 3545, "Method"], [3548, 3550, "Method"], [3570, 3571, "Method"], [3598, 3600, "Method"], [3606, 3607, "Method"], [3622, 3623, "Method"], [3627, 3629, "Method"], [3667, 3671, "Method"], [3683, 3685, "Method"], [3689, 3691, "Method"], [3702, 3704, "Method"], [3706, 3708, "Method"], [3763, 3764, "Method"], [3765, 3766, "Method"], [3767, 3768, "Method"], [3786, 3788, "Method"], [3811, 3813, "Method"], [3814, 3816, "Metric"], [3825, 3827, "Method"], [3854, 3856, "Method"], [3882, 3883, "Method"], [3886, 3887, "Task"], [3922, 3923, "Method"], [3931, 3933, "Metric"], [3945, 3947, "Metric"], [3963, 3965, "Method"], [3972, 3973, "Method"], [3990, 3993, "Method"], [4000, 4001, "Method"], [4006, 4007, "Method"], [4008, 4012, "Method"], [4028, 4029, "Method"], [4037, 4038, "Method"], [4039, 4040, "Method"], [4049, 4050, "Method"], [4052, 4054, "Metric"], [4081, 4085, "Method"], [4090, 4091, "Method"], [4094, 4095, "Method"], [4106, 4110, "Method"], [4113, 4115, "Task"], [4130, 4131, "Method"], [4140, 4143, "Method"], [4146, 4147, "Method"], [4148, 4149, "Task"], [4156, 4160, "Method"], [4166, 4168, "Task"], [4179, 4180, "Method"], [4277, 4279, "Material"], [4285, 4289, "Method"], [4303, 4304, "Method"], [4328, 4334, "Method"], [4336, 4337, "Method"], [4345, 4346, "Method"], [4357, 4362, "Task"], [4363, 4366, "Task"], [4368, 4371, "Task"], [4393, 4396, "Task"], [4404, 4406, "Method"], [4417, 4422, "Method"], [4425, 4426, "Method"], [4435, 4436, "Method"], [4443, 4445, "Material"], [4455, 4457, "Method"], [4469, 4470, "Method"], [4509, 4510, "Method"], [4531, 4535, "Method"], [4543, 4544, "Method"], [4561, 4566, "Method"], [4567, 4571, "Method"], [4573, 4576, "Method"], [4593, 4596, "Method"], [4604, 4605, "Method"], [4622, 4623, "Method"], [4625, 4627, "Method"], [4631, 4635, "Method"], [4636, 4638, "Task"], [4642, 4643, "Method"], [4650, 4652, "Method"], [4656, 4658, "Method"], [4664, 4668, "Material"], [4670, 4672, "Material"], [4673, 4676, "Material"], [4679, 4681, "Method"], [4696, 4699, "Method"], [4702, 4703, "Method"], [4705, 4707, "Method"], [4713, 4717, "Method"], [4718, 4719, "Method"], [4723, 4724, "Method"], [4725, 4726, "Metric"], [4728, 4731, "Method"], [4742, 4744, "Method"], [4745, 4750, "Task"], [4751, 4756, "Task"], [4757, 4759, "Task"], [4760, 4764, "Metric"], [4772, 4773, "Method"], [4778, 4779, "Method"], [4781, 4782, "Method"], [4784, 4785, "Metric"], [4787, 4789, "Method"], [4792, 4794, "Task"], [4804, 4805, "Method"], [4815, 4817, "Method"], [4824, 4826, "Task"], [4831, 4834, "Method"], [4838, 4839, "Task"], [4840, 4842, "Task"], [4845, 4846, "Method"], [4851, 4852, "Method"], [4853, 4854, "Method"], [4855, 4857, "Task"], [4860, 4862, "Metric"], [4863, 4864, "Method"], [4869, 4871, "Metric"], [4894, 4897, "Metric"], [4902, 4904, "Method"], [4907, 4909, "Metric"], [4915, 4916, "Method"], [4919, 4920, "Method"], [4953, 4957, "Method"], [4966, 4967, "Method"], [4979, 4980, "Method"], [4999, 5000, "Method"], [5025, 5027, "Task"], [5029, 5031, "Method"], [5042, 5043, "Metric"], [5078, 5080, "Method"], [5083, 5085, "Method"], [5087, 5088, "Task"], [5089, 5090, "Task"], [5138, 5144, "Task"], [5145, 5146, "Method"], [5153, 5154, "Method"], [5159, 5161, "Task"], [5175, 5177, "Method"], [5212, 5216, "Method"], [5222, 5224, "Method"], [5225, 5227, "Method"], [5263, 5267, "Method"], [5271, 5273, "Method"], [5274, 5275, "Method"], [5280, 5282, "Method"], [5284, 5285, "Method"], [5288, 5291, "Task"], [5303, 5304, "Method"], [5305, 5308, "Material"], [5314, 5315, "Metric"], [5332, 5333, "Method"], [5359, 5361, "Task"], [5380, 5382, "Material"], [5383, 5385, "Task"], [5391, 5392, "Metric"], [5400, 5401, "Method"], [5405, 5407, "Task"], [5408, 5410, "Task"], [5418, 5419, "Method"], [5423, 5424, "Method"], [5425, 5427, "Metric"], [5444, 5446, "Task"], [5448, 5449, "Metric"], [5456, 5459, "Method"], [5488, 5489, "Task"], [5497, 5500, "Method"], [5502, 5505, "Method"], [5541, 5542, "Method"], [5543, 5544, "Task"], [5547, 5548, "Method"], [5551, 5553, "Task"], [5554, 5557, "Method"], [5559, 5562, "Method"], [5563, 5564, "Method"], [5576, 5581, "Method"], [5585, 5589, "Method"], [5596, 5597, "Task"], [5602, 5605, "Method"], [5610, 5612, "Method"], [5615, 5616, "Method"], [5624, 5627, "Method"], [5629, 5632, "Method"], [5633, 5635, "Method"], [5640, 5642, "Method"], [5650, 5652, "Metric"], [5659, 5660, "Metric"], [5679, 5681, "Method"], [5692, 5695, "Method"], [5716, 5718, "Method"], [5740, 5742, "Method"], [5750, 5751, "Metric"], [5762, 5763, "Method"], [5765, 5767, "Method"], [5768, 5769, "Metric"], [5791, 5793, "Method"], [5796, 5797, "Metric"], [5803, 5805, "Task"], [5806, 5808, "Method"], [5816, 5817, "Method"], [5837, 5841, "Method"], [5844, 5847, "Method"], [5849, 5852, "Method"], [5854, 5856, "Method"], [5864, 5869, "Method"], [5873, 5878, "Method"], [5879, 5880, "Method"], [5890, 5892, "Method"], [5893, 5895, "Method"], [5897, 5902, "Method"], [5905, 5906, "Method"], [5910, 5912, "Method"], [5921, 5923, "Method"], [5924, 5926, "Method"], [5934, 5936, "Method"], [5947, 5949, "Method"], [5959, 5961, "Method"], [5975, 5976, "Method"], [5981, 5983, "Method"], [5989, 5990, "Method"], [6003, 6006, "Method"], [6037, 6041, "Method"], [6049, 6052, "Method"], [6071, 6073, "Material"], [6095, 6096, "Task"], [6105, 6106, "Method"], [6110, 6111, "Metric"], [6139, 6142, "Task"], [6144, 6146, "Method"], [6151, 6152, "Method"], [6158, 6164, "Method"], [6167, 6169, "Method"], [6182, 6185, "Method"], [6188, 6191, "Method"], [6201, 6202, "Method"], [6215, 6217, "Method"], [6222, 6224, "Method"], [6225, 6227, "Method"], [6229, 6230, "Method"], [6231, 6235, "Method"], [6237, 6239, "Method"], [6245, 6247, "Method"], [6250, 6252, "Method"], [6267, 6269, "Method"], [6272, 6275, "Method"], [6294, 6295, "Method"], [6297, 6299, "Metric"], [6302, 6303, "Method"], [6310, 6312, "Method"], [6320, 6322, "Method"], [6329, 6332, "Method"], [6350, 6352, "Metric"], [6383, 6384, "Method"], [6400, 6401, "Method"], [6421, 6423, "Metric"], [6425, 6428, "Method"], [6429, 6430, "Method"], [6435, 6438, "Metric"], [6440, 6441, "Metric"], [6454, 6460, "Task"], [6469, 6470, "Metric"], [6472, 6476, "Method"], [6478, 6479, "Method"], [6482, 6484, "Task"], [6485, 6488, "Method"], [6490, 6497, "Method"], [6498, 6499, "Method"], [6512, 6513, "Method"], [6518, 6521, "Method"], [6524, 6526, "Method"], [6534, 6535, "Method"], [6538, 6540, "Method"], [6548, 6550, "Method"], [6580, 6582, "Metric"], [6596, 6597, "Metric"], [6599, 6600, "Method"], [6605, 6606, "Method"], [6625, 6628, "Task"], [6630, 6632, "Method"], [6642, 6643, "Method"], [6645, 6649, "Method"], [6651, 6653, "Method"], [6655, 6658, "Method"], [6660, 6662, "Method"], [6668, 6670, "Method"], [6672, 6674, "Method"], [6676, 6678, "Method"], [6682, 6684, "Method"], [6701, 6703, "Method"], [6704, 6706, "Method"], [6713, 6715, "Method"], [6720, 6721, "Task"], [6723, 6725, "Method"], [6735, 6737, "Method"], [6744, 6746, "Method"], [6762, 6763, "Method"], [6765, 6767, "Metric"], [6774, 6776, "Method"], [6784, 6786, "Method"], [6793, 6797, "Metric"], [6805, 6806, "Method"], [6813, 6817, "Method"], [6820, 6825, "Material"], [6860, 6862, "Metric"], [6864, 6870, "Method"], [6879, 6880, "Metric"], [6897, 6898, "Method"], [6906, 6907, "Method"], [6914, 6916, "Task"], [6921, 6925, "Method"], [6927, 6928, "Method"], [6936, 6938, "Method"], [6941, 6943, "Method"], [6946, 6948, "Method"], [6949, 6950, "Method"], [6952, 6954, "Method"], [6956, 6961, "Method"], [6964, 6966, "Metric"], [6981, 6982, "Method"], [6986, 6989, "Task"], [7011, 7012, "Method"], [7018, 7019, "Method"], [7021, 7023, "Metric"], [7038, 7039, "Method"], [7047, 7048, "Method"], [7058, 7059, "Method"]], "sections": [[0, 154], [154, 663], [663, 1223], [1223, 1341], [1341, 1620], [1620, 1791], [1791, 2160], [2160, 2511], [2511, 3483], [3483, 3665], [3665, 3967], [3967, 4132], [4132, 4858], [4858, 5055], [5055, 5125], [5125, 5203], [5203, 5206], [5206, 5286], [5286, 5462], [5462, 5549], [5549, 5801], [5801, 6137], [6137, 6480], [6480, 6623], [6623, 6912], [6912, 7065]], "sentences": [[0, 6], [6, 54], [54, 85], [85, 121], [121, 154], [154, 157], [157, 163], [163, 176], [176, 186], [186, 226], [226, 275], [275, 301], [301, 327], [327, 343], [343, 378], [378, 411], [411, 425], [425, 491], [491, 521], [521, 533], [533, 564], [564, 590], [590, 614], [614, 639], [639, 663], [663, 667], [667, 724], [724, 744], [744, 767], [767, 771], [771, 784], [784, 788], [788, 822], [822, 835], [835, 862], [862, 897], [897, 907], [907, 929], [929, 958], [958, 982], [982, 999], [999, 1049], [1049, 1088], [1088, 1098], [1098, 1111], [1111, 1152], [1152, 1175], [1175, 1223], [1223, 1231], [1231, 1256], [1256, 1272], [1272, 1289], [1289, 1294], [1294, 1317], [1317, 1336], [1336, 1341], [1341, 1346], [1346, 1373], [1373, 1415], [1415, 1430], [1430, 1449], [1449, 1505], [1505, 1515], [1515, 1562], [1562, 1591], [1591, 1620], [1620, 1623], [1623, 1638], [1638, 1663], [1663, 1681], [1681, 1702], [1702, 1724], [1724, 1743], [1743, 1760], [1760, 1783], [1783, 1791], [1791, 1799], [1799, 1830], [1830, 1846], [1846, 1866], [1866, 1879], [1879, 1904], [1904, 1938], [1938, 1962], [1962, 1996], [1996, 2035], [2035, 2058], [2058, 2068], [2068, 2098], [2098, 2129], [2129, 2149], [2149, 2160], [2160, 2169], [2169, 2205], [2205, 2233], [2233, 2278], [2278, 2302], [2302, 2331], [2331, 2360], [2360, 2380], [2380, 2413], [2413, 2434], [2434, 2446], [2446, 2472], [2472, 2486], [2486, 2505], [2505, 2511], [2511, 2514], [2514, 2533], [2533, 2568], [2568, 2585], [2585, 2612], [2612, 2622], [2622, 2644], [2644, 2663], [2663, 2667], [2667, 2698], [2698, 2704], [2704, 2753], [2753, 2762], [2762, 2779], [2779, 2804], [2804, 2827], [2827, 2846], [2846, 2894], [2894, 2915], [2915, 2937], [2937, 2940], [2940, 2965], [2965, 2972], [2972, 2998], [2998, 3059], [3059, 3081], [3081, 3099], [3099, 3124], [3124, 3175], [3175, 3178], [3178, 3183], [3183, 3208], [3208, 3220], [3220, 3265], [3265, 3288], [3288, 3329], [3329, 3334], [3334, 3353], [3353, 3355], [3355, 3396], [3396, 3420], [3420, 3453], [3453, 3476], [3476, 3482], [3482, 3483], [3483, 3486], [3486, 3507], [3507, 3509], [3509, 3556], [3556, 3558], [3558, 3601], [3601, 3614], [3614, 3632], [3632, 3643], [3643, 3660], [3660, 3665], [3665, 3671], [3671, 3686], [3686, 3713], [3713, 3755], [3755, 3769], [3769, 3798], [3798, 3823], [3823, 3842], [3842, 3874], [3874, 3906], [3906, 3921], [3921, 3945], [3945, 3967], [3967, 3974], [3974, 3994], [3994, 4019], [4019, 4071], [4071, 4111], [4111, 4132], [4132, 4144], [4144, 4176], [4176, 4191], [4191, 4201], [4201, 4219], [4219, 4264], [4264, 4298], [4298, 4321], [4321, 4338], [4338, 4372], [4372, 4393], [4393, 4427], [4427, 4461], [4461, 4504], [4504, 4528], [4528, 4549], [4549, 4592], [4592, 4636], [4636, 4677], [4677, 4700], [4700, 4720], [4720, 4760], [4760, 4790], [4790, 4827], [4827, 4858], [4858, 4862], [4862, 4898], [4898, 4917], [4917, 4950], [4950, 4968], [4968, 4997], [4997, 5023], [5023, 5055], [5055, 5061], [5061, 5072], [5072, 5086], [5086, 5108], [5108, 5125], [5125, 5128], [5128, 5162], [5162, 5203], [5203, 5206], [5206, 5216], [5216, 5248], [5248, 5286], [5286, 5291], [5291, 5296], [5296, 5316], [5316, 5349], [5349, 5362], [5362, 5376], [5376, 5402], [5402, 5414], [5414, 5441], [5441, 5462], [5462, 5472], [5472, 5477], [5477, 5506], [5506, 5519], [5519, 5523], [5523, 5549], [5549, 5553], [5553, 5565], [5565, 5595], [5595, 5616], [5616, 5628], [5628, 5636], [5636, 5671], [5671, 5677], [5677, 5707], [5707, 5743], [5743, 5759], [5759, 5801], [5801, 5805], [5805, 5818], [5818, 5857], [5857, 5881], [5881, 5927], [5927, 5947], [5947, 5967], [5967, 5984], [5984, 5997], [5997, 6022], [6022, 6032], [6032, 6061], [6061, 6085], [6085, 6118], [6118, 6137], [6137, 6142], [6142, 6153], [6153, 6186], [6186, 6198], [6198, 6221], [6221, 6227], [6227, 6236], [6236, 6261], [6261, 6280], [6280, 6304], [6304, 6320], [6320, 6333], [6333, 6342], [6342, 6363], [6363, 6374], [6374, 6390], [6390, 6414], [6414, 6434], [6434, 6480], [6480, 6484], [6484, 6508], [6508, 6541], [6541, 6551], [6551, 6563], [6563, 6598], [6598, 6623], [6623, 6628], [6628, 6642], [6642, 6650], [6650, 6675], [6675, 6707], [6707, 6735], [6735, 6747], [6747, 6770], [6770, 6782], [6782, 6805], [6805, 6818], [6818, 6853], [6853, 6874], [6874, 6894], [6894, 6912], [6912, 6916], [6916, 6951], [6951, 6983], [6983, 7002], [7002, 7026], [7026, 7065]], "words": ["document", ":", "Deep", "contextualized", "word", "representations", "We", "introduce", "a", "new", "type", "of", "deep", "contextualized", "word", "representation", "that", "models", "both", "(", "1", ")", "complex", "characteristics", "of", "word", "use", "(", "e.g.", ",", "syntax", "and", "semantics", ")", ",", "and", "(", "2", ")", "how", "these", "uses", "vary", "across", "linguistic", "contexts", "(", "i.e.", ",", "to", "model", "polysemy", ")", ".", "Our", "word", "vectors", "are", "learned", "functions", "of", "the", "internal", "states", "of", "a", "deep", "bidirectional", "language", "model", "(", "biLM", ")", ",", "which", "is", "pre", "-", "trained", "on", "a", "large", "text", "corpus", ".", "We", "show", "that", "these", "representations", "can", "be", "easily", "added", "to", "existing", "models", "and", "significantly", "improve", "the", "state", "of", "the", "art", "across", "six", "challenging", "NLP", "problems", ",", "including", "question", "answering", ",", "textual", "entailment", "and", "sentiment", "analysis", ".", "We", "also", "present", "an", "analysis", "showing", "that", "exposing", "the", "deep", "internals", "of", "the", "pre", "-", "trained", "network", "is", "crucial", ",", "allowing", "downstream", "models", "to", "mix", "different", "types", "of", "semi", "-", "supervision", "signals", ".", "section", ":", "Introduction", "Pre", "-", "trained", "word", "representations", "word2vec", ",", "Pennington2014GloveGV", "are", "a", "key", "component", "in", "many", "neural", "language", "understanding", "models", ".", "However", ",", "learning", "high", "quality", "representations", "can", "be", "challenging", ".", "They", "should", "ideally", "model", "both", "(", "1", ")", "complex", "characteristics", "of", "word", "use", "(", "e.g.", ",", "syntax", "and", "semantics", ")", ",", "and", "(", "2", ")", "how", "these", "uses", "vary", "across", "linguistic", "contexts", "(", "i.e.", ",", "to", "model", "polysemy", ")", ".", "In", "this", "paper", ",", "we", "introduce", "a", "new", "type", "of", "deep", "contextualized", "word", "representation", "that", "directly", "addresses", "both", "challenges", ",", "can", "be", "easily", "integrated", "into", "existing", "models", ",", "and", "significantly", "improves", "the", "state", "of", "the", "art", "in", "every", "considered", "case", "across", "a", "range", "of", "challenging", "language", "understanding", "problems", ".", "Our", "representations", "differ", "from", "traditional", "word", "type", "embeddings", "in", "that", "each", "token", "is", "assigned", "a", "representation", "that", "is", "a", "function", "of", "the", "entire", "input", "sentence", ".", "We", "use", "vectors", "derived", "from", "a", "bidirectional", "LSTM", "that", "is", "trained", "with", "a", "coupled", "language", "model", "(", "LM", ")", "objective", "on", "a", "large", "text", "corpus", ".", "For", "this", "reason", ",", "we", "call", "them", "ELMo", "(", "Embeddings", "from", "Language", "Models", ")", "representations", ".", "Unlike", "previous", "approaches", "for", "learning", "contextualized", "word", "vectors", "Peters2017SemisupervisedST", ",", "McCann2017LearnedIT", ",", "ELMo", "representations", "are", "deep", ",", "in", "the", "sense", "that", "they", "are", "a", "function", "of", "all", "of", "the", "internal", "layers", "of", "the", "biLM", ".", "More", "specifically", ",", "we", "learn", "a", "linear", "combination", "of", "the", "vectors", "stacked", "above", "each", "input", "word", "for", "each", "end", "task", ",", "which", "markedly", "improves", "performance", "over", "just", "using", "the", "top", "LSTM", "layer", ".", "Combining", "the", "internal", "states", "in", "this", "manner", "allows", "for", "very", "rich", "word", "representations", ".", "Using", "intrinsic", "evaluations", ",", "we", "show", "that", "the", "higher", "-", "level", "LSTM", "states", "capture", "context", "-", "dependent", "aspects", "of", "word", "meaning", "(", "e.g.", ",", "they", "can", "be", "used", "without", "modification", "to", "perform", "well", "on", "supervised", "word", "sense", "disambiguation", "tasks", ")", "while", "lower", "-", "level", "states", "model", "aspects", "of", "syntax", "(", "e.g.", ",", "they", "can", "be", "used", "to", "do", "part", "-", "of", "-", "speech", "tagging", ")", ".", "Simultaneously", "exposing", "all", "of", "these", "signals", "is", "highly", "beneficial", ",", "allowing", "the", "learned", "models", "select", "the", "types", "of", "semi", "-", "supervision", "that", "are", "most", "useful", "for", "each", "end", "task", ".", "Extensive", "experiments", "demonstrate", "that", "ELMo", "representations", "work", "extremely", "well", "in", "practice", ".", "We", "first", "show", "that", "they", "can", "be", "easily", "added", "to", "existing", "models", "for", "six", "diverse", "and", "challenging", "language", "understanding", "problems", ",", "including", "textual", "entailment", ",", "question", "answering", "and", "sentiment", "analysis", ".", "The", "addition", "of", "ELMo", "representations", "alone", "significantly", "improves", "the", "state", "of", "the", "art", "in", "every", "case", ",", "including", "up", "to", "20", "%", "relative", "error", "reductions", ".", "For", "tasks", "where", "direct", "comparisons", "are", "possible", ",", "ELMo", "outperforms", "CoVe", "McCann2017LearnedIT", ",", "which", "computes", "contextualized", "representations", "using", "a", "neural", "machine", "translation", "encoder", ".", "Finally", ",", "an", "analysis", "of", "both", "ELMo", "and", "CoVe", "reveals", "that", "deep", "representations", "outperform", "those", "derived", "from", "just", "the", "top", "layer", "of", "an", "LSTM", ".", "Our", "trained", "models", "and", "code", "are", "publicly", "available", ",", "and", "we", "expect", "that", "ELMo", "will", "provide", "similar", "gains", "for", "many", "other", "NLP", "problems", ".", "section", ":", "Related", "work", "Due", "to", "their", "ability", "to", "capture", "syntactic", "and", "semantic", "information", "of", "words", "from", "large", "scale", "unlabeled", "text", ",", "pretrained", "word", "vectors", "Turian2010WordRA", ",", "word2vec", ",", "Pennington2014GloveGV", "are", "a", "standard", "component", "of", "most", "state", "-", "of", "-", "the", "-", "art", "NLP", "architectures", ",", "including", "for", "question", "answering", "liu2017stochastic", ",", "textual", "entailment", "Chen2017EnhancedLF", "and", "semantic", "role", "labeling", "He2017DeepSR", ".", "However", ",", "these", "approaches", "for", "learning", "word", "vectors", "only", "allow", "a", "single", "context", "-", "independent", "representation", "for", "each", "word", ".", "Previously", "proposed", "methods", "overcome", "some", "of", "the", "shortcomings", "of", "traditional", "word", "vectors", "by", "either", "enriching", "them", "with", "subword", "information", "[", "e.g.", ",", "]", "[", "]", "Wieting2016CharagramEW", ",", "Bojanowski2017EnrichingWV", "or", "learning", "separate", "vectors", "for", "each", "word", "sense", "[", "e.g.", ",", "]", "[", "]", "Neelakantan2014EfficientNE", ".", "Our", "approach", "also", "benefits", "from", "subword", "units", "through", "the", "use", "of", "character", "convolutions", ",", "and", "we", "seamlessly", "incorporate", "multi", "-", "sense", "information", "into", "downstream", "tasks", "without", "explicitly", "training", "to", "predict", "predefined", "sense", "classes", ".", "Other", "recent", "work", "has", "also", "focused", "on", "learning", "context", "-", "dependent", "representations", ".", "context2vec", "Melamud2016context2vecLG", "uses", "a", "bidirectional", "Long", "Short", "Term", "Memory", "[", "LSTM", ";", "]", "[", "]", "LSTM", ":", "Hochreiter1997", "to", "encode", "the", "context", "around", "a", "pivot", "word", ".", "Other", "approaches", "for", "learning", "contextual", "embeddings", "include", "the", "pivot", "word", "itself", "in", "the", "representation", "and", "are", "computed", "with", "the", "encoder", "of", "either", "a", "supervised", "neural", "machine", "translation", "(", "MT", ")", "system", "[", "CoVe", ";", "]", "[", "]", "McCann2017LearnedIT", "or", "an", "unsupervised", "language", "model", "Peters2017SemisupervisedST", ".", "Both", "of", "these", "approaches", "benefit", "from", "large", "datasets", ",", "although", "the", "MT", "approach", "is", "limited", "by", "the", "size", "of", "parallel", "corpora", ".", "In", "this", "paper", ",", "we", "take", "full", "advantage", "of", "access", "to", "plentiful", "monolingual", "data", ",", "and", "train", "our", "biLM", "on", "a", "corpus", "with", "approximately", "30", "million", "sentences", "Chelba2014OneBW", ".", "We", "also", "generalize", "these", "approaches", "to", "deep", "contextual", "representations", ",", "which", "we", "show", "work", "well", "across", "a", "broad", "range", "of", "diverse", "NLP", "tasks", ".", "Previous", "work", "has", "also", "shown", "that", "different", "layers", "of", "deep", "biRNNs", "encode", "different", "types", "of", "information", ".", "For", "example", ",", "introducing", "multi", "-", "task", "syntactic", "supervision", "(", "e.g.", ",", "part", "-", "of", "-", "speech", "tags", ")", "at", "the", "lower", "levels", "of", "a", "deep", "LSTM", "can", "improve", "overall", "performance", "of", "higher", "level", "tasks", "such", "as", "dependency", "parsing", "joint", "-", "many", "-", "iclr07", "or", "CCG", "super", "tagging", "Sgaard2016DeepML", ".", "In", "an", "RNN", "-", "based", "encoder", "-", "decoder", "machine", "translation", "system", ",", "Belinkov2017WhatDN", "showed", "that", "the", "representations", "learned", "at", "the", "first", "layer", "in", "a", "2", "-", "layer", "LSTM", "encoder", "are", "better", "at", "predicting", "POS", "tags", "then", "second", "layer", ".", "Finally", ",", "the", "top", "layer", "of", "an", "LSTM", "for", "encoding", "word", "context", "Melamud2016context2vecLG", "has", "been", "shown", "to", "learn", "representations", "of", "word", "sense", ".", "We", "show", "that", "similar", "signals", "are", "also", "induced", "by", "the", "modified", "language", "model", "objective", "of", "our", "ELMo", "representations", ",", "and", "it", "can", "be", "very", "beneficial", "to", "learn", "models", "for", "downstream", "tasks", "that", "mix", "these", "different", "types", "of", "semi", "-", "supervision", ".", "Dai2015SemisupervisedSL", "and", "Ramachandran2017ImproveSeq2SeqLMGal2016ATG", "pretrain", "encoder", "-", "decoder", "pairs", "using", "language", "models", "and", "sequence", "autoencoders", "and", "then", "fine", "tune", "with", "task", "specific", "supervision", ".", "In", "contrast", ",", "after", "pretraining", "the", "biLM", "with", "unlabeled", "data", ",", "we", "fix", "the", "weights", "and", "add", "additional", "task", "-", "specific", "model", "capacity", ",", "allowing", "us", "to", "leverage", "large", ",", "rich", "and", "universal", "biLM", "representations", "for", "cases", "where", "downstream", "training", "data", "size", "dictates", "a", "smaller", "supervised", "model", ".", "section", ":", "ELMo", ":", "Embeddings", "from", "Language", "Models", "Unlike", "most", "widely", "used", "word", "embeddings", "Pennington2014GloveGV", ",", "ELMo", "word", "representations", "are", "functions", "of", "the", "entire", "input", "sentence", ",", "as", "described", "in", "this", "section", ".", "They", "are", "computed", "on", "top", "of", "two", "-", "layer", "biLMs", "with", "character", "convolutions", "(", "Sec", ".", "[", "reference", "]", ")", ",", "as", "a", "linear", "function", "of", "the", "internal", "network", "states", "(", "Sec", ".", "[", "reference", "]", ")", ".", "This", "setup", "allows", "us", "to", "do", "semi", "-", "supervised", "learning", ",", "where", "the", "biLM", "is", "pretrained", "at", "a", "large", "scale", "(", "Sec", ".", "[", "reference", "]", ")", "and", "easily", "incorporated", "into", "a", "wide", "range", "of", "existing", "neural", "NLP", "architectures", "(", "Sec", ".", "[", "reference", "]", ")", ".", "subsection", ":", "Bidirectional", "language", "models", "Given", "a", "sequence", "of", "tokens", ",", ",", "a", "forward", "language", "model", "computes", "the", "probability", "of", "the", "sequence", "by", "modeling", "the", "probability", "of", "token", "given", "the", "history", ":", "Recent", "state", "-", "of", "-", "the", "-", "art", "neural", "language", "models", "Jzefowicz2016ExploringTL", ",", "Melis2017OnTS", ",", "Merity2017RegularizingAO", "compute", "a", "context", "-", "independent", "token", "representation", "(", "via", "token", "embeddings", "or", "a", "CNN", "over", "characters", ")", "then", "pass", "it", "through", "layers", "of", "forward", "LSTMs", ".", "At", "each", "position", ",", "each", "LSTM", "layer", "outputs", "a", "context", "-", "dependent", "representation", "where", ".", "The", "top", "layer", "LSTM", "output", ",", ",", "is", "used", "to", "predict", "the", "next", "token", "with", "a", "Softmax", "layer", ".", "A", "backward", "LM", "is", "similar", "to", "a", "forward", "LM", ",", "except", "it", "runs", "over", "the", "sequence", "in", "reverse", ",", "predicting", "the", "previous", "token", "given", "the", "future", "context", ":", "It", "can", "be", "implemented", "in", "an", "analogous", "way", "to", "a", "forward", "LM", ",", "with", "each", "backward", "LSTM", "layer", "in", "a", "layer", "deep", "model", "producing", "representations", "of", "given", ".", "A", "biLM", "combines", "both", "a", "forward", "and", "backward", "LM", ".", "Our", "formulation", "jointly", "maximizes", "the", "log", "likelihood", "of", "the", "forward", "and", "backward", "directions", ":", "We", "tie", "the", "parameters", "for", "both", "the", "token", "representation", "(", ")", "and", "Softmax", "layer", "(", ")", "in", "the", "forward", "and", "backward", "direction", "while", "maintaining", "separate", "parameters", "for", "the", "LSTMs", "in", "each", "direction", ".", "Overall", ",", "this", "formulation", "is", "similar", "to", "the", "approach", "of", "Peters2017SemisupervisedST", ",", "with", "the", "exception", "that", "we", "share", "some", "weights", "between", "directions", "instead", "of", "using", "completely", "independent", "parameters", ".", "In", "the", "next", "section", ",", "we", "depart", "from", "previous", "work", "by", "introducing", "a", "new", "approach", "for", "learning", "word", "representations", "that", "are", "a", "linear", "combination", "of", "the", "biLM", "layers", ".", "subsection", ":", "ELMo", "ELMo", "is", "a", "task", "specific", "combination", "of", "the", "intermediate", "layer", "representations", "in", "the", "biLM", ".", "For", "each", "token", ",", "a", "-", "layer", "biLM", "computes", "a", "set", "of", "representations", "where", "is", "the", "token", "layer", "and", ",", "for", "each", "biLSTM", "layer", ".", "For", "inclusion", "in", "a", "downstream", "model", ",", "ELMo", "collapses", "all", "layers", "in", "into", "a", "single", "vector", ",", ".", "In", "the", "simplest", "case", ",", "ELMo", "just", "selects", "the", "top", "layer", ",", ",", "as", "in", "TagLM", "Peters2017SemisupervisedST", "and", "CoVe", "McCann2017LearnedIT", ".", "More", "generally", ",", "we", "compute", "a", "task", "specific", "weighting", "of", "all", "biLM", "layers", ":", "In", "(", "[", "reference", "]", ")", ",", "are", "softmax", "-", "normalized", "weights", "and", "the", "scalar", "parameter", "allows", "the", "task", "model", "to", "scale", "the", "entire", "ELMo", "vector", ".", "is", "of", "practical", "importance", "to", "aid", "the", "optimization", "process", "(", "see", "supplemental", "material", "for", "details", ")", ".", "Considering", "that", "the", "activations", "of", "each", "biLM", "layer", "have", "a", "different", "distribution", ",", "in", "some", "cases", "it", "also", "helped", "to", "apply", "layer", "normalization", "Ba2016LayerN", "to", "each", "biLM", "layer", "before", "weighting", ".", "subsection", ":", "Using", "biLMs", "for", "supervised", "NLP", "tasks", "Given", "a", "pre", "-", "trained", "biLM", "and", "a", "supervised", "architecture", "for", "a", "target", "NLP", "task", ",", "it", "is", "a", "simple", "process", "to", "use", "the", "biLM", "to", "improve", "the", "task", "model", ".", "We", "simply", "run", "the", "biLM", "and", "record", "all", "of", "the", "layer", "representations", "for", "each", "word", ".", "Then", ",", "we", "let", "the", "end", "task", "model", "learn", "a", "linear", "combination", "of", "these", "representations", ",", "as", "described", "below", ".", "First", "consider", "the", "lowest", "layers", "of", "the", "supervised", "model", "without", "the", "biLM", ".", "Most", "supervised", "NLP", "models", "share", "a", "common", "architecture", "at", "the", "lowest", "layers", ",", "allowing", "us", "to", "add", "ELMo", "in", "a", "consistent", ",", "unified", "manner", ".", "Given", "a", "sequence", "of", "tokens", ",", "it", "is", "standard", "to", "form", "a", "context", "-", "independent", "token", "representation", "for", "each", "token", "position", "using", "pre", "-", "trained", "word", "embeddings", "and", "optionally", "character", "-", "based", "representations", ".", "Then", ",", "the", "model", "forms", "a", "context", "-", "sensitive", "representation", ",", "typically", "using", "either", "bidirectional", "RNNs", ",", "CNNs", ",", "or", "feed", "forward", "networks", ".", "To", "add", "ELMo", "to", "the", "supervised", "model", ",", "we", "first", "freeze", "the", "weights", "of", "the", "biLM", "and", "then", "concatenate", "the", "ELMo", "vector", "with", "and", "pass", "the", "ELMo", "enhanced", "representation", "into", "the", "task", "RNN", ".", "For", "some", "tasks", "(", "e.g.", ",", "SNLI", ",", "SQuAD", ")", ",", "we", "observe", "further", "improvements", "by", "also", "including", "ELMo", "at", "the", "output", "of", "the", "task", "RNN", "by", "introducing", "another", "set", "of", "output", "specific", "linear", "weights", "and", "replacing", "with", ".", "As", "the", "remainder", "of", "the", "supervised", "model", "remains", "unchanged", ",", "these", "additions", "can", "happen", "within", "the", "context", "of", "more", "complex", "neural", "models", ".", "For", "example", ",", "see", "the", "SNLI", "experiments", "in", "Sec", ".", "[", "reference", "]", "where", "a", "bi", "-", "attention", "layer", "follows", "the", "biLSTMs", ",", "or", "the", "coreference", "resolution", "experiments", "where", "a", "clustering", "model", "is", "layered", "on", "top", "of", "the", "biLSTMs", ".", "Finally", ",", "we", "found", "it", "beneficial", "to", "add", "a", "moderate", "amount", "of", "dropout", "to", "ELMo", "Srivastava2014DropoutAS", "and", "in", "some", "cases", "to", "regularize", "the", "ELMo", "weights", "by", "adding", "to", "the", "loss", ".", "This", "imposes", "an", "inductive", "bias", "on", "the", "ELMo", "weights", "to", "stay", "close", "to", "an", "average", "of", "all", "biLM", "layers", ".", "Our", "baseline", "ELMo", "+", "baseline", "Increase", "(", "absolute", "/", "relative", ")", "subsection", ":", "Pre", "-", "trained", "bidirectional", "language", "model", "architecture", "The", "pre", "-", "trained", "biLMs", "in", "this", "paper", "are", "similar", "to", "the", "architectures", "in", "Jzefowicz2016ExploringTL", "and", "kim2015characterNeuralLM", ",", "but", "modified", "to", "support", "joint", "training", "of", "both", "directions", "and", "add", "a", "residual", "connection", "between", "LSTM", "layers", ".", "We", "focus", "on", "large", "scale", "biLMs", "in", "this", "work", ",", "as", "Peters2017SemisupervisedST", "highlighted", "the", "importance", "of", "using", "biLMs", "over", "forward", "-", "only", "LMs", "and", "large", "scale", "training", ".", "To", "balance", "overall", "language", "model", "perplexity", "with", "model", "size", "and", "computational", "requirements", "for", "downstream", "tasks", "while", "maintaining", "a", "purely", "character", "-", "based", "input", "representation", ",", "we", "halved", "all", "embedding", "and", "hidden", "dimensions", "from", "the", "single", "best", "model", "CNN", "-", "BIG", "-", "LSTM", "in", "Jzefowicz2016ExploringTL", ".", "The", "final", "model", "uses", "biLSTM", "layers", "with", "4096", "units", "and", "512", "dimension", "projections", "and", "a", "residual", "connection", "from", "the", "first", "to", "second", "layer", ".", "The", "context", "insensitive", "type", "representation", "uses", "2048", "character", "n", "-", "gram", "convolutional", "filters", "followed", "by", "two", "highway", "layers", "Srivastava2015TrainingVD", "and", "a", "linear", "projection", "down", "to", "a", "512", "representation", ".", "As", "a", "result", ",", "the", "biLM", "provides", "three", "layers", "of", "representations", "for", "each", "input", "token", ",", "including", "those", "outside", "the", "training", "set", "due", "to", "the", "purely", "character", "input", ".", "In", "contrast", ",", "traditional", "word", "embedding", "methods", "only", "provide", "one", "layer", "of", "representation", "for", "tokens", "in", "a", "fixed", "vocabulary", ".", "After", "training", "for", "10", "epochs", "on", "the", "1B", "Word", "Benchmark", "Chelba2014OneBW", ",", "the", "average", "forward", "and", "backward", "perplexities", "is", "39.7", ",", "compared", "to", "30.0", "for", "the", "forward", "CNN", "-", "BIG", "-", "LSTM", ".", "Generally", ",", "we", "found", "the", "forward", "and", "backward", "perplexities", "to", "be", "approximately", "equal", ",", "with", "the", "backward", "value", "slightly", "lower", ".", "Once", "pretrained", ",", "the", "biLM", "can", "compute", "representations", "for", "any", "task", ".", "In", "some", "cases", ",", "fine", "tuning", "the", "biLM", "on", "domain", "specific", "data", "leads", "to", "significant", "drops", "in", "perplexity", "and", "an", "increase", "in", "downstream", "task", "performance", ".", "This", "can", "be", "seen", "as", "a", "type", "of", "domain", "transfer", "for", "the", "biLM", ".", "As", "a", "result", ",", "in", "most", "cases", "we", "used", "a", "fine", "-", "tuned", "biLM", "in", "the", "downstream", "task", ".", "See", "supplemental", "material", "for", "details", ".", "section", ":", "Evaluation", "Table", "[", "reference", "]", "shows", "the", "performance", "of", "ELMo", "across", "a", "diverse", "set", "of", "six", "benchmark", "NLP", "tasks", ".", "In", "every", "task", "considered", ",", "simply", "adding", "ELMo", "establishes", "a", "new", "state", "-", "of", "-", "the", "-", "art", "result", ",", "with", "relative", "error", "reductions", "ranging", "from", "6", "-", "20", "%", "over", "strong", "base", "models", ".", "This", "is", "a", "very", "general", "result", "across", "a", "diverse", "set", "model", "architectures", "and", "language", "understanding", "tasks", ".", "In", "the", "remainder", "of", "this", "section", "we", "provide", "high", "-", "level", "sketches", "of", "the", "individual", "task", "results", ";", "see", "the", "supplemental", "material", "for", "full", "experimental", "details", ".", "Question", "answering", "The", "Stanford", "Question", "Answering", "Dataset", "(", "SQuAD", ")", "Rajpurkar2016SQuAD10", "contains", "100K", "+", "crowd", "sourced", "question", "-", "answer", "pairs", "where", "the", "answer", "is", "a", "span", "in", "a", "given", "Wikipedia", "paragraph", ".", "Our", "baseline", "model", "ClarkAdvancingRC", "is", "an", "improved", "version", "of", "the", "Bidirectional", "Attention", "Flow", "model", "in", "[", "BiDAF", ";", "]", "[", "]", "Seo2016BidirectionalAF", ".", "It", "adds", "a", "self", "-", "attention", "layer", "after", "the", "bidirectional", "attention", "component", ",", "simplifies", "some", "of", "the", "pooling", "operations", "and", "substitutes", "the", "LSTMs", "for", "gated", "recurrent", "units", "[", "GRUs", ";", "]", "[", "]", "GRU", ":", "Cho2014", ".", "After", "adding", "ELMo", "to", "the", "baseline", "model", ",", "test", "set", "F", "improved", "by", "4.7", "%", "from", "81.1", "%", "to", "85.8", "%", ",", "a", "24.9", "%", "relative", "error", "reduction", "over", "the", "baseline", ",", "and", "improving", "the", "overall", "single", "model", "state", "-", "of", "-", "the", "-", "art", "by", "1.4", "%", ".", "A", "11", "member", "ensemble", "pushes", "F", "to", "87.4", ",", "the", "overall", "state", "-", "of", "-", "the", "-", "art", "at", "time", "of", "submission", "to", "the", "leaderboard", ".", "The", "increase", "of", "4.7", "%", "with", "ELMo", "is", "also", "significantly", "larger", "then", "the", "1.8", "%", "improvement", "from", "adding", "CoVe", "to", "a", "baseline", "model", "McCann2017LearnedIT", ".", "Textual", "entailment", "Textual", "entailment", "is", "the", "task", "of", "determining", "whether", "a", "\u201c", "hypothesis", "\u201d", "is", "true", ",", "given", "a", "\u201c", "premise", "\u201d", ".", "The", "Stanford", "Natural", "Language", "Inference", "(", "SNLI", ")", "corpus", "snliemnlp2015", "provides", "approximately", "550", "K", "hypothesis", "/", "premise", "pairs", ".", "Our", "baseline", ",", "the", "ESIM", "sequence", "model", "from", "Chen2017EnhancedLF", ",", "uses", "a", "biLSTM", "to", "encode", "the", "premise", "and", "hypothesis", ",", "followed", "by", "a", "matrix", "attention", "layer", ",", "a", "local", "inference", "layer", ",", "another", "biLSTM", "inference", "composition", "layer", ",", "and", "finally", "a", "pooling", "operation", "before", "the", "output", "layer", ".", "Overall", ",", "adding", "ELMo", "to", "the", "ESIM", "model", "improves", "accuracy", "by", "an", "average", "of", "0.7", "%", "across", "five", "random", "seeds", ".", "A", "five", "member", "ensemble", "pushes", "the", "overall", "accuracy", "to", "89.3", "%", ",", "exceeding", "the", "previous", "ensemble", "best", "of", "88.9", "%", "Gong2017NaturalLI", ".", "Semantic", "role", "labeling", "A", "semantic", "role", "labeling", "(", "SRL", ")", "system", "models", "the", "predicate", "-", "argument", "structure", "of", "a", "sentence", ",", "and", "is", "often", "described", "as", "answering", "\u201c", "Who", "did", "what", "to", "whom", "\u201d", ".", "He2017DeepSR", "modeled", "SRL", "as", "a", "BIO", "tagging", "problem", "and", "used", "an", "8", "-", "layer", "deep", "biLSTM", "with", "forward", "and", "backward", "directions", "interleaved", ",", "following", "Zhou2015EndtoendLO", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "when", "adding", "ELMo", "to", "a", "re", "-", "implementation", "of", "He2017DeepSR", "the", "single", "model", "test", "set", "F", "jumped", "3.2", "%", "from", "81.4", "%", "to", "84.6", "%", "\u2013", "a", "new", "state", "-", "of", "-", "the", "-", "art", "on", "the", "OntoNotes", "benchmark", "Pradhan2013TowardsRL", ",", "even", "improving", "over", "the", "previous", "best", "ensemble", "result", "by", "1.2", "%", ".", "Coreference", "resolution", "Coreference", "resolution", "is", "the", "task", "of", "clustering", "mentions", "in", "text", "that", "refer", "to", "the", "same", "underlying", "real", "world", "entities", ".", "Our", "baseline", "model", "is", "the", "end", "-", "to", "-", "end", "span", "-", "based", "neural", "model", "of", "Lee2017EndtoendNC", ".", "It", "uses", "a", "biLSTM", "and", "attention", "mechanism", "to", "first", "compute", "span", "representations", "and", "then", "applies", "a", "softmax", "mention", "ranking", "model", "to", "find", "coreference", "chains", ".", "In", "our", "experiments", "with", "the", "OntoNotes", "coreference", "annotations", "from", "the", "CoNLL", "2012", "shared", "task", "Pradhan2012CoNLL2012ST", ",", "adding", "ELMo", "improved", "the", "average", "F", "by", "3.2", "%", "from", "67.2", "to", "70.4", ",", "establishing", "a", "new", "state", "of", "the", "art", ",", "again", "improving", "over", "the", "previous", "best", "ensemble", "result", "by", "1.6", "%", "F", ".", "Named", "entity", "extraction", "The", "CoNLL", "2003", "NER", "task", "CoNLL2003NER", "consists", "of", "newswire", "from", "the", "Reuters", "RCV1", "corpus", "tagged", "with", "four", "different", "entity", "types", "(", "PER", ",", "LOC", ",", "ORG", ",", "MISC", ")", ".", "Following", "recent", "state", "-", "of", "-", "the", "-", "art", "systems", "lample", "-", "EtAl:2016:N16", "-", "1", ",", "Peters2017SemisupervisedST", ",", "the", "baseline", "model", "uses", "pre", "-", "trained", "word", "embeddings", ",", "a", "character", "-", "based", "CNN", "representation", ",", "two", "biLSTM", "layers", "and", "a", "conditional", "random", "field", "(", "CRF", ")", "loss", "CRF", ":", "Lafferty2001", ",", "similar", "to", "NLPfromScratch", ":", "Collobert2011", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "our", "ELMo", "enhanced", "biLSTM", "-", "CRF", "achieves", "92.22", "%", "F", "averaged", "over", "five", "runs", ".", "The", "key", "difference", "between", "our", "system", "and", "the", "previous", "state", "of", "the", "art", "from", "Peters2017SemisupervisedST", "is", "that", "we", "allowed", "the", "task", "model", "to", "learn", "a", "weighted", "average", "of", "all", "biLM", "layers", ",", "whereas", "Peters2017SemisupervisedST", "only", "use", "the", "top", "biLM", "layer", ".", "As", "shown", "in", "Sec", ".", "[", "reference", "]", ",", "using", "all", "layers", "instead", "of", "just", "the", "last", "layer", "improves", "performance", "across", "multiple", "tasks", ".", "Sentiment", "analysis", "The", "fine", "-", "grained", "sentiment", "classification", "task", "in", "the", "Stanford", "Sentiment", "Treebank", "[", "SST", "-", "5;", "][]", "socher2013recursive", "involves", "selecting", "one", "of", "five", "labels", "(", "from", "very", "negative", "to", "very", "positive", ")", "to", "describe", "a", "sentence", "from", "a", "movie", "review", ".", "The", "sentences", "contain", "diverse", "linguistic", "phenomena", "such", "as", "idioms", "and", "complex", "syntactic", "constructions", "such", "as", "negations", "that", "are", "difficult", "for", "models", "to", "learn", ".", "Our", "baseline", "model", "is", "the", "biattentive", "classification", "network", "(", "BCN", ")", "from", "McCann2017LearnedIT", ",", "which", "also", "held", "the", "prior", "state", "-", "of", "-", "the", "-", "art", "result", "when", "augmented", "with", "CoVe", "embeddings", ".", "Replacing", "CoVe", "with", "ELMo", "in", "the", "BCN", "model", "results", "in", "a", "1.0", "%", "absolute", "accuracy", "improvement", "over", "the", "state", "of", "the", "art", ".", "Input", "Only", "Input", "&", "Output", "Output", "Only", "section", ":", "Analysis", "This", "section", "provides", "an", "ablation", "analysis", "to", "validate", "our", "chief", "claims", "and", "to", "elucidate", "some", "interesting", "aspects", "of", "ELMo", "representations", ".", "Sec", ".", "[", "reference", "]", "shows", "that", "using", "deep", "contextual", "representations", "in", "downstream", "tasks", "improves", "performance", "over", "previous", "work", "that", "uses", "just", "the", "top", "layer", ",", "regardless", "of", "whether", "they", "are", "produced", "from", "a", "biLM", "or", "MT", "encoder", ",", "and", "that", "ELMo", "representations", "provide", "the", "best", "overall", "performance", ".", "Sec", ".", "[", "reference", "]", "explores", "the", "different", "types", "of", "contextual", "information", "captured", "in", "biLMs", "and", "uses", "two", "intrinsic", "evaluations", "to", "show", "that", "syntactic", "information", "is", "better", "represented", "at", "lower", "layers", "while", "semantic", "information", "is", "captured", "a", "higher", "layers", ",", "consistent", "with", "MT", "encoders", ".", "It", "also", "shows", "that", "our", "biLM", "consistently", "provides", "richer", "representations", "then", "CoVe", ".", "Additionally", ",", "we", "analyze", "the", "sensitivity", "to", "where", "ELMo", "is", "included", "in", "the", "task", "model", "(", "Sec", ".", "[", "reference", "]", ")", ",", "training", "set", "size", "(", "Sec", ".", "[", "reference", "]", ")", ",", "and", "visualize", "the", "ELMo", "learned", "weights", "across", "the", "tasks", "(", "Sec", ".", "[", "reference", "]", ")", ".", "subsection", ":", "Alternate", "layer", "weighting", "schemes", "There", "are", "many", "alternatives", "to", "Equation", "[", "reference", "]", "for", "combining", "the", "biLM", "layers", ".", "Previous", "work", "on", "contextual", "representations", "used", "only", "the", "last", "layer", ",", "whether", "it", "be", "from", "a", "biLM", "Peters2017SemisupervisedST", "or", "an", "MT", "encoder", "[", "CoVe;", "][]", "McCann2017LearnedIT", ".", "The", "choice", "of", "the", "regularization", "parameter", "is", "also", "important", ",", "as", "large", "values", "such", "as", "effectively", "reduce", "the", "weighting", "function", "to", "a", "simple", "average", "over", "the", "layers", ",", "while", "smaller", "values", "(", "e.g.", ",", ")", "allow", "the", "layer", "weights", "to", "vary", ".", "Table", "[", "reference", "]", "compares", "these", "alternatives", "for", "SQuAD", ",", "SNLI", "and", "SRL", ".", "Including", "representations", "from", "all", "layers", "improves", "overall", "performance", "over", "just", "using", "the", "last", "layer", ",", "and", "including", "contextual", "representations", "from", "the", "last", "layer", "improves", "performance", "over", "the", "baseline", ".", "For", "example", ",", "in", "the", "case", "of", "SQuAD", ",", "using", "just", "the", "last", "biLM", "layer", "improves", "development", "F", "by", "3.9", "%", "over", "the", "baseline", ".", "Averaging", "all", "biLM", "layers", "instead", "of", "using", "just", "the", "last", "layer", "improves", "F", "another", "0.3", "%", "(", "comparing", "\u201c", "Last", "Only", "\u201d", "to", "=", "1", "columns", ")", ",", "and", "allowing", "the", "task", "model", "to", "learn", "individual", "layer", "weights", "improves", "F", "another", "0.2", "%", "(", "=", "1", "vs.", "=", "0.001", ")", ".", "A", "small", "is", "preferred", "in", "most", "cases", "with", "ELMo", ",", "although", "for", "NER", ",", "a", "task", "with", "a", "smaller", "training", "set", ",", "the", "results", "are", "insensitive", "to", "(", "not", "shown", ")", ".", "The", "overall", "trend", "is", "similar", "with", "CoVe", "but", "with", "smaller", "increases", "over", "the", "baseline", ".", "For", "SNLI", ",", "averaging", "all", "layers", "with", "=", "1", "improves", "development", "accuracy", "from", "88.2", "to", "88.7", "%", "over", "using", "just", "the", "last", "layer", ".", "SRL", "F", "increased", "a", "marginal", "0.1", "%", "to", "82.2", "for", "the", "=", "1", "case", "compared", "to", "using", "the", "last", "layer", "only", ".", "subsection", ":", "Where", "to", "include", "ELMo", "?", "All", "of", "the", "task", "architectures", "in", "this", "paper", "include", "word", "embeddings", "only", "as", "input", "to", "the", "lowest", "layer", "biRNN", ".", "However", ",", "we", "find", "that", "including", "ELMo", "at", "the", "output", "of", "the", "biRNN", "in", "task", "-", "specific", "architectures", "improves", "overall", "results", "for", "some", "tasks", ".", "As", "shown", "in", "Table", "[", "reference", "]", ",", "including", "ELMo", "at", "both", "the", "input", "and", "output", "layers", "for", "SNLI", "and", "SQuAD", "improves", "over", "just", "the", "input", "layer", ",", "but", "for", "SRL", "(", "and", "coreference", "resolution", ",", "not", "shown", ")", "performance", "is", "highest", "when", "it", "is", "included", "at", "just", "the", "input", "layer", ".", "One", "possible", "explanation", "for", "this", "result", "is", "that", "both", "the", "SNLI", "and", "SQuAD", "architectures", "use", "attention", "layers", "after", "the", "biRNN", ",", "so", "introducing", "ELMo", "at", "this", "layer", "allows", "the", "model", "to", "attend", "directly", "to", "the", "biLM", "\u2019s", "internal", "representations", ".", "In", "the", "SRL", "case", ",", "the", "task", "-", "specific", "context", "representations", "are", "likely", "more", "important", "than", "those", "from", "the", "biLM", ".", "subsection", ":", "What", "information", "is", "captured", "by", "the", "biLM", "\u2019s", "representations", "?", "Since", "adding", "ELMo", "improves", "task", "performance", "over", "word", "vectors", "alone", ",", "the", "biLM", "\u2019s", "contextual", "representations", "must", "encode", "information", "generally", "useful", "for", "NLP", "tasks", "that", "is", "not", "captured", "in", "word", "vectors", ".", "Intuitively", ",", "the", "biLM", "must", "be", "disambiguating", "the", "meaning", "of", "words", "using", "their", "context", ".", "Consider", "\u201c", "play", "\u201d", ",", "a", "highly", "polysemous", "word", ".", "The", "top", "of", "Table", "[", "reference", "]", "lists", "nearest", "neighbors", "to", "\u201c", "play", "\u201d", "using", "GloVe", "vectors", ".", "They", "are", "spread", "across", "several", "parts", "of", "speech", "(", "e.g.", ",", "\u201c", "played", "\u201d", ",", "\u201c", "playing", "\u201d", "as", "verbs", ",", "and", "\u201c", "player", "\u201d", ",", "\u201c", "game", "\u201d", "as", "nouns", ")", "but", "concentrated", "in", "the", "sports", "-", "related", "senses", "of", "\u201c", "play", "\u201d", ".", "In", "contrast", ",", "the", "bottom", "two", "rows", "show", "nearest", "neighbor", "sentences", "from", "the", "SemCor", "dataset", "(", "see", "below", ")", "using", "the", "biLM", "\u2019s", "context", "representation", "of", "\u201c", "play", "\u201d", "in", "the", "source", "sentence", ".", "In", "these", "cases", ",", "the", "biLM", "is", "able", "to", "disambiguate", "both", "the", "part", "of", "speech", "and", "word", "sense", "in", "the", "source", "sentence", ".", "These", "observations", "can", "be", "quantified", "using", "an", "intrinsic", "evaluation", "of", "the", "contextual", "representations", "similar", "to", "Belinkov2017WhatDN", ".", "To", "isolate", "the", "information", "encoded", "by", "the", "biLM", ",", "the", "representations", "are", "used", "to", "directly", "make", "predictions", "for", "a", "fine", "grained", "word", "sense", "disambiguation", "(", "WSD", ")", "task", "and", "a", "POS", "tagging", "task", ".", "Using", "this", "approach", ",", "it", "is", "also", "possible", "to", "compare", "to", "CoVe", ",", "and", "across", "each", "of", "the", "individual", "layers", ".", "Word", "sense", "disambiguation", "Given", "a", "sentence", ",", "we", "can", "use", "the", "biLM", "representations", "to", "predict", "the", "sense", "of", "a", "target", "word", "using", "a", "simple", "1", "-", "nearest", "neighbor", "approach", ",", "similar", "to", "Melamud2016context2vecLG", ".", "To", "do", "so", ",", "we", "first", "use", "the", "biLM", "to", "compute", "representations", "for", "all", "words", "in", "SemCor", "3.0", ",", "our", "training", "corpus", "Miller1994UsingAS", ",", "and", "then", "take", "the", "average", "representation", "for", "each", "sense", ".", "At", "test", "time", ",", "we", "again", "use", "the", "biLM", "to", "compute", "representations", "for", "a", "given", "target", "word", "and", "take", "the", "nearest", "neighbor", "sense", "from", "the", "training", "set", ",", "falling", "back", "to", "the", "first", "sense", "from", "WordNet", "for", "lemmas", "not", "observed", "during", "training", ".", "Table", "[", "reference", "]", "compares", "WSD", "results", "using", "the", "evaluation", "framework", "from", "Raganato2017WordSD", "across", "the", "same", "suite", "of", "four", "test", "sets", "in", "Raganato2017NeuralSL", ".", "Overall", ",", "the", "biLM", "top", "layer", "representations", "have", "F", "of", "69.0", "and", "are", "better", "at", "WSD", "then", "the", "first", "layer", ".", "This", "is", "competitive", "with", "a", "state", "-", "of", "-", "the", "-", "art", "WSD", "-", "specific", "supervised", "model", "using", "hand", "crafted", "features", "Iacobacci2016EmbeddingsFW", "and", "a", "task", "specific", "biLSTM", "that", "is", "also", "trained", "with", "auxiliary", "coarse", "-", "grained", "semantic", "labels", "and", "POS", "tags", "Raganato2017NeuralSL", ".", "The", "CoVe", "biLSTM", "layers", "follow", "a", "similar", "pattern", "to", "those", "from", "the", "biLM", "(", "higher", "overall", "performance", "at", "the", "second", "layer", "compared", "to", "the", "first", ")", ";", "however", ",", "our", "biLM", "outperforms", "the", "CoVe", "biLSTM", ",", "which", "trails", "the", "WordNet", "first", "sense", "baseline", ".", "POS", "tagging", "To", "examine", "whether", "the", "biLM", "captures", "basic", "syntax", ",", "we", "used", "the", "context", "representations", "as", "input", "to", "a", "linear", "classifier", "that", "predicts", "POS", "tags", "with", "the", "Wall", "Street", "Journal", "portion", "of", "the", "Penn", "Treebank", "(", "PTB", ")", "Marcus1993BuildingAL", ".", "As", "the", "linear", "classifier", "adds", "only", "a", "small", "amount", "of", "model", "capacity", ",", "this", "is", "direct", "test", "of", "the", "biLM", "\u2019s", "representations", ".", "Similar", "to", "WSD", ",", "the", "biLM", "representations", "are", "competitive", "with", "carefully", "tuned", ",", "task", "specific", "biLSTMs", "Ling2015FindingFI", ",", "Ma2016EndtoendSL", ".", "However", ",", "unlike", "WSD", ",", "accuracies", "using", "the", "first", "biLM", "layer", "are", "higher", "than", "the", "top", "layer", ",", "consistent", "with", "results", "from", "deep", "biLSTMs", "in", "multi", "-", "task", "training", "Sgaard2016DeepML", ",", "joint", "-", "many", "-", "iclr07", "and", "MT", "Belinkov2017WhatDN", ".", "CoVe", "POS", "tagging", "accuracies", "follow", "the", "same", "pattern", "as", "those", "from", "the", "biLM", ",", "and", "just", "like", "for", "WSD", ",", "the", "biLM", "achieves", "higher", "accuracies", "than", "the", "CoVe", "encoder", ".", "Implications", "for", "supervised", "tasks", "Taken", "together", ",", "these", "experiments", "confirm", "different", "layers", "in", "the", "biLM", "represent", "different", "types", "of", "information", "and", "explain", "why", "including", "all", "biLM", "layers", "is", "important", "for", "the", "highest", "performance", "in", "downstream", "tasks", ".", "In", "addition", ",", "the", "biLM", "\u2019s", "representations", "are", "more", "transferable", "to", "WSD", "and", "POS", "tagging", "than", "those", "in", "CoVe", ",", "helping", "to", "illustrate", "why", "ELMo", "outperforms", "CoVe", "in", "downstream", "tasks", ".", "subsection", ":", "Sample", "efficiency", "Adding", "ELMo", "to", "a", "model", "increases", "the", "sample", "efficiency", "considerably", ",", "both", "in", "terms", "of", "number", "of", "parameter", "updates", "to", "reach", "state", "-", "of", "-", "the", "-", "art", "performance", "and", "the", "overall", "training", "set", "size", ".", "For", "example", ",", "the", "SRL", "model", "reaches", "a", "maximum", "development", "F", "after", "486", "epochs", "of", "training", "without", "ELMo", ".", "After", "adding", "ELMo", ",", "the", "model", "exceeds", "the", "baseline", "maximum", "at", "epoch", "10", ",", "a", "98", "%", "relative", "decrease", "in", "the", "number", "of", "updates", "needed", "to", "reach", "the", "same", "level", "of", "performance", ".", "In", "addition", ",", "ELMo", "-", "enhanced", "models", "use", "smaller", "training", "sets", "more", "efficiently", "than", "models", "without", "ELMo", ".", "Figure", "1", "compares", "the", "performance", "of", "baselines", "models", "with", "and", "without", "ELMo", "as", "the", "percentage", "of", "the", "full", "training", "set", "is", "varied", "from", "0.1", "%", "to", "100", "%", ".", "Improvements", "with", "ELMo", "are", "largest", "for", "smaller", "training", "sets", "and", "significantly", "reduce", "the", "amount", "of", "training", "data", "needed", "to", "reach", "a", "given", "level", "of", "performance", ".", "In", "the", "SRL", "case", ",", "the", "ELMo", "model", "with", "1", "%", "of", "the", "training", "set", "has", "about", "the", "same", "F", "as", "the", "baseline", "model", "with", "10", "%", "of", "the", "training", "set", ".", "subsection", ":", "Visualization", "of", "learned", "weights", "Figure", "2", "visualizes", "the", "softmax", "-", "normalized", "learned", "layer", "weights", ".", "At", "the", "input", "layer", ",", "the", "task", "model", "favors", "the", "first", "biLSTM", "layer", ".", "For", "coreference", "and", "SQuAD", ",", "the", "this", "is", "strongly", "favored", ",", "but", "the", "distribution", "is", "less", "peaked", "for", "the", "other", "tasks", ".", "The", "output", "layer", "weights", "are", "relatively", "balanced", ",", "with", "a", "slight", "preference", "for", "the", "lower", "layers", ".", "section", ":", "Conclusion", "We", "have", "introduced", "a", "general", "approach", "for", "learning", "high", "-", "quality", "deep", "context", "-", "dependent", "representations", "from", "biLMs", ",", "and", "shown", "large", "improvements", "when", "applying", "ELMo", "to", "a", "broad", "range", "of", "NLP", "tasks", ".", "Through", "ablations", "and", "other", "controlled", "experiments", ",", "we", "have", "also", "confirmed", "that", "the", "biLM", "layers", "efficiently", "encode", "different", "types", "of", "syntactic", "and", "semantic", "information", "about", "words", "-", "in", "-", "context", ",", "and", "that", "using", "all", "layers", "improves", "overall", "task", "performance", ".", "bibliography", ":", "References", "appendix", ":", "Supplemental", "Material", "to", "accompany", "Deep", "contextualized", "word", "representations", "This", "supplement", "contains", "details", "of", "the", "model", "architectures", ",", "training", "routines", "and", "hyper", "-", "parameter", "choices", "for", "the", "state", "-", "of", "-", "the", "-", "art", "models", "in", "Section", "[", "reference", "]", ".", "All", "of", "the", "individual", "models", "share", "a", "common", "architecture", "in", "the", "lowest", "layers", "with", "a", "context", "independent", "token", "representation", "below", "several", "layers", "of", "stacked", "RNNs", "\u2013", "LSTMs", "in", "every", "case", "except", "the", "SQuAD", "model", "that", "uses", "GRUs", ".", "subsection", ":", "Fine", "tuning", "biLM", "As", "noted", "in", "Sec", ".", "[", "reference", "]", ",", "fine", "tuning", "the", "biLM", "on", "task", "specific", "data", "typically", "resulted", "in", "significant", "drops", "in", "perplexity", ".", "To", "fine", "tune", "on", "a", "given", "task", ",", "the", "supervised", "labels", "were", "temporarily", "ignored", ",", "the", "biLM", "fine", "tuned", "for", "one", "epoch", "on", "the", "training", "split", "and", "evaluated", "on", "the", "development", "split", ".", "Once", "fine", "tuned", ",", "the", "biLM", "weights", "were", "fixed", "during", "task", "training", ".", "Table", "[", "reference", "]", "lists", "the", "development", "set", "perplexities", "for", "the", "considered", "tasks", ".", "In", "every", "case", "except", "CoNLL", "2012", ",", "fine", "tuning", "results", "in", "a", "large", "improvement", "in", "perplexity", ",", "e.g.", ",", "from", "72.1", "to", "16.8", "for", "SNLI", ".", "The", "impact", "of", "fine", "tuning", "on", "supervised", "performance", "is", "task", "dependent", ".", "In", "the", "case", "of", "SNLI", ",", "fine", "tuning", "the", "biLM", "increased", "development", "accuracy", "0.6", "%", "from", "88.9", "%", "to", "89.5", "%", "for", "our", "single", "best", "model", ".", "However", ",", "for", "sentiment", "classification", "development", "set", "accuracy", "is", "approximately", "the", "same", "regardless", "whether", "a", "fine", "tuned", "biLM", "was", "used", ".", "subsection", ":", "Importance", "of", "in", "Eqn", ".", "(", "1", ")", "The", "parameter", "in", "Eqn", ".", "(", "[", "reference", "]", ")", "was", "of", "practical", "importance", "to", "aid", "optimization", ",", "due", "to", "the", "different", "distributions", "between", "the", "biLM", "internal", "representations", "and", "the", "task", "specific", "representations", ".", "It", "is", "especially", "important", "in", "the", "last", "-", "only", "case", "in", "Sec", ".", "[", "reference", "]", ".", "Without", "this", "parameter", ",", "the", "last", "-", "only", "case", "performed", "poorly", "(", "well", "below", "the", "baseline", ")", "for", "SNLI", "and", "training", "failed", "completely", "for", "SRL", ".", "subsection", ":", "Textual", "Entailment", "Our", "baseline", "SNLI", "model", "is", "the", "ESIM", "sequence", "model", "from", "Chen2017EnhancedLF", ".", "Following", "the", "original", "implementation", ",", "we", "used", "300", "dimensions", "for", "all", "LSTM", "and", "feed", "forward", "layers", "and", "pre", "-", "trained", "300", "dimensional", "GloVe", "embeddings", "that", "were", "fixed", "during", "training", ".", "For", "regularization", ",", "we", "added", "50", "%", "variational", "dropout", "Gal2016ATG", "to", "the", "input", "of", "each", "LSTM", "layer", "and", "50", "%", "dropout", "Srivastava2014DropoutAS", "at", "the", "input", "to", "the", "final", "two", "fully", "connected", "layers", ".", "All", "feed", "forward", "layers", "use", "ReLU", "activations", ".", "Parameters", "were", "optimized", "using", "Adam", "Kingma2014AdamAM", "with", "gradient", "norms", "clipped", "at", "5.0", "and", "initial", "learning", "rate", "0.0004", ",", "decreasing", "by", "half", "each", "time", "accuracy", "on", "the", "development", "set", "did", "not", "increase", "in", "subsequent", "epochs", ".", "The", "batch", "size", "was", "32", ".", "The", "best", "ELMo", "configuration", "added", "ELMo", "vectors", "to", "both", "the", "input", "and", "output", "of", "the", "lowest", "layer", "LSTM", ",", "using", "(", "[", "reference", "]", ")", "with", "layer", "normalization", "and", ".", "Due", "to", "the", "increased", "number", "of", "parameters", "in", "the", "ELMo", "model", ",", "we", "added", "regularization", "with", "regularization", "coefficient", "0.0001", "to", "all", "recurrent", "and", "feed", "forward", "weight", "matrices", "and", "50", "%", "dropout", "after", "the", "attention", "layer", ".", "Table", "[", "reference", "]", "compares", "test", "set", "accuracy", "of", "our", "system", "to", "previously", "published", "systems", ".", "Overall", ",", "adding", "ELMo", "to", "the", "ESIM", "model", "improved", "accuracy", "by", "0.7", "%", "establishing", "a", "new", "single", "model", "state", "-", "of", "-", "the", "-", "art", "of", "88.7", "%", ",", "and", "a", "five", "member", "ensemble", "pushes", "the", "overall", "accuracy", "to", "89.3", "%", ".", "subsection", ":", "Question", "Answering", "Our", "QA", "model", "is", "a", "simplified", "version", "of", "the", "model", "from", "ClarkAdvancingRC", ".", "It", "embeds", "tokens", "by", "concatenating", "each", "token", "\u2019s", "case", "-", "sensitive", "300", "dimensional", "GloVe", "word", "vector", "Pennington2014GloveGV", "with", "a", "character", "-", "derived", "embedding", "produced", "using", "a", "convolutional", "neural", "network", "followed", "by", "max", "-", "pooling", "on", "learned", "character", "embeddings", ".", "The", "token", "embeddings", "are", "passed", "through", "a", "shared", "bi", "-", "directional", "GRU", ",", "and", "then", "the", "bi", "-", "directional", "attention", "mechanism", "from", "BiDAF", ".", "The", "augmented", "context", "vectors", "are", "then", "passed", "through", "a", "linear", "layer", "with", "ReLU", "activations", ",", "a", "residual", "self", "-", "attention", "layer", "that", "uses", "a", "GRU", "followed", "by", "the", "same", "attention", "mechanism", "applied", "context", "-", "to", "-", "context", ",", "and", "another", "linear", "layer", "with", "ReLU", "activations", ".", "Finally", ",", "the", "results", "are", "fed", "through", "linear", "layers", "to", "predict", "the", "start", "and", "end", "token", "of", "the", "answer", ".", "Variational", "dropout", "is", "used", "before", "the", "input", "to", "the", "GRUs", "and", "the", "linear", "layers", "at", "a", "rate", "of", "0.2", ".", "A", "dimensionality", "of", "90", "is", "used", "for", "the", "GRUs", ",", "and", "180", "for", "the", "linear", "layers", ".", "We", "optimize", "the", "model", "using", "Adadelta", "with", "a", "batch", "size", "of", "45", ".", "At", "test", "time", "we", "use", "an", "exponential", "moving", "average", "of", "the", "weights", "and", "limit", "the", "output", "span", "to", "be", "of", "at", "most", "size", "17", ".", "We", "do", "not", "update", "the", "word", "vectors", "during", "training", ".", "Performance", "was", "highest", "when", "adding", "ELMo", "without", "layer", "normalization", "to", "both", "the", "input", "and", "output", "of", "the", "contextual", "GRU", "layer", "and", "leaving", "the", "ELMo", "weights", "unregularized", "(", ")", ".", "Table", "[", "reference", "]", "compares", "test", "set", "results", "from", "the", "SQuAD", "leaderboard", "as", "of", "November", "17", ",", "2017", "when", "we", "submitted", "our", "system", ".", "Overall", ",", "our", "submission", "had", "the", "highest", "single", "model", "and", "ensemble", "results", ",", "improving", "the", "previous", "single", "model", "result", "(", "SAN", ")", "by", "1.4", "%", "F", "and", "our", "baseline", "by", "4.2", "%", ".", "A", "11", "member", "ensemble", "pushes", "F", "to", "87.4", "%", ",", "1.0", "%", "increase", "over", "the", "previous", "ensemble", "best", ".", "subsection", ":", "Semantic", "Role", "Labeling", "Our", "baseline", "SRL", "model", "is", "an", "exact", "reimplementation", "of", "He2017DeepSR", ".", "Words", "are", "represented", "using", "a", "concatenation", "of", "100", "dimensional", "vector", "representations", ",", "initialized", "using", "GloVe", "Pennington2014GloveGV", "and", "a", "binary", ",", "per", "-", "word", "predicate", "feature", ",", "represented", "using", "an", "100", "dimensional", "embedding", ".", "This", "200", "dimensional", "token", "representation", "is", "then", "passed", "through", "an", "8", "layer", "\u201c", "interleaved", "\u201d", "biLSTM", "with", "a", "300", "dimensional", "hidden", "size", ",", "in", "which", "the", "directions", "of", "the", "LSTM", "layers", "alternate", "per", "layer", ".", "This", "deep", "LSTM", "uses", "Highway", "connections", "Srivastava2015TrainingVD", "between", "layers", "and", "variational", "recurrent", "dropout", "Gal2016ATG", ".", "This", "deep", "representation", "is", "then", "projected", "using", "a", "final", "dense", "layer", "followed", "by", "a", "softmax", "activation", "to", "form", "a", "distribution", "over", "all", "possible", "tags", ".", "Labels", "consist", "of", "semantic", "roles", "from", "PropBank", "Palmer2005propbank", "augmented", "with", "a", "BIO", "labeling", "scheme", "to", "represent", "argument", "spans", ".", "During", "training", ",", "we", "minimize", "the", "negative", "log", "likelihood", "of", "the", "tag", "sequence", "using", "Adadelta", "with", "a", "learning", "rate", "of", "1.0", "and", "Zeiler2012ADADELTAAA", ".", "At", "test", "time", ",", "we", "perform", "Viterbi", "decoding", "to", "enforce", "valid", "spans", "using", "BIO", "constraints", ".", "Variational", "dropout", "of", "10", "%", "is", "added", "to", "all", "LSTM", "hidden", "layers", ".", "Gradients", "are", "clipped", "if", "their", "value", "exceeds", "1.0", ".", "Models", "are", "trained", "for", "500", "epochs", "or", "until", "validation", "F1", "does", "not", "improve", "for", "200", "epochs", ",", "whichever", "is", "sooner", ".", "The", "pretrained", "GloVe", "vectors", "are", "fine", "-", "tuned", "during", "training", ".", "The", "final", "dense", "layer", "and", "all", "cells", "of", "all", "LSTMs", "are", "initialized", "to", "be", "orthogonal", ".", "The", "forget", "gate", "bias", "is", "initialized", "to", "1", "for", "all", "LSTMs", ",", "with", "all", "other", "gates", "initialized", "to", "0", ",", "as", "per", "Jzefowicz2015AnEE", ".", "Table", "[", "reference", "]", "compares", "test", "set", "F1", "scores", "of", "our", "ELMo", "augmented", "implementation", "of", "He2017DeepSR", "with", "previous", "results", ".", "Our", "single", "model", "score", "of", "84.6", "F1", "represents", "a", "new", "state", "-", "of", "-", "the", "-", "art", "result", "on", "the", "CONLL", "2012", "Semantic", "Role", "Labeling", "task", ",", "surpassing", "the", "previous", "single", "model", "result", "by", "2.9", "F1", "and", "a", "5", "-", "model", "ensemble", "by", "1.2", "F1", ".", "subsection", ":", "Coreference", "resolution", "Our", "baseline", "coreference", "model", "is", "the", "end", "-", "to", "-", "end", "neural", "model", "from", "Lee2017EndtoendNC", "with", "all", "hyperparameters", "exactly", "following", "the", "original", "implementation", ".", "The", "best", "configuration", "added", "ELMo", "to", "the", "input", "of", "the", "lowest", "layer", "biLSTM", "and", "weighted", "the", "biLM", "layers", "using", "(", "[", "reference", "]", ")", "without", "any", "regularization", "(", ")", "or", "layer", "normalization", ".", "50", "%", "dropout", "was", "added", "to", "the", "ELMo", "representations", ".", "Table", "[", "reference", "]", "compares", "our", "results", "with", "previously", "published", "results", ".", "Overall", ",", "we", "improve", "the", "single", "model", "state", "-", "of", "-", "the", "-", "art", "by", "3.2", "%", "average", "F", ",", "and", "our", "single", "model", "result", "improves", "the", "previous", "ensemble", "best", "by", "1.6", "%", "F", ".", "Adding", "ELMo", "to", "the", "output", "from", "the", "biLSTM", "in", "addition", "to", "the", "biLSTM", "input", "reduced", "F", "by", "approximately", "0.7", "%", "(", "not", "shown", ")", ".", "subsection", ":", "Named", "Entity", "Recognition", "Our", "baseline", "NER", "model", "concatenates", "50", "dimensional", "pre", "-", "trained", "Senna", "vectors", "NLPfromScratch", ":", "Collobert2011", "with", "a", "CNN", "character", "based", "representation", ".", "The", "character", "representation", "uses", "16", "dimensional", "character", "embeddings", "and", "128", "convolutional", "filters", "of", "width", "three", "characters", ",", "a", "ReLU", "activation", "and", "by", "max", "pooling", ".", "The", "token", "representation", "is", "passed", "through", "two", "biLSTM", "layers", ",", "the", "first", "with", "200", "hidden", "units", "and", "the", "second", "with", "100", "hidden", "units", "before", "a", "final", "dense", "layer", "and", "softmax", "layer", ".", "During", "training", ",", "we", "use", "a", "CRF", "loss", "and", "at", "test", "time", "perform", "decoding", "using", "the", "Viterbi", "algorithm", "while", "ensuring", "that", "the", "output", "tag", "sequence", "is", "valid", ".", "Variational", "dropout", "is", "added", "to", "the", "input", "of", "both", "biLSTM", "layers", ".", "During", "training", "the", "gradients", "are", "rescaled", "if", "their", "norm", "exceeds", "5.0", "and", "parameters", "updated", "using", "Adam", "with", "constant", "learning", "rate", "of", "0.001", ".", "The", "pre", "-", "trained", "Senna", "embeddings", "are", "fine", "tuned", "during", "training", ".", "We", "employ", "early", "stopping", "on", "the", "development", "set", "and", "report", "the", "averaged", "test", "set", "score", "across", "five", "runs", "with", "different", "random", "seeds", ".", "ELMo", "was", "added", "to", "the", "input", "of", "the", "lowest", "layer", "task", "biLSTM", ".", "As", "the", "CoNLL", "2003", "NER", "data", "set", "is", "relatively", "small", ",", "we", "found", "the", "best", "performance", "by", "constraining", "the", "trainable", "layer", "weights", "to", "be", "effectively", "constant", "by", "setting", "with", "(", "[", "reference", "]", ")", ".", "Table", "[", "reference", "]", "compares", "test", "set", "F", "scores", "of", "our", "ELMo", "enhanced", "biLSTM", "-", "CRF", "tagger", "with", "previous", "results", ".", "Overall", ",", "the", "92.22", "%", "F", "from", "our", "system", "establishes", "a", "new", "state", "-", "of", "-", "the", "-", "art", ".", "When", "compared", "to", "Peters2017SemisupervisedST", ",", "using", "representations", "from", "all", "layers", "of", "the", "biLM", "provides", "a", "modest", "improvement", ".", "subsection", ":", "Sentiment", "classification", "We", "use", "almost", "the", "same", "biattention", "classification", "network", "architecture", "described", "in", "McCann2017LearnedIT", ",", "with", "the", "exception", "of", "replacing", "the", "final", "maxout", "network", "with", "a", "simpler", "feedforward", "network", "composed", "of", "two", "ReLu", "layers", "with", "dropout", ".", "A", "BCN", "model", "with", "a", "batch", "-", "normalized", "maxout", "network", "reached", "significantly", "lower", "validation", "accuracies", "in", "our", "experiments", ",", "although", "there", "may", "be", "discrepancies", "between", "our", "implementation", "and", "that", "of", "McCann2017LearnedIT", ".", "To", "match", "the", "CoVe", "training", "setup", ",", "we", "only", "train", "on", "phrases", "that", "contain", "four", "or", "more", "tokens", ".", "We", "use", "300", "-", "d", "hidden", "states", "for", "the", "biLSTM", "and", "optimize", "the", "model", "parameters", "with", "Adam", "using", "a", "learning", "rate", "of", "0.0001", ".", "The", "trainable", "biLM", "layer", "weights", "are", "regularized", "by", ",", "and", "we", "add", "ELMo", "to", "both", "the", "input", "and", "output", "of", "the", "biLSTM", ";", "the", "output", "ELMo", "vectors", "are", "computed", "with", "a", "second", "biLSTM", "and", "concatenated", "to", "the", "input", "."]}