{"coref": {"Accuracy": [[1482, 1483], [1653, 1654], [1680, 1681], [1875, 1876], [2128, 2129], [2160, 2161], [2520, 2521], [1860, 1861]], "Avg_accuracy": [], "Bi-LSTM": [[10, 17], [20, 26], [27, 30], [186, 193], [1327, 1330], [1752, 1755], [1877, 1880], [1963, 1966], [2005, 2008], [2015, 2018], [2502, 2508], [107, 110], [194, 197], [405, 408], [531, 534], [872, 875], [880, 883], [896, 899], [919, 922], [947, 950], [1035, 1038], [1162, 1165], [1540, 1543], [1550, 1553], [1568, 1571], [1730, 1733], [1825, 1828], [2024, 2027], [2286, 2289], [2571, 2574]], "Part-Of-Speech_Tagging": [[2, 9], [84, 86], [205, 207], [278, 280], [291, 293], [544, 545], [2214, 2216], [2307, 2310], [2341, 2343], [2485, 2495], [115, 117]], "Penn_Treebank": [[2249, 2250]], "UD": [[1692, 1693], [1717, 1720]]}, "coref_non_salient": {"0": [[228, 229], [835, 836], [848, 850], [868, 869], [1819, 1821], [73, 74], [91, 92], [160, 161], [247, 248], [276, 277], [378, 379], [501, 502], [548, 549], [859, 860], [2067, 2068], [2120, 2121], [2151, 2152], [2316, 2317], [2339, 2340]], "1": [[1367, 1371], [2563, 2565]], "10": [[75, 83], [1257, 1258], [1662, 1664], [2179, 2181], [2295, 2297], [2543, 2545], [2559, 2561]], "11": [[636, 638], [2209, 2212]], "12": [[224, 227], [1101, 1103]], "13": [[1013, 1017], [1923, 1927]], "14": [[1282, 1284], [1666, 1667], [1968, 1970]], "15": [[1449, 1450], [1452, 1453], [1799, 1800]], "16": [[1575, 1577], [2292, 2294]], "17": [[234, 235], [553, 554], [572, 573], [654, 655], [748, 749], [840, 841], [846, 847], [1804, 1805]], "18": [[1484, 1485], [1832, 1833], [2364, 2365]], "19": [[1942, 1946]], "2": [[1096, 1099], [1881, 1884]], "20": [[1169, 1170], [2237, 2238], [2319, 2320]], "21": [[2195, 2197], [2384, 2387]], "22": [[2074, 2077]], "23": [[18, 20], [410, 412], [536, 538], [2509, 2511], [2513, 2515], [121, 123], [435, 437]], "24": [[1423, 1425]], "25": [[202, 204], [2189, 2191], [2304, 2306], [2418, 2420]], "26": [[2478, 2484], [2525, 2527]], "27": [[2332, 2334]], "28": [[94, 96], [393, 395], [1321, 1322], [1353, 1354], [1561, 1562], [2051, 2052], [2269, 2270], [2539, 2541]], "29": [[2335, 2336]], "3": [[230, 233], [549, 552], [640, 644]], "30": [[2225, 2229]], "31": [[2440, 2446]], "32": [[1904, 1906]], "33": [[1599, 1600], [1895, 1896], [2553, 2555]], "34": [[1793, 1794]], "35": [[2580, 2584]], "36": [[1331, 1332], [1345, 1346], [1525, 1526], [1592, 1593], [1998, 1999], [2122, 2123], [2163, 2164]], "37": [[2252, 2253]], "38": [[1656, 1657]], "39": [[220, 222]], "4": [[1337, 1340], [1812, 1813]], "40": [[1658, 1659]], "41": [[214, 219]], "42": [[1197, 1199]], "43": [[208, 213]], "44": [[1341, 1342]], "45": [[611, 615]], "46": [[1775, 1777]], "47": [[1795, 1796]], "48": [[625, 627]], "49": [[1363, 1366]], "5": [[606, 610], [2379, 2383], [2452, 2458]], "50": [[539, 541]], "51": [[1200, 1204]], "52": [[1601, 1603]], "53": [[1070, 1074]], "54": [[1864, 1866]], "55": [[1174, 1178]], "56": [[38, 42]], "57": [[1416, 1419]], "58": [[1897, 1901], [2039, 2043], [2549, 2552]], "59": [[1674, 1678]], "6": [[645, 648], [682, 685], [706, 710], [714, 718], [719, 722], [765, 768], [800, 803], [700, 703], [761, 764], [818, 821], [862, 865]], "60": [[2240, 2241]], "61": [[1772, 1774]], "62": [[1383, 1384]], "63": [[559, 564]], "64": [[358, 360]], "65": [[1607, 1612]], "66": [[788, 790]], "67": [[285, 290]], "68": [[1487, 1488]], "69": [[1110, 1114]], "7": [[1373, 1375], [1537, 1538], [2021, 2022]], "70": [[1454, 1455]], "71": [[603, 604]], "72": [[1390, 1392]], "73": [[1244, 1245]], "74": [[2233, 2236]], "75": [[1171, 1172]], "76": [[1333, 1336]], "77": [[1214, 1216]], "78": [[1626, 1627]], "8": [[1192, 1194], [2247, 2248]], "9": [[2114, 2117], [2603, 2605]]}, "doc_id": "29c19276b8fff231717c3e342cb24144d2b77726", "method_subrelations": {"Bi-LSTM": [[[0, 7], "Bi-LSTM"]]}, "n_ary_relations": [{"Material": "Penn_Treebank", "Method": "Bi-LSTM", "Metric": "Accuracy", "Task": "Part-Of-Speech_Tagging", "score": "97.22"}, {"Material": "UD", "Method": "Bi-LSTM", "Metric": "Avg_accuracy", "Task": "Part-Of-Speech_Tagging", "score": "96.40"}], "ner": [[2, 9, "Task"], [10, 17, "Method"], [18, 20, "Method"], [20, 26, "Method"], [27, 30, "Method"], [38, 42, "Task"], [75, 83, "Method"], [84, 86, "Task"], [94, 96, "Method"], [186, 193, "Method"], [202, 204, "Task"], [205, 207, "Task"], [208, 213, "Task"], [214, 219, "Task"], [220, 222, "Task"], [224, 227, "Task"], [228, 229, "Method"], [230, 233, "Method"], [234, 235, "Method"], [278, 280, "Task"], [285, 290, "Method"], [291, 293, "Task"], [358, 360, "Method"], [393, 395, "Method"], [410, 412, "Method"], [536, 538, "Method"], [539, 541, "Method"], [544, 545, "Task"], [549, 552, "Method"], [553, 554, "Method"], [559, 564, "Task"], [572, 573, "Method"], [603, 604, "Method"], [606, 610, "Method"], [611, 615, "Method"], [625, 627, "Method"], [636, 638, "Task"], [640, 644, "Method"], [645, 648, "Method"], [654, 655, "Method"], [682, 685, "Method"], [706, 710, "Method"], [714, 718, "Method"], [719, 722, "Method"], [748, 749, "Method"], [765, 768, "Method"], [788, 790, "Method"], [800, 803, "Method"], [835, 836, "Method"], [840, 841, "Method"], [846, 847, "Method"], [848, 850, "Method"], [868, 869, "Method"], [1013, 1017, "Method"], [1070, 1074, "Metric"], [1096, 1099, "Method"], [1101, 1103, "Task"], [1110, 1114, "Task"], [1169, 1170, "Method"], [1171, 1172, "Method"], [1174, 1178, "Method"], [1192, 1194, "Material"], [1197, 1199, "Method"], [1200, 1204, "Method"], [1214, 1216, "Metric"], [1244, 1245, "Task"], [1257, 1258, "Method"], [1282, 1284, "Method"], [1321, 1322, "Method"], [1327, 1330, "Method"], [1331, 1332, "Method"], [1333, 1336, "Method"], [1337, 1340, "Method"], [1341, 1342, "Method"], [1345, 1346, "Method"], [1353, 1354, "Method"], [1363, 1366, "Method"], [1367, 1371, "Method"], [1373, 1375, "Method"], [1383, 1384, "Method"], [1390, 1392, "Task"], [1416, 1419, "Task"], [1423, 1425, "Method"], [1449, 1450, "Material"], [1452, 1453, "Material"], [1454, 1455, "Material"], [1482, 1483, "Metric"], [1484, 1485, "Material"], [1487, 1488, "Material"], [1525, 1526, "Method"], [1537, 1538, "Method"], [1561, 1562, "Method"], [1575, 1577, "Method"], [1592, 1593, "Method"], [1599, 1600, "Material"], [1601, 1603, "Material"], [1607, 1612, "Method"], [1626, 1627, "Material"], [1653, 1654, "Metric"], [1656, 1657, "Material"], [1658, 1659, "Material"], [1662, 1664, "Method"], [1666, 1667, "Method"], [1674, 1678, "Method"], [1680, 1681, "Metric"], [1692, 1693, "Material"], [1717, 1720, "Material"], [1752, 1755, "Method"], [1772, 1774, "Task"], [1775, 1777, "Task"], [1793, 1794, "Material"], [1795, 1796, "Material"], [1799, 1800, "Material"], [1804, 1805, "Method"], [1812, 1813, "Method"], [1819, 1821, "Method"], [1832, 1833, "Material"], [1864, 1866, "Metric"], [1875, 1876, "Metric"], [1877, 1880, "Method"], [1881, 1884, "Method"], [1895, 1896, "Material"], [1897, 1901, "Material"], [1904, 1906, "Metric"], [1923, 1927, "Method"], [1942, 1946, "Method"], [1963, 1966, "Method"], [1968, 1970, "Method"], [1998, 1999, "Method"], [2005, 2008, "Method"], [2015, 2018, "Method"], [2021, 2022, "Method"], [2039, 2043, "Material"], [2051, 2052, "Method"], [2074, 2077, "Method"], [2114, 2117, "Metric"], [2122, 2123, "Method"], [2128, 2129, "Metric"], [2160, 2161, "Metric"], [2163, 2164, "Method"], [2179, 2181, "Method"], [2189, 2191, "Task"], [2195, 2197, "Task"], [2209, 2212, "Task"], [2214, 2216, "Task"], [2225, 2229, "Method"], [2233, 2236, "Method"], [2237, 2238, "Method"], [2240, 2241, "Method"], [2247, 2248, "Material"], [2249, 2250, "Material"], [2252, 2253, "Material"], [2269, 2270, "Method"], [2292, 2294, "Method"], [2295, 2297, "Method"], [2304, 2306, "Task"], [2307, 2310, "Task"], [2332, 2334, "Task"], [2335, 2336, "Material"], [2341, 2343, "Task"], [2364, 2365, "Material"], [2379, 2383, "Method"], [2384, 2387, "Task"], [2440, 2446, "Method"], [2452, 2458, "Method"], [2478, 2484, "Method"], [2485, 2495, "Task"], [2502, 2508, "Method"], [2509, 2511, "Method"], [2513, 2515, "Method"], [2520, 2521, "Metric"], [2525, 2527, "Method"], [2539, 2541, "Method"], [2543, 2545, "Method"], [2549, 2552, "Material"], [2553, 2555, "Material"], [2559, 2561, "Method"], [2563, 2565, "Method"], [2580, 2584, "Method"], [2603, 2605, "Metric"], [73, 74, "Method"], [91, 92, "Method"], [107, 110, "Method"], [115, 117, "Task"], [121, 123, "Method"], [160, 161, "Method"], [194, 197, "Method"], [247, 248, "Method"], [276, 277, "Method"], [378, 379, "Method"], [405, 408, "Method"], [435, 437, "Method"], [501, 502, "Method"], [531, 534, "Method"], [548, 549, "Method"], [700, 703, "Method"], [761, 764, "Method"], [818, 821, "Method"], [859, 860, "Method"], [862, 865, "Method"], [872, 875, "Method"], [880, 883, "Method"], [896, 899, "Method"], [919, 922, "Method"], [947, 950, "Method"], [1035, 1038, "Method"], [1162, 1165, "Method"], [1540, 1543, "Method"], [1550, 1553, "Method"], [1568, 1571, "Method"], [1730, 1733, "Method"], [1825, 1828, "Method"], [1860, 1861, "Metric"], [2024, 2027, "Method"], [2067, 2068, "Method"], [2120, 2121, "Method"], [2151, 2152, "Method"], [2286, 2289, "Method"], [2316, 2317, "Method"], [2319, 2320, "Method"], [2339, 2340, "Method"], [2418, 2420, "Task"], [2571, 2574, "Method"]], "sections": [[0, 181], [181, 482], [482, 542], [542, 1158], [1158, 1301], [1301, 1385], [1385, 1506], [1506, 1841], [1841, 1928], [1928, 2088], [2088, 2175], [2175, 2473], [2473, 2607], [2607, 2653], [2653, 2656]], "sentences": [[0, 20], [20, 65], [65, 87], [87, 102], [102, 130], [130, 154], [154, 181], [181, 184], [184, 228], [228, 246], [246, 265], [265, 271], [271, 281], [281, 307], [307, 316], [316, 338], [338, 356], [356, 371], [371, 396], [396, 413], [413, 427], [427, 455], [455, 470], [470, 482], [482, 485], [485, 526], [526, 542], [542, 549], [549, 571], [571, 593], [593, 616], [616, 639], [639, 677], [677, 712], [712, 758], [758, 793], [793, 813], [813, 835], [835, 858], [858, 870], [870, 889], [889, 901], [901, 927], [927, 955], [955, 979], [979, 1019], [1019, 1026], [1026, 1031], [1031, 1068], [1068, 1094], [1094, 1122], [1122, 1158], [1158, 1161], [1161, 1179], [1179, 1243], [1243, 1257], [1257, 1272], [1272, 1285], [1285, 1295], [1295, 1301], [1301, 1304], [1304, 1314], [1314, 1333], [1333, 1343], [1343, 1358], [1358, 1360], [1360, 1385], [1385, 1388], [1388, 1413], [1413, 1426], [1426, 1457], [1457, 1479], [1479, 1494], [1494, 1506], [1506, 1509], [1509, 1519], [1519, 1539], [1539, 1567], [1567, 1578], [1578, 1588], [1588, 1599], [1599, 1605], [1605, 1639], [1639, 1660], [1660, 1682], [1682, 1702], [1702, 1721], [1721, 1743], [1743, 1780], [1780, 1784], [1784, 1801], [1801, 1822], [1822, 1836], [1836, 1841], [1841, 1845], [1845, 1867], [1867, 1890], [1890, 1918], [1918, 1928], [1928, 1933], [1933, 1947], [1947, 1983], [1983, 1998], [1998, 2023], [2023, 2038], [2038, 2061], [2061, 2088], [2088, 2092], [2092, 2108], [2108, 2135], [2135, 2165], [2165, 2175], [2175, 2179], [2179, 2187], [2187, 2192], [2192, 2198], [2198, 2213], [2213, 2230], [2230, 2276], [2276, 2278], [2278, 2298], [2298, 2326], [2326, 2337], [2337, 2349], [2349, 2367], [2367, 2400], [2400, 2427], [2427, 2462], [2462, 2473], [2473, 2476], [2476, 2512], [2512, 2525], [2525, 2556], [2556, 2570], [2570, 2607], [2607, 2610], [2610, 2619], [2619, 2627], [2627, 2628], [2628, 2632], [2632, 2653], [2653, 2656]], "words": ["document", ":", "Multilingual", "Part", "-", "of", "-", "Speech", "Tagging", "with", "Bidirectional", "Long", "Short", "-", "Term", "Memory", "Models", "and", "Auxiliary", "Loss", "Bidirectional", "long", "short", "-", "term", "memory", "(", "bi", "-", "LSTM", ")", "networks", "have", "recently", "proven", "successful", "for", "various", "NLP", "sequence", "modeling", "tasks", ",", "but", "little", "is", "known", "about", "their", "reliance", "to", "input", "representations", ",", "target", "languages", ",", "data", "set", "size", ",", "and", "label", "noise", ".", "We", "address", "these", "issues", "and", "evaluate", "bi", "-", "LSTMs", "with", "word", ",", "character", ",", "and", "unicode", "byte", "embeddings", "for", "POS", "tagging", ".", "We", "compare", "bi", "-", "LSTMs", "to", "traditional", "POS", "taggers", "across", "languages", "and", "data", "sizes", ".", "We", "also", "present", "a", "novel", "bi", "-", "LSTM", "model", ",", "which", "combines", "the", "POS", "tagging", "loss", "function", "with", "an", "auxiliary", "loss", "function", "that", "accounts", "for", "rare", "words", ".", "The", "model", "obtains", "state", "-", "of", "-", "the", "-", "art", "performance", "across", "22", "languages", ",", "and", "works", "especially", "well", "for", "morphologically", "complex", "languages", ".", "Our", "analysis", "suggests", "that", "bi", "-", "LSTMs", "are", "less", "sensitive", "to", "training", "data", "size", "and", "label", "corruptions", "(", "at", "small", "noise", "levels", ")", "than", "previously", "assumed", ".", "section", ":", "Introduction", "Recently", ",", "bidirectional", "long", "short", "-", "term", "memory", "networks", "(", "bi", "-", "LSTM", ")", "have", "been", "used", "for", "language", "modelling", ",", "POS", "tagging", ",", "transition", "-", "based", "dependency", "parsing", ",", "fine", "-", "grained", "sentiment", "analysis", ",", "syntactic", "chunking", ",", "and", "semantic", "role", "labeling", ".", "LSTMs", "are", "recurrent", "neural", "networks", "(", "RNNs", ")", "in", "which", "layers", "are", "designed", "to", "prevent", "vanishing", "gradients", ".", "Bidirectional", "LSTMs", "make", "a", "backward", "and", "forward", "pass", "through", "the", "sequence", "before", "passing", "on", "to", "the", "next", "layer", ".", "For", "further", "details", ",", "see", ".", "We", "consider", "using", "bi", "-", "LSTMs", "for", "POS", "tagging", ".", "Previous", "work", "on", "using", "deep", "learning", "-", "based", "methods", "for", "POS", "tagging", "has", "focused", "either", "on", "a", "single", "language", "or", "a", "small", "set", "of", "languages", ".", "Instead", "we", "evaluate", "our", "models", "across", "22", "languages", ".", "In", "addition", ",", "we", "compare", "performance", "with", "representations", "at", "different", "levels", "of", "granularity", "(", "words", ",", "characters", ",", "and", "bytes", ")", ".", "These", "levels", "of", "representation", "were", "previously", "introduced", "in", "different", "efforts", ",", "but", "a", "comparative", "evaluation", "was", "missing", ".", "Moreover", ",", "deep", "networks", "are", "often", "said", "to", "require", "large", "volumes", "of", "training", "data", ".", "We", "investigate", "to", "what", "extent", "bi", "-", "LSTMs", "are", "more", "sensitive", "to", "the", "amount", "of", "training", "data", "and", "label", "noise", "than", "standard", "POS", "taggers", ".", "Finally", ",", "we", "introduce", "a", "novel", "model", ",", "a", "bi", "-", "LSTM", "trained", "with", "auxiliary", "loss", ".", "The", "model", "jointly", "predicts", "the", "POS", "and", "the", "log", "frequency", "of", "the", "word", ".", "The", "intuition", "behind", "this", "model", "is", "that", "the", "auxiliary", "loss", ",", "being", "predictive", "of", "word", "frequency", ",", "helps", "to", "differentiate", "the", "representations", "of", "rare", "and", "common", "words", ".", "We", "indeed", "observe", "performance", "gains", "on", "rare", "and", "out", "-", "of", "-", "vocabulary", "words", ".", "These", "performance", "gains", "transfer", "into", "general", "improvements", "for", "morphologically", "rich", "languages", ".", "paragraph", ":", "Contributions", "In", "this", "paper", ",", "we", "a", ")", "evaluate", "the", "effectiveness", "of", "different", "representations", "in", "bi", "-", "LSTMs", ",", "b", ")", "compare", "these", "models", "across", "a", "large", "set", "of", "languages", "and", "under", "varying", "conditions", "(", "data", "size", ",", "label", "noise", ")", "and", "c", ")", "propose", "a", "novel", "bi", "-", "LSTM", "model", "with", "auxiliary", "loss", "(", "Logfreq", ")", ".", "section", ":", "Tagging", "with", "bi", "-", "LSTMs", "Recurrent", "neural", "networks", "(", "RNNs", ")", "allow", "the", "computation", "of", "fixed", "-", "size", "vector", "representations", "for", "word", "sequences", "of", "arbitrary", "length", ".", "An", "RNN", "is", "a", "function", "that", "reads", "in", "vectors", "and", "produces", "an", "output", "vector", ",", "that", "depends", "on", "the", "entire", "sequence", ".", "The", "vector", "is", "then", "fed", "as", "an", "input", "to", "some", "classifier", ",", "or", "higher", "-", "level", "RNNs", "in", "stacked", "/", "hierarchical", "models", ".", "The", "entire", "network", "is", "trained", "jointly", "such", "that", "the", "hidden", "representation", "captures", "the", "important", "information", "from", "the", "sequence", "for", "the", "prediction", "task", ".", "A", "bidirectional", "recurrent", "neural", "network", "(", "bi", "-", "RNN", ")", "is", "an", "extension", "of", "an", "RNN", "that", "reads", "the", "input", "sequence", "twice", ",", "from", "left", "to", "right", "and", "right", "to", "left", ",", "and", "the", "encodings", "are", "concatenated", ".", "The", "literature", "uses", "the", "term", "bi", "-", "RNN", "to", "refer", "to", "two", "related", "architectures", ",", "which", "we", "refer", "to", "here", "as", "\u201c", "context", "bi", "-", "RNN", "\u201d", "and", "\u201c", "sequence", "bi", "-", "RNN", "\u201d", ".", "In", "a", "sequence", "bi", "-", "RNN", "(", "bi", "-", "RNN", ")", ",", "the", "input", "is", "a", "sequence", "of", "vectors", "and", "the", "output", "is", "a", "concatenation", "(", ")", "of", "a", "forward", "(", ")", "and", "reverse", "(", ")", "RNN", "each", "reading", "the", "sequence", "in", "a", "different", "directions", ":", "In", "a", "context", "bi", "-", "RNN", "(", "bi", "-", "RNN", ")", ",", "we", "get", "an", "additional", "input", "indicating", "a", "sequence", "position", ",", "and", "the", "resulting", "vectors", "result", "from", "concatenating", "the", "RNN", "encodings", "up", "to", ":", "Thus", ",", "the", "state", "vector", "in", "this", "bi", "-", "RNN", "encodes", "information", "at", "position", "and", "its", "entire", "sequential", "context", ".", "Another", "view", "of", "the", "context", "bi", "-", "RNN", "is", "of", "taking", "a", "sequence", "and", "returning", "the", "corresponding", "sequence", "of", "state", "vectors", ".", "LSTMs", "are", "a", "variant", "of", "RNNs", "that", "replace", "the", "cells", "of", "RNNs", "with", "LSTM", "cells", "that", "were", "designed", "to", "prevent", "vanishing", "gradients", ".", "Bidirectional", "LSTMs", "are", "the", "bi", "-", "RNN", "counterpart", "based", "on", "LSTMs", ".", "Our", "basic", "bi", "-", "LSTM", "tagging", "model", "is", "a", "context", "bi", "-", "LSTM", "taking", "as", "input", "word", "embeddings", ".", "We", "incorporate", "subtoken", "information", "using", "an", "hierarchical", "bi", "-", "LSTM", "architecture", ".", "We", "compute", "subtoken", "-", "level", "(", "either", "characters", "or", "unicode", "byte", ")", "embeddings", "of", "words", "using", "a", "sequence", "bi", "-", "LSTM", "at", "the", "lower", "level", ".", "This", "representation", "is", "then", "concatenated", "with", "the", "(", "learned", ")", "word", "embeddings", "vector", "which", "forms", "the", "input", "to", "the", "context", "bi", "-", "LSTM", "at", "the", "next", "layer", ".", "This", "model", ",", "illustrated", "in", "Figure", "[", "reference", "]", "(", "lower", "part", "in", "left", "figure", ")", ",", "is", "inspired", "by", "ballesteros", ":", "ea:2015", ".", "We", "also", "test", "models", "in", "which", "we", "only", "keep", "sub", "-", "token", "information", ",", "e.g.", ",", "either", "both", "byte", "and", "character", "embeddings", "(", "Figure", "[", "reference", "]", ",", "right", ")", "or", "a", "single", "(", "sub", "-)", "token", "representation", "alone", ".", "In", "our", "novel", "model", ",", "cf", ".", "Figure", "[", "reference", "]", "left", ",", "we", "train", "the", "bi", "-", "LSTM", "tagger", "to", "predict", "both", "the", "tags", "of", "the", "sequence", ",", "as", "well", "as", "a", "label", "that", "represents", "the", "log", "frequency", "of", "the", "token", "as", "estimated", "from", "the", "training", "data", ".", "Our", "combined", "cross", "-", "entropy", "loss", "is", "now", ":", ",", "where", "stands", "for", "a", "POS", "tag", "and", "is", "the", "log", "frequency", "label", ",", "i.e.", ",", ".", "Combining", "this", "log", "frequency", "objective", "with", "the", "tagging", "task", "can", "be", "seen", "as", "an", "instance", "of", "multi", "-", "task", "learning", "in", "which", "the", "labels", "are", "predicted", "jointly", ".", "The", "idea", "behind", "this", "model", "is", "to", "make", "the", "representation", "predictive", "for", "frequency", ",", "which", "encourages", "the", "model", "to", "not", "share", "representations", "between", "common", "and", "rare", "words", ",", "thus", "benefiting", "the", "handling", "of", "rare", "tokens", ".", "section", ":", "Experiments", "All", "bi", "-", "LSTM", "models", "were", "implemented", "in", "CNN", "/", "pycnn", ",", "a", "flexible", "neural", "network", "library", ".", "For", "all", "models", "we", "use", "the", "same", "hyperparameters", ",", "which", "were", "set", "on", "English", "dev", ",", "i.e.", ",", "SGD", "training", "with", "cross", "-", "entropy", "loss", ",", "no", "mini", "-", "batches", ",", "20", "epochs", ",", "default", "learning", "rate", "(", "0.1", ")", ",", "128", "dimensions", "for", "word", "embeddings", ",", "100", "for", "character", "and", "byte", "embeddings", ",", "100", "hidden", "states", "and", "Gaussian", "noise", "with", "=", "0.2", ".", "As", "training", "is", "stochastic", "in", "nature", ",", "we", "use", "a", "fixed", "seed", "throughout", ".", "Embeddings", "are", "not", "initialized", "with", "pre", "-", "trained", "embeddings", ",", "except", "when", "reported", "otherwise", ".", "In", "that", "case", "we", "use", "off", "-", "the", "-", "shelf", "polyglot", "embeddings", ".", "No", "further", "unlabeled", "data", "is", "considered", "in", "this", "paper", ".", "The", "code", "is", "released", "at", ":", "paragraph", ":", "Taggers", "We", "want", "to", "compare", "POS", "taggers", "under", "varying", "conditions", ".", "We", "hence", "use", "three", "different", "types", "of", "taggers", ":", "our", "implementation", "of", "a", "bi", "-", "LSTM", ";", "Tnt", "\u2014a", "second", "order", "HMM", "with", "suffix", "trie", "handling", "for", "OOVs", ".", "We", "use", "Tnt", "as", "it", "was", "among", "the", "best", "performing", "taggers", "evaluated", "in", "horsmann", ":", "ea:2015", ".", "We", "complement", "the", "NN", "-", "based", "and", "HMM", "-", "based", "tagger", "with", "a", "CRF", "tagger", ",", "using", "a", "freely", "available", "implementation", "based", "on", "crfsuite", ".", "subsection", ":", "Datasets", "For", "the", "multilingual", "experiments", ",", "we", "use", "the", "data", "from", "the", "Universal", "Dependencies", "project", "v1.2", "(", "17", "POS", ")", "with", "the", "canonical", "data", "splits", ".", "For", "languages", "with", "token", "segmentation", "ambiguity", "we", "use", "the", "provided", "gold", "segmentation", ".", "If", "there", "is", "more", "than", "one", "treebank", "per", "language", ",", "we", "use", "the", "treebank", "that", "has", "the", "canonical", "language", "name", "(", "e.g.", ",", "Finnish", "instead", "of", "Finnish", "-", "FTB", ")", ".", "We", "consider", "all", "languages", "that", "have", "at", "least", "60k", "tokens", "and", "are", "distributed", "with", "word", "forms", ",", "resulting", "in", "22", "languages", ".", "We", "also", "report", "accuracies", "on", "WSJ", "(", "45", "POS", ")", "using", "the", "standard", "splits", ".", "The", "overview", "of", "languages", "is", "provided", "in", "Table", "[", "reference", "]", ".", "subsection", ":", "Results", "Our", "results", "are", "given", "in", "Table", "[", "reference", "]", ".", "First", "of", "all", ",", "notice", "that", "TnT", "performs", "remarkably", "well", "across", "the", "22", "languages", ",", "closely", "followed", "by", "CRF", ".", "The", "bi", "-", "LSTM", "tagger", "(", ")", "without", "lower", "-", "level", "bi", "-", "LSTM", "for", "subtokens", "falls", "short", ",", "outperforms", "the", "traditional", "taggers", "only", "on", "3", "languages", ".", "The", "bi", "-", "LSTM", "model", "clearly", "benefits", "from", "character", "representations", ".", "The", "model", "using", "characters", "alone", "(", ")", "works", "remarkably", "well", ",", "it", "improves", "over", "TnT", "on", "9", "languages", "(", "incl", ".", "Slavic", "and", "Nordic", "languages", ")", ".", "The", "combined", "word", "+", "character", "representation", "model", "is", "the", "best", "representation", ",", "outperforming", "the", "baseline", "on", "all", "except", "one", "language", "(", "Indonesian", ")", ",", "providing", "strong", "results", "already", "without", "pre", "-", "trained", "embeddings", ".", "This", "model", "(", ")", "reaches", "the", "biggest", "improvement", "(", "more", "than", "+", "2", "%", "accuracy", ")", "on", "Hebrew", "and", "Slovene", ".", "Initializing", "the", "word", "embeddings", "(", "+", "Polyglot", ")", "with", "off", "-", "the", "-", "shelf", "language", "-", "specific", "embeddings", "further", "improves", "accuracy", ".", "The", "only", "system", "we", "are", "aware", "of", "that", "evaluates", "on", "UD", "is", "gillick", ":", "ea:2016", "(", "last", "column", ")", ".", "However", ",", "note", "that", "these", "results", "are", "not", "strictly", "comparable", "as", "they", "use", "the", "earlier", "UD", "v1.1", "version", ".", "The", "overall", "best", "system", "is", "the", "multi", "-", "task", "bi", "-", "LSTM", "freqbin", "(", "it", "uses", "and", "Polyglot", "initialization", "for", ")", ".", "While", "on", "macro", "average", "it", "is", "on", "par", "with", "bi", "-", "LSTM", ",", "it", "obtains", "the", "best", "results", "on", "12", "/", "22", "languages", ",", "and", "it", "is", "successful", "in", "predicting", "POS", "for", "OOV", "tokens", "(", "cf", ".", "Table", "[", "reference", "]", "OOV", "Acc", "columns", ")", ",", "especially", "for", "languages", "like", "Arabic", ",", "Farsi", ",", "Hebrew", ",", "Finnish", ".", "We", "examined", "simple", "RNNs", "and", "confirm", "the", "finding", "of", "ling", ":", "ea:2015", "that", "they", "performed", "worse", "than", "their", "LSTM", "counterparts", ".", "Finally", ",", "the", "bi", "-", "LSTM", "tagger", "is", "competitive", "on", "WSJ", ",", "cf", ".", "Table", "[", "reference", "]", ".", "paragraph", ":", "Rare", "words", "In", "order", "to", "evaluate", "the", "effect", "of", "modeling", "sub", "-", "token", "information", ",", "we", "examine", "accuracy", "rates", "at", "different", "frequency", "rates", ".", "Figure", "[", "reference", "]", "shows", "absolute", "improvements", "in", "accuracy", "of", "bi", "-", "LSTM", "over", "mean", "log", "frequency", ",", "for", "different", "language", "families", ".", "We", "see", "that", "especially", "for", "Slavic", "and", "non", "-", "Indoeuropean", "languages", ",", "having", "high", "morphologic", "complexity", ",", "most", "of", "the", "improvement", "is", "obtained", "in", "the", "Zipfian", "tail", ".", "Rare", "tokens", "benefit", "from", "the", "sub", "-", "token", "representations", ".", "paragraph", ":", "Data", "set", "size", "Prior", "work", "mostly", "used", "large", "data", "sets", "when", "applying", "neural", "network", "based", "approaches", ".", "We", "evaluate", "how", "brittle", "such", "models", "are", "with", "respect", "to", "their", "more", "traditional", "counterparts", "by", "training", "bi", "-", "LSTM", "(", "without", "Polyglot", "embeddings", ")", "for", "increasing", "amounts", "of", "training", "instances", "(", "number", "of", "sentences", ")", ".", "The", "learning", "curves", "in", "Figure", "[", "reference", "]", "show", "similar", "trends", "across", "language", "families", ".", "TnT", "is", "better", "with", "little", "data", ",", "bi", "-", "LSTM", "is", "better", "with", "more", "data", ",", "and", "bi", "-", "LSTM", "always", "wins", "over", "CRF", ".", "The", "bi", "-", "LSTM", "model", "performs", "already", "surprisingly", "well", "after", "only", "500", "training", "sentences", ".", "For", "non", "-", "Indoeuropean", "languages", "it", "is", "on", "par", "and", "above", "the", "other", "taggers", "with", "even", "less", "data", "(", "100", "sentences", ")", ".", "This", "shows", "that", "the", "bi", "-", "LSTMs", "often", "needs", "more", "data", "than", "the", "generative", "markovian", "model", ",", "but", "this", "is", "definitely", "less", "than", "what", "we", "expected", ".", "paragraph", ":", "Label", "Noise", "We", "investigated", "the", "susceptibility", "of", "the", "models", "to", "noise", ",", "by", "artificially", "corrupting", "training", "labels", ".", "Our", "initial", "results", "show", "that", "at", "low", "noise", "rates", ",", "bi", "-", "LSTMs", "and", "TnT", "are", "affected", "similarly", ",", "their", "accuracies", "drop", "to", "a", "similar", "degree", ".", "Only", "at", "higher", "noise", "levels", "(", "more", "than", "30", "%", "corrupted", "labels", ")", ",", "bi", "-", "LSTMs", "are", "less", "robust", ",", "showing", "higher", "drops", "in", "accuracy", "compared", "to", "TnT", ".", "This", "is", "the", "case", "for", "all", "investigated", "language", "families", ".", "section", ":", "Related", "Work", "Character", "embeddings", "were", "first", "introduced", "by", "sutskever", ":", "ea:2011", "for", "language", "modeling", ".", "Early", "applications", "include", "text", "classification", ".", "Recently", ",", "these", "representations", "were", "successfully", "applied", "to", "a", "range", "of", "structured", "prediction", "tasks", ".", "For", "POS", "tagging", ",", "santos", ":", "zadrozny:2014", "were", "the", "first", "to", "propose", "character", "-", "based", "models", ".", "They", "use", "a", "convolutional", "neural", "network", "(", "CNN", ";", "or", "convnet", ")", "and", "evaluated", "their", "model", "on", "English", "(", "PTB", ")", "and", "Portuguese", ",", "showing", "that", "the", "model", "achieves", "state", "-", "of", "-", "the", "-", "art", "performance", "close", "to", "taggers", "using", "carefully", "designed", "feature", "templates", ".", "ling", ":", "ea:2015", "extend", "this", "line", "and", "compare", "a", "novel", "bi", "-", "LSTM", "model", ",", "learning", "word", "representations", "through", "character", "embeddings", ".", "They", "evaluate", "their", "model", "on", "a", "language", "modeling", "and", "POS", "tagging", "setup", ",", "and", "show", "that", "bi", "-", "LSTMs", "outperform", "the", "CNN", "approach", "of", "santos", ":", "zadrozny:2014", ".", "Similarly", ",", "labeau", ":", "ea:2015", "evaluate", "character", "embeddings", "for", "German", ".", "Bi", "-", "LSTMs", "for", "POS", "tagging", "are", "also", "reported", "in", "wang", ":", "ea:2015:arxiv", ",", "however", ",", "they", "only", "explore", "word", "embeddings", ",", "orthographic", "information", "and", "evaluate", "on", "WSJ", "only", ".", "A", "related", "study", "is", "cheng", ":", "fang", ":", "ostendorf:2015", "who", "propose", "a", "multi", "-", "task", "RNN", "for", "named", "entity", "recognition", "by", "jointly", "predicting", "the", "next", "token", "and", "current", "token", "\u2019s", "name", "label", ".", "Our", "model", "is", "simpler", ",", "it", "uses", "a", "very", "coarse", "set", "of", "labels", "rather", "then", "integrating", "an", "entire", "language", "modeling", "task", "which", "is", "computationally", "more", "expensive", ".", "An", "interesting", "recent", "study", "is", "gillick", ":", "ea:2016", ",", "they", "build", "a", "single", "byte", "-", "to", "-", "span", "model", "for", "multiple", "languages", "based", "on", "a", "sequence", "-", "to", "-", "sequence", "RNN", "achieving", "impressive", "results", ".", "We", "would", "like", "to", "extend", "this", "work", "in", "their", "direction", ".", "section", ":", "Conclusions", "We", "evaluated", "token", "and", "subtoken", "-", "level", "representations", "for", "neural", "network", "-", "based", "part", "-", "of", "-", "speech", "tagging", "across", "22", "languages", "and", "proposed", "a", "novel", "multi", "-", "task", "bi", "-", "LSTM", "with", "auxiliary", "loss", ".", "The", "auxiliary", "loss", "is", "effective", "at", "improving", "the", "accuracy", "of", "rare", "words", ".", "Subtoken", "representations", "are", "necessary", "to", "obtain", "a", "state", "-", "of", "-", "the", "-", "art", "POS", "tagger", ",", "and", "character", "embeddings", "are", "particularly", "helpful", "for", "non", "-", "Indoeuropean", "and", "Slavic", "languages", ".", "Combining", "them", "with", "word", "embeddings", "in", "a", "hierarchical", "network", "provides", "the", "best", "representation", ".", "The", "bi", "-", "LSTM", "tagger", "is", "as", "effective", "as", "the", "CRF", "and", "HMM", "taggers", "with", "already", "as", "little", "as", "500", "training", "sentences", ",", "but", "is", "less", "robust", "to", "label", "noise", "(", "at", "higher", "noise", "rates", ")", ".", "section", ":", "Acknowledgments", "We", "thank", "the", "anonymous", "reviewers", "for", "their", "feedback", ".", "AS", "is", "funded", "by", "the", "ERC", "Starting", "Grant", "LOWLANDS", "No", ".", "313695", ".", "YG", "is", "supported", "by", "The", "Israeli", "Science", "Foundation", "(", "grant", "number", "1555", "/", "15", ")", "and", "a", "Google", "Research", "Award", ".", "bibliography", ":", "References"]}