{"coref": {"AWD-LSTM-DOC": [[2588, 2593], [2654, 2659], [2724, 2729], [2757, 2762], [2880, 2885], [2933, 2938], [2960, 2965], [2970, 2975], [3140, 3145], [3159, 3164], [3169, 3174], [4144, 4149], [4318, 4323], [4356, 4361], [4528, 4533]], "AWD-LSTM-DOC_x5": [], "Constituency_Parsing": [[4001, 4003]], "F1_score": [[3773, 3775], [3914, 3916], [4008, 4010], [4313, 4315], [4401, 4403], [4513, 4515], [4521, 4523], [3830, 3832], [4207, 4209], [4420, 4422]], "LSTM-LM": [[4290, 4293], [4900, 4903]], "LSTM_Encoder-Decoder": [[188, 193], [655, 658], [3264, 3269], [3286, 3290], [3296, 3297], [3522, 3526], [3534, 3537], [3558, 3559], [3590, 3591], [3596, 3599], [3600, 3601], [3683, 3684], [3759, 3763], [3790, 3794], [3821, 3822], [3887, 3888], [3894, 3895], [3954, 3955], [3957, 3958], [3970, 3975], [4506, 4507], [5269, 5270]], "LSTM_Encoder-Decoder___LSTM-LM": [], "Language_Modelling": [[272, 274], [292, 294], [595, 597], [872, 874], [1934, 1936], [2446, 2448], [2913, 2915], [4170, 4172], [4579, 4581], [4636, 4638], [4690, 4692], [4933, 4935], [5237, 5239], [67, 69], [1944, 1946], [2025, 2027], [3737, 3739], [4234, 4236], [4341, 4343], [5038, 5040]], "Number_of_params": [], "Params": [], "Penn_Treebank": [[105, 107], [599, 601], [2003, 2005], [2006, 2007], [2046, 2048], [2182, 2185], [2230, 2232], [2513, 2515], [2704, 2705], [3149, 3150], [3181, 3182], [4211, 4214], [4920, 4921], [5240, 5241], [681, 683], [3238, 3239], [4012, 4014], [4071, 4073]], "Penn_Treebank__Word_Level_": [], "Test_perplexity": [[622, 623], [2283, 2284], [2486, 2487], [2840, 2841], [2853, 2854], [2872, 2873], [2888, 2889], [3122, 3123], [3167, 3168], [3176, 3177], [3215, 3216], [4036, 4037], [4188, 4189], [4910, 4911], [5230, 5231], [2221, 2222], [3138, 3139]], "Validation_perplexity": [[622, 623], [2283, 2284], [2710, 2712], [4188, 4189], [4910, 4911], [5230, 5231], [2221, 2222], [3138, 3139]], "WikiText-2": [[108, 111], [602, 605], [3151, 3154], [3185, 3188], [4922, 4925], [5242, 5245], [2009, 2012], [2049, 2052], [3244, 3247]]}, "coref_non_salient": {"0": [[3068, 3071], [3231, 3234]], "1": [[4300, 4301], [4304, 4309]], "10": [[758, 760], [1331, 1333], [1512, 1517]], "11": [[2648, 2653], [2717, 2722], [2740, 2745], [2781, 2786], [2862, 2867], [2926, 2931], [2944, 2949], [2979, 2984], [4154, 4159], [4328, 4333], [4367, 4372]], "12": [[2, 5], [531, 534], [1281, 1284], [5181, 5184], [1344, 1347]], "13": [[1565, 1567], [5041, 5043]], "14": [[417, 418], [1845, 1847], [2109, 2110], [4788, 4789], [4889, 4891]], "15": [[843, 845], [4408, 4413]], "16": [[177, 180], [200, 204]], "17": [[4388, 4390], [4454, 4455], [4546, 4548]], "18": [[825, 827], [4872, 4873]], "19": [[1203, 1204], [1240, 1241]], "2": [[849, 851], [4986, 4988]], "20": [[58, 60], [96, 98], [893, 895], [2312, 2314], [2348, 2350], [2368, 2370], [4054, 4056], [5014, 5016]], "21": [[234, 237], [773, 776], [4587, 4589]], "22": [[135, 137], [642, 644], [3407, 3409], [3631, 3633], [3670, 3672], [5256, 5258]], "23": [[208, 209], [3415, 3417]], "24": [[1848, 1849], [4626, 4627], [4852, 4853]], "25": [[5135, 5137]], "26": [[4849, 4851]], "27": [[572, 575], [1737, 1740]], "28": [[2135, 2137], [2151, 2153], [2957, 2959]], "29": [[576, 577], [1741, 1742]], "3": [[4701, 4702], [4720, 4725], [4796, 4798]], "30": [[4597, 4598], [4613, 4614], [4624, 4625]], "31": [[5116, 5118]], "32": [[835, 836], [2106, 2107], [4683, 4684], [4785, 4786], [4977, 4978], [859, 860], [2225, 2226], [2443, 2444], [3101, 3102], [3554, 3555], [3565, 3566]], "33": [[660, 662], [3539, 3541], [3567, 3568]], "34": [[2808, 2809], [2900, 2901], [2813, 2814], [2998, 2999], [4897, 4898]], "35": [[3647, 3648], [3655, 3656]], "36": [[839, 842], [4938, 4941]], "37": [[3907, 3910], [3911, 3914]], "38": [[1354, 1356], [5189, 5191]], "39": [[1891, 1894]], "4": [[217, 221], [3274, 3277]], "40": [[4943, 4945]], "41": [[535, 536], [546, 547], [578, 579], [610, 611], [634, 635], [648, 649], [691, 692], [1395, 1396], [1471, 1472], [1580, 1581], [1701, 1702], [1719, 1720], [1734, 1735], [1812, 1813], [1941, 1942], [1961, 1962], [2672, 2673], [2907, 2908], [2992, 2993], [3283, 3284], [3293, 3294], [3679, 3680], [3949, 3950], [3966, 3967], [4049, 4050], [5074, 5075], [5157, 5158], [5185, 5186], [5207, 5208], [5223, 5224], [5263, 5264], [1348, 1349], [1358, 1359], [1377, 1378], [2227, 2228], [2396, 2397], [3103, 3104], [3688, 3689], [3819, 3820], [3885, 3886], [3987, 3988], [3993, 3994]], "42": [[475, 478], [875, 877], [900, 903], [1094, 1097]], "43": [[2092, 2095], [2100, 2103], [2285, 2290], [2638, 2641], [2713, 2716], [2736, 2739], [2777, 2780], [2858, 2861], [2922, 2925], [4150, 4153], [4324, 4327], [4363, 4366], [4905, 4908]], "44": [[166, 170], [222, 225], [245, 248], [258, 261], [4040, 4043], [4568, 4571]], "45": [[4374, 4376]], "46": [[181, 182], [288, 289]], "47": [[41, 43], [987, 989], [1322, 1324], [1645, 1647], [1663, 1667]], "48": [[4737, 4738], [4739, 4741]], "49": [[740, 741], [798, 799], [819, 820], [4486, 4487], [4634, 4635], [4688, 4689], [5066, 5067]], "5": [[23, 31], [363, 366], [383, 386], [408, 411], [439, 442], [458, 461], [470, 473], [695, 698], [705, 708], [754, 757], [1065, 1068], [1085, 1088], [1114, 1117], [1121, 1124], [1576, 1579], [1612, 1615], [4671, 4674], [4696, 4699], [4777, 4780], [5119, 5122], [5213, 5216]], "50": [[4816, 4817], [4864, 4865]], "51": [[1153, 1155]], "52": [[4812, 4815]], "53": [[1757, 1759]], "54": [[3049, 3052]], "55": [[4577, 4578]], "56": [[497, 500], [1304, 1307]], "57": [[242, 243]], "58": [[501, 502], [612, 613], [1311, 1312], [1966, 1967], [2675, 2676], [5192, 5193], [5225, 5226], [1308, 1309], [3706, 3707]], "59": [[3429, 3432]], "6": [[132, 134], [205, 207], [639, 641], [3309, 3315], [3317, 3319], [3626, 3628], [3663, 3665], [5253, 5255]], "60": [[4652, 4661]], "61": [[1953, 1957]], "62": [[4136, 4138]], "63": [[4087, 4088]], "64": [[4892, 4893]], "65": [[482, 484]], "66": [[5005, 5006]], "67": [[4097, 4099]], "68": [[4414, 4415]], "69": [[3253, 3255]], "7": [[2799, 2801], [4768, 4772]], "70": [[3543, 3544], [3611, 3612]], "71": [[266, 269]], "72": [[4775, 4776]], "73": [[3641, 3646]], "74": [[4591, 4596]], "75": [[4952, 4954]], "76": [[4606, 4611]], "77": [[444, 446]], "78": [[3200, 3202]], "79": [[228, 230]], "8": [[3046, 3048], [4509, 4511]], "80": [[7, 12]], "81": [[285, 287]], "82": [[4127, 4128]], "83": [[4476, 4480]], "84": [[401, 403], [1040, 1042]], "85": [[1505, 1507]], "86": [[4958, 4960]], "87": [[238, 241]], "88": [[2083, 2091]], "89": [[543, 545]], "9": [[829, 834], [4868, 4871]], "90": [[4444, 4451]], "91": [[211, 212]], "92": [[539, 542]], "93": [[2803, 2807]], "94": [[2994, 2995]], "95": [[46, 48], [1744, 1746], [2315, 2317], [5273, 5275]]}, "doc_id": "0be9ca65ad318ee3729928882ef2c403d4b6d24e", "method_subrelations": {"AWD-LSTM-DOC": [[[0, 12], "AWD-LSTM-DOC"]], "AWD-LSTM-DOC_x5": [[[0, 15], "AWD-LSTM-DOC_x5"]], "LSTM_Encoder-Decoder___LSTM-LM": [[[0, 20], "LSTM_Encoder-Decoder"], [[23, 30], "LSTM-LM"]]}, "n_ary_relations": [{"Material": "Penn_Treebank", "Method": "LSTM_Encoder-Decoder___LSTM-LM", "Metric": "F1_score", "Task": "Constituency_Parsing", "score": "94.47"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-DOC", "Metric": "Params", "Task": "Language_Modelling", "score": "23M"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-DOC_x5", "Metric": "Params", "Task": "Language_Modelling", "score": "185M"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-DOC", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "52.38"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-DOC_x5", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "47.17"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-DOC", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "54.12"}, {"Material": "Penn_Treebank__Word_Level_", "Method": "AWD-LSTM-DOC_x5", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "48.63"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-DOC", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "37M"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-DOC_x5", "Metric": "Number_of_params", "Task": "Language_Modelling", "score": "185M"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-DOC", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "58.03"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-DOC_x5", "Metric": "Test_perplexity", "Task": "Language_Modelling", "score": "53.09"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-DOC", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "60.29"}, {"Material": "WikiText-2", "Method": "AWD-LSTM-DOC_x5", "Metric": "Validation_perplexity", "Task": "Language_Modelling", "score": "54.19"}], "ner": [[2, 5, "Method"], [7, 12, "Method"], [23, 31, "Method"], [41, 43, "Method"], [46, 48, "Method"], [58, 60, "Method"], [96, 98, "Method"], [105, 107, "Material"], [108, 111, "Material"], [132, 134, "Task"], [135, 137, "Task"], [166, 170, "Method"], [177, 180, "Task"], [181, 182, "Task"], [188, 193, "Method"], [200, 204, "Task"], [205, 207, "Task"], [208, 209, "Task"], [211, 212, "Task"], [217, 221, "Method"], [222, 225, "Method"], [228, 230, "Task"], [234, 237, "Method"], [238, 241, "Method"], [242, 243, "Method"], [245, 248, "Method"], [258, 261, "Method"], [266, 269, "Method"], [272, 274, "Task"], [285, 287, "Method"], [292, 294, "Task"], [363, 366, "Method"], [383, 386, "Method"], [401, 403, "Method"], [408, 411, "Method"], [417, 418, "Method"], [439, 442, "Method"], [444, 446, "Metric"], [458, 461, "Method"], [470, 473, "Method"], [475, 478, "Task"], [482, 484, "Task"], [497, 500, "Method"], [501, 502, "Method"], [531, 534, "Method"], [535, 536, "Method"], [539, 542, "Method"], [543, 545, "Method"], [546, 547, "Method"], [572, 575, "Task"], [576, 577, "Method"], [578, 579, "Method"], [595, 597, "Task"], [599, 601, "Material"], [602, 605, "Material"], [610, 611, "Method"], [612, 613, "Method"], [622, 623, "Metric"], [634, 635, "Method"], [639, 641, "Task"], [642, 644, "Task"], [648, 649, "Method"], [655, 658, "Method"], [660, 662, "Method"], [691, 692, "Method"], [695, 698, "Method"], [705, 708, "Method"], [740, 741, "Method"], [754, 757, "Method"], [758, 760, "Method"], [773, 776, "Method"], [798, 799, "Method"], [819, 820, "Method"], [825, 827, "Method"], [829, 834, "Method"], [835, 836, "Method"], [839, 842, "Method"], [843, 845, "Method"], [849, 851, "Method"], [872, 874, "Task"], [875, 877, "Task"], [893, 895, "Method"], [900, 903, "Task"], [987, 989, "Method"], [1040, 1042, "Method"], [1065, 1068, "Method"], [1085, 1088, "Method"], [1094, 1097, "Task"], [1114, 1117, "Method"], [1121, 1124, "Method"], [1153, 1155, "Metric"], [1203, 1204, "Task"], [1240, 1241, "Task"], [1281, 1284, "Method"], [1311, 1312, "Method"], [1322, 1324, "Method"], [1331, 1333, "Method"], [1354, 1356, "Method"], [1395, 1396, "Method"], [1471, 1472, "Method"], [1505, 1507, "Method"], [1512, 1517, "Method"], [1565, 1567, "Method"], [1576, 1579, "Method"], [1580, 1581, "Method"], [1612, 1615, "Method"], [1645, 1647, "Method"], [1663, 1667, "Method"], [1701, 1702, "Method"], [1719, 1720, "Method"], [1734, 1735, "Method"], [1737, 1740, "Task"], [1741, 1742, "Method"], [1744, 1746, "Method"], [1757, 1759, "Method"], [1812, 1813, "Method"], [1845, 1847, "Method"], [1848, 1849, "Method"], [1891, 1894, "Task"], [1934, 1936, "Task"], [1941, 1942, "Method"], [1953, 1957, "Task"], [1961, 1962, "Method"], [1966, 1967, "Method"], [2003, 2005, "Material"], [2006, 2007, "Material"], [2046, 2048, "Material"], [2083, 2091, "Method"], [2092, 2095, "Method"], [2100, 2103, "Method"], [2106, 2107, "Method"], [2109, 2110, "Method"], [2135, 2137, "Metric"], [2151, 2153, "Metric"], [2182, 2185, "Material"], [2230, 2232, "Material"], [2283, 2284, "Metric"], [2285, 2290, "Method"], [2312, 2314, "Method"], [2315, 2317, "Method"], [2348, 2350, "Method"], [2368, 2370, "Method"], [2446, 2448, "Task"], [2486, 2487, "Metric"], [2513, 2515, "Material"], [2588, 2593, "Method"], [2638, 2641, "Method"], [2648, 2653, "Method"], [2654, 2659, "Method"], [2672, 2673, "Method"], [2675, 2676, "Method"], [2704, 2705, "Material"], [2710, 2712, "Metric"], [2713, 2716, "Method"], [2717, 2722, "Method"], [2724, 2729, "Method"], [2736, 2739, "Method"], [2740, 2745, "Method"], [2757, 2762, "Method"], [2777, 2780, "Method"], [2781, 2786, "Method"], [2799, 2801, "Method"], [2803, 2807, "Method"], [2808, 2809, "Method"], [2840, 2841, "Metric"], [2853, 2854, "Metric"], [2858, 2861, "Method"], [2862, 2867, "Method"], [2872, 2873, "Metric"], [2880, 2885, "Method"], [2888, 2889, "Metric"], [2900, 2901, "Method"], [2907, 2908, "Method"], [2913, 2915, "Task"], [2922, 2925, "Method"], [2926, 2931, "Method"], [2933, 2938, "Method"], [2944, 2949, "Method"], [2957, 2959, "Metric"], [2960, 2965, "Method"], [2970, 2975, "Method"], [2979, 2984, "Method"], [2992, 2993, "Method"], [2994, 2995, "Method"], [3046, 3048, "Method"], [3049, 3052, "Task"], [3068, 3071, "Method"], [3122, 3123, "Metric"], [3140, 3145, "Method"], [3149, 3150, "Material"], [3151, 3154, "Material"], [3159, 3164, "Method"], [3167, 3168, "Metric"], [3169, 3174, "Method"], [3176, 3177, "Metric"], [3181, 3182, "Material"], [3185, 3188, "Material"], [3200, 3202, "Method"], [3215, 3216, "Metric"], [3231, 3234, "Method"], [3253, 3255, "Task"], [3264, 3269, "Method"], [3274, 3277, "Method"], [3283, 3284, "Method"], [3286, 3290, "Method"], [3293, 3294, "Method"], [3296, 3297, "Method"], [3309, 3315, "Task"], [3317, 3319, "Task"], [3407, 3409, "Task"], [3415, 3417, "Task"], [3429, 3432, "Method"], [3522, 3526, "Method"], [3534, 3537, "Method"], [3539, 3541, "Method"], [3543, 3544, "Method"], [3558, 3559, "Method"], [3567, 3568, "Method"], [3590, 3591, "Method"], [3596, 3599, "Method"], [3600, 3601, "Method"], [3611, 3612, "Method"], [3626, 3628, "Task"], [3631, 3633, "Task"], [3641, 3646, "Method"], [3647, 3648, "Method"], [3663, 3665, "Task"], [3670, 3672, "Task"], [3679, 3680, "Method"], [3683, 3684, "Method"], [3759, 3763, "Method"], [3773, 3775, "Metric"], [3790, 3794, "Method"], [3821, 3822, "Method"], [3887, 3888, "Method"], [3894, 3895, "Method"], [3907, 3910, "Metric"], [3911, 3914, "Metric"], [3914, 3916, "Metric"], [3949, 3950, "Method"], [3954, 3955, "Method"], [3957, 3958, "Method"], [3966, 3967, "Method"], [3970, 3975, "Method"], [4001, 4003, "Task"], [4008, 4010, "Metric"], [4036, 4037, "Metric"], [4040, 4043, "Method"], [4049, 4050, "Method"], [4054, 4056, "Method"], [4087, 4088, "Task"], [4097, 4099, "Method"], [4127, 4128, "Task"], [4136, 4138, "Method"], [4144, 4149, "Method"], [4150, 4153, "Method"], [4154, 4159, "Method"], [4170, 4172, "Task"], [4188, 4189, "Metric"], [4211, 4214, "Material"], [4290, 4293, "Method"], [4300, 4301, "Method"], [4304, 4309, "Method"], [4313, 4315, "Metric"], [4318, 4323, "Method"], [4324, 4327, "Method"], [4328, 4333, "Method"], [4356, 4361, "Method"], [4363, 4366, "Method"], [4367, 4372, "Method"], [4374, 4376, "Task"], [4388, 4390, "Method"], [4401, 4403, "Metric"], [4408, 4413, "Method"], [4414, 4415, "Method"], [4444, 4451, "Method"], [4454, 4455, "Method"], [4476, 4480, "Method"], [4486, 4487, "Method"], [4506, 4507, "Method"], [4509, 4511, "Method"], [4513, 4515, "Metric"], [4521, 4523, "Metric"], [4528, 4533, "Method"], [4546, 4548, "Method"], [4568, 4571, "Method"], [4577, 4578, "Task"], [4579, 4581, "Task"], [4587, 4589, "Method"], [4591, 4596, "Method"], [4597, 4598, "Method"], [4606, 4611, "Method"], [4613, 4614, "Method"], [4624, 4625, "Method"], [4626, 4627, "Method"], [4634, 4635, "Method"], [4636, 4638, "Task"], [4652, 4661, "Method"], [4671, 4674, "Method"], [4683, 4684, "Method"], [4688, 4689, "Method"], [4690, 4692, "Task"], [4696, 4699, "Method"], [4701, 4702, "Method"], [4720, 4725, "Method"], [4737, 4738, "Method"], [4739, 4741, "Method"], [4768, 4772, "Method"], [4775, 4776, "Method"], [4777, 4780, "Method"], [4785, 4786, "Method"], [4788, 4789, "Method"], [4796, 4798, "Method"], [4812, 4815, "Method"], [4816, 4817, "Method"], [4849, 4851, "Metric"], [4852, 4853, "Method"], [4864, 4865, "Method"], [4868, 4871, "Method"], [4872, 4873, "Method"], [4889, 4891, "Method"], [4892, 4893, "Method"], [4900, 4903, "Method"], [4905, 4908, "Method"], [4910, 4911, "Metric"], [4920, 4921, "Material"], [4922, 4925, "Material"], [4933, 4935, "Task"], [4938, 4941, "Method"], [4943, 4945, "Method"], [4952, 4954, "Method"], [4958, 4960, "Method"], [4977, 4978, "Method"], [4986, 4988, "Method"], [5005, 5006, "Method"], [5014, 5016, "Method"], [5041, 5043, "Method"], [5066, 5067, "Method"], [5074, 5075, "Method"], [5116, 5118, "Method"], [5119, 5122, "Method"], [5135, 5137, "Method"], [5157, 5158, "Method"], [5181, 5184, "Method"], [5185, 5186, "Method"], [5189, 5191, "Method"], [5192, 5193, "Method"], [5207, 5208, "Method"], [5213, 5216, "Method"], [5223, 5224, "Method"], [5225, 5226, "Method"], [5230, 5231, "Metric"], [5237, 5239, "Task"], [5240, 5241, "Material"], [5242, 5245, "Material"], [5253, 5255, "Task"], [5256, 5258, "Task"], [5263, 5264, "Method"], [5269, 5270, "Method"], [5273, 5275, "Method"], [67, 69, "Task"], [288, 289, "Task"], [681, 683, "Material"], [859, 860, "Method"], [1304, 1307, "Method"], [1308, 1309, "Method"], [1344, 1347, "Method"], [1348, 1349, "Method"], [1358, 1359, "Method"], [1377, 1378, "Method"], [1944, 1946, "Task"], [2009, 2012, "Material"], [2025, 2027, "Task"], [2049, 2052, "Material"], [2221, 2222, "Metric"], [2225, 2226, "Method"], [2227, 2228, "Method"], [2396, 2397, "Method"], [2443, 2444, "Method"], [2813, 2814, "Method"], [2998, 2999, "Method"], [3101, 3102, "Method"], [3103, 3104, "Method"], [3138, 3139, "Metric"], [3238, 3239, "Material"], [3244, 3247, "Material"], [3554, 3555, "Method"], [3565, 3566, "Method"], [3655, 3656, "Method"], [3688, 3689, "Method"], [3706, 3707, "Method"], [3737, 3739, "Task"], [3819, 3820, "Method"], [3830, 3832, "Metric"], [3885, 3886, "Method"], [3987, 3988, "Method"], [3993, 3994, "Method"], [4012, 4014, "Material"], [4071, 4073, "Material"], [4207, 4209, "Metric"], [4234, 4236, "Task"], [4341, 4343, "Task"], [4420, 4422, "Metric"], [4897, 4898, "Method"], [5038, 5040, "Task"]], "sections": [[0, 163], [163, 693], [693, 870], [870, 1276], [1276, 1930], [1930, 1997], [1997, 2074], [2074, 2212], [2212, 3249], [3249, 3302], [3302, 3520], [3520, 3764], [3764, 3997], [3997, 4060], [4060, 4139], [4139, 4197], [4197, 4560], [4560, 5176], [5176, 5281], [5281, 5284]], "sentences": [[0, 12], [12, 49], [49, 83], [83, 119], [119, 138], [138, 163], [163, 166], [166, 185], [185, 222], [222, 231], [231, 258], [258, 270], [270, 291], [291, 306], [306, 315], [315, 347], [347, 363], [363, 377], [377, 404], [404, 423], [423, 479], [479, 490], [490, 525], [525, 559], [559, 587], [587, 606], [606, 627], [627, 645], [645, 672], [672, 693], [693, 698], [698, 709], [709, 726], [726, 752], [752, 802], [802, 812], [812, 852], [852, 870], [870, 877], [877, 879], [879, 904], [904, 914], [914, 920], [920, 965], [965, 993], [993, 1014], [1014, 1027], [1027, 1059], [1059, 1098], [1098, 1118], [1118, 1135], [1135, 1137], [1137, 1170], [1170, 1199], [1199, 1207], [1207, 1244], [1244, 1252], [1252, 1276], [1276, 1284], [1284, 1297], [1297, 1311], [1311, 1338], [1338, 1373], [1373, 1387], [1387, 1413], [1413, 1445], [1445, 1462], [1462, 1470], [1470, 1508], [1508, 1518], [1518, 1532], [1532, 1546], [1546, 1573], [1573, 1597], [1597, 1616], [1616, 1626], [1626, 1635], [1635, 1654], [1654, 1674], [1674, 1684], [1684, 1693], [1693, 1698], [1698, 1717], [1717, 1724], [1724, 1763], [1763, 1806], [1806, 1822], [1822, 1856], [1856, 1882], [1882, 1914], [1914, 1930], [1930, 1936], [1936, 1948], [1948, 1980], [1980, 1997], [1997, 2000], [2000, 2029], [2029, 2039], [2039, 2042], [2042, 2054], [2054, 2062], [2062, 2074], [2074, 2077], [2077, 2100], [2100, 2111], [2111, 2123], [2123, 2146], [2146, 2171], [2171, 2186], [2186, 2201], [2201, 2212], [2212, 2215], [2215, 2233], [2233, 2247], [2247, 2271], [2271, 2306], [2306, 2325], [2325, 2362], [2362, 2398], [2398, 2422], [2422, 2435], [2435, 2449], [2449, 2465], [2465, 2476], [2476, 2494], [2494, 2516], [2516, 2529], [2529, 2547], [2547, 2573], [2573, 2594], [2594, 2611], [2611, 2629], [2629, 2645], [2645, 2668], [2668, 2692], [2692, 2706], [2706, 2734], [2734, 2763], [2763, 2795], [2795, 2822], [2822, 2842], [2842, 2858], [2858, 2877], [2877, 2902], [2902, 2916], [2916, 2943], [2943, 2970], [2970, 2988], [2988, 2995], [2995, 3021], [3021, 3042], [3042, 3053], [3053, 3075], [3075, 3093], [3093, 3127], [3127, 3155], [3155, 3169], [3169, 3199], [3199, 3225], [3225, 3249], [3249, 3255], [3255, 3278], [3278, 3302], [3302, 3305], [3305, 3316], [3316, 3342], [3342, 3364], [3364, 3407], [3407, 3422], [3422, 3451], [3451, 3472], [3472, 3484], [3484, 3520], [3520, 3526], [3526, 3545], [3545, 3576], [3576, 3592], [3592, 3602], [3602, 3622], [3622, 3634], [3634, 3650], [3650, 3673], [3673, 3685], [3685, 3717], [3717, 3746], [3746, 3750], [3750, 3764], [3764, 3767], [3767, 3779], [3779, 3811], [3811, 3823], [3823, 3836], [3836, 3862], [3862, 3883], [3883, 3886], [3886, 3892], [3892, 3917], [3917, 3928], [3928, 3945], [3945, 3962], [3962, 3976], [3976, 3988], [3988, 3997], [3997, 4003], [4003, 4044], [4044, 4060], [4060, 4063], [4063, 4075], [4075, 4094], [4094, 4126], [4126, 4139], [4139, 4142], [4142, 4160], [4160, 4180], [4180, 4197], [4197, 4200], [4200, 4215], [4215, 4272], [4272, 4294], [4294, 4316], [4316, 4333], [4333, 4351], [4351, 4377], [4377, 4395], [4395, 4426], [4426, 4452], [4452, 4464], [4464, 4488], [4488, 4508], [4508, 4516], [4516, 4534], [4534, 4560], [4560, 4564], [4564, 4572], [4572, 4600], [4600, 4622], [4622, 4628], [4628, 4648], [4648, 4662], [4662, 4675], [4675, 4677], [4677, 4680], [4680, 4712], [4712, 4726], [4726, 4737], [4737, 4760], [4760, 4762], [4762, 4794], [4794, 4846], [4846, 4885], [4885, 4904], [4904, 4926], [4926, 4936], [4936, 4950], [4950, 4961], [4961, 4984], [4984, 5017], [5017, 5031], [5031, 5052], [5052, 5068], [5068, 5092], [5092, 5107], [5107, 5109], [5109, 5123], [5123, 5125], [5125, 5148], [5148, 5176], [5176, 5179], [5179, 5207], [5207, 5223], [5223, 5246], [5246, 5259], [5259, 5281], [5281, 5284]], "words": ["document", ":", "Direct", "Output", "Connection", "for", "a", "High", "-", "Rank", "Language", "Model", "This", "paper", "proposes", "a", "state", "-", "of", "-", "the", "-", "art", "recurrent", "neural", "network", "(", "RNN", ")", "language", "model", "that", "combines", "probability", "distributions", "computed", "not", "only", "from", "a", "final", "RNN", "layer", "but", "also", "from", "middle", "layers", ".", "Our", "proposed", "method", "raises", "the", "expressive", "power", "of", "a", "language", "model", "based", "on", "the", "matrix", "factorization", "interpretation", "of", "language", "modeling", "introduced", "by", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", ".", "The", "proposed", "method", "improves", "the", "current", "state", "-", "of", "-", "the", "-", "art", "language", "model", "and", "achieves", "the", "best", "score", "on", "the", "Penn", "Treebank", "and", "WikiText", "-", "2", ",", "which", "are", "the", "standard", "benchmark", "datasets", ".", "Moreover", ",", "we", "indicate", "our", "proposed", "method", "contributes", "to", "two", "application", "tasks", ":", "machine", "translation", "and", "headline", "generation", ".", "Our", "code", "is", "publicly", "available", "at", ":", "https:", "//", "github.com", "/", "nttcslab", "-", "nlp", "/", "doc_lmhttps:", "//", "github.com", "/", "nttcslab", "-", "nlp", "/", "doc_lm", ".", "section", ":", "Introduction", "Neural", "network", "language", "models", "have", "played", "a", "central", "role", "in", "recent", "natural", "language", "processing", "(", "NLP", ")", "advances", ".", "For", "example", ",", "neural", "encoder", "-", "decoder", "models", ",", "which", "were", "successfully", "applied", "to", "various", "natural", "language", "generation", "tasks", "including", "machine", "translation", ",", "summarization", ",", "and", "dialogue", ",", "can", "be", "interpreted", "as", "conditional", "neural", "language", "models", ".", "Neural", "language", "models", "also", "positively", "influence", "syntactic", "parsing", ".", "Moreover", ",", "such", "word", "embedding", "methods", "as", "Skip", "-", "gram", "and", "vLBL", "originated", "from", "neural", "language", "models", "designed", "to", "handle", "much", "larger", "vocabulary", "and", "data", "sizes", ".", "Neural", "language", "models", "can", "also", "be", "used", "as", "contextualized", "word", "representations", ".", "Thus", ",", "language", "modeling", "is", "a", "good", "benchmark", "task", "for", "investigating", "the", "general", "frameworks", "of", "neural", "methods", "in", "NLP", "field", ".", "In", "language", "modeling", ",", "we", "compute", "joint", "probability", "using", "the", "product", "of", "conditional", "probabilities", ".", "Let", "be", "a", "word", "sequence", "with", "length", ":", ".", "We", "obtain", "the", "joint", "probability", "of", "word", "sequence", "as", "follows", ":", "is", "generally", "assumed", "to", "be", "in", "this", "literature", ",", "that", "is", ",", ",", "and", "thus", "we", "can", "ignore", "its", "calculation", ".", "See", "the", "implementation", "of", "DBLP", ":", "journals", "/", "corr", "/", "ZarembaSV14", ",", "for", "an", "example", ".", "RNN", "language", "models", "obtain", "conditional", "probability", "from", "the", "probability", "distribution", "of", "each", "word", ".", "To", "compute", "the", "probability", "distribution", ",", "RNN", "language", "models", "encode", "sequence", "into", "a", "fixed", "-", "length", "vector", "and", "apply", "a", "transformation", "matrix", "and", "the", "softmax", "function", ".", "Previous", "researches", "demonstrated", "that", "RNN", "language", "models", "achieve", "high", "performance", "by", "using", "several", "regularizations", "and", "selecting", "appropriate", "hyperparameters", ".", "However", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "proved", "that", "existing", "RNN", "language", "models", "have", "low", "expressive", "power", "due", "to", "the", "Softmax", "bottleneck", ",", "which", "means", "the", "output", "matrix", "of", "RNN", "language", "models", "is", "low", "rank", "when", "we", "interpret", "the", "training", "of", "RNN", "language", "models", "as", "a", "matrix", "factorization", "problem", ".", "To", "solve", "the", "Softmax", "bottleneck", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "proposed", "Mixture", "of", "Softmaxes", "(", "MoS", ")", ",", "which", "increases", "the", "rank", "of", "the", "matrix", "by", "combining", "multiple", "probability", "distributions", "computed", "from", "the", "encoded", "fixed", "-", "length", "vector", ".", "In", "this", "study", ",", "we", "propose", "Direct", "Output", "Connection", "(", "DOC", ")", "as", "a", "generalization", "of", "MoS.", "For", "stacked", "RNNs", ",", "DOC", "computes", "the", "probability", "distributions", "from", "the", "middle", "layers", "including", "input", "embeddings", ".", "In", "addition", "to", "raising", "the", "rank", ",", "the", "proposed", "method", "helps", "weaken", "the", "vanishing", "gradient", "problem", "in", "backpropagation", "because", "DOC", "provides", "a", "shortcut", "connection", "to", "the", "output", ".", "We", "conduct", "experiments", "on", "standard", "benchmark", "datasets", "for", "language", "modeling", ":", "the", "Penn", "Treebank", "and", "WikiText", "-", "2", ".", "Our", "experiments", "demonstrate", "that", "DOC", "outperforms", "MoS", "and", "achieves", "state", "-", "of", "-", "the", "-", "art", "perplexities", "on", "each", "dataset", ".", "Moreover", ",", "we", "investigate", "the", "effect", "of", "DOC", "on", "two", "applications", ":", "machine", "translation", "and", "headline", "generation", ".", "We", "indicate", "that", "DOC", "can", "improve", "the", "performance", "of", "an", "encoder", "-", "decoder", "with", "an", "attention", "mechanism", ",", "which", "is", "a", "strong", "baseline", "for", "such", "applications", ".", "In", "addition", ",", "we", "conduct", "an", "experiment", "on", "the", "Penn", "Treebank", "constituency", "parsing", "task", "to", "investigate", "the", "effectiveness", "of", "DOC", ".", "section", ":", "RNN", "Language", "Model", "In", "this", "section", ",", "we", "briefly", "overview", "RNN", "language", "models", ".", "Let", "be", "the", "vocabulary", "size", "and", "let", "be", "the", "probability", "distribution", "of", "the", "vocabulary", "at", "timestep", ".", "Moreover", ",", "let", "be", "the", "dimension", "of", "the", "hidden", "state", "of", "the", "-", "th", "RNN", ",", "and", "let", "be", "the", "dimensions", "of", "the", "embedding", "vectors", ".", "Then", "the", "RNN", "language", "models", "predict", "probability", "distribution", "by", "the", "following", "equation", ":", "where", "is", "a", "weight", "matrix", ",", "is", "a", "word", "embedding", "matrix", ",", "is", "a", "one", "-", "hot", "vector", "of", "input", "word", "at", "timestep", ",", "and", "is", "the", "hidden", "state", "of", "the", "-", "th", "RNN", "at", "timestep", ".", "We", "define", "at", "timestep", "as", "a", "zero", "vector", ":", ".", "Let", "represent", "an", "abstract", "function", "of", "an", "RNN", ",", "which", "might", "be", "the", "Elman", "network", ",", "the", "Long", "Short", "-", "Term", "Memory", "(", "LSTM", ")", ",", "the", "Recurrent", "Highway", "Network", "(", "RHN", ")", ",", "or", "any", "other", "RNN", "variant", ".", "In", "this", "research", ",", "we", "stack", "three", "LSTM", "layers", "based", "on", "merityRegOpt", "because", "they", "achieved", "high", "performance", ".", "section", ":", "Language", "Modeling", "as", "Matrix", "Factorization", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "indicated", "that", "the", "training", "of", "language", "models", "can", "be", "interpreted", "as", "a", "matrix", "factorization", "problem", ".", "In", "this", "section", ",", "we", "briefly", "introduce", "their", "description", ".", "Let", "word", "sequence", "be", "context", ".", "Then", "we", "can", "regard", "a", "natural", "language", "as", "a", "finite", "set", "of", "the", "pairs", "of", "a", "context", "and", "its", "conditional", "probability", "distribution", ":", ",", "where", "is", "the", "number", "of", "possible", "contexts", "and", "is", "a", "variable", "representing", "a", "one", "-", "hot", "vector", "of", "a", "word", ".", "Here", ",", "we", "consider", "matrix", "that", "represents", "the", "true", "log", "probability", "distributions", "and", "matrix", "that", "contains", "the", "hidden", "states", "of", "the", "final", "RNN", "layer", "for", "each", "context", ":", "Then", "we", "obtain", "set", "of", "matrices", ",", "where", "is", "an", "all", "-", "ones", "matrix", ",", "and", "is", "a", "diagonal", "matrix", ".", "contains", "matrices", "that", "shifted", "each", "row", "of", "by", "an", "arbitrary", "real", "number", ".", "In", "other", "words", ",", "if", "we", "take", "a", "matrix", "from", "and", "apply", "the", "softmax", "function", "to", "each", "of", "its", "rows", ",", "we", "obtain", "a", "matrix", "that", "consists", "of", "true", "probability", "distributions", ".", "Therefore", ",", "for", "some", ",", "training", "RNN", "language", "models", "is", "to", "find", "the", "parameters", "satisfying", "the", "following", "equation", ":", "Equation", "[", "reference", "]", "indicates", "that", "training", "RNN", "language", "models", "can", "also", "be", "interpreted", "as", "a", "matrix", "factorization", "problem", ".", "In", "most", "cases", ",", "the", "rank", "of", "matrix", "is", "because", "is", "smaller", "than", "and", "in", "common", "RNN", "language", "models", ".", "Thus", ",", "an", "RNN", "language", "model", "can", "not", "express", "true", "distributions", "if", "is", "much", "smaller", "than", ".", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "also", "argued", "that", "is", "as", "high", "as", "vocabulary", "size", "based", "on", "the", "following", "two", "assumptions", ":", "Natural", "language", "is", "highly", "context", "-", "dependent", ".", "In", "addition", ",", "since", "we", "can", "imagine", "many", "kinds", "of", "contexts", ",", "it", "is", "difficult", "to", "assume", "a", "basis", "that", "represents", "a", "conditional", "probability", "distribution", "for", "any", "contexts", ".", "In", "other", "words", ",", "compressing", "is", "difficult", ".", "Since", "we", "also", "have", "many", "kinds", "of", "semantic", "meanings", ",", "it", "is", "difficult", "to", "assume", "basic", "meanings", "that", "can", "create", "all", "other", "semantic", "meanings", "by", "such", "simple", "operations", "as", "addition", "and", "subtraction", ";", "compressing", "is", "difficult", ".", "In", "summary", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "indicated", "that", "is", "much", "smaller", "than", "because", "its", "scale", "is", "usually", "and", "vocabulary", "size", "is", "at", "least", ".", "section", ":", "Proposed", "Method", ":", "Direct", "Output", "Connection", "To", "construct", "a", "high", "-", "rank", "matrix", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "proposed", "Mixture", "of", "Softmaxes", "(", "MoS", ")", ".", "MoS", "computes", "multiple", "probability", "distributions", "from", "the", "hidden", "state", "of", "final", "RNN", "layer", "and", "regards", "the", "weighted", "average", "of", "the", "probability", "distributions", "as", "the", "final", "distribution", ".", "In", "this", "study", ",", "we", "propose", "Direct", "Output", "Connection", "(", "DOC", ")", ",", "which", "is", "a", "generalization", "method", "of", "MoS.", "DOC", "computes", "probability", "distributions", "from", "the", "middle", "layers", "in", "addition", "to", "the", "final", "layer", ".", "In", "other", "words", ",", "DOC", "directly", "connects", "the", "middle", "layers", "to", "the", "output", ".", "Figure", "[", "reference", "]", "shows", "an", "overview", "of", "DOC", ",", "that", "uses", "the", "middle", "layers", "(", "including", "word", "embeddings", ")", "to", "compute", "the", "probability", "distributions", ".", "Figure", "[", "reference", "]", "computes", "three", "probability", "distributions", "from", "all", "the", "layers", ",", "but", "we", "can", "vary", "the", "number", "of", "probability", "distributions", "for", "each", "layer", "and", "select", "some", "layers", "to", "avoid", ".", "In", "our", "experiments", ",", "we", "search", "for", "the", "appropriate", "number", "of", "probability", "distributions", "for", "each", "layer", ".", "Formally", ",", "instead", "of", "Equation", "[", "reference", "]", ",", "DOC", "computes", "the", "output", "probability", "distribution", "at", "timestep", "by", "the", "following", "equation", ":", "where", "is", "a", "weight", "for", "each", "probability", "distribution", ",", "is", "a", "vector", "computed", "from", "each", "hidden", "state", ",", "and", "is", "a", "weight", "matrix", ".", "Thus", ",", "is", "the", "weighted", "average", "of", "probability", "distributions", ".", "We", "define", "the", "diagonal", "matrix", "whose", "elements", "are", "weight", "for", "each", "context", "as", ".", "Then", "we", "obtain", "matrix", ":", "where", "is", "a", "matrix", "whose", "rows", "are", "vector", ".", "can", "be", "an", "arbitrary", "high", "rank", "because", "the", "righthand", "side", "of", "Equation", "[", "reference", "]", "computes", "not", "only", "the", "matrix", "multiplication", "but", "also", "a", "nonlinear", "function", ".", "Therefore", ",", "an", "RNN", "language", "model", "with", "DOC", "can", "output", "a", "distribution", "matrix", "whose", "rank", "is", "identical", "to", "one", "of", "the", "true", "distributions", ".", "In", "other", "words", ",", "is", "a", "better", "approximation", "of", "than", "the", "output", "of", "a", "standard", "RNN", "language", "model", ".", "Next", "we", "describe", "how", "to", "acquire", "weight", "and", "vector", ".", "Let", "be", "a", "vector", "whose", "elements", "are", "weight", ".", "Then", "we", "compute", "from", "the", "hidden", "state", "of", "the", "final", "RNN", "layer", ":", "where", "is", "a", "weight", "matrix", ".", "We", "next", "compute", "from", "the", "hidden", "state", "of", "the", "-", "th", "RNN", "layer", ":", "where", "is", "a", "weight", "matrix", ".", "In", "addition", ",", "let", "be", "the", "number", "of", "from", ".", "Then", "we", "define", "the", "sum", "of", "for", "all", "as", ";", "that", "is", ",", ".", "In", "short", ",", "DOC", "computes", "probability", "distributions", "from", "all", "the", "layers", ",", "including", "the", "input", "embedding", "(", ")", ".", "For", ",", "DOC", "becomes", "identical", "to", "MoS.", "In", "addition", "to", "increasing", "the", "rank", ",", "we", "expect", "that", "DOC", "weakens", "the", "vanishing", "gradient", "problem", "during", "backpropagation", "because", "a", "middle", "layer", "is", "directly", "connected", "to", "the", "output", ",", "such", "as", "with", "the", "auxiliary", "classifiers", "described", "in", "43022", ".", "For", "a", "network", "that", "computes", "the", "weights", "for", "several", "vectors", ",", "such", "as", "Equation", "[", "reference", "]", ",", "DBLP", ":", "journals", "/", "corr", "/", "ShazeerMMDLHD17", "indicated", "that", "it", "often", "converges", "to", "a", "state", "where", "it", "always", "produces", "large", "weights", "for", "few", "vectors", ".", "In", "fact", ",", "we", "observed", "that", "DOC", "tends", "to", "assign", "large", "weights", "to", "shallow", "layers", ".", "To", "prevent", "this", "phenomenon", ",", "we", "compute", "the", "coefficient", "of", "variation", "of", "Equation", "[", "reference", "]", "in", "each", "mini", "-", "batch", "as", "a", "regularization", "term", "following", "DBLP", ":", "journals", "/", "corr", "/", "ShazeerMMDLHD17", ".", "In", "other", "words", ",", "we", "try", "to", "adjust", "the", "sum", "of", "the", "weights", "for", "each", "probability", "distribution", "with", "identical", "values", "in", "each", "mini", "-", "batch", ".", "Formally", ",", "we", "compute", "the", "following", "equation", "for", "a", "mini", "-", "batch", "consisting", "of", ":", "where", "functions", "and", "are", "functions", "that", "respectively", "return", "an", "input", "\u2019s", "standard", "deviation", "and", "its", "average", ".", "In", "the", "training", "step", ",", "we", "add", "multiplied", "by", "weight", "coefficient", "to", "the", "loss", "function", ".", "section", ":", "Experiments", "on", "Language", "Modeling", "We", "investigate", "the", "effect", "of", "DOC", "on", "the", "language", "modeling", "task", ".", "In", "detail", ",", "we", "conduct", "word", "-", "level", "prediction", "experiments", "and", "show", "that", "DOC", "improves", "the", "performance", "of", "MoS", ",", "which", "only", "uses", "the", "final", "layer", "to", "compute", "the", "probability", "distributions", ".", "Moreover", ",", "we", "evaluate", "various", "combinations", "of", "layers", "to", "explore", "which", "combination", "achieves", "the", "best", "score", ".", "subsection", ":", "Datasets", "We", "used", "the", "Penn", "Treebank", "(", "PTB", ")", "and", "WikiText", "-", "2", "datasets", ",", "which", "are", "the", "standard", "benchmark", "datasets", "for", "the", "word", "-", "level", "language", "modeling", "task", ".", "DBLP", ":", "conf", "/", "interspeech", "/", "MikolovKBCK10", "and", "DBLP", ":", "journals", "/", "corr", "/", "MerityXBS16", "respectively", "published", "preprocessed", "PTB", "and", "WikiText", "-", "2", "datasets", ".", "Table", "[", "reference", "]", "describes", "their", "statistics", ".", "We", "used", "these", "preprocessed", "datasets", "for", "fair", "comparisons", "with", "previous", "studies", ".", "subsection", ":", "Hyperparameters", "Our", "implementation", "is", "based", "on", "the", "averaged", "stochastic", "gradient", "descent", "Weight", "-", "Dropped", "LSTM", "(", "AWD", "-", "LSTM", ")", "proposed", "by", "merityRegOpt", ".", "AWD", "-", "LSTM", "consists", "of", "three", "LSTMs", "with", "various", "regularizations", ".", "For", "the", "hyperparameters", ",", "we", "used", "the", "same", "values", "as", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "except", "for", "the", "dropout", "rate", "for", "vector", "and", "the", "non", "-", "monotone", "interval", ".", "Since", "we", "found", "that", "the", "dropout", "rate", "for", "vector", "greatly", "influences", "in", "Equation", "[", "reference", "]", ",", "we", "varied", "it", "from", "to", "with", "intervals", ".", "We", "selected", "because", "this", "value", "achieved", "the", "best", "score", "on", "the", "PTB", "validation", "dataset", ".", "For", "the", "non", "-", "monotone", "interval", ",", "we", "adopted", "the", "same", "value", "as", "fraternal", ".", "Table", "[", "reference", "]", "summarizes", "the", "hyperparameters", "of", "our", "experiments", ".", "subsection", ":", "Results", "Table", "[", "reference", "]", "shows", "the", "perplexities", "of", "AWD", "-", "LSTM", "with", "DOC", "on", "the", "PTB", "dataset", ".", "Each", "value", "of", "columns", "represents", "the", "number", "of", "probability", "distributions", "from", "hidden", "state", ".", "To", "find", "the", "best", "combination", ",", "we", "varied", "the", "number", "of", "probability", "distributions", "from", "each", "layer", "by", "fixing", "their", "total", "to", "20", ":", ".", "Moreover", ",", "the", "top", "row", "of", "Table", "[", "reference", "]", "shows", "the", "perplexity", "of", "AWD", "-", "LSTM", "with", "MoS", "reported", "in", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "for", "comparison", ".", "Table", "[", "reference", "]", "indicates", "that", "language", "models", "using", "middle", "layers", "outperformed", "one", "using", "only", "the", "final", "layer", ".", "In", "addition", ",", "Table", "[", "reference", "]", "shows", "that", "increasing", "the", "distributions", "from", "the", "final", "layer", "(", ")", "degraded", "the", "score", "from", "the", "language", "model", "with", "(", "the", "top", "row", "of", "Table", "[", "reference", "]", ")", ".", "Thus", ",", "to", "obtain", "a", "superior", "language", "model", ",", "we", "should", "not", "increase", "the", "number", "of", "distributions", "from", "the", "final", "layer", ";", "we", "should", "instead", "use", "the", "middle", "layers", ",", "as", "with", "our", "proposed", "DOC", ".", "Table", "[", "reference", "]", "shows", "that", "the", "setting", "achieved", "the", "best", "performance", "and", "the", "other", "settings", "with", "shallow", "layers", "have", "a", "little", "effect", ".", "This", "result", "implies", "that", "we", "need", "some", "layers", "to", "output", "accurate", "distributions", ".", "In", "fact", ",", "most", "previous", "studies", "adopted", "two", "LSTM", "layers", "for", "language", "modeling", ".", "This", "suggests", "that", "we", "need", "at", "least", "two", "layers", "to", "obtain", "high", "-", "quality", "distributions", ".", "For", "the", "setting", ",", "we", "explored", "the", "effect", "of", "in", ".", "Although", "Table", "[", "reference", "]", "shows", "that", "achieved", "the", "best", "perplexity", ",", "the", "effect", "is", "not", "consistent", ".", "Table", "[", "reference", "]", "shows", "the", "coefficient", "of", "variation", "of", "Equation", "[", "reference", "]", ",", "i.e.", ",", "in", "the", "PTB", "dataset", ".", "This", "table", "demonstrates", "that", "the", "coefficient", "of", "variation", "decreases", "with", "growth", "in", ".", "In", "other", "words", ",", "the", "model", "trained", "with", "a", "large", "assigns", "balanced", "weights", "to", "each", "probability", "distribution", ".", "These", "results", "indicate", "that", "it", "is", "not", "always", "necessary", "to", "equally", "use", "each", "probability", "distribution", ",", "but", "we", "can", "acquire", "a", "better", "model", "in", "some", ".", "Hereafter", ",", "we", "refer", "to", "the", "setting", "that", "achieved", "the", "best", "score", "(", ")", "as", "AWD", "-", "LSTM", "-", "DOC", ".", "Table", "[", "reference", "]", "shows", "the", "ranks", "of", "matrices", "containing", "log", "probability", "distributions", "from", "each", "method", ".", "In", "other", "words", ",", "Table", "[", "reference", "]", "describes", "in", "Equation", "[", "reference", "]", "for", "each", "method", ".", "As", "shown", "by", "this", "table", ",", "the", "output", "of", "AWD", "-", "LSTM", "is", "restricted", "to", ".", "In", "contrast", ",", "AWD", "-", "LSTM", "-", "MoS", "and", "AWD", "-", "LSTM", "-", "DOC", "outputted", "matrices", "whose", "ranks", "equal", "the", "vocabulary", "size", ".", "This", "fact", "indicates", "that", "DOC", "(", "including", "MoS", ")", "can", "output", "the", "same", "matrix", "as", "the", "true", "distributions", "in", "view", "of", "a", "rank", ".", "Figure", "[", "reference", "]", "illustrates", "the", "learning", "curves", "of", "each", "method", "on", "PTB", ".", "This", "figure", "contains", "the", "validation", "scores", "of", "AWD", "-", "LSTM", ",", "AWD", "-", "LSTM", "-", "MoS", ",", "and", "AWD", "-", "LSTM", "-", "DOC", "at", "each", "training", "epoch", ".", "We", "trained", "AWD", "-", "LSTM", "and", "AWD", "-", "LSTM", "-", "MoS", "by", "setting", "the", "non", "-", "monotone", "interval", "to", "60", ",", "as", "with", "AWD", "-", "LSTM", "-", "DOC", ".", "In", "other", "words", ",", "we", "used", "hyperparameters", "identical", "to", "the", "original", "ones", "to", "train", "AWD", "-", "LSTM", "and", "AWD", "-", "LSTM", "-", "MoS", ",", "except", "for", "the", "non", "-", "monotone", "interval", ".", "We", "note", "that", "the", "optimization", "method", "converts", "the", "ordinary", "stochastic", "gradient", "descent", "(", "SGD", ")", "into", "the", "averaged", "SGD", "at", "the", "point", "where", "convergence", "almost", "occurs", ".", "In", "Figure", "[", "reference", "]", ",", "the", "turning", "point", "is", "the", "epoch", "when", "each", "method", "drastically", "decreases", "the", "perplexity", ".", "Figure", "[", "reference", "]", "shows", "that", "each", "method", "similarly", "reduces", "the", "perplexity", "at", "the", "beginning", ".", "AWD", "-", "LSTM", "and", "AWD", "-", "LSTM", "-", "MoS", "were", "slow", "to", "decrease", "the", "perplexity", "from", "50", "epochs", ".", "In", "contrast", ",", "AWD", "-", "LSTM", "-", "DOC", "constantly", "decreased", "the", "perplexity", "and", "achieved", "a", "lower", "value", "than", "the", "other", "methods", "with", "ordinary", "SGD", ".", "Therefore", ",", "we", "conclude", "that", "DOC", "positively", "affects", "the", "training", "of", "language", "modeling", ".", "Table", "[", "reference", "]", "shows", "the", "AWD", "-", "LSTM", ",", "AWD", "-", "LSTM", "-", "MoS", ",", "and", "AWD", "-", "LSTM", "-", "DOC", "results", "in", "our", "configurations", ".", "For", "AWD", "-", "LSTM", "-", "MoS", ",", "we", "trained", "our", "implementation", "with", "the", "same", "dropout", "rates", "as", "AWD", "-", "LSTM", "-", "DOC", "for", "a", "fair", "comparison", ".", "AWD", "-", "LSTM", "-", "DOC", "outperformed", "both", "the", "original", "AWD", "-", "LSTM", "-", "MoS", "and", "our", "implementation", ".", "In", "other", "words", ",", "DOC", "outperformed", "MoS.", "Since", "the", "averaged", "SGD", "uses", "the", "averaged", "parameters", "from", "each", "update", "step", ",", "the", "parameters", "of", "the", "early", "steps", "are", "harmful", "to", "the", "final", "parameters", ".", "Therefore", ",", "when", "the", "model", "converges", ",", "recent", "studies", "and", "ours", "eliminate", "the", "history", "of", "and", "then", "retrains", "the", "model", ".", "merityRegOpt", "referred", "to", "this", "retraining", "process", "as", "fine", "-", "tuning", ".", "Although", "most", "previous", "studies", "only", "conducted", "fine", "-", "tuning", "once", ",", "fraternal", "argued", "that", "two", "fine", "-", "tunings", "provided", "additional", "improvement", ".", "Thus", ",", "we", "repeated", "fine", "-", "tuning", "until", "we", "achieved", "no", "more", "improvements", "in", "the", "validation", "data", ".", "We", "refer", "to", "the", "model", "as", "AWD", "-", "LSTM", "-", "DOC", "(", "fin", ")", "in", "Table", "[", "reference", "]", ",", "which", "shows", "that", "repeated", "fine", "-", "tunings", "improved", "the", "perplexity", "by", "about", "0.5", ".", "Tables", "[", "reference", "]", "and", "[", "reference", "]", "respectively", "show", "the", "perplexities", "of", "AWD", "-", "LSTM", "-", "DOC", "and", "previous", "studies", "on", "PTB", "and", "WikiText", "-", "2", ".", "These", "tables", "show", "that", "AWD", "-", "LSTM", "-", "DOC", "achieved", "the", "best", "perplexity", ".", "AWD", "-", "LSTM", "-", "DOC", "improved", "the", "perplexity", "by", "almost", "2.0", "on", "PTB", "and", "3.5", "on", "WikiText", "-", "2", "from", "the", "state", "-", "of", "-", "the", "-", "art", "scores", ".", "The", "ensemble", "technique", "provided", "further", "improvement", ",", "as", "described", "in", "previous", "studies", ",", "and", "improved", "the", "perplexity", "by", "at", "least", "4", "points", "on", "both", "datasets", ".", "Finally", ",", "the", "ensemble", "of", "the", "repeated", "finetuning", "models", "achieved", "47.17", "on", "the", "PTB", "test", "and", "53.09", "on", "the", "WikiText", "-", "2", "test", ".", "section", ":", "Experiments", "on", "Application", "Tasks", "As", "described", "in", "Section", "[", "reference", "]", ",", "a", "neural", "encoder", "-", "decoder", "model", "can", "be", "interpreted", "as", "a", "conditional", "language", "model", ".", "To", "investigate", "the", "effect", "of", "DOC", "on", "an", "encoder", "-", "decoder", "model", ",", "we", "incorporate", "DOC", "into", "the", "decoder", "and", "examine", "its", "performance", ".", "subsection", ":", "Dataset", "We", "conducted", "experiments", "on", "machine", "translation", "and", "headline", "generation", "tasks", ".", "For", "machine", "translation", ",", "we", "used", "two", "kinds", "of", "sentence", "pairs", "(", "English", "-", "German", "and", "English", "-", "French", ")", "in", "the", "IWSLT", "2016", "dataset", ".", "The", "training", "set", "respectively", "contains", "about", "189", "K", "and", "208", "K", "sentence", "pairs", "of", "English", "-", "German", "and", "English", "-", "French", ".", "We", "experimented", "in", "four", "settings", ":", "from", "English", "to", "German", "(", "En", "-", "De", ")", ",", "its", "reverse", "(", "De", "-", "En", ")", ",", "from", "English", "to", "French", "(", "En", "-", "Fr", ")", ",", "and", "its", "reverse", "(", "Fr", "-", "En", ")", ".", "Headline", "generation", "is", "a", "task", "that", "creates", "a", "short", "summarization", "of", "an", "input", "sentence", ".", "rush", "-", "chopra", "-", "weston:2015:EMNLP", "constructed", "a", "headline", "generation", "dataset", "by", "extracting", "pairs", "of", "first", "sentences", "of", "news", "articles", "and", "their", "headlines", "from", "the", "annotated", "English", "Gigaword", "corpus", ".", "They", "also", "divided", "the", "extracted", "sentence", "-", "headline", "pairs", "into", "three", "parts", ":", "training", ",", "validation", ",", "and", "test", "sets", ".", "The", "training", "set", "contains", "about", "3.8", "M", "sentence", "-", "headline", "pairs", ".", "For", "our", "evaluation", ",", "we", "used", "the", "test", "set", "constructed", "by", "zhou", "-", "EtAl:2017:Long", "because", "the", "one", "constructed", "by", "rush", "-", "chopra", "-", "weston:2015:EMNLP", "contains", "some", "invalid", "instances", ",", "as", "reported", "in", "zhou", "-", "EtAl:2017:Long", ".", "subsection", ":", "Encoder", "-", "Decoder", "Model", "For", "the", "base", "model", ",", "we", "adopted", "an", "encoder", "-", "decoder", "with", "an", "attention", "mechanism", "described", "in", "kiyono", ".", "The", "encoder", "consists", "of", "a", "2", "-", "layer", "bidirectional", "LSTM", ",", "and", "the", "decoder", "consists", "of", "a", "2", "-", "layer", "LSTM", "with", "attention", "proposed", "by", "luong", "-", "pham", "-", "manning:2015:EMNLP", ".", "We", "interpreted", "the", "layer", "after", "computing", "the", "attention", "as", "the", "3rd", "layer", "of", "the", "decoder", ".", "We", "refer", "to", "this", "encoder", "-", "decoder", "as", "EncDec", ".", "For", "the", "hyperparameters", ",", "we", "followed", "the", "setting", "of", "kiyono", "except", "for", "the", "sizes", "of", "hidden", "states", "and", "embeddings", ".", "We", "used", "500", "for", "machine", "translation", "and", "400", "for", "headline", "generation", ".", "We", "constructed", "a", "vocabulary", "set", "by", "using", "Byte", "-", "Pair", "-", "Encoding", "(", "BPE", ")", ".", "We", "set", "the", "number", "of", "BPE", "merge", "operations", "at", "16", "K", "for", "the", "machine", "translation", "and", "5", "K", "for", "the", "headline", "generation", ".", "In", "this", "experiment", ",", "we", "compare", "DOC", "to", "the", "base", "EncDec", ".", "We", "prepared", "two", "DOC", "settings", ":", "using", "only", "the", "final", "layer", ",", "that", "is", ",", "a", "setting", "that", "is", "identical", "to", "MoS", ",", "and", "using", "both", "the", "final", "and", "middle", "layers", ".", "We", "used", "the", "2nd", "and", "3rd", "layers", "in", "the", "latter", "setting", "because", "this", "case", "achieved", "the", "best", "performance", "on", "the", "language", "modeling", "task", "in", "Section", "[", "reference", "]", ".", "We", "set", "and", ".", "For", "this", "experiment", ",", "we", "modified", "a", "publicly", "available", "encode", "-", "decoder", "implementation", ".", "subsection", ":", "Results", "Table", "[", "reference", "]", "shows", "the", "BLEU", "scores", "of", "each", "method", ".", "Since", "an", "initial", "value", "often", "drastically", "varies", "the", "result", "of", "a", "neural", "encoder", "-", "decoder", ",", "we", "reported", "the", "average", "of", "three", "models", "trained", "from", "different", "initial", "values", "and", "random", "seeds", ".", "Table", "[", "reference", "]", "indicates", "that", "EncDec", "+", "DOC", "outperformed", "EncDec", ".", "Table", "[", "reference", "]", "shows", "the", "ROUGE", "F1", "scores", "of", "each", "method", ".", "In", "addition", "to", "the", "results", "of", "our", "implementations", "(", "the", "upper", "part", ")", ",", "the", "lower", "part", "represents", "the", "published", "scores", "reported", "in", "previous", "studies", ".", "For", "the", "upper", "part", ",", "we", "reported", "the", "average", "of", "three", "models", "(", "as", "in", "Table", "[", "reference", "]", ")", ".", "EncDec", "+", "DOC", "outperformed", "EncDec", "on", "all", "scores", ".", "Moreover", ",", "EncDec", "outperformed", "the", "state", "-", "of", "-", "the", "-", "art", "method", "on", "the", "ROUGE", "-", "2", "and", "ROUGE", "-", "L", "F1", "scores", ".", "In", "other", "words", ",", "our", "baseline", "is", "already", "very", "strong", ".", "We", "believe", "that", "this", "is", "because", "we", "adopted", "a", "larger", "embedding", "size", "than", "zhou", "-", "EtAl:2017:Long", ".", "It", "is", "noteworthy", "that", "DOC", "improved", "the", "performance", "of", "EncDec", "even", "though", "EncDec", "is", "very", "strong", ".", "These", "results", "indicate", "that", "DOC", "positively", "influences", "a", "neural", "encoder", "-", "decoder", "model", ".", "Using", "the", "middle", "layer", "also", "yields", "further", "improvement", "because", "EncDec", "+", "DOC", "(", ")", "outperformed", "EncDec", "+", "DOC", "(", ")", ".", "section", ":", "Experiments", "on", "Constituency", "Parsing", "choe", "-", "charniak:2016:EMNLP2016", "achieved", "high", "F1", "scores", "on", "the", "Penn", "Treebank", "constituency", "parsing", "task", "by", "transforming", "candidate", "trees", "into", "a", "symbol", "sequence", "(", "S", "-", "expression", ")", "and", "reranking", "them", "based", "on", "the", "perplexity", "obtained", "by", "a", "neural", "language", "model", ".", "To", "investigate", "the", "effectiveness", "of", "DOC", ",", "we", "evaluate", "our", "language", "models", "following", "their", "configurations", ".", "subsection", ":", "Dataset", "We", "used", "the", "Wall", "Street", "Journal", "of", "the", "Penn", "Treebank", "dataset", ".", "We", "used", "the", "section", "2", "-", "21", "for", "training", ",", "22", "for", "validation", ",", "and", "23", "for", "testing", ".", "We", "applied", "the", "preprocessing", "codes", "of", "choe", "-", "charniak:2016:EMNLP2016", "to", "the", "dataset", "and", "converted", "a", "token", "that", "appears", "fewer", "than", "ten", "times", "in", "the", "training", "dataset", "into", "a", "special", "token", "unk", ".", "For", "reranking", ",", "we", "prepared", "500", "candidates", "obtained", "by", "the", "Charniak", "parser", ".", "subsection", ":", "Models", "We", "compare", "AWD", "-", "LSTM", "-", "DOC", "with", "AWD", "-", "LSTM", "and", "AWD", "-", "LSTM", "-", "MoS", ".", "We", "trained", "each", "model", "with", "the", "same", "hyperparameters", "from", "our", "language", "modeling", "experiments", "(", "Section", "[", "reference", "]", ")", ".", "We", "selected", "the", "model", "that", "achieved", "the", "best", "perplexity", "on", "the", "validation", "set", "during", "the", "training", ".", "subsection", ":", "Results", "Table", "[", "reference", "]", "shows", "the", "bracketing", "F1", "scores", "on", "the", "PTB", "test", "set", ".", "This", "table", "is", "divided", "into", "three", "parts", "by", "horizontal", "lines", ";", "the", "upper", "part", "describes", "the", "scores", "by", "single", "language", "modeling", "based", "rerankers", ",", "the", "middle", "part", "shows", "the", "results", "by", "ensembling", "five", "rerankers", ",", "and", "the", "lower", "part", "represents", "the", "current", "state", "-", "of", "-", "the", "-", "art", "scores", "in", "the", "setting", "without", "external", "data", ".", "The", "upper", "part", "also", "contains", "the", "score", "reported", "in", "choe", "-", "charniak:2016:EMNLP2016", "that", "reranked", "candidates", "by", "the", "simple", "LSTM", "language", "model", ".", "This", "part", "indicates", "that", "our", "implemented", "rerankers", "outperformed", "the", "simple", "LSTM", "language", "model", "based", "reranker", ",", "which", "achieved", "92.6", "F1", "score", ".", "Moreover", ",", "AWD", "-", "LSTM", "-", "DOC", "outperformed", "AWD", "-", "LSTM", "and", "AWD", "-", "LSTM", "-", "MoS.", "These", "results", "correspond", "to", "the", "performance", "on", "the", "language", "modeling", "task", "(", "Section", "[", "reference", "]", ")", ".", "The", "middle", "part", "shows", "that", "AWD", "-", "LSTM", "-", "DOC", "also", "outperformed", "AWD", "-", "LSTM", "and", "AWD", "-", "LSTM", "-", "MoS", "in", "the", "ensemble", "setting", ".", "In", "addition", ",", "we", "can", "improve", "the", "performance", "by", "exchanging", "the", "base", "parser", "with", "a", "stronger", "one", ".", "In", "fact", ",", "we", "achieved", "94.29", "F1", "score", "by", "reranking", "the", "candidates", "from", "retrained", "Recurrent", "Neural", "Network", "Grammars", "(", "RNNG", ")", ",", "that", "achieved", "91.2", "F1", "score", "in", "our", "configuration", ".", "Moreover", ",", "the", "lowest", "row", "of", "the", "middle", "part", "indicates", "the", "result", "by", "reranking", "the", "candidates", "from", "the", "retrained", "neural", "encoder", "-", "decoder", "based", "parser", ".", "Our", "base", "parser", "has", "two", "different", "parts", "from", "P18", "-", "2097", ".", "First", ",", "we", "used", "the", "sum", "of", "the", "hidden", "states", "of", "the", "forward", "and", "backward", "RNNs", "as", "the", "hidden", "layer", "for", "each", "RNN", ".", "Second", ",", "we", "tied", "the", "embedding", "matrix", "to", "the", "weight", "matrix", "to", "compute", "the", "probability", "distributions", "in", "the", "decoder", ".", "The", "retrained", "parser", "achieved", "93.12", "F1", "score", ".", "Finally", ",", "we", "achieved", "94.47", "F1", "score", "by", "reranking", "its", "candidates", "with", "AWD", "-", "LSTM", "-", "DOC", ".", "We", "expect", "that", "we", "can", "achieve", "even", "better", "score", "by", "replacing", "the", "base", "parser", "with", "the", "current", "state", "-", "of", "-", "the", "-", "art", "one", ".", "section", ":", "Related", "Work", "Bengio:2003:NPL:944919.944966", "are", "pioneers", "of", "neural", "language", "models", ".", "To", "address", "the", "curse", "of", "dimensionality", "in", "language", "modeling", ",", "they", "proposed", "a", "method", "using", "word", "embeddings", "and", "a", "feed", "-", "forward", "neural", "network", "(", "FFNN", ")", ".", "They", "demonstrated", "that", "their", "approach", "outperformed", "n", "-", "gram", "language", "models", ",", "but", "FFNN", "can", "only", "handle", "fixed", "-", "length", "contexts", ".", "Instead", "of", "FFNN", ",", "DBLP", ":", "conf", "/", "interspeech", "/", "MikolovKBCK10", "applied", "RNN", "to", "language", "modeling", "to", "address", "the", "entire", "given", "sequence", "as", "a", "context", ".", "Their", "method", "outperformed", "the", "Kneser", "-", "Ney", "smoothed", "5", "-", "gram", "language", "model", ".", "Researchers", "continue", "to", "try", "to", "improve", "the", "performance", "of", "RNN", "language", "models", ".", "DBLP", ":", "journals", "/", "corr", "/", "ZarembaSV14", "used", "LSTM", "instead", "of", "a", "simple", "RNN", "for", "language", "modeling", "and", "significantly", "improved", "an", "RNN", "language", "model", "by", "applying", "dropout", "to", "all", "the", "connections", "except", "for", "the", "recurrent", "connections", ".", "To", "regularize", "the", "recurrent", "connections", ",", "Gal2016Theoretically", "proposed", "variational", "inference", "-", "based", "dropout", ".", "Their", "method", "uses", "the", "same", "dropout", "mask", "at", "each", "timestep", ".", "fraternal", "proposed", "fraternal", "dropout", ",", "which", "minimizes", "the", "differences", "between", "outputs", "from", "different", "dropout", "masks", "to", "be", "invariant", "to", "the", "dropout", "mask", ".", "DBLP", ":", "journals", "/", "corr", "/", "MelisDB17", "used", "black", "-", "box", "optimization", "to", "find", "appropriate", "hyperparameters", "for", "RNN", "language", "models", "and", "demonstrated", "that", "the", "standard", "LSTM", "with", "proper", "regularizations", "can", "outperform", "other", "architectures", ".", "Apart", "from", "dropout", "techniques", ",", "DBLP", ":", "journals", "/", "corr", "/", "InanKS16", "and", "press", "-", "wolf:2017:EACLshort", "proposed", "the", "word", "tying", "method", "(", "WT", ")", ",", "which", "unifies", "word", "embeddings", "(", "in", "Equation", "[", "reference", "]", ")", "with", "the", "weight", "matrix", "to", "compute", "probability", "distributions", "(", "in", "Equation", "[", "reference", "]", ")", ".", "In", "addition", "to", "quantitative", "evaluation", ",", "DBLP", ":", "journals", "/", "corr", "/", "InanKS16", "provided", "a", "theoretical", "justification", "for", "WT", "and", "proposed", "the", "augmented", "loss", "technique", "(", "AL", ")", ",", "which", "computes", "an", "objective", "probability", "based", "on", "word", "embeddings", ".", "In", "addition", "to", "these", "regularization", "techniques", ",", "merityRegOpt", "used", "DropConnect", "and", "averaged", "SGD", "for", "an", "LSTM", "language", "model", ".", "Their", "AWD", "-", "LSTM", "achieved", "lower", "perplexity", "than", "DBLP", ":", "journals", "/", "corr", "/", "MelisDB17", "on", "PTB", "and", "WikiText", "-", "2", ".", "Previous", "studies", "also", "explored", "superior", "architecture", "for", "language", "modeling", ".", "zilly2016recurrent", "proposed", "recurrent", "highway", "networks", "that", "use", "highway", "layers", "to", "deepen", "recurrent", "connections", ".", "45826", "adopted", "reinforcement", "learning", "to", "construct", "the", "best", "RNN", "structure", ".", "However", ",", "as", "mentioned", ",", "DBLP", ":", "journals", "/", "corr", "/", "MelisDB17", "established", "that", "the", "standard", "LSTM", "is", "superior", "to", "these", "architectures", ".", "Apart", "from", "RNN", "architecture", ",", "takase", "-", "suzuki", "-", "nagata:2017:I17", "-", "2", "proposed", "the", "input", "-", "to", "-", "output", "gate", "(", "IOG", ")", ",", "which", "boosts", "the", "performance", "of", "trained", "language", "models", ".", "As", "described", "in", "Section", "[", "reference", "]", ",", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", "interpreted", "training", "language", "modeling", "as", "matrix", "factorization", "and", "improved", "performance", "by", "computing", "multiple", "probability", "distributions", ".", "In", "this", "study", ",", "we", "generalized", "their", "approach", "to", "use", "the", "middle", "layers", "of", "RNNs", ".", "Finally", ",", "our", "proposed", "method", ",", "DOC", ",", "achieved", "the", "state", "-", "of", "-", "the", "-", "art", "score", "on", "the", "standard", "benchmark", "datasets", ".", "Some", "studies", "provided", "methods", "that", "boost", "performance", "by", "using", "statistics", "obtained", "from", "test", "data", ".", "DBLP", ":", "journals", "/", "corr", "/", "GraveJU16", "extended", "a", "cache", "model", "for", "RNN", "language", "models", ".", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1709", "-", "07432", "proposed", "dynamic", "evaluation", "that", "updates", "parameters", "based", "on", "a", "recent", "sequence", "during", "testing", ".", "Although", "these", "methods", "might", "also", "improve", "the", "performance", "of", "DOC", ",", "we", "omitted", "such", "investigation", "to", "focus", "on", "comparisons", "among", "methods", "trained", "only", "on", "the", "training", "set", ".", "section", ":", "Conclusion", "We", "proposed", "Direct", "Output", "Connection", "(", "DOC", ")", ",", "a", "generalization", "method", "of", "MoS", "introduced", "by", "DBLP", ":", "journals", "/", "corr", "/", "abs", "-", "1711", "-", "03953", ".", "DOC", "raises", "the", "expressive", "power", "of", "RNN", "language", "models", "and", "improves", "quality", "of", "the", "model", ".", "DOC", "outperformed", "MoS", "and", "achieved", "the", "best", "perplexities", "on", "the", "standard", "benchmark", "datasets", "of", "language", "modeling", ":", "PTB", "and", "WikiText", "-", "2", ".", "Moreover", ",", "we", "investigated", "its", "effectiveness", "on", "machine", "translation", "and", "headline", "generation", ".", "Our", "results", "show", "that", "DOC", "also", "improved", "the", "performance", "of", "EncDec", "and", "using", "a", "middle", "layer", "positively", "affected", "such", "application", "tasks", ".", "bibliography", ":", "References"]}